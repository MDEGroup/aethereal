[
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec$$anonfun$2/apply(org.apache.spark.sql.catalyst.expressions.Attribute)|",
    "called": "|java+method:///org/apache/spark/sql/types/MetadataBuilder/build()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/types/MetadataBuilder/putLong(java.lang.String,long)|",
      "|java+method:///org/apache/spark/unsafe/types/CalendarInterval/milliseconds()|",
      "|java+method:///org/apache/spark/sql/types/MetadataBuilder/build()|",
      "|java+method:///org/apache/spark/sql/types/MetadataBuilder/withMetadata(org.apache.spark.sql.types.Metadata)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/eventTime()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/EventTimeWatermark$/delayKey()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Attribute/withMetadata(org.apache.spark.sql.types.Metadata)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Attribute/semanticEquals(org.apache.spark.sql.catalyst.expressions.Expression)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Attribute/metadata()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/delay()|",
      "|java+constructor:///org/apache/spark/sql/types/MetadataBuilder/MetadataBuilder()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/types/MetadataBuilder/putLong(java.lang.String,long)|",
      "|java+method:///org/apache/spark/sql/types/MetadataBuilder/build()|",
      "|java+method:///org/apache/spark/sql/types/MetadataBuilder/withMetadata(org.apache.spark.sql.types.Metadata)|",
      "|java+constructor:///org/apache/spark/sql/types/MetadataBuilder/MetadataBuilder()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/eventTime()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/delayMs()|",
      "|java+method:///org/apache/spark/sql/types/Metadata/contains(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/EventTimeWatermark$/delayKey()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Attribute/withMetadata(org.apache.spark.sql.types.Metadata)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Attribute/semanticEquals(org.apache.spark.sql.catalyst.expressions.Expression)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Attribute/metadata()|",
      "|java+method:///org/apache/spark/sql/types/MetadataBuilder/remove(java.lang.String)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$10/UDFRegistration$$anonfun$register$10(org.apache.spark.sql.UDFRegistration,scala.Function8,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$9/UDFRegistration$$anonfun$9(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$10/UDFRegistration$$anonfun$register$10(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function8,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$9/UDFRegistration$$anonfun$9(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)|",
    "called": "|java+method:///org/apache/spark/sql/internal/SQLConf/isParquetBinaryAsString()|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions$default$2()|",
      "|java+method:///java/lang/Math/max(int,int)|",
      "|java+method:///org/apache/spark/SparkContext/defaultParallelism()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///scala/collection/mutable/ArrayOps/tail()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$mergeSchemasInParallel$1/ParquetFileFormat$$anonfun$mergeSchemasInParallel$1(scala.runtime.ObjectRef)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sparkContext()|",
      "|java+method:///scala/collection/mutable/ArrayOps/isEmpty()|",
      "|java+method:///java/lang/Math/min(int,int)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/head()|",
      "|java+method:///org/apache/spark/rdd/RDD/collect()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/writeLegacyParquetFormat()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/isParquetBinaryAsString()|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$18/ParquetFileFormat$$anonfun$18()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/isParquetINT96AsTimestamp()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$19/ParquetFileFormat$$anonfun$19(boolean,boolean,boolean,org.apache.spark.util.SerializableConfiguration)|",
      "|java+method:///scala/runtime/ObjectRef/create(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$11/ParquetFileFormat$$anonfun$11()|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions$default$2()|",
      "|java+method:///java/lang/Math/max(int,int)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///scala/collection/mutable/ArrayOps/tail()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$mergeSchemasInParallel$1/ParquetFileFormat$$anonfun$mergeSchemasInParallel$1(scala.runtime.ObjectRef)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sparkContext()|",
      "|java+method:///scala/collection/mutable/ArrayOps/isEmpty()|",
      "|java+method:///java/lang/Math/min(int,int)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/head()|",
      "|java+method:///org/apache/spark/rdd/RDD/collect()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/writeLegacyParquetFormat()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/isParquetBinaryAsString()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$12/ParquetFileFormat$$anonfun$12(boolean,boolean,boolean,org.apache.spark.util.SerializableConfiguration,boolean)|",
      "|java+method:///org/apache/spark/SparkContext/defaultParallelism()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/isParquetINT96AsTimestamp()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/ignoreCorruptFiles()|",
      "|java+method:///scala/runtime/ObjectRef/create(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/quotedString()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/table(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/TruncateTableCommand$$anonfun$5/TruncateTableCommand$$anonfun$5(org.apache.spark.sql.execution.command.TruncateTableCommand)|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/command/TruncateTableCommand/tableName()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/TruncateTableCommand$$anonfun$7/TruncateTableCommand$$anonfun$7(org.apache.spark.sql.execution.command.TruncateTableCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///scala/Option$WithFilter/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/Seq/isEmpty()|",
      "|java+method:///org/slf4j/Logger/warn(java.lang.String,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/uncacheQuery$default$2()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///scala/Option/withFilter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/listPartitions(org.apache.spark.sql.catalyst.TableIdentifier,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/TruncateTableCommand$$anonfun$4/TruncateTableCommand$$anonfun$4(org.apache.spark.sql.execution.command.TruncateTableCommand,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/uncacheQuery(org.apache.spark.sql.Dataset,boolean)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/internal/SharedState/cacheManager()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/unquotedString()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/refreshTable(java.lang.String)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/quotedString()|",
      "|java+method:///org/apache/spark/sql/execution/command/TruncateTableCommand/log()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/command/TruncateTableCommand/partitionSpec()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sharedState()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/TruncateTableCommand$$anonfun$6/TruncateTableCommand$$anonfun$6(org.apache.spark.sql.execution.command.TruncateTableCommand,scala.collection.Seq,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/TruncateTableCommand$$anonfun$run$2/TruncateTableCommand$$anonfun$run$2(org.apache.spark.sql.execution.command.TruncateTableCommand,java.lang.String,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/table(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/TruncateTableCommand$$anonfun$5/TruncateTableCommand$$anonfun$5(org.apache.spark.sql.execution.command.TruncateTableCommand)|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/command/TruncateTableCommand/tableName()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/TruncateTableCommand$$anonfun$7/TruncateTableCommand$$anonfun$7(org.apache.spark.sql.execution.command.TruncateTableCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///scala/Option$WithFilter/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/Seq/isEmpty()|",
      "|java+method:///org/slf4j/Logger/warn(java.lang.String,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/uncacheQuery$default$2()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///scala/Option/withFilter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/listPartitions(org.apache.spark.sql.catalyst.TableIdentifier,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/TruncateTableCommand$$anonfun$4/TruncateTableCommand$$anonfun$4(org.apache.spark.sql.execution.command.TruncateTableCommand,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.Seq)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/uncacheQuery(org.apache.spark.sql.Dataset,boolean)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/internal/SharedState/cacheManager()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/unquotedString()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/refreshTable(java.lang.String)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/quotedString()|",
      "|java+method:///org/apache/spark/sql/execution/command/TruncateTableCommand/log()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/command/TruncateTableCommand/partitionSpec()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sharedState()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/TruncateTableCommand$$anonfun$6/TruncateTableCommand$$anonfun$6(org.apache.spark.sql.execution.command.TruncateTableCommand,scala.collection.Seq,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/TruncateTableCommand$$anonfun$run$2/TruncateTableCommand$$anonfun$run$2(org.apache.spark.sql.execution.command.TruncateTableCommand,java.lang.String,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$38/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$38$$anonfun$apply$14/UDFRegistration$$anonfun$register$38$$anonfun$apply$14(org.apache.spark.sql.UDFRegistration$$anonfun$register$38)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterViewAsCommand/originalText()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/Dataset/queryExecution()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterViewAsCommand/name()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/SQLBuilder/SQLBuilder(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/assertAnalyzed()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/SQLBuilder/toSQL()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sql(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+constructor:///java/lang/RuntimeException/RuntimeException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterViewAsCommand/originalText()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/Dataset/queryExecution()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterViewAsCommand/name()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/SQLBuilder/SQLBuilder(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/assertAnalyzed()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/SQLBuilder/toSQL()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sql(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+constructor:///java/lang/RuntimeException/RuntimeException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$18()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function20,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$21/UDFRegistration$$anonfun$21(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$22/UDFRegistration$$anonfun$register$22(org.apache.spark.sql.UDFRegistration,scala.Function20,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$21/UDFRegistration$$anonfun$21(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$22/UDFRegistration$$anonfun$register$22(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function20,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator$$anonfun$2/apply(org.apache.spark.sql.types.StructField)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceObj(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/types/DecimalType/scale()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/types/StructField/dataType()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/types/StructField/name()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/types/DecimalType/precision()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/types/DecimalType/scale()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/types/StructField/dataType()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///org/apache/spark/sql/types/StructField/name()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/types/DecimalType/precision()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceObj(java.lang.Object)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/internal/SessionState/catalog$lzycompute()|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/SessionCatalog(org.apache.spark.sql.catalyst.catalog.ExternalCatalog,org.apache.spark.sql.catalyst.catalog.GlobalTempViewManager,org.apache.spark.sql.catalyst.catalog.FunctionResourceLoader,org.apache.spark.sql.catalyst.analysis.FunctionRegistry,org.apache.spark.sql.internal.SQLConf,org.apache.hadoop.conf.Configuration)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/internal/SessionState/functionResourceLoader()|",
      "|java+method:///org/apache/spark/sql/internal/SharedState/globalTempViewManager()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/SessionCatalog(org.apache.spark.sql.catalyst.catalog.ExternalCatalog,org.apache.spark.sql.catalyst.catalog.GlobalTempViewManager,org.apache.spark.sql.catalyst.catalog.FunctionResourceLoader,org.apache.spark.sql.catalyst.analysis.FunctionRegistry,org.apache.spark.sql.catalyst.CatalystConf,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sharedState()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/functionRegistry()|",
      "|java+method:///org/apache/spark/sql/internal/SharedState/externalCatalog()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/internal/SessionState/functionResourceLoader()|",
      "|java+method:///org/apache/spark/sql/internal/SharedState/globalTempViewManager()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sharedState()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/functionRegistry()|",
      "|java+method:///org/apache/spark/sql/internal/SharedState/externalCatalog()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/SessionCatalog(org.apache.spark.sql.catalyst.catalog.ExternalCatalog,org.apache.spark.sql.catalyst.catalog.GlobalTempViewManager,org.apache.spark.sql.catalyst.catalog.FunctionResourceLoader,org.apache.spark.sql.catalyst.analysis.FunctionRegistry,org.apache.spark.sql.internal.SQLConf,org.apache.hadoop.conf.Configuration)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$7/UDFRegistration$$anonfun$register$7(org.apache.spark.sql.UDFRegistration,scala.Function5,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$6/UDFRegistration$$anonfun$6(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$7/UDFRegistration$$anonfun$register$7(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function5,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$6/UDFRegistration$$anonfun$6(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/SQLContext/tableNames(java.lang.String)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/listTables(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/sql/SQLContext$$anonfun$tableNames$2/SQLContext$$anonfun$tableNames$2(org.apache.spark.sql.SQLContext)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/SparkSession/catalog()|",
      "|java+method:///org/apache/spark/sql/catalog/Catalog/listTables(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/SQLContext/sparkSession()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/sql/Dataset/collect()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/listTables(java.lang.String)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/sql/SQLContext$$anonfun$tableNames$1/SQLContext$$anonfun$tableNames$1(org.apache.spark.sql.SQLContext)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/sql/SQLContext/sessionState()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable/org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkPartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/types/StructType/length()|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/HIVE_PROVIDER()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkPartitionColumns$1/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkPartitionColumns$1(org.apache.spark.sql.execution.datasources.AnalyzeCreateTable,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable$$anonfun$8/AnalyzeCreateTable$$anonfun$8(org.apache.spark.sql.execution.datasources.AnalyzeCreateTable,org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/types/StructType/filter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkPartitionColumns$3/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkPartitionColumns$3(org.apache.spark.sql.execution.datasources.AnalyzeCreateTable)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkPartitionColumns$2/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkPartitionColumns$2(org.apache.spark.sql.execution.datasources.AnalyzeCreateTable)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable/org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$failAnalysis(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable/org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkDuplication(scala.collection.Seq,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///scala/collection/Seq/length()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/spark/sql/types/StructType/nonEmpty()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/types/StructType/length()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkPartitionColumns$1/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkPartitionColumns$1(org.apache.spark.sql.execution.datasources.AnalyzeCreateTable,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/unquotedString()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/types/StructType/filter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkPartitionColumns$3/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkPartitionColumns$3(org.apache.spark.sql.execution.datasources.AnalyzeCreateTable)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkPartitionColumns$2/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkPartitionColumns$2(org.apache.spark.sql.execution.datasources.AnalyzeCreateTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable/org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$failAnalysis(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/resolver()|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/HIVE_PROVIDER()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable/org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkDuplication(scala.collection.Seq,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///scala/collection/Seq/length()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/spark/sql/types/StructType/nonEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable/sparkSession()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$18()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable$$anonfun$8/AnalyzeCreateTable$$anonfun$8(org.apache.spark.sql.execution.datasources.AnalyzeCreateTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogUtils$/normalizePartCols(java.lang.String,scala.collection.Seq,scala.collection.Seq,scala.Function2)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function13,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$14/UDFRegistration$$anonfun$14(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$15/UDFRegistration$$anonfun$register$15(org.apache.spark.sql.UDFRegistration,scala.Function13,org.apache.spark.sql.types.DataType,scala.Option)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$14/UDFRegistration$$anonfun$14(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$15/UDFRegistration$$anonfun$register$15(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function13,org.apache.spark.sql.types.DataType,scala.Option)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitExplain$1/apply()|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$ExplainContext/EXTENDED()|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder/isExplainableStatement(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$ExplainContext/FORMATTED()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/ExplainCommand/ExplainCommand(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/command/ExplainCommand$/apply$default$2()|",
      "|java+method:///org/apache/spark/sql/execution/command/ExplainCommand$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$ExplainContext/LOGICAL()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$ExplainContext/statement()|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder/plan(org.antlr.v4.runtime.ParserRuleContext)|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitExplain$1/apply()|",
      "|java+method:///org/apache/spark/sql/execution/command/ExplainCommand$/apply$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$ExplainContext/CODEGEN()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$ExplainContext/EXTENDED()|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder/isExplainableStatement(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$ExplainContext/FORMATTED()|",
      "|java+method:///org/apache/spark/sql/execution/command/ExplainCommand$/apply$default$2()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/ExplainCommand/ExplainCommand(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$ExplainContext/LOGICAL()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$ExplainContext/statement()|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder/plan(org.antlr.v4.runtime.ParserRuleContext)|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitExplain$1/apply()|",
      "|java+method:///org/apache/spark/sql/execution/command/ExplainCommand$/apply$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$ExplainContext/CODEGEN()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$apply$3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/quotedString()|",
    "v1Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$20/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$20(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogRelation/catalogTable()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$18/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$18(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$19/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$19(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/LogicalRelation/relation()|",
      "|java+method:///org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/HadoopFsRelation/partitionSchema()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/resolved()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$applyOrElse$3/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$applyOrElse$3(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/table()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/quotedString()|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$17/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$17(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/LogicalRelation/catalogTable()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogRelation/catalogTable()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$15/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$15(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$16/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$16(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$18/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$18(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/LogicalRelation/relation()|",
      "|java+method:///org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/HadoopFsRelation/partitionSchema()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/resolved()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$applyOrElse$3/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$applyOrElse$3(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/table()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/quotedString()|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$17/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$17(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/LogicalRelation/catalogTable()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/SparkStrategies$BasicOperators$/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/plans/physical/HashPartitioning/HashPartitioning(scala.collection.Seq,int)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/execution/AppendColumnsWithObjectExec/AppendColumnsWithObjectExec(scala.Function1,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/outputObjAttr()|",
      "|java+constructor:///org/apache/spark/sql/execution/RangeExec/RangeExec(org.apache.spark.sql.catalyst.plans.logical.Range)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitions/func()|",
      "|java+constructor:///org/apache/spark/sql/execution/MapGroupsExec/MapGroupsExec(scala.Function2,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/leftGroup()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/IntegerLiteral$/unapply(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Expand/projections()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapGroups/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/DeserializeToObjectExec/DeserializeToObjectExec(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/func()|",
      "|java+method:///org/apache/spark/sql/execution/LogicalRDD/output()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitions/outputObjAttr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/AppendColumns/func()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkStrategies$BasicOperators$$anonfun$8/SparkStrategies$BasicOperators$$anonfun$8(org.apache.spark.sql.execution.SparkStrategies$BasicOperators$)|",
      "|java+constructor:///java/lang/IllegalStateException/IllegalStateException(java.lang.String)|",
      "|java+constructor:///org/apache/spark/sql/execution/window/WindowExec/WindowExec(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LocalLimit/limitExpr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapElements/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/right()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/ExternalRDD/rdd()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LocalRelation/data()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/inputSchema()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Project/projectList()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Sample/upperBound()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/MemoryPlan/sink()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Repartition/shuffle()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/leftAttr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/SortPartitions/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Sample/lowerBound()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/broadcastVars()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/ExpandExec/ExpandExec(scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/SortPartitions/sortExpressions()|",
      "|java+constructor:///org/apache/spark/sql/execution/MapElementsExec/MapElementsExec(java.lang.Object,org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+constructor:///org/apache/spark/sql/execution/FilterExec/FilterExec(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/leftDeserializer()|",
      "|java+method:///org/apache/spark/sql/execution/LogicalRDD/rdd()|",
      "|java+constructor:///org/apache/spark/sql/execution/GenerateExec/GenerateExec(org.apache.spark.sql.catalyst.expressions.Generator,boolean,boolean,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Generate/join()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/AppendColumnsWithObject/newColumnsSerializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Repartition/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/MapPartitionsExec/MapPartitionsExec(scala.Function1,org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/GlobalLimit/child()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/MemorySink/schema()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapGroups/valueDeserializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Sample/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/CoalesceExec/CoalesceExec(int,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/RepartitionByExpression/numPartitions()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/SerializeFromObject/serializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/DeserializeToObject/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Project/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/LocalLimitExec/LocalLimitExec(int,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Generate/generator()|",
      "|java+method:///org/apache/spark/sql/catalyst/encoders/RowEncoder$/apply(org.apache.spark.sql.types.StructType)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapGroups/groupingAttributes()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/AppendColumnsWithObject/childSerializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/keyDeserializer()|",
      "|java+constructor:///org/apache/spark/sql/execution/FlatMapGroupsInRExec/FlatMapGroupsInRExec(byte%5B%5D,byte%5B%5D,org.apache.spark.broadcast.Broadcast%5B%5D,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapGroups/outputObjAttr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/RepartitionByExpression/partitionExpressions()|",
      "|java+method:///org/apache/spark/sql/execution/RDDScanExec$/apply$default$4()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkStrategies$BasicOperators$$anonfun$7/SparkStrategies$BasicOperators$$anonfun$7(org.apache.spark.sql.execution.SparkStrategies$BasicOperators$,org.apache.spark.sql.catalyst.encoders.ExpressionEncoder)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/outputSchema()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/packageNames()|",
      "|java+method:///org/apache/spark/sql/execution/RDDScanExec$/apply$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LocalLimit/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Window/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/SampleExec/SampleExec(double,double,boolean,long,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/outputObjAttr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Sort/order()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapElements/outputObjAttr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/func()|",
      "|java+constructor:///org/apache/spark/sql/execution/ProjectExec/ProjectExec(scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/execution/SparkStrategies$BasicOperators$/planLater(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/AppendColumns/serializer()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/physical/HashPartitioning/HashPartitioning(scala.collection.Seq,int)|",
      "|java+constructor:///org/apache/spark/sql/execution/SerializeFromObjectExec/SerializeFromObjectExec(scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/rightGroup()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Union/children()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/AppendColumns/deserializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/TypedFilter/deserializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapGroups/dataAttributes()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapElements/func()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/func()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/outputObjAttr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Generate/outer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/TypedFilter/child()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/MemoryPlan/output()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkStrategies$BasicOperators$$anonfun$1/SparkStrategies$BasicOperators$$anonfun$1(org.apache.spark.sql.execution.SparkStrategies$BasicOperators$)|",
      "|java+constructor:///org/apache/spark/sql/execution/r/MapPartitionsRWrapper/MapPartitionsRWrapper(byte%5B%5D,byte%5B%5D,org.apache.spark.broadcast.Broadcast%5B%5D,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/AppendColumns/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Generate/child()|",
      "|java+method:///org/apache/spark/sql/execution/SparkPlanner/singleRowRdd()|",
      "|java+constructor:///org/apache/spark/sql/execution/AppendColumnsExec/AppendColumnsExec(scala.Function1,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/execution/LogicalRDD/outputOrdering()|",
      "|java+method:///org/apache/spark/sql/execution/exchange/ShuffleExchange$/apply(org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/DeserializeToObject/outputObjAttr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/left()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapGroups/func()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/rightAttr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Generate/qualifiedGeneratorOutput()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Filter/condition()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/inputSchema()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitions/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/groupingAttributes()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/TypedFilter/typedCondition(org.apache.spark.sql.catalyst.expressions.Expression)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapGroups/keyDeserializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/RepartitionByExpression/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/rightDeserializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Filter/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Sort/global()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/broadcastVars()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Window/partitionSpec()|",
      "|java+constructor:///org/apache/spark/sql/execution/LocalTableScanExec/LocalTableScanExec(scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/LogicalRDD/outputPartitioning()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Expand/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/BroadcastHint/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Sample/withReplacement()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/MemorySink/allData()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/AppendColumnsWithObject/child()|",
      "|java+method:///org/apache/spark/sql/execution/ExternalRDD/outputObjAttr()|",
      "|java+constructor:///org/apache/spark/sql/execution/CoGroupExec/CoGroupExec(scala.Function3,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.sql.execution.SparkPlan,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/SerializeFromObject/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/keyDeserializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Sample/seed()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Expand/output()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/ExecutedCommandExec/ExecutedCommandExec(org.apache.spark.sql.execution.command.RunnableCommand)|",
      "|java+method:///org/apache/spark/sql/execution/SortExec$/apply$default$4()|",
      "|java+constructor:///org/apache/spark/sql/execution/UnionExec/UnionExec(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/physical/RoundRobinPartitioning/RoundRobinPartitioning(int)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LocalRelation/output()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Repartition/numPartitions()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/DeserializeToObject/deserializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Window/orderSpec()|",
      "|java+constructor:///org/apache/spark/sql/execution/ExternalRDDScanExec/ExternalRDDScanExec(org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.rdd.RDD)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/AppendColumnsWithObject/func()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Sort/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/RDDScanExec/RDDScanExec(scala.collection.Seq,org.apache.spark.rdd.RDD,java.lang.String,org.apache.spark.sql.catalyst.plans.physical.Partitioning,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/GlobalLimit/limitExpr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/valueDeserializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/dataAttributes()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Window/windowExpressions()|",
      "|java+constructor:///org/apache/spark/sql/execution/GlobalLimitExec/GlobalLimitExec(int,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/packageNames()|",
      "|java+constructor:///org/apache/spark/sql/execution/SortExec/SortExec(scala.collection.Seq,boolean,org.apache.spark.sql.execution.SparkPlan,int)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/outputSchema()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/execution/AppendColumnsWithObjectExec/AppendColumnsWithObjectExec(scala.Function1,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/outputObjAttr()|",
      "|java+constructor:///org/apache/spark/sql/execution/RangeExec/RangeExec(org.apache.spark.sql.catalyst.plans.logical.Range)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitions/func()|",
      "|java+constructor:///org/apache/spark/sql/execution/MapGroupsExec/MapGroupsExec(scala.Function2,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/leftGroup()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/IntegerLiteral$/unapply(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Expand/projections()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapGroups/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/DeserializeToObjectExec/DeserializeToObjectExec(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/func()|",
      "|java+method:///org/apache/spark/sql/execution/LogicalRDD/output()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitions/outputObjAttr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/AppendColumns/func()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkStrategies$BasicOperators$$anonfun$8/SparkStrategies$BasicOperators$$anonfun$8(org.apache.spark.sql.execution.SparkStrategies$BasicOperators$)|",
      "|java+constructor:///java/lang/IllegalStateException/IllegalStateException(java.lang.String)|",
      "|java+constructor:///org/apache/spark/sql/execution/window/WindowExec/WindowExec(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LocalLimit/limitExpr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapElements/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/right()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/ExternalRDD/rdd()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LocalRelation/data()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/inputSchema()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Project/projectList()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Sample/upperBound()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/MemoryPlan/sink()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Repartition/shuffle()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/leftAttr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Sample/lowerBound()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/broadcastVars()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/ExpandExec/ExpandExec(scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+constructor:///org/apache/spark/sql/execution/MapElementsExec/MapElementsExec(java.lang.Object,org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+constructor:///org/apache/spark/sql/execution/FilterExec/FilterExec(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/leftDeserializer()|",
      "|java+method:///org/apache/spark/sql/execution/LogicalRDD/rdd()|",
      "|java+constructor:///org/apache/spark/sql/execution/GenerateExec/GenerateExec(org.apache.spark.sql.catalyst.expressions.Generator,boolean,boolean,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Generate/join()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/AppendColumnsWithObject/newColumnsSerializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Repartition/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/MapPartitionsExec/MapPartitionsExec(scala.Function1,org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/GlobalLimit/child()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/MemorySink/schema()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapGroups/valueDeserializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Sample/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/CoalesceExec/CoalesceExec(int,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/RepartitionByExpression/numPartitions()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/SerializeFromObject/serializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/DeserializeToObject/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Project/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/LocalLimitExec/LocalLimitExec(int,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Generate/generator()|",
      "|java+method:///org/apache/spark/sql/catalyst/encoders/RowEncoder$/apply(org.apache.spark.sql.types.StructType)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapGroups/groupingAttributes()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/AppendColumnsWithObject/childSerializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/keyDeserializer()|",
      "|java+constructor:///org/apache/spark/sql/execution/FlatMapGroupsInRExec/FlatMapGroupsInRExec(byte%5B%5D,byte%5B%5D,org.apache.spark.broadcast.Broadcast%5B%5D,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapGroups/outputObjAttr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/RepartitionByExpression/partitionExpressions()|",
      "|java+method:///org/apache/spark/sql/execution/RDDScanExec$/apply$default$4()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkStrategies$BasicOperators$$anonfun$7/SparkStrategies$BasicOperators$$anonfun$7(org.apache.spark.sql.execution.SparkStrategies$BasicOperators$,org.apache.spark.sql.catalyst.encoders.ExpressionEncoder)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/outputSchema()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/packageNames()|",
      "|java+method:///org/apache/spark/sql/execution/RDDScanExec$/apply$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LocalLimit/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Window/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/SampleExec/SampleExec(double,double,boolean,long,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/outputObjAttr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Sort/order()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapElements/outputObjAttr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/func()|",
      "|java+constructor:///org/apache/spark/sql/execution/ProjectExec/ProjectExec(scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/execution/SparkStrategies$BasicOperators$/planLater(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/AppendColumns/serializer()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/physical/HashPartitioning/HashPartitioning(scala.collection.Seq,int)|",
      "|java+constructor:///org/apache/spark/sql/execution/SerializeFromObjectExec/SerializeFromObjectExec(scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/rightGroup()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Union/children()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/AppendColumns/deserializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/TypedFilter/deserializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapGroups/dataAttributes()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapElements/func()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/func()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/outputObjAttr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Generate/outer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/TypedFilter/child()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/MemoryPlan/output()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkStrategies$BasicOperators$$anonfun$1/SparkStrategies$BasicOperators$$anonfun$1(org.apache.spark.sql.execution.SparkStrategies$BasicOperators$)|",
      "|java+constructor:///org/apache/spark/sql/execution/r/MapPartitionsRWrapper/MapPartitionsRWrapper(byte%5B%5D,byte%5B%5D,org.apache.spark.broadcast.Broadcast%5B%5D,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/AppendColumns/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Generate/child()|",
      "|java+method:///org/apache/spark/sql/execution/SparkPlanner/singleRowRdd()|",
      "|java+constructor:///org/apache/spark/sql/execution/AppendColumnsExec/AppendColumnsExec(scala.Function1,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/execution/LogicalRDD/outputOrdering()|",
      "|java+method:///org/apache/spark/sql/execution/exchange/ShuffleExchange$/apply(org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/DeserializeToObject/outputObjAttr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/left()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapGroups/func()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/rightAttr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Generate/qualifiedGeneratorOutput()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Filter/condition()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/inputSchema()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitions/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/groupingAttributes()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/TypedFilter/typedCondition(org.apache.spark.sql.catalyst.expressions.Expression)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapGroups/keyDeserializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/RepartitionByExpression/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/rightDeserializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Filter/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Sort/global()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/broadcastVars()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Window/partitionSpec()|",
      "|java+constructor:///org/apache/spark/sql/execution/LocalTableScanExec/LocalTableScanExec(scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/LogicalRDD/outputPartitioning()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Expand/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/BroadcastHint/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Sample/withReplacement()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/MemorySink/allData()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/AppendColumnsWithObject/child()|",
      "|java+method:///org/apache/spark/sql/execution/ExternalRDD/outputObjAttr()|",
      "|java+constructor:///org/apache/spark/sql/execution/CoGroupExec/CoGroupExec(scala.Function3,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.sql.execution.SparkPlan,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/SerializeFromObject/child()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/CoGroup/keyDeserializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Sample/seed()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Expand/output()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/ExecutedCommandExec/ExecutedCommandExec(org.apache.spark.sql.execution.command.RunnableCommand)|",
      "|java+method:///org/apache/spark/sql/execution/SortExec$/apply$default$4()|",
      "|java+constructor:///org/apache/spark/sql/execution/UnionExec/UnionExec(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/physical/RoundRobinPartitioning/RoundRobinPartitioning(int)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LocalRelation/output()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Repartition/numPartitions()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/DeserializeToObject/deserializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Window/orderSpec()|",
      "|java+constructor:///org/apache/spark/sql/execution/ExternalRDDScanExec/ExternalRDDScanExec(org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.rdd.RDD)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/AppendColumnsWithObject/func()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Sort/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/RDDScanExec/RDDScanExec(scala.collection.Seq,org.apache.spark.rdd.RDD,java.lang.String,org.apache.spark.sql.catalyst.plans.physical.Partitioning,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/GlobalLimit/limitExpr()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/valueDeserializer()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/dataAttributes()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Window/windowExpressions()|",
      "|java+constructor:///org/apache/spark/sql/execution/GlobalLimitExec/GlobalLimitExec(int,org.apache.spark.sql.execution.SparkPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/packageNames()|",
      "|java+constructor:///org/apache/spark/sql/execution/SortExec/SortExec(scala.collection.Seq,boolean,org.apache.spark.sql.execution.SparkPlan,int)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/outputSchema()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$27/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$27$$anonfun$apply$3/UDFRegistration$$anonfun$register$27$$anonfun$apply$3(org.apache.spark.sql.UDFRegistration$$anonfun$register$27)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$32/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$32$$anonfun$apply$8/UDFRegistration$$anonfun$register$32$$anonfun$apply$8(org.apache.spark.sql.UDFRegistration$$anonfun$register$32)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$45/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$45$$anonfun$apply$21/UDFRegistration$$anonfun$register$45$$anonfun$apply$21(org.apache.spark.sql.UDFRegistration$$anonfun$register$45)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$43/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$43$$anonfun$apply$19/UDFRegistration$$anonfun$register$43$$anonfun$apply$19(org.apache.spark.sql.UDFRegistration$$anonfun$register$43)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable/org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkBucketColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
    "v1Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/sql/types/StructType/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/BucketSpec/sortColumnNames()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkBucketColumns$1/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkBucketColumns$1(org.apache.spark.sql.execution.datasources.AnalyzeCreateTable,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkBucketColumns$3/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkBucketColumns$3(org.apache.spark.sql.execution.datasources.AnalyzeCreateTable)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/BucketSpec/numBuckets()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkBucketColumns$2/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkBucketColumns$2(org.apache.spark.sql.execution.datasources.AnalyzeCreateTable)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable$$anonfun$10/AnalyzeCreateTable$$anonfun$10(org.apache.spark.sql.execution.datasources.AnalyzeCreateTable,org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable$$anonfun$9/AnalyzeCreateTable$$anonfun$9(org.apache.spark.sql.execution.datasources.AnalyzeCreateTable,org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/catalog/BucketSpec/BucketSpec(int,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable/org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkDuplication(scala.collection.Seq,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/BucketSpec/bucketColumnNames()|"
    ],
    "v2Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/BucketSpec/sortColumnNames()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkBucketColumns$1/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkBucketColumns$1(org.apache.spark.sql.execution.datasources.AnalyzeCreateTable,org.apache.spark.sql.types.StructType)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogUtils$/normalizeBucketSpec(java.lang.String,scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.BucketSpec,scala.Function2)|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/unquotedString()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkBucketColumns$3/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkBucketColumns$3(org.apache.spark.sql.execution.datasources.AnalyzeCreateTable)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/resolver()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkBucketColumns$2/AnalyzeCreateTable$$anonfun$org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkBucketColumns$2(org.apache.spark.sql.execution.datasources.AnalyzeCreateTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable/org$apache$spark$sql$execution$datasources$AnalyzeCreateTable$$checkDuplication(scala.collection.Seq,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/BucketSpec/bucketColumnNames()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable/sparkSession()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$18()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable$$anonfun$9/AnalyzeCreateTable$$anonfun$9(org.apache.spark.sql.execution.datasources.AnalyzeCreateTable)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/tableExists(org.apache.spark.sql.catalyst.TableIdentifier)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/execution/command/DropTableCommand/log()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///java/lang/Throwable/toString()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/command/DropTableCommand/ifExists()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sharedState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/tableExists(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/slf4j/Logger/warn(java.lang.String,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///org/apache/spark/sql/execution/command/DropTableCommand/purge()|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/uncacheQuery$default$2()|",
      "|java+method:///org/apache/spark/sql/execution/command/DropTableCommand/isView()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/uncacheQuery(org.apache.spark.sql.Dataset,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/isTemporaryTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/internal/SharedState/cacheManager()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/dropTable(org.apache.spark.sql.catalyst.TableIdentifier,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/command/DropTableCommand/tableName()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///org/apache/spark/sql/SparkSession/table(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/quotedString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/table(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/execution/command/DropTableCommand/log()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///java/lang/Throwable/toString()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/command/DropTableCommand/ifExists()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sharedState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/tableExists(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/slf4j/Logger/warn(java.lang.String,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///org/apache/spark/sql/execution/command/DropTableCommand/purge()|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/uncacheQuery$default$2()|",
      "|java+method:///org/apache/spark/sql/execution/command/DropTableCommand/isView()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/uncacheQuery(org.apache.spark.sql.Dataset,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/isTemporaryTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/internal/SharedState/cacheManager()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/dropTable(org.apache.spark.sql.catalyst.TableIdentifier,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/command/DropTableCommand/tableName()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$2/UDFRegistration$$anonfun$2(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$3/UDFRegistration$$anonfun$register$3(org.apache.spark.sql.UDFRegistration,scala.Function1,org.apache.spark.sql.types.DataType,scala.Option)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$3/UDFRegistration$$anonfun$register$3(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function1,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$2/UDFRegistration$$anonfun$2(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/internal/SQLConf/resolver()|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions(org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/tableName()|",
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/cmd()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$4/AlterTableRecoverPartitionsCommand$$anonfun$run$4(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,org.apache.hadoop.fs.Path)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$6/AlterTableRecoverPartitionsCommand$$anonfun$run$6(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,int)|",
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///org/apache/spark/sql/SQLContext/conf()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/Seq/isEmpty()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/RuntimeConfig/get(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/location()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/collection/GenMap$/empty()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sqlContext()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/SparkSession/conf()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$getPathFilter(org.apache.hadoop.conf.Configuration)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/quotedString()|",
      "|java+method:///org/apache/spark/SparkContext/hadoopConfiguration()|",
      "|java+method:///scala/collection/GenSeq/length()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sparkContext()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/addPartitions(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.GenSeq,scala.collection.GenMap)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$5/AlterTableRecoverPartitionsCommand$$anonfun$run$5(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,org.apache.hadoop.fs.Path,int)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/gatherFastStats()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$7/AlterTableRecoverPartitionsCommand$$anonfun$run$7(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,int)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/internal/SQLConf/resolver()|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$7/AlterTableRecoverPartitionsCommand$$anonfun$run$7(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,int)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions(org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/tableName()|",
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/cmd()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$4/AlterTableRecoverPartitionsCommand$$anonfun$run$4(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,org.apache.hadoop.fs.Path)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$6/AlterTableRecoverPartitionsCommand$$anonfun$run$6(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,int)|",
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///org/apache/spark/sql/SQLContext/conf()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/Seq/isEmpty()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/RuntimeConfig/get(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/location()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/collection/GenMap$/empty()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sqlContext()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/SparkSession/conf()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$getPathFilter(org.apache.hadoop.conf.Configuration)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/quotedString()|",
      "|java+method:///org/apache/spark/SparkContext/hadoopConfiguration()|",
      "|java+method:///scala/collection/GenSeq/length()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sparkContext()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/addPartitions(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.GenSeq,scala.collection.GenMap)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$18()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$5/AlterTableRecoverPartitionsCommand$$anonfun$run$5(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,org.apache.hadoop.fs.Path,int)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/gatherFastStats()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function12,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$13/UDFRegistration$$anonfun$13(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$14/UDFRegistration$$anonfun$register$14(org.apache.spark.sql.UDFRegistration,scala.Function12,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$13/UDFRegistration$$anonfun$13(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$14/UDFRegistration$$anonfun$register$14(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function12,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/DataFrameWriter/mode()|",
      "|java+method:///org/apache/spark/sql/DataFrameWriter/org$apache$spark$sql$DataFrameWriter$$df()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/CreateTable/CreateTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.SaveMode,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/Tuple2/_1$mcZ$sp()|",
      "|java+method:///org/apache/spark/sql/DataFrameWriter/getBucketSpec()|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/toRdd()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/tableExists(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/HIVE_PROVIDER()|",
      "|java+method:///java/lang/String/toLowerCase()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+method:///org/apache/spark/sql/DataFrameWriter/extraOptions()|",
      "|java+method:///org/apache/spark/sql/Dataset/logicalPlan()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/types/StructType/StructType()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/buildStorageFormatFromOptions(scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/Dataset/sparkSession()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///scala/Predef$/$conforms()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$13()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+constructor:///org/apache/spark/sql/DataFrameWriter$$anonfun$3/DataFrameWriter$$anonfun$3(org.apache.spark.sql.DataFrameWriter)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$14()|",
      "|java+method:///org/apache/spark/sql/DataFrameWriter/source()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$15()|",
      "|java+method:///org/apache/spark/sql/DataFrameWriter/org$apache$spark$sql$DataFrameWriter$$partitioningColumns()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/executePlan(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()|",
      "|java+method:///scala/collection/mutable/HashMap/toMap(scala.Predef$$less$colon$less)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/DataFrameWriter/mode()|",
      "|java+method:///org/apache/spark/sql/DataFrameWriter/org$apache$spark$sql$DataFrameWriter$$df()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/CreateTable/CreateTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.SaveMode,scala.Option)|",
      "|java+method:///scala/Tuple2/_1$mcZ$sp()|",
      "|java+method:///org/apache/spark/sql/DataFrameWriter/getBucketSpec()|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/toRdd()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$18()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/tableExists(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/HIVE_PROVIDER()|",
      "|java+method:///java/lang/String/toLowerCase()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+method:///org/apache/spark/sql/DataFrameWriter/extraOptions()|",
      "|java+method:///org/apache/spark/sql/Dataset/logicalPlan()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/types/StructType/StructType()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/buildStorageFormatFromOptions(scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/Dataset/sparkSession()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///scala/Predef$/$conforms()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$13()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+constructor:///org/apache/spark/sql/DataFrameWriter$$anonfun$3/DataFrameWriter$$anonfun$3(org.apache.spark.sql.DataFrameWriter)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$14()|",
      "|java+method:///org/apache/spark/sql/DataFrameWriter/source()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$15()|",
      "|java+method:///org/apache/spark/sql/DataFrameWriter/org$apache$spark$sql$DataFrameWriter$$partitioningColumns()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/executePlan(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()|",
      "|java+method:///scala/collection/mutable/HashMap/toMap(scala.Predef$$less$colon$less)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function11,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$12/UDFRegistration$$anonfun$12(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$13/UDFRegistration$$anonfun$register$13(org.apache.spark.sql.UDFRegistration,scala.Function11,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$12/UDFRegistration$$anonfun$12(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$13/UDFRegistration$$anonfun$register$13(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function11,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function19,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$21/UDFRegistration$$anonfun$register$21(org.apache.spark.sql.UDFRegistration,scala.Function19,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$20/UDFRegistration$$anonfun$20(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$21/UDFRegistration$$anonfun$register$21(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function19,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$20/UDFRegistration$$anonfun$20(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch()|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/expressions/AttributeMap$/apply(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///java/util/concurrent/locks/Condition/signalAll()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$3/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$3(org.apache.spark.sql.execution.streaming.StreamExecution,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/AttributeMap$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$2/StreamExecution$$anonfun$2(org.apache.spark.sql.execution.streaming.StreamExecution,scala.runtime.ObjectRef)|",
      "|java+method:///org/apache/spark/sql/catalyst/encoders/RowEncoder$/apply(org.apache.spark.sql.types.StructType)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/awaitBatchLock()|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/analyzed()|",
      "|java+method:///java/util/concurrent/locks/ReentrantLock/unlock()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/lastExecution()|",
      "|java+method:///java/util/concurrent/locks/ReentrantLock/lock()|",
      "|java+method:///scala/runtime/ObjectRef/create(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/awaitBatchLockCondition()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/transform(scala.PartialFunction)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/transformAllExpressions(scala.PartialFunction)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/logicalPlan()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$1/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$1(org.apache.spark.sql.execution.streaming.StreamExecution,org.apache.spark.sql.Dataset)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/newData_$eq(scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$2/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$2(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$3/StreamExecution$$anonfun$3(org.apache.spark.sql.execution.streaming.StreamExecution,org.apache.spark.sql.catalyst.expressions.AttributeMap)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/reportTimeTaken(java.lang.String,scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/sparkSession()|",
      "|java+constructor:///org/apache/spark/sql/Dataset/Dataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.Encoder)|"
    ],
    "v2Body": [
      "|java+method:///java/util/concurrent/locks/Condition/signalAll()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$3/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$3(org.apache.spark.sql.execution.streaming.StreamExecution,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/AttributeMap$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$3/StreamExecution$$anonfun$3(org.apache.spark.sql.execution.streaming.StreamExecution,scala.runtime.ObjectRef)|",
      "|java+method:///org/apache/spark/sql/catalyst/encoders/RowEncoder$/apply(org.apache.spark.sql.types.StructType)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/awaitBatchLock()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/lastExecution()|",
      "|java+method:///java/util/concurrent/locks/ReentrantLock/lock()|",
      "|java+method:///scala/runtime/ObjectRef/create(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/awaitBatchLockCondition()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/transform(scala.PartialFunction)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/transformAllExpressions(scala.PartialFunction)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/logicalPlan()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$1/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$1(org.apache.spark.sql.execution.streaming.StreamExecution,org.apache.spark.sql.Dataset)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/newData_$eq(scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$2/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$2(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$4/StreamExecution$$anonfun$4(org.apache.spark.sql.execution.streaming.StreamExecution,org.apache.spark.sql.catalyst.expressions.AttributeMap)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/reportTimeTaken(java.lang.String,scala.Function0)|",
      "|java+method:///java/util/concurrent/locks/ReentrantLock/unlock()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/IncrementalExecution/analyzed()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/sparkSession()|",
      "|java+constructor:///org/apache/spark/sql/Dataset/Dataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.Encoder)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/Dataset$$anonfun$repartition$3/apply()|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/RepartitionByExpression/RepartitionByExpression(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Option)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/Dataset$$anonfun$repartition$3/apply()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/Dataset/logicalPlan()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/RepartitionByExpression/RepartitionByExpression(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$repartition$3$$anonfun$apply$16/Dataset$$anonfun$repartition$3$$anonfun$apply$16(org.apache.spark.sql.Dataset$$anonfun$repartition$3)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/Dataset$$anonfun$repartition$3/apply()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/Dataset/logicalPlan()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/RepartitionByExpression/RepartitionByExpression(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$repartition$3$$anonfun$apply$17/Dataset$$anonfun$repartition$3$$anonfun$apply$17(org.apache.spark.sql.Dataset$$anonfun$repartition$3)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1/apply()|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableUsingContext/tableProvider()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$QualifiedNameContext/getText()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableUsingContext/query()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder/plan(org.antlr.v4.runtime.ParserRuleContext)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/buildStorageFormatFromOptions(scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableUsingContext/tablePropertyList()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$11/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$11(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$12/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$12(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/HIVE_PROVIDER()|",
      "|java+method:///java/lang/String/toLowerCase()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///scala/Tuple4/_1()|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1/apply()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/CreateTable/CreateTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.SaveMode,scala.Option)|",
      "|java+method:///scala/Tuple4/_2()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/Tuple4/_3()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple4/_4()|",
      "|java+method:///scala/Option/nonEmpty()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$TableProviderContext/qualifiedName()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/CreateTempViewUsing/CreateTempViewUsing(org.apache.spark.sql.catalyst.TableIdentifier,scala.Option,boolean,boolean,java.lang.String,scala.collection.immutable.Map)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableUsingContext/colTypeList()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$13()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$14()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder/visitCreateTableHeader(org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateTableHeaderContext)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableUsingContext/bucketSpec()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$10/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$10(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$apply$8/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$apply$8(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableUsingContext/createTableHeader()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$9/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$9(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$7/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$7(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$6/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$6(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableUsingContext/tableProvider()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$8/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$8(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1)|",
      "|java+constructor:///scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder/logWarning(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$QualifiedNameContext/getText()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableUsingContext/query()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder/plan(org.antlr.v4.runtime.ParserRuleContext)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/buildStorageFormatFromOptions(scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$18()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableUsingContext/tablePropertyList()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$11/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$11(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$12/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$12(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1)|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/HIVE_PROVIDER()|",
      "|java+method:///java/lang/String/toLowerCase()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///scala/Tuple4/_1()|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1/apply()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/CreateTable/CreateTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.SaveMode,scala.Option)|",
      "|java+method:///scala/Tuple4/_2()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/Tuple4/_3()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple4/_4()|",
      "|java+method:///scala/Option/nonEmpty()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$TableProviderContext/qualifiedName()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/CreateTempViewUsing/CreateTempViewUsing(org.apache.spark.sql.catalyst.TableIdentifier,scala.Option,boolean,boolean,java.lang.String,scala.collection.immutable.Map)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableUsingContext/colTypeList()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$13()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$14()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder/visitCreateTableHeader(org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateTableHeaderContext)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableUsingContext/bucketSpec()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$10/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$10(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$apply$8/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$apply$8(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableUsingContext/createTableHeader()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$9/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$9(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$7/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$7(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$6/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$6(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableUsingContext/tableProvider()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$8/SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1$$anonfun$8(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTableUsing$1)|",
      "|java+constructor:///scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder/logWarning(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$46/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$46$$anonfun$apply$22/UDFRegistration$$anonfun$register$46$$anonfun$apply$22(org.apache.spark.sql.UDFRegistration$$anonfun$register$46)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/internal/CatalogImpl/refreshTable(java.lang.String)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/lookupRelation$default$2()|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/lookupCachedData(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/Option/nonEmpty()|",
      "|java+method:///org/apache/spark/sql/internal/SharedState/cacheManager()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/sqlParser()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/table()|",
      "|java+method:///org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$sessionCatalog()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/cacheQuery$default$3()|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/uncacheQuery(org.apache.spark.sql.Dataset,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/cacheQuery(org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.storage.StorageLevel)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/ParserInterface/parseTableIdentifier(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/lookupRelation(org.apache.spark.sql.catalyst.TableIdentifier,scala.Option)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sharedState()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/table(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/internal/SharedState/cacheManager()|",
      "|java+method:///org/apache/spark/sql/internal/CatalogImpl/isCached(org.apache.spark.sql.Dataset)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sharedState()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/sqlParser()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/table()|",
      "|java+method:///org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$sessionCatalog()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/cacheQuery$default$3()|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/uncacheQuery(org.apache.spark.sql.Dataset,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/cacheQuery(org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.storage.StorageLevel)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/ParserInterface/parseTableIdentifier(java.lang.String)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/command/AnalyzeTableCommand/updateTableStats$1(org.apache.spark.sql.catalyst.catalog.CatalogTable,long,org.apache.spark.sql.SparkSession,org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/stats()|",
      "|java+method:///org/apache/spark/sql/execution/command/AnalyzeTableCommand/noscan()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/Statistics/Statistics(scala.math.BigInt,scala.Option,scala.collection.immutable.Map,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Statistics$/apply$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Statistics$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$5/AnalyzeTableCommand$$anonfun$5(org.apache.spark.sql.execution.command.AnalyzeTableCommand)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$4/AnalyzeTableCommand$$anonfun$4(org.apache.spark.sql.execution.command.AnalyzeTableCommand)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$2/AnalyzeTableCommand$$anonfun$2(org.apache.spark.sql.execution.command.AnalyzeTableCommand)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$1/AnalyzeTableCommand$$anonfun$1(org.apache.spark.sql.execution.command.AnalyzeTableCommand)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/sql/Dataset/count()|",
      "|java+method:///scala/Option/flatMap(scala.Function1)|",
      "|java+method:///scala/math/BigInt$/long2bigInt(long)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$updateTableStats$1$1/AnalyzeTableCommand$$anonfun$updateTableStats$1$1(org.apache.spark.sql.execution.command.AnalyzeTableCommand,long)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Statistics$/apply$default$3()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///scala/package$/BigInt()|",
      "|java+method:///org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///scala/math/BigInt$/apply(long)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/stats()|",
      "|java+method:///org/apache/spark/sql/execution/command/AnalyzeTableCommand/noscan()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/Statistics/Statistics(scala.math.BigInt,scala.Option,scala.collection.immutable.Map,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Statistics$/apply$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Statistics$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$18()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$5/AnalyzeTableCommand$$anonfun$5(org.apache.spark.sql.execution.command.AnalyzeTableCommand)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$4/AnalyzeTableCommand$$anonfun$4(org.apache.spark.sql.execution.command.AnalyzeTableCommand)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$2/AnalyzeTableCommand$$anonfun$2(org.apache.spark.sql.execution.command.AnalyzeTableCommand)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$1/AnalyzeTableCommand$$anonfun$1(org.apache.spark.sql.execution.command.AnalyzeTableCommand)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/sql/Dataset/count()|",
      "|java+method:///scala/Option/flatMap(scala.Function1)|",
      "|java+method:///scala/math/BigInt$/long2bigInt(long)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$updateTableStats$1$1/AnalyzeTableCommand$$anonfun$updateTableStats$1$1(org.apache.spark.sql.execution.command.AnalyzeTableCommand,long)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Statistics$/apply$default$3()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///scala/package$/BigInt()|",
      "|java+method:///org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///scala/math/BigInt$/apply(long)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat$/empty()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/SQLBuilder/SQLBuilder(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateViewCommand/originalText()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/Dataset/queryExecution()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateViewCommand/name()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateViewCommand/comment()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$6()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/assertAnalyzed()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/SQLBuilder/toSQL()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sql(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateViewCommand/properties()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()|",
      "|java+constructor:///java/lang/RuntimeException/RuntimeException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat$/empty()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/SQLBuilder/SQLBuilder(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$18()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateViewCommand/originalText()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/Dataset/queryExecution()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateViewCommand/name()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateViewCommand/comment()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$6()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/assertAnalyzed()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/SQLBuilder/toSQL()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sql(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateViewCommand/properties()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()|",
      "|java+constructor:///java/lang/RuntimeException/RuntimeException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/CacheManager$$anonfun$cacheQuery$1/apply()|",
    "called": "|java+method:///org/apache/spark/sql/internal/SQLConf/columnBatchSize()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/execution/CacheManager/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/lookupCachedData(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///org/apache/spark/sql/Dataset/queryExecution()|",
      "|java+method:///scala/Option/nonEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/org$apache$spark$sql$execution$CacheManager$$cachedData()|",
      "|java+constructor:///org/apache/spark/sql/execution/CachedData/CachedData(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.execution.columnar.InMemoryRelation)|",
      "|java+method:///org/apache/spark/sql/Dataset/sparkSession()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/useCompression()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/executedPlan()|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/analyzed()|",
      "|java+method:///org/apache/spark/sql/execution/columnar/InMemoryRelation$/apply(boolean,int,org.apache.spark.storage.StorageLevel,org.apache.spark.sql.execution.SparkPlan,scala.Option)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/columnBatchSize()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/executePlan(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+constructor:///org/apache/spark/sql/execution/CacheManager$$anonfun$cacheQuery$1$$anonfun$apply$2/CacheManager$$anonfun$cacheQuery$1$$anonfun$apply$2(org.apache.spark.sql.execution.CacheManager$$anonfun$cacheQuery$1)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/lookupCachedData(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///org/apache/spark/sql/Dataset/logicalPlan()|",
      "|java+method:///scala/Option/nonEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/org$apache$spark$sql$execution$CacheManager$$cachedData()|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/executedPlan()|",
      "|java+constructor:///org/apache/spark/sql/execution/CachedData/CachedData(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.execution.columnar.InMemoryRelation)|",
      "|java+method:///org/apache/spark/sql/Dataset/sparkSession()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/executePlan(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/useCompression()|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/logWarning(scala.Function0)|",
      "|java+constructor:///org/apache/spark/sql/execution/CacheManager$$anonfun$cacheQuery$1$$anonfun$apply$1/CacheManager$$anonfun$cacheQuery$1$$anonfun$apply$1(org.apache.spark.sql.execution.CacheManager$$anonfun$cacheQuery$1)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///java/util/LinkedList/add(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/columnar/InMemoryRelation$/apply(boolean,int,org.apache.spark.storage.StorageLevel,org.apache.spark.sql.execution.SparkPlan,scala.Option)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/columnBatchSize()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$4/UDFRegistration$$anonfun$register$4(org.apache.spark.sql.UDFRegistration,scala.Function2,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$3/UDFRegistration$$anonfun$3(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$4/UDFRegistration$$anonfun$register$4(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function2,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$3/UDFRegistration$$anonfun$3(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$9/UDFRegistration$$anonfun$register$9(org.apache.spark.sql.UDFRegistration,scala.Function7,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$8/UDFRegistration$$anonfun$8(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$9/UDFRegistration$$anonfun$register$9(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function7,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$8/UDFRegistration$$anonfun$8(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$42/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$42$$anonfun$apply$18/UDFRegistration$$anonfun$register$42$$anonfun$apply$18(org.apache.spark.sql.UDFRegistration$$anonfun$register$42)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+constructor:///org/apache/spark/sql/internal/SessionState$$anon$1/SessionState$$anon$1(org.apache.spark.sql.internal.SessionState)|",
    "called": "|java+method:///org/apache/spark/sql/internal/SQLConf/runSQLonFile()|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/execution/datasources/ResolveDataSource/ResolveDataSource(org.apache.spark.sql.SparkSession)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/FindDataSourceTable/FindDataSourceTable(org.apache.spark.sql.SparkSession)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable/AnalyzeCreateTable(org.apache.spark.sql.SparkSession)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/PreprocessTableInsertion(org.apache.spark.sql.internal.SQLConf)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/runSQLonFile()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreWriteCheck/PreWriteCheck(org.apache.spark.sql.internal.SQLConf,org.apache.spark.sql.catalyst.catalog.SessionCatalog)|",
      "|java+method:///scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSourceAnalysis/DataSourceAnalysis(org.apache.spark.sql.catalyst.CatalystConf)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/analysis/Analyzer/Analyzer(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.CatalystConf)|",
      "|java+method:///scala/collection/immutable/List/$colon$colon(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/execution/datasources/ResolveDataSource/ResolveDataSource(org.apache.spark.sql.SparkSession)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/FindDataSourceTable/FindDataSourceTable(org.apache.spark.sql.SparkSession)|",
      "|java+method:///scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/AnalyzeCreateTable/AnalyzeCreateTable(org.apache.spark.sql.SparkSession)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/collection/immutable/List/$colon$colon(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/runSQLonFile()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreWriteCheck/PreWriteCheck(org.apache.spark.sql.internal.SQLConf,org.apache.spark.sql.catalyst.catalog.SessionCatalog)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/analysis/Analyzer/Analyzer(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.internal.SQLConf)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/PreprocessTableInsertion(org.apache.spark.sql.internal.SQLConf)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSourceAnalysis/DataSourceAnalysis(org.apache.spark.sql.internal.SQLConf)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/Dataset$$anonfun$dropDuplicates$1$$anonfun$41/apply(org.apache.spark.sql.catalyst.expressions.Attribute)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/Alias/Alias(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String,org.apache.spark.sql.catalyst.expressions.ExprId,scala.Option,scala.Option,java.lang.Boolean)|",
    "v1Body": [
      "|java+method:///scala/collection/Seq/contains(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/Alias/Alias(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String,org.apache.spark.sql.catalyst.expressions.ExprId,scala.Option,scala.Option,java.lang.Boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/aggregate/First/toAggregateExpression()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/aggregate/First/First(org.apache.spark.sql.catalyst.expressions.Expression)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$6(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$5(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$4(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Attribute/name()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Attribute/exprId()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Seq/contains(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/Alias/Alias(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String,org.apache.spark.sql.catalyst.expressions.ExprId,scala.Option,scala.Option,java.lang.Boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/aggregate/First/toAggregateExpression()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/aggregate/First/First(org.apache.spark.sql.catalyst.expressions.Expression)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$6(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$5(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$4(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$3(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Attribute/name()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Attribute/exprId()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$37/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$37$$anonfun$apply$13/UDFRegistration$$anonfun$register$37$$anonfun$apply$13(org.apache.spark.sql.UDFRegistration$$anonfun$register$37)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$33/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$33$$anonfun$apply$9/UDFRegistration$$anonfun$register$33$$anonfun$apply$9(org.apache.spark.sql.UDFRegistration$$anonfun$register$33)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/FindDataSourceTable/org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.SimpleCatalogRelation)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/SimpleCatalogRelation/catalogTable()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SimpleCatalogRelation/output()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/LogicalRelation/LogicalRelation(org.apache.spark.sql.sources.BaseRelation,scala.Option,scala.Option)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource/DataSource(org.apache.spark.sql.SparkSession,java.lang.String,scala.collection.Seq,scala.Option,scala.collection.Seq,scala.Option,scala.collection.immutable.Map,scala.Option)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$3()|",
      "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/resolveRelation$default$1()|",
      "|java+method:///scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/FindDataSourceTable$$anonfun$9/FindDataSourceTable$$anonfun$9(org.apache.spark.sql.execution.datasources.FindDataSourceTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SimpleCatalogRelation/catalogTable()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()|",
      "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SimpleCatalogRelation/output()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/LogicalRelation/LogicalRelation(org.apache.spark.sql.sources.BaseRelation,scala.Option,scala.Option)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource/DataSource(org.apache.spark.sql.SparkSession,java.lang.String,scala.collection.Seq,scala.Option,scala.collection.Seq,scala.Option,scala.collection.immutable.Map,scala.Option)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$3()|",
      "|java+method:///scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/FindDataSourceTable$$anonfun$9/FindDataSourceTable$$anonfun$9(org.apache.spark.sql.execution.datasources.FindDataSourceTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SimpleCatalogRelation/catalogTable()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$28/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$28$$anonfun$apply$4/UDFRegistration$$anonfun$register$28$$anonfun$apply$4(org.apache.spark.sql.UDFRegistration$$anonfun$register$28)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateTableLikeCommand/ifNotExists()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateTableLikeCommand/sourceTable()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/defaultDataSourceName()|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTempViewOrPermanentTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/createTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateTableLikeCommand/targetTable()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$11()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$13()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$14()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateTableLikeCommand/ifNotExists()|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateTableLikeCommand/sourceTable()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$18()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/defaultDataSourceName()|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTempViewOrPermanentTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/createTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateTableLikeCommand/targetTable()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$11()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$13()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$14()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/Dataset$$anonfun$withWatermark$1/apply()|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/EventTimeWatermark/EventTimeWatermark(org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.unsafe.types.CalendarInterval,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/EventTimeWatermark/EventTimeWatermark(org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.unsafe.types.CalendarInterval,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$withWatermark$1$$anonfun$13/Dataset$$anonfun$withWatermark$1$$anonfun$13(org.apache.spark.sql.Dataset$$anonfun$withWatermark$1)|",
      "|java+method:///org/apache/spark/sql/Dataset$$anonfun$withWatermark$1/apply()|",
      "|java+method:///org/apache/spark/unsafe/types/CalendarInterval/fromString(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/Dataset/logicalPlan()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/UnresolvedAttribute$/apply(java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/EventTimeWatermark/EventTimeWatermark(org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.unsafe.types.CalendarInterval,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/unsafe/types/CalendarInterval/milliseconds()|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$withWatermark$1$$anonfun$13/Dataset$$anonfun$withWatermark$1$$anonfun$13(org.apache.spark.sql.Dataset$$anonfun$withWatermark$1)|",
      "|java+method:///org/apache/spark/unsafe/types/CalendarInterval/fromString(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/Dataset/logicalPlan()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$withWatermark$1$$anonfun$apply$7/Dataset$$anonfun$withWatermark$1$$anonfun$apply$7(org.apache.spark.sql.Dataset$$anonfun$withWatermark$1)|",
      "|java+method:///org/apache/spark/sql/Dataset$$anonfun$withWatermark$1/apply()|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/UnresolvedAttribute$/apply(java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function22,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$24/UDFRegistration$$anonfun$register$24(org.apache.spark.sql.UDFRegistration,scala.Function22,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$23/UDFRegistration$$anonfun$23(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$24/UDFRegistration$$anonfun$register$24(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function22,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$23/UDFRegistration$$anonfun$23(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/QueryExecution/assertAnalyzed()|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/analysis/Analyzer/checkAnalysis(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/AnalysisException/startPosition()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/AnalysisException/setStackTrace(java.lang.StackTraceElement%5B%5D)|",
      "|java+method:///org/apache/spark/sql/AnalysisException/getStackTrace()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/analyzer()|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/Analyzer/checkAnalysis(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/analyzed()|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/sparkSession()|",
      "|java+method:///org/apache/spark/sql/AnalysisException/line()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/AnalysisException/message()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/AnalysisException/startPosition()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/AnalysisException/setStackTrace(java.lang.StackTraceElement%5B%5D)|",
      "|java+method:///org/apache/spark/sql/AnalysisException/getStackTrace()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/analyzer()|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/analyzed()|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/sparkSession()|",
      "|java+method:///org/apache/spark/sql/AnalysisException/line()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/AnalysisException/message()|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/Analyzer/checkAnalysis(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$30/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$30$$anonfun$apply$6/UDFRegistration$$anonfun$register$30$$anonfun$apply$6(org.apache.spark.sql.UDFRegistration$$anonfun$register$30)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/properties()|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand$$anonfun$1/AlterTableUnsetPropertiesCommand$$anonfun$1(org.apache.spark.sql.execution.command.AlterTableUnsetPropertiesCommand)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/isView()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/tableName()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/propKeys()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand$$anonfun$run$3/AlterTableUnsetPropertiesCommand$$anonfun$run$3(org.apache.spark.sql.execution.command.AlterTableUnsetPropertiesCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/ifExists()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///scala/collection/immutable/Map/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/properties()|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$18()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand$$anonfun$1/AlterTableUnsetPropertiesCommand$$anonfun$1(org.apache.spark.sql.execution.command.AlterTableUnsetPropertiesCommand)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/isView()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/tableName()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/propKeys()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand$$anonfun$run$3/AlterTableUnsetPropertiesCommand$$anonfun$run$3(org.apache.spark.sql.execution.command.AlterTableUnsetPropertiesCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/ifExists()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///scala/collection/immutable/Map/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$44/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$44$$anonfun$apply$20/UDFRegistration$$anonfun$register$44$$anonfun$apply$20(org.apache.spark.sql.UDFRegistration$$anonfun$register$44)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogRelation/catalogTable()|",
      "|java+method:///scala/Option$/empty()|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/tableExists(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/SQLContext/conf()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/LogicalRelation/relation()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/AlterTableRecoverPartitionsCommand(org.apache.spark.sql.catalyst.TableIdentifier,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/manageFilesourcePartitions()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/defaultTablePath(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/lookupRelation$default$2()|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/HIVE_PROVIDER()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/lookupDataSource(java.lang.String)|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sqlContext()|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/EliminateSubqueryAliases$/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/LogicalRelation/schema()|",
      "|java+method:///org/apache/spark/sql/types/StructType/fieldNames()|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/query()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/isDatasourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/HadoopFsRelation/fileFormat()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$/apply$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/copy(java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/table()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/createTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$4/CreateDataSourceTableAsSelectCommand$$anonfun$4(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$3/CreateDataSourceTableAsSelectCommand$$anonfun$3(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand,org.apache.spark.sql.internal.SessionState)|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/unquotedString()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///java/lang/Class/getName()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/toRdd()|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/dropTable(org.apache.spark.sql.catalyst.TableIdentifier,boolean,boolean)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$5/CreateDataSourceTableAsSelectCommand$$anonfun$5(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/Dataset/selectExpr(scala.collection.Seq)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SimpleCatalogRelation/metadata()|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/mode()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/database()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/toString()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/lookupRelation(org.apache.spark.sql.catalyst.TableIdentifier,scala.Option)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)|",
      "|java+method:///org/apache/spark/sql/types/StructType/size()|",
      "|java+method:///scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/sources/BaseRelation/schema()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource/DataSource(org.apache.spark.sql.SparkSession,java.lang.String,scala.collection.Seq,scala.Option,scala.collection.Seq,scala.Option,scala.collection.immutable.Map,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()|",
      "|java+method:///org/apache/spark/sql/types/StructType/isEmpty()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/executePlan(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/copy$default$1()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/query()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/Option$/empty()|",
      "|java+method:///scala/Some/x()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$6/CreateDataSourceTableAsSelectCommand$$anonfun$6(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand,java.lang.String,scala.Function2,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/types/StructType/length()|",
      "|java+method:///org/apache/spark/sql/Dataset/select(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$run$1/CreateDataSourceTableAsSelectCommand$$anonfun$run$1(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand,scala.Function2)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/tableExists(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/SQLContext/conf()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/AlterTableRecoverPartitionsCommand(org.apache.spark.sql.catalyst.TableIdentifier,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$4()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///java/lang/Class/getSimpleName()|",
      "|java+method:///org/apache/spark/sql/types/StructType/catalogString()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/manageFilesourcePartitions()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/resolver()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/defaultTablePath(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/HIVE_PROVIDER()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$9/CreateDataSourceTableAsSelectCommand$$anonfun$9(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/lookupDataSource(java.lang.String)|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$8/CreateDataSourceTableAsSelectCommand$$anonfun$8(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand)|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sqlContext()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$7/CreateDataSourceTableAsSelectCommand$$anonfun$7(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand)|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$13/CreateDataSourceTableAsSelectCommand$$anonfun$13(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$/apply$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/copy(java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/table()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/createTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$4/CreateDataSourceTableAsSelectCommand$$anonfun$4(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$3/CreateDataSourceTableAsSelectCommand$$anonfun$3(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand,org.apache.spark.sql.internal.SessionState)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/unquotedString()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/toRdd()|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/dropTable(org.apache.spark.sql.catalyst.TableIdentifier,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$11/CreateDataSourceTableAsSelectCommand$$anonfun$11(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand)|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/mode()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$12/CreateDataSourceTableAsSelectCommand$$anonfun$12(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$10/CreateDataSourceTableAsSelectCommand$$anonfun$10(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/database()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$18()|",
      "|java+method:///scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/sources/BaseRelation/schema()|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource/DataSource(org.apache.spark.sql.SparkSession,java.lang.String,scala.collection.Seq,scala.Option,scala.collection.Seq,scala.Option,scala.collection.immutable.Map,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()|",
      "|java+method:///org/apache/spark/sql/types/StructType/isEmpty()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/executePlan(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogUtils$/normalizePartCols(java.lang.String,scala.collection.Seq,scala.collection.Seq,scala.Function2)|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/copy$default$1()|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/FileFormatWriter$DynamicPartitionWriteTask$$anonfun$partitionStringExpression$1/apply(scala.Tuple2)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils$/DEFAULT_PARTITION_NAME()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Literal$/apply(java.lang.Object)|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/Cast/Cast(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.types.DataType)|",
      "|java+method:///scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/IsNull/IsNull(org.apache.spark.sql.catalyst.expressions.Expression)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/If/If(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/FileFormatWriter$DynamicPartitionWriteTask$$anonfun$partitionStringExpression$1$$anonfun$9/FileFormatWriter$DynamicPartitionWriteTask$$anonfun$partitionStringExpression$1$$anonfun$9(org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask$$anonfun$partitionStringExpression$1)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/List/$colon$colon(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Attribute/name()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/Cast/Cast(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.types.DataType)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/FileFormatWriter$DynamicPartitionWriteTask$$anonfun$partitionStringExpression$1$$anonfun$9/FileFormatWriter$DynamicPartitionWriteTask$$anonfun$partitionStringExpression$1$$anonfun$9(org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask$$anonfun$partitionStringExpression$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Attribute/name()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Literal$/apply(java.lang.Object)|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputOrdering()|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/plans/LeftExistence$/unapply(org.apache.spark.sql.catalyst.plans.JoinType)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/execution/joins/SortMergeJoinExec/requiredOrders(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/joins/SortMergeJoinExec/leftKeys()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/execution/joins/SortMergeJoinExec/joinType()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/LeftExistence$/unapply(org.apache.spark.sql.catalyst.plans.JoinType)|",
      "|java+method:///org/apache/spark/sql/execution/joins/SortMergeJoinExec/requiredOrders(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/joins/SortMergeJoinExec/rightKeys()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/joins/SortMergeJoinExec/leftKeys()|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///java/lang/Class/getSimpleName()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$10/UDFRegistration$$anonfun$10(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$11/UDFRegistration$$anonfun$register$11(org.apache.spark.sql.UDFRegistration,scala.Function9,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$11/UDFRegistration$$anonfun$register$11(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function9,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$10/UDFRegistration$$anonfun$10(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator$$anonfun$1/apply(org.apache.spark.sql.types.StructField)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceObj(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/types/DecimalType/scale()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/types/StructField/dataType()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/types/StructField/name()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/types/DecimalType/precision()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/types/DecimalType/scale()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/types/StructField/dataType()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///org/apache/spark/sql/types/StructField/name()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/types/DecimalType/precision()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceObj(java.lang.Object)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$replaceCol(org.apache.spark.sql.types.StructField,scala.collection.immutable.Map)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/expressions/CaseKeyWhen$/apply(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/immutable/Map/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/CaseKeyWhen$/apply(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/Column/Column(org.apache.spark.sql.catalyst.expressions.Expression)|",
      "|java+method:///org/apache/spark/sql/types/StructField/name()|",
      "|java+constructor:///org/apache/spark/sql/DataFrameNaFunctions$$anonfun$8/DataFrameNaFunctions$$anonfun$8(org.apache.spark.sql.DataFrameNaFunctions,org.apache.spark.sql.catalyst.expressions.Expression)|",
      "|java+method:///scala/collection/immutable/Iterable$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/Column/expr()|",
      "|java+method:///org/apache/spark/sql/Dataset/col(java.lang.String)|",
      "|java+method:///scala/collection/TraversableOnce/toSeq()|",
      "|java+method:///scala/collection/Seq/$colon$plus(java.lang.Object,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/Column/as(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/immutable/Map/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/CaseKeyWhen$/apply(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/Column/Column(org.apache.spark.sql.catalyst.expressions.Expression)|",
      "|java+method:///scala/collection/immutable/Iterable$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/Column/expr()|",
      "|java+method:///org/apache/spark/sql/types/StructField/name()|",
      "|java+constructor:///org/apache/spark/sql/DataFrameNaFunctions$$anonfun$6/DataFrameNaFunctions$$anonfun$6(org.apache.spark.sql.DataFrameNaFunctions,org.apache.spark.sql.catalyst.expressions.Expression)|",
      "|java+method:///org/apache/spark/sql/Dataset/col(java.lang.String)|",
      "|java+method:///scala/collection/TraversableOnce/toSeq()|",
      "|java+method:///scala/collection/Seq/$colon$plus(java.lang.Object,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/Column/as(java.lang.String)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/DataSourceAnalysis/resolver()|",
    "called": "|java+method:///org/apache/spark/sql/internal/SQLConf/resolver()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/catalyst/CatalystConf/resolver()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSourceAnalysis/conf()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/internal/SQLConf/resolver()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSourceAnalysis/conf()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)|",
    "called": "|java+method:///org/apache/spark/sql/internal/SQLConf/isUnsupportedOperationCheckEnabled()|",
    "v1Body": [
      "|java+method:///org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/sql/Dataset/queryExecution()|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/analyzed()|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryManager$$anonfun$2/StreamingQueryManager$$anonfun$2(org.apache.spark.sql.streaming.StreamingQueryManager,scala.Option,org.apache.spark.sql.Dataset)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryManager$$anonfun$1/StreamingQueryManager$$anonfun$1(org.apache.spark.sql.streaming.StreamingQueryManager)|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryManager$$anonfun$3/StreamingQueryManager$$anonfun$3(org.apache.spark.sql.streaming.StreamingQueryManager,boolean)|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/UnsupportedOperationChecker$/checkForStreaming(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.streaming.OutputMode)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///scala/Option/orNull(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/isUnsupportedOperationCheckEnabled()|",
      "|java+method:///org/apache/spark/sql/Dataset/sparkSession()|",
      "|java+method:///scala/Predef$/$conforms()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/assertAnalyzed()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution/StreamExecution(org.apache.spark.sql.SparkSession,java.lang.String,java.lang.String,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock,org.apache.spark.sql.streaming.OutputMode)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/internal/SQLConf$/ADAPTIVE_EXECUTION_ENABLED()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/sql/Dataset/queryExecution()|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/analyzed()|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryManager$$anonfun$2/StreamingQueryManager$$anonfun$2(org.apache.spark.sql.streaming.StreamingQueryManager,scala.Option,org.apache.spark.sql.Dataset)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///scala/runtime/BooleanRef/create(boolean)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryManager$$anonfun$1/StreamingQueryManager$$anonfun$1(org.apache.spark.sql.streaming.StreamingQueryManager)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/UnsupportedOperationChecker$/checkForStreaming(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.streaming.OutputMode)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamingQueryWrapper/StreamingQueryWrapper(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+method:///scala/Option/orNull(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/isUnsupportedOperationCheckEnabled()|",
      "|java+method:///org/apache/spark/sql/Dataset/sparkSession()|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryManager$$anonfun$3/StreamingQueryManager$$anonfun$3(org.apache.spark.sql.streaming.StreamingQueryManager,boolean,scala.runtime.BooleanRef)|",
      "|java+method:///scala/Predef$/$conforms()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/assertAnalyzed()|",
      "|java+method:///org/apache/spark/internal/config/ConfigEntry/key()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution/StreamExecution(org.apache.spark.sql.SparkSession,java.lang.String,java.lang.String,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock,org.apache.spark.sql.streaming.OutputMode,boolean)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/adaptiveExecutionEnabled()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$1()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/types/StructType/length()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/immutable/Map/size()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$5()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/partition()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/Map/nonEmpty()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/resolver()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/child()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/immutable/MapLike/keySet()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/Predef$/$conforms()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/table()|",
      "|java+method:///scala/collection/immutable/Map/keys()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///scala/collection/Seq/filterNot(scala.Function1)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/mkString(java.lang.String)|",
      "|java+method:///scala/collection/immutable/Map/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitioningUtils$/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/Seq/length()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$15/PreprocessTableInsertion$$anonfun$15(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$13/PreprocessTableInsertion$$anonfun$13(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.immutable.Map,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.catalyst.plans.logical.OverwriteOptions,boolean)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/castAndRenameChildOutput(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$14/PreprocessTableInsertion$$anonfun$14(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion,scala.collection.immutable.Set)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/conf()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/types/StructType/length()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/immutable/Map/size()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$5()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/partition()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/Map/nonEmpty()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/resolver()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/child()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/immutable/MapLike/keySet()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/Predef$/$conforms()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/table()|",
      "|java+method:///scala/collection/immutable/Map/keys()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///scala/collection/Seq/filterNot(scala.Function1)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/mkString(java.lang.String)|",
      "|java+method:///scala/collection/immutable/Map/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitioningUtils$/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/Seq/length()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$13/PreprocessTableInsertion$$anonfun$13(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.immutable.Map,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.catalyst.plans.logical.OverwriteOptions,boolean)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$11/PreprocessTableInsertion$$anonfun$11(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/castAndRenameChildOutput(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$12/PreprocessTableInsertion$$anonfun$12(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion,scala.collection.immutable.Set)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/conf()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)|",
    "called": "|java+method:///org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/Dataset/logicalPlan()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource$$anonfun$16/DataSource$$anonfun$16(org.apache.spark.sql.execution.datasources.DataSource,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource$$anonfun$write$2/DataSource$$anonfun$write$2(org.apache.spark.sql.execution.datasources.DataSource)|",
      "|java+method:///scala/collection/IterableLike/exists(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/paths()|",
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///scala/util/Try/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/copy$default$1()|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource$$anonfun$17/DataSource$$anonfun$17(org.apache.spark.sql.execution.datasources.DataSource)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource$$anonfun$21/DataSource$$anonfun$21(org.apache.spark.sql.execution.datasources.DataSource)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource$$anonfun$18/DataSource$$anonfun$18(org.apache.spark.sql.execution.datasources.DataSource)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource$$anonfun$19/DataSource$$anonfun$19(org.apache.spark.sql.execution.datasources.DataSource)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/copy$default$3()|",
      "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/resolveRelation$default$1()|",
      "|java+method:///java/lang/Class/newInstance()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/getUri()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/copy(org.apache.spark.sql.SparkSession,java.lang.String,scala.collection.Seq,scala.Option,scala.collection.Seq,scala.Option,scala.collection.immutable.Map,scala.Option)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/sources/CreatableRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/bucketSpec()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sqlContext()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/org$apache$spark$sql$execution$datasources$DataSource$$caseInsensitiveOptions()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/InsertIntoHadoopFsRelationCommand(org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.immutable.Map,scala.collection.Seq,scala.Option,org.apache.spark.sql.execution.datasources.FileFormat,scala.Function1,scala.collection.immutable.Map,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.SaveMode,scala.Option)|",
      "|java+method:///java/lang/Class/getCanonicalName()|",
      "|java+method:///org/apache/spark/sql/types/StructType/asNullable()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitioningUtils$/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/equals(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/options()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource$$anonfun$20/DataSource$$anonfun$20(org.apache.spark.sql.execution.datasources.DataSource,org.apache.spark.sql.Dataset)|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/toRdd()|",
      "|java+method:///scala/collection/Seq/head()|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource$$anonfun$write$1/DataSource$$anonfun$write$1(org.apache.spark.sql.execution.datasources.DataSource)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/partitionColumns()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///scala/sys/package$/error(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///scala/collection/immutable/Map$/empty()|",
      "|java+method:///org/apache/hadoop/fs/Path/makeQualified(java.net.URI,org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/caseSensitiveAnalysis()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/getWorkingDirectory()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/catalogTable()|",
      "|java+method:///org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/get(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/Seq/length()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/sparkSession()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/providingClass()|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///org/apache/spark/sql/Dataset/schema()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/executePlan(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/IterableLike/exists(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///java/lang/Class/newInstance()|",
      "|java+method:///org/apache/spark/sql/sources/CreatableRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sqlContext()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///java/lang/Class/getCanonicalName()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource$$anonfun$write$2/DataSource$$anonfun$write$2(org.apache.spark.sql.execution.datasources.DataSource)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/sparkSession()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/org$apache$spark$sql$execution$datasources$DataSource$$caseInsensitiveOptions()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource$$anonfun$write$1/DataSource$$anonfun$write$1(org.apache.spark.sql.execution.datasources.DataSource)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/sys/package$/error(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/providingClass()|",
      "|java+method:///org/apache/spark/sql/Dataset/schema()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function18,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$19/UDFRegistration$$anonfun$19(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$20/UDFRegistration$$anonfun$register$20(org.apache.spark.sql.UDFRegistration,scala.Function18,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$20/UDFRegistration$$anonfun$register$20(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function18,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$19/UDFRegistration$$anonfun$19(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()|",
    "called": "|java+method:///org/apache/spark/sql/internal/SQLConf/streamingMetricsEnabled()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/terminationLatch()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/triggerExecutor()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/TERMINATED()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/exception()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/ACTIVE()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/logicalPlan()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$2/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$2(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/OffsetSeq/toString()|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryStatus/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/id()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/updateStatusMessage(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/SparkSession/streams()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/startLatch()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/committedOffsets()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamProgress/toOffsetSeq(scala.collection.Seq,org.apache.spark.sql.execution.streaming.OffsetSeqMetadata)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/state()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/currentStatus_$eq(org.apache.spark.sql.streaming.StreamingQueryStatus)|",
      "|java+method:///java/util/concurrent/CountDownLatch/countDown()|",
      "|java+method:///scala/util/control/NonFatal$/apply(java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/name()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/sources()|",
      "|java+method:///java/lang/Throwable/getMessage()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$3/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$3(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryManager/notifyQueryTermination(org.apache.spark.sql.streaming.StreamingQuery)|",
      "|java+method:///org/apache/spark/sql/SparkSession$/setActiveSession(org.apache.spark.sql.SparkSession)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/registerSource(org.apache.spark.metrics.source.Source)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$4/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$4(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/streamDeathCause_$eq(org.apache.spark.sql.streaming.StreamingQueryException)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/removeSource(org.apache.spark.metrics.source.Source)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/offsetSeqMetadata()|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryListener$QueryTerminatedEvent/StreamingQueryListener$QueryTerminatedEvent(java.util.UUID,java.util.UUID,scala.Option)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/status()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/postEvent(org.apache.spark.sql.streaming.StreamingQueryListener$Event)|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryStatus/copy(java.lang.String,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/ProcessingTimeExecutor/execute(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryListener$QueryStartedEvent/StreamingQueryListener$QueryStartedEvent(java.util.UUID,java.util.UUID,java.lang.String)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryException/StreamingQueryException(org.apache.spark.sql.streaming.StreamingQuery,java.lang.String,java.lang.Throwable,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/streamingMetricsEnabled()|",
      "|java+method:///org/apache/spark/SparkEnv/metricsSystem()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/availableOffsets()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sparkContext()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$prettyIdString()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/sparkSession()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/streamMetrics()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/runId()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/state_$eq(org.apache.spark.sql.execution.streaming.StreamExecution$State)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/triggerExecutor()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/exception()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/ACTIVE()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/logicalPlan()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/INITIALIZING()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/id()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/terminationLatch()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/isInitialized()|",
      "|java+method:///java/util/concurrent/locks/Condition/signalAll()|",
      "|java+method:///org/apache/spark/sql/SparkSession/streams()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/startLatch()|",
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/awaitBatchLockCondition()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/TERMINATED()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/committedOffsets()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamProgress/toOffsetSeq(scala.collection.Seq,org.apache.spark.sql.execution.streaming.OffsetSeqMetadata)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/state()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/currentStatus_$eq(org.apache.spark.sql.streaming.StreamingQueryStatus)|",
      "|java+method:///java/util/concurrent/CountDownLatch/countDown()|",
      "|java+method:///java/util/concurrent/atomic/AtomicReference/compareAndSet(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/util/control/NonFatal$/apply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/checkpointRoot()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$5/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$5(org.apache.spark.sql.execution.streaming.StreamExecution,org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/name()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/stopSources()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$2/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$2(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/sources()|",
      "|java+method:///java/lang/Throwable/getMessage()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$3/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$3(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryManager/notifyQueryTermination(org.apache.spark.sql.streaming.StreamingQuery)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/awaitBatchLock()|",
      "|java+method:///org/apache/spark/sql/SparkSession$/setActiveSession(org.apache.spark.sql.SparkSession)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/registerSource(org.apache.spark.metrics.source.Source)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$4/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$4(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/streamDeathCause_$eq(org.apache.spark.sql.streaming.StreamingQueryException)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///java/util/concurrent/atomic/AtomicReference/set(java.lang.Object)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/removeSource(org.apache.spark.metrics.source.Source)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/offsetSeqMetadata()|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryListener$QueryTerminatedEvent/StreamingQueryListener$QueryTerminatedEvent(java.util.UUID,java.util.UUID,scala.Option)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/status()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/postEvent(org.apache.spark.sql.streaming.StreamingQueryListener$Event)|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryStatus/copy(java.lang.String,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/ProcessingTimeExecutor/execute(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryListener$QueryStartedEvent/StreamingQueryListener$QueryStartedEvent(java.util.UUID,java.util.UUID,java.lang.String)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/streamingMetricsEnabled()|",
      "|java+method:///java/util/concurrent/locks/ReentrantLock/unlock()|",
      "|java+method:///org/apache/spark/SparkEnv/metricsSystem()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/availableOffsets()|",
      "|java+method:///java/util/concurrent/atomic/AtomicReference/get()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/OffsetSeq/toString()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sparkContext()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$prettyIdString()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/sparkSession()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/streamMetrics()|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryStatus/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/initializationLatch()|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryException/StreamingQueryException(java.lang.String,java.lang.String,java.lang.Throwable,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/runId()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/delete(org.apache.hadoop.fs.Path,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/updateStatusMessage(java.lang.String)|",
      "|java+method:///java/util/concurrent/locks/ReentrantLock/lock()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/toDebugString(boolean)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction/inputTypes()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/Column$/apply(org.apache.spark.sql.catalyst.expressions.Expression)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction/dataType()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction/f()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/expressions/UserDefinedFunction$$anonfun$apply$1/UserDefinedFunction$$anonfun$apply$1(org.apache.spark.sql.expressions.UserDefinedFunction)|",
      "|java+constructor:///org/apache/spark/sql/expressions/UserDefinedFunction$$anonfun$apply$2/UserDefinedFunction$$anonfun$apply$2(org.apache.spark.sql.expressions.UserDefinedFunction)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/Column$/apply(org.apache.spark.sql.catalyst.expressions.Expression)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction/f()|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction/dataType()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction/inputTypes()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/expressions/UserDefinedFunction$$anonfun$apply$1/UserDefinedFunction$$anonfun$apply$1(org.apache.spark.sql.expressions.UserDefinedFunction)|",
      "|java+constructor:///org/apache/spark/sql/expressions/UserDefinedFunction$$anonfun$apply$2/UserDefinedFunction$$anonfun$apply$2(org.apache.spark.sql.expressions.UserDefinedFunction)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$36/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$36$$anonfun$apply$12/UDFRegistration$$anonfun$register$36$$anonfun$apply$12(org.apache.spark.sql.UDFRegistration$$anonfun$register$36)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)|",
    "called": "|java+method:///org/apache/spark/sql/types/StructType/length()|",
    "v1Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///java/net/URI/URI(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/RecordReaderIterator/map(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initBatch(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.InternalRow)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/enableReturningBatches()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/parquet/hadoop/ParquetRecordReader/ParquetRecordReader(org.apache.parquet.hadoop.api.ReadSupport,org.apache.parquet.filter2.compat.FilterCompat$Filter)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/ParquetReadSupport()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/JoinedRow/JoinedRow()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getPath()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionedFile/partitionValues()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/VectorizedParquetRecordReader()|",
      "|java+constructor:///org/apache/hadoop/mapreduce/lib/input/FileSplit/FileSplit(org.apache.hadoop.fs.Path,long,long,java.lang.String%5B%5D)|",
      "|java+method:///scala/Some/x()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$apply$4/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$apply$4(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1,org.apache.spark.sql.execution.datasources.RecordReaderIterator)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)|",
      "|java+method:///org/apache/spark/sql/types/StructType/length()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$8/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$8(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1,org.apache.spark.sql.execution.datasources.PartitionedFile)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/parquet/filter2/compat/FilterCompat/get(org.apache.parquet.filter2.predicate.FilterPredicate,org.apache.parquet.filter.UnboundRecordFilter)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/JobID/JobID()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getLocations()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/generate(java.lang.Object,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/TaskContext$/get()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getStart()|",
      "|java+method:///org/apache/spark/broadcast/Broadcast/value()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionedFile/filePath()|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionedFile/length()|",
      "|java+method:///org/apache/spark/sql/types/StructType/toAttributes()|",
      "|java+method:///org/apache/spark/util/SerializableConfiguration/value()|",
      "|java+constructor:///org/apache/hadoop/mapreduce/task/TaskAttemptContextImpl/TaskAttemptContextImpl(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Array$/empty(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionedFile/start()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.net.URI)|",
      "|java+method:///org/apache/parquet/hadoop/ParquetInputFormat/setFilterPredicate(org.apache.hadoop.conf.Configuration,org.apache.parquet.filter2.predicate.FilterPredicate)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/TaskAttemptID/TaskAttemptID(org.apache.hadoop.mapreduce.TaskID,int)|",
      "|java+constructor:///org/apache/parquet/hadoop/ParquetInputSplit/ParquetInputSplit(org.apache.hadoop.fs.Path,long,long,long,java.lang.String%5B%5D,long%5B%5D)|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getLength()|",
      "|java+method:///org/apache/hadoop/mapreduce/task/TaskAttemptContextImpl/getConfiguration()|",
      "|java+method:///org/apache/parquet/hadoop/ParquetRecordReader/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$apply$6/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$apply$6(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1,org.apache.spark.sql.catalyst.expressions.JoinedRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,org.apache.spark.sql.execution.datasources.PartitionedFile)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/RecordReaderIterator/RecordReaderIterator(org.apache.hadoop.mapreduce.RecordReader)|",
      "|java+method:///org/apache/spark/sql/types/StructType/size()|",
      "|java+method:///org/apache/spark/sql/catalyst/InternalRow/numFields()|",
      "|java+constructor:///org/apache/hadoop/mapreduce/TaskID/TaskID(org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.mapreduce.TaskType,int)|",
      "|java+constructor:///org/apache/parquet/hadoop/ParquetRecordReader/ParquetRecordReader(org.apache.parquet.hadoop.api.ReadSupport)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$9/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$9(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///java/net/URI/URI(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/RecordReaderIterator/map(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initBatch(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.InternalRow)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/enableReturningBatches()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/parquet/hadoop/ParquetRecordReader/ParquetRecordReader(org.apache.parquet.hadoop.api.ReadSupport,org.apache.parquet.filter2.compat.FilterCompat$Filter)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/ParquetReadSupport()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/JoinedRow/JoinedRow()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getPath()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/VectorizedParquetRecordReader()|",
      "|java+method:///scala/Some/x()|",
      "|java+constructor:///org/apache/hadoop/mapreduce/lib/input/FileSplit/FileSplit(org.apache.hadoop.fs.Path,long,long,java.lang.String%5B%5D)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)|",
      "|java+method:///org/apache/spark/sql/types/StructType/length()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$8/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$8(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1,org.apache.spark.sql.execution.datasources.PartitionedFile)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$apply$3/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$apply$3(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1,org.apache.spark.sql.execution.datasources.RecordReaderIterator)|",
      "|java+method:///org/apache/parquet/filter2/compat/FilterCompat/get(org.apache.parquet.filter2.predicate.FilterPredicate,org.apache.parquet.filter.UnboundRecordFilter)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/JobID/JobID()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getLocations()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/generate(java.lang.Object,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/TaskContext$/get()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getStart()|",
      "|java+method:///org/apache/spark/broadcast/Broadcast/value()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionedFile/filePath()|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionedFile/length()|",
      "|java+method:///org/apache/spark/sql/types/StructType/toAttributes()|",
      "|java+method:///org/apache/spark/util/SerializableConfiguration/value()|",
      "|java+constructor:///org/apache/hadoop/mapreduce/task/TaskAttemptContextImpl/TaskAttemptContextImpl(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Array$/empty(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionedFile/start()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.net.URI)|",
      "|java+method:///org/apache/parquet/hadoop/ParquetInputFormat/setFilterPredicate(org.apache.hadoop.conf.Configuration,org.apache.parquet.filter2.predicate.FilterPredicate)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/TaskAttemptID/TaskAttemptID(org.apache.hadoop.mapreduce.TaskID,int)|",
      "|java+constructor:///org/apache/parquet/hadoop/ParquetInputSplit/ParquetInputSplit(org.apache.hadoop.fs.Path,long,long,long,java.lang.String%5B%5D,long%5B%5D)|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getLength()|",
      "|java+method:///org/apache/hadoop/mapreduce/task/TaskAttemptContextImpl/getConfiguration()|",
      "|java+method:///org/apache/parquet/hadoop/ParquetRecordReader/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$apply$5/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$apply$5(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1,org.apache.spark.sql.catalyst.expressions.JoinedRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,org.apache.spark.sql.execution.datasources.PartitionedFile)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionedFile/partitionValues()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/RecordReaderIterator/RecordReaderIterator(org.apache.hadoop.mapreduce.RecordReader)|",
      "|java+method:///org/apache/spark/sql/types/StructType/size()|",
      "|java+method:///org/apache/spark/sql/catalyst/InternalRow/numFields()|",
      "|java+constructor:///org/apache/hadoop/mapreduce/TaskID/TaskID(org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.mapreduce.TaskType,int)|",
      "|java+constructor:///org/apache/parquet/hadoop/ParquetRecordReader/ParquetRecordReader(org.apache.parquet.hadoop.api.ReadSupport)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$9/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$9(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/internal/CatalogImpl/createExternalTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/table(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/CreateTable/CreateTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.SaveMode,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/buildStorageFormatFromOptions(scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/ParserInterface/parseTableIdentifier(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/toRdd()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/executePlan(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|",
      "|java+method:///java/lang/String/toLowerCase()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$13()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$14()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/sqlParser()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/table(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///java/lang/String/toLowerCase()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/CreateTable/CreateTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.SaveMode,scala.Option)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/buildStorageFormatFromOptions(scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/ParserInterface/parseTableIdentifier(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/QueryExecution/toRdd()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$18()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/executePlan(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$13()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$14()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/sqlParser()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$6/UDFRegistration$$anonfun$register$6(org.apache.spark.sql.UDFRegistration,scala.Function4,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$5/UDFRegistration$$anonfun$5(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$6/UDFRegistration$$anonfun$register$6(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function4,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$5/UDFRegistration$$anonfun$5(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function0,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$1/UDFRegistration$$anonfun$1(org.apache.spark.sql.UDFRegistration)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$2/UDFRegistration$$anonfun$register$2(org.apache.spark.sql.UDFRegistration,scala.Function0,org.apache.spark.sql.types.DataType,scala.Option)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$2/UDFRegistration$$anonfun$register$2(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function0,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$1/UDFRegistration$$anonfun$1(org.apache.spark.sql.UDFRegistration)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/Dataset/randomSplit(double%5B%5D,long)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/Sort/Sort(scala.collection.Seq,boolean,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$2/Dataset$$anonfun$2(org.apache.spark.sql.Dataset,double)|",
      "|java+method:///scala/collection/Iterator/toArray(scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$22/Dataset$$anonfun$22(org.apache.spark.sql.Dataset)|",
      "|java+method:///scala/collection/mutable/ArrayOps/sliding(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToDouble(java.lang.Object)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$randomSplit$4/Dataset$$anonfun$randomSplit$4(org.apache.spark.sql.Dataset,long,org.apache.spark.sql.catalyst.plans.logical.Sort)|",
      "|java+method:///scala/collection/mutable/ArrayOps/forall(scala.Function1)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/Predef$/doubleArrayOps(double%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/Dataset/logicalPlan()|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$23/Dataset$$anonfun$23(org.apache.spark.sql.Dataset)|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/collection/mutable/ArrayOps/scanLeft(java.lang.Object,scala.Function2,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|",
      "|java+method:///scala/reflect/ClassTag$/Double()|",
      "|java+method:///scala/collection/mutable/ArrayOps/sum(scala.math.Numeric)|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$3/Dataset$$anonfun$3(org.apache.spark.sql.Dataset)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$randomSplit$1/Dataset$$anonfun$randomSplit$1(org.apache.spark.sql.Dataset)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/Sort/Sort(scala.collection.Seq,boolean,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///scala/collection/Seq/filterNot(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$randomSplit$2/Dataset$$anonfun$randomSplit$2(org.apache.spark.sql.Dataset,double%5B%5D)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$randomSplit$3/Dataset$$anonfun$randomSplit$3(org.apache.spark.sql.Dataset,double%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$2/Dataset$$anonfun$2(org.apache.spark.sql.Dataset,double)|",
      "|java+method:///scala/collection/Iterator/toArray(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq/filter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$22/Dataset$$anonfun$22(org.apache.spark.sql.Dataset)|",
      "|java+method:///scala/collection/mutable/ArrayOps/sliding(int)|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToDouble(java.lang.Object)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/Sort/Sort(scala.collection.Seq,boolean,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/Dataset/cache()|",
      "|java+method:///scala/collection/mutable/ArrayOps/forall(scala.Function1)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$randomSplit$4/Dataset$$anonfun$randomSplit$4(org.apache.spark.sql.Dataset,long,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///scala/Predef$/doubleArrayOps(double%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/Dataset/logicalPlan()|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$23/Dataset$$anonfun$23(org.apache.spark.sql.Dataset)|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/collection/mutable/ArrayOps/scanLeft(java.lang.Object,scala.Function2,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|",
      "|java+method:///scala/reflect/ClassTag$/Double()|",
      "|java+method:///scala/collection/mutable/ArrayOps/sum(scala.math.Numeric)|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$3/Dataset$$anonfun$3(org.apache.spark.sql.Dataset)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$randomSplit$1/Dataset$$anonfun$randomSplit$1(org.apache.spark.sql.Dataset)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$randomSplit$2/Dataset$$anonfun$randomSplit$2(org.apache.spark.sql.Dataset,double%5B%5D)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$randomSplit$3/Dataset$$anonfun$randomSplit$3(org.apache.spark.sql.Dataset,double%5B%5D)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/api/r/SQLUtils$/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/listTables(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/sql/api/r/SQLUtils$$anonfun$getTableNames$1/SQLUtils$$anonfun$getTableNames$1()|",
      "|java+constructor:///org/apache/spark/sql/api/r/SQLUtils$$anonfun$getTableNames$2/SQLUtils$$anonfun$getTableNames$2()|",
      "|java+method:///org/apache/spark/sql/catalog/Catalog/listTables(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalog/Catalog/listTables()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/immutable/StringOps/nonEmpty()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/sql/Dataset/collect()|",
      "|java+method:///java/lang/String/trim()|",
      "|java+method:///org/apache/spark/sql/SparkSession/catalog()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/sql/api/r/SQLUtils$$anonfun$getTableNames$1/SQLUtils$$anonfun$getTableNames$1()|",
      "|java+method:///scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/sql/catalog/Catalog/currentDatabase()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/StringOps/nonEmpty()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/listTables(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///java/lang/String/trim()|",
      "|java+method:///org/apache/spark/sql/SparkSession/catalog()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1/apply()|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)|",
    "v1Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/CatalogStorageFormat(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)|",
      "|java+method:///scala/Tuple4/_4()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$validateRowFormatFileFormat(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateFileFormatContext,org.antlr.v4.runtime.ParserRuleContext)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat$/empty()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/rowFormat()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/HIVE_PROVIDER()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/parser/ParseException/ParseException(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/defaultDataSourceName()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/outputFormat()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/bucketSpec()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/inputFormat()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/skewSpec()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/serde()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///scala/Tuple4/_1()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/CreateTable/CreateTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.SaveMode,scala.Option)|",
      "|java+method:///scala/Option/flatMap(scala.Function1)|",
      "|java+method:///scala/Tuple4/_2()|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1/apply()|",
      "|java+method:///scala/Tuple4/_3()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/convertCTAS()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/internal/HiveSerDe$/sourceToSerDe(java.lang.String)|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/createTableHeader()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/createFileFormat()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/locationSpec()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/STRING()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$41/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$41(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$40/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$40(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$39/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$39(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$38/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$38(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/tablePropertyList()|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$30/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$30(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$27/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$27(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$28/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$28(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$29/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$29(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$26/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$26(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$37/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$37(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$31/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$31(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$42/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$42(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$13()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$20/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$20(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$21/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$21(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$32/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$32(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$22/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$22(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$33/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$33(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$23/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$23(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$34/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$34(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$24/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$24(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$35/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$35(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+method:///org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$25/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$25(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$36/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$36(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$14()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/getConfString(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/query()|",
      "|java+method:///org/apache/spark/sql/types/StructType/nonEmpty()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()|",
      "|java+method:///scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder/visitCreateTableHeader(org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateTableHeaderContext)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()|",
      "|java+constructor:///scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|"
    ],
    "v2Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/HIVE_PROVIDER()|",
      "|java+method:///scala/Tuple4/_4()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()|",
      "|java+constructor:///scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat$/empty()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/CatalogStorageFormat(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/rowFormat()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/parser/ParseException/ParseException(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/defaultDataSourceName()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/outputFormat()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/bucketSpec()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/inputFormat()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/skewSpec()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/serde()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///scala/Tuple4/_1()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/CreateTable/CreateTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.SaveMode,scala.Option)|",
      "|java+method:///scala/Option/flatMap(scala.Function1)|",
      "|java+method:///scala/Tuple4/_2()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1/apply()|",
      "|java+method:///scala/Tuple4/_3()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/convertCTAS()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/internal/HiveSerDe$/sourceToSerDe(java.lang.String)|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/createTableHeader()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/createFileFormat()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/locationSpec()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/STRING()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$41/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$41(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$40/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$40(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$39/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$39(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$38/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$38(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat)|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/tablePropertyList()|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$30/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$30(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$27/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$27(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$28/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$28(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$29/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$29(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$26/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$26(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$37/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$37(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$31/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$31(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$42/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$42(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$13()|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$20/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$20(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$21/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$21(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$32/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$32(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$22/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$22(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$33/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$33(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$23/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$23(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$34/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$34(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$24/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$24(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$35/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$35(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+method:///org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$25/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$25(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$36/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$36(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$14()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/getConfString(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/query()|",
      "|java+method:///org/apache/spark/sql/types/StructType/nonEmpty()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$18()|",
      "|java+method:///scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder/visitCreateTableHeader(org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateTableHeaderContext)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()|",
      "|java+method:///org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$validateRowFormatFileFormat(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateFileFormatContext,org.antlr.v4.runtime.ParserRuleContext)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$18()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+constructor:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/EventTimeWatermarkExec(org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.unsafe.types.CalendarInterval,org.apache.spark.sql.execution.SparkPlan)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/plans/logical/EventTimeWatermark$/getDelayMs(org.apache.spark.unsafe.types.CalendarInterval)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/sparkContext()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/EventTimeStatsAccum$/$lessinit$greater$default$1()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/EventTimeStatsAccum/EventTimeStatsAccum(org.apache.spark.sql.execution.streaming.EventTimeStats)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec$$anonfun$2/EventTimeWatermarkExec$$anonfun$2(org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec)|",
      "|java+method:///org/apache/spark/SparkContext/register(org.apache.spark.util.AccumulatorV2)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkPlan/SparkPlan()|",
      "|java+method:///org/apache/spark/sql/execution/SparkPlan/output()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/eventTimeStats()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/sparkContext()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/EventTimeStatsAccum$/$lessinit$greater$default$1()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/EventTimeStatsAccum/EventTimeStatsAccum(org.apache.spark.sql.execution.streaming.EventTimeStats)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec$$anonfun$2/EventTimeWatermarkExec$$anonfun$2(org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/EventTimeWatermark$/getDelayMs(org.apache.spark.unsafe.types.CalendarInterval)|",
      "|java+method:///org/apache/spark/SparkContext/register(org.apache.spark.util.AccumulatorV2)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkPlan/SparkPlan()|",
      "|java+method:///org/apache/spark/sql/execution/SparkPlan/output()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/eventTimeStats()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$/org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)|",
    "called": "|java+method:///org/apache/spark/sql/internal/SQLConf/parallelPartitionDiscoveryParallelism()|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions$default$2()|",
      "|java+method:///scala/Predef$DummyImplicit$/dummyImplicit()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sparkContext()|",
      "|java+method:///java/lang/Math/min(int,int)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/rdd/RDD/collect()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$13/PartitioningAwareFileIndex$$anonfun$13()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$3/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$3()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$11/PartitioningAwareFileIndex$$anonfun$11()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$12/PartitioningAwareFileIndex$$anonfun$12(org.apache.hadoop.fs.PathFilter,org.apache.spark.util.SerializableConfiguration)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/parallelPartitionDiscoveryThreshold()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$2/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$2(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/metrics/source/HiveCatalogMetrics$/incrementParallelListingJobCount(int)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$1/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$1(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions$default$2()|",
      "|java+method:///scala/Predef$DummyImplicit$/dummyImplicit()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sparkContext()|",
      "|java+method:///java/lang/Math/min(int,int)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/rdd/RDD/collect()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/parallelPartitionDiscoveryParallelism()|",
      "|java+method:///org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$13/PartitioningAwareFileIndex$$anonfun$13()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$3/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$3()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$11/PartitioningAwareFileIndex$$anonfun$11()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$12/PartitioningAwareFileIndex$$anonfun$12(org.apache.hadoop.fs.PathFilter,org.apache.spark.util.SerializableConfiguration)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/parallelPartitionDiscoveryThreshold()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$2/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$2(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/metrics/source/HiveCatalogMetrics$/incrementParallelListingJobCount(int)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$1/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$1(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$8/UDFRegistration$$anonfun$register$8(org.apache.spark.sql.UDFRegistration,scala.Function6,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$7/UDFRegistration$$anonfun$7(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$8/UDFRegistration$$anonfun$register$8(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function6,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$7/UDFRegistration$$anonfun$7(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function16,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$18/UDFRegistration$$anonfun$register$18(org.apache.spark.sql.UDFRegistration,scala.Function16,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$17/UDFRegistration$$anonfun$17(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$18/UDFRegistration$$anonfun$register$18(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function16,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$17/UDFRegistration$$anonfun$17(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function17,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$19/UDFRegistration$$anonfun$register$19(org.apache.spark.sql.UDFRegistration,scala.Function17,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$18/UDFRegistration$$anonfun$18(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$19/UDFRegistration$$anonfun$register$19(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function17,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$18/UDFRegistration$$anonfun$18(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+constructor:///org/apache/spark/sql/execution/SparkOptimizer/SparkOptimizer(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.internal.SQLConf,org.apache.spark.sql.ExperimentalMethods)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/optimizer/Optimizer/Optimizer(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.internal.SQLConf)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/optimizer/Optimizer/Optimizer(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.CatalystConf)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/optimizer/Optimizer/Optimizer(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.internal.SQLConf)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$34/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$34$$anonfun$apply$10/UDFRegistration$$anonfun$register$34$$anonfun$apply$10(org.apache.spark.sql.UDFRegistration$$anonfun$register$34)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/getOrElse(java.lang.Object,scala.Function0)|",
    "v1Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///java/lang/Class/newInstance()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sqlContext()|",
      "|java+method:///org/apache/spark/sql/streaming/OutputMode/Append()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/org$apache$spark$sql$execution$datasources$DataSource$$caseInsensitiveOptions()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/sparkSession()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource$$anonfun$11/DataSource$$anonfun$11(org.apache.spark.sql.execution.datasources.DataSource)|",
      "|java+method:///org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///org/apache/spark/sql/sources/StreamSinkProvider/createSink(org.apache.spark.sql.SQLContext,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.streaming.OutputMode)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/className()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/partitionColumns()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/providingClass()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSink/FileStreamSink(org.apache.spark.sql.SparkSession,java.lang.String,org.apache.spark.sql.execution.datasources.FileFormat,scala.collection.Seq,scala.collection.immutable.Map)|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///java/lang/Class/newInstance()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sqlContext()|",
      "|java+method:///org/apache/spark/sql/streaming/OutputMode/Append()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/sparkSession()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource$$anonfun$11/DataSource$$anonfun$11(org.apache.spark.sql.execution.datasources.DataSource)|",
      "|java+method:///org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/org$apache$spark$sql$execution$datasources$DataSource$$caseInsensitiveOptions()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/sql/sources/StreamSinkProvider/createSink(org.apache.spark.sql.SQLContext,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.streaming.OutputMode)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/className()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/partitionColumns()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/providingClass()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSink/FileStreamSink(org.apache.spark.sql.SparkSession,java.lang.String,org.apache.spark.sql.execution.datasources.FileFormat,scala.collection.Seq,scala.collection.immutable.Map)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$41/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$41$$anonfun$apply$17/UDFRegistration$$anonfun$register$41$$anonfun$apply$17(org.apache.spark.sql.UDFRegistration$$anonfun$register$41)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$29/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$29$$anonfun$apply$5/UDFRegistration$$anonfun$register$29$$anonfun$apply$5(org.apache.spark.sql.UDFRegistration$$anonfun$register$29)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function21,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$22/UDFRegistration$$anonfun$22(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$23/UDFRegistration$$anonfun$register$23(org.apache.spark.sql.UDFRegistration,scala.Function21,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$22/UDFRegistration$$anonfun$22(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$23/UDFRegistration$$anonfun$register$23(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function21,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+constructor:///org/apache/spark/sql/execution/datasources/HadoopFsRelation/HadoopFsRelation(org.apache.spark.sql.execution.datasources.FileIndex,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.Option,org.apache.spark.sql.execution.datasources.FileFormat,scala.collection.immutable.Map,org.apache.spark.sql.SparkSession)|",
    "called": "|java+method:///org/apache/spark/sql/types/StructType/foreach(scala.Function1)|",
    "v1Body": [
      "|java+method:///scala/Product$class/$init$(scala.Product)|",
      "|java+method:///org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/HadoopFsRelation$$anonfun$2/HadoopFsRelation$$anonfun$2(org.apache.spark.sql.execution.datasources.HadoopFsRelation,scala.collection.immutable.Set)|",
      "|java+method:///org/apache/spark/sql/types/StructType/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/sql/sources/BaseRelation/BaseRelation()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/HadoopFsRelation$$anonfun$1/HadoopFsRelation$$anonfun$1(org.apache.spark.sql.execution.datasources.HadoopFsRelation)|",
      "|java+method:///org/apache/spark/sql/types/StructType/filterNot(scala.Function1)|",
      "|java+method:///scala/collection/TraversableOnce/toSet()|",
      "|java+method:///org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///scala/Product$class/$init$(scala.Product)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/HadoopFsRelation$$anonfun$3/HadoopFsRelation$$anonfun$3(org.apache.spark.sql.execution.datasources.HadoopFsRelation,scala.Function1,scala.collection.mutable.Map)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/HadoopFsRelation$$anonfun$2/HadoopFsRelation$$anonfun$2(org.apache.spark.sql.execution.datasources.HadoopFsRelation)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/HadoopFsRelation$$anonfun$1/HadoopFsRelation$$anonfun$1(org.apache.spark.sql.execution.datasources.HadoopFsRelation)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/HadoopFsRelation$$anonfun$5/HadoopFsRelation$$anonfun$5(org.apache.spark.sql.execution.datasources.HadoopFsRelation,scala.Function1,scala.collection.mutable.Map)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/types/StructType/filterNot(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/types/StructType/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/sources/BaseRelation/BaseRelation()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/HadoopFsRelation$$anonfun$4/HadoopFsRelation$$anonfun$4(org.apache.spark.sql.execution.datasources.HadoopFsRelation,scala.Function1,scala.collection.mutable.Map)|",
      "|java+method:///scala/collection/mutable/Map$/empty()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/caseSensitiveAnalysis()|",
      "|java+method:///org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$40/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$40$$anonfun$apply$16/UDFRegistration$$anonfun$register$40$$anonfun$apply$16(org.apache.spark.sql.UDFRegistration$$anonfun$register$40)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/TableIdentifier/TableIdentifier(java.lang.String,scala.Option)|",
    "v1Body": [
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AnalyzeColumnCommand$$anonfun$2/AnalyzeColumnCommand$$anonfun$2(org.apache.spark.sql.execution.command.AnalyzeColumnCommand)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Statistics$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/stats()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/table()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/lookupRelation$default$2()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/Statistics/Statistics(scala.math.BigInt,scala.Option,scala.collection.immutable.Map,boolean)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/EliminateSubqueryAliases$/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/math/BigInt$/long2bigInt(long)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AnalyzeColumnCommand$$anonfun$3/AnalyzeColumnCommand$$anonfun$3(org.apache.spark.sql.execution.command.AnalyzeColumnCommand)|",
      "|java+method:///org/apache/spark/sql/execution/command/AnalyzeColumnCommand/columnNames()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogRelation/catalogTable()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/immutable/MapLike/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AnalyzeColumnCommand$$anonfun$1/AnalyzeColumnCommand$$anonfun$1(org.apache.spark.sql.execution.command.AnalyzeColumnCommand,org.apache.spark.sql.internal.SessionState)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/database()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/lookupRelation(org.apache.spark.sql.catalyst.TableIdentifier,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/TableIdentifier/TableIdentifier(java.lang.String,scala.Option)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Tuple2/_2$mcJ$sp()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/LogicalRelation/catalogTable()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/nodeName()|",
      "|java+method:///scala/Tuple2/_1$mcJ$sp()|",
      "|java+method:///org/apache/spark/sql/execution/command/AnalyzeColumnCommand$/computeColumnStats(org.apache.spark.sql.SparkSession,java.lang.String,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/command/AnalyzeColumnCommand/tableIdent()|",
      "|java+method:///org/apache/spark/sql/execution/command/AnalyzeTableCommand$/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AnalyzeColumnCommand$$anonfun$2/AnalyzeColumnCommand$$anonfun$2(org.apache.spark.sql.execution.command.AnalyzeColumnCommand)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/Statistics$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/stats()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/table()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/lookupRelation$default$2()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/Statistics/Statistics(scala.math.BigInt,scala.Option,scala.collection.immutable.Map,boolean)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/EliminateSubqueryAliases$/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/math/BigInt$/long2bigInt(long)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AnalyzeColumnCommand$$anonfun$3/AnalyzeColumnCommand$$anonfun$3(org.apache.spark.sql.execution.command.AnalyzeColumnCommand)|",
      "|java+method:///org/apache/spark/sql/execution/command/AnalyzeColumnCommand/columnNames()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogRelation/catalogTable()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/immutable/MapLike/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AnalyzeColumnCommand$$anonfun$1/AnalyzeColumnCommand$$anonfun$1(org.apache.spark.sql.execution.command.AnalyzeColumnCommand,org.apache.spark.sql.internal.SessionState)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/database()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/lookupRelation(org.apache.spark.sql.catalyst.TableIdentifier,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/TableIdentifier/TableIdentifier(java.lang.String,scala.Option)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$18()|",
      "|java+method:///scala/Tuple2/_2$mcJ$sp()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/LogicalRelation/catalogTable()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/nodeName()|",
      "|java+method:///scala/Tuple2/_1$mcJ$sp()|",
      "|java+method:///org/apache/spark/sql/execution/command/AnalyzeColumnCommand$/computeColumnStats(org.apache.spark.sql.SparkSession,java.lang.String,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/command/AnalyzeColumnCommand/tableIdent()|",
      "|java+method:///org/apache/spark/sql/execution/command/AnalyzeTableCommand$/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/Dataset$$anonfun$select$1/apply()|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/Project/Project(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/Dataset$$anonfun$select$1/apply()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/Project/Project(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/Dataset/logicalPlan()|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$select$1$$anonfun$apply$7/Dataset$$anonfun$select$1$$anonfun$apply$7(org.apache.spark.sql.Dataset$$anonfun$select$1)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/Dataset$$anonfun$select$1/apply()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/Project/Project(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/Dataset/logicalPlan()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$select$1$$anonfun$apply$8/Dataset$$anonfun$select$1$$anonfun$apply$8(org.apache.spark.sql.Dataset$$anonfun$select$1)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$11/UDFRegistration$$anonfun$11(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$12/UDFRegistration$$anonfun$register$12(org.apache.spark.sql.UDFRegistration,scala.Function10,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$11/UDFRegistration$$anonfun$11(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$12/UDFRegistration$$anonfun$register$12(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function10,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$31/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$31$$anonfun$apply$7/UDFRegistration$$anonfun$register$31$$anonfun$apply$7(org.apache.spark.sql.UDFRegistration$$anonfun$register$31)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function14,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$15/UDFRegistration$$anonfun$15(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$16/UDFRegistration$$anonfun$register$16(org.apache.spark.sql.UDFRegistration,scala.Function14,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$15/UDFRegistration$$anonfun$15(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$16/UDFRegistration$$anonfun$register$16(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function14,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/castAndRenameChildOutput(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,scala.collection.Seq)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$1()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/child()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/Project/Project(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.immutable.Map,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.catalyst.plans.logical.OverwriteOptions,boolean)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$16/PreprocessTableInsertion$$anonfun$16(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/child()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/Project/Project(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$4()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$14/PreprocessTableInsertion$$anonfun$14(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.immutable.Map,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.catalyst.plans.logical.OverwriteOptions,boolean)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$25/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$25$$anonfun$apply$1/UDFRegistration$$anonfun$register$25$$anonfun$apply$1(org.apache.spark.sql.UDFRegistration$$anonfun$register$25)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$39/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$39$$anonfun$apply$15/UDFRegistration$$anonfun$register$39$$anonfun$apply$15(org.apache.spark.sql.UDFRegistration$$anonfun$register$39)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/properties()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/properties()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/isView()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/tableName()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/properties()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/properties()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/isView()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/tableName()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$18()|",
      "|java+method:///scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator$$anonfun$2/apply(org.apache.spark.sql.types.StructField)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceObj(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/types/DecimalType/scale()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/types/StructField/dataType()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/types/StructField/name()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/types/DecimalType/precision()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/types/DecimalType/scale()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/types/StructField/dataType()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///org/apache/spark/sql/types/StructField/name()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/types/DecimalType/precision()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceObj(java.lang.Object)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator$$anonfun$1/apply(org.apache.spark.sql.types.StructField)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceObj(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/types/DecimalType/scale()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/types/StructField/dataType()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/types/StructField/name()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/types/DecimalType/precision()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/types/DecimalType/scale()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/types/StructField/dataType()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///org/apache/spark/sql/types/StructField/name()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/types/DecimalType/precision()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceObj(java.lang.Object)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/quotedString()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///org/apache/spark/util/Utils$/resolveURI(java.lang.String)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///java/nio/file/Path/toAbsolutePath()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/isLocal()|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/isOverwrite()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/LoadDataCommand$$anonfun$run$1/LoadDataCommand$$anonfun$run$1(org.apache.spark.sql.execution.command.LoadDataCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)|",
      "|java+constructor:///java/net/URI/URI(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/TraversableOnce/size()|",
      "|java+constructor:///java/net/URI/URI(java.lang.String)|",
      "|java+method:///java/nio/file/FileSystem/getPathMatcher(java.lang.String)|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///java/net/URI/toString()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///java/net/URI/getAuthority()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/LoadDataCommand$$anonfun$3/LoadDataCommand$$anonfun$3(org.apache.spark.sql.execution.command.LoadDataCommand,java.nio.file.FileSystem,java.nio.file.PathMatcher)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/partition()|",
      "|java+method:///scala/Option/nonEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/isDatasourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/table()|",
      "|java+method:///java/net/URI/getQuery()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/loadPartition(org.apache.spark.sql.catalyst.TableIdentifier,java.lang.String,scala.collection.immutable.Map,boolean,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///java/net/URI/getPath()|",
      "|java+method:///java/net/URI/getScheme()|",
      "|java+method:///java/lang/System/getProperty(java.lang.String)|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///scala/collection/MapLike/keys()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///java/nio/file/FileSystems/getDefault()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/quotedString()|",
      "|java+method:///java/lang/String/startsWith(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/path()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/loadTable(org.apache.spark.sql.catalyst.TableIdentifier,java.lang.String,boolean,boolean)|",
      "|java+method:///scala/collection/mutable/ArrayOps/exists(scala.Function1)|",
      "|java+method:///java/lang/String/contains(java.lang.CharSequence)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/hadoop/conf/Configuration/get(java.lang.String)|",
      "|java+method:///java/nio/file/Path/getParent()|",
      "|java+method:///java/io/File/listFiles()|",
      "|java+method:///java/nio/file/FileSystem/getPath(java.lang.String,java.lang.String%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///java/net/URI/getFragment()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///org/apache/spark/util/Utils$/resolveURI(java.lang.String)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///java/nio/file/Path/toAbsolutePath()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/isLocal()|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/isOverwrite()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/LoadDataCommand$$anonfun$run$1/LoadDataCommand$$anonfun$run$1(org.apache.spark.sql.execution.command.LoadDataCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)|",
      "|java+constructor:///java/net/URI/URI(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/TraversableOnce/size()|",
      "|java+constructor:///java/net/URI/URI(java.lang.String)|",
      "|java+method:///java/nio/file/FileSystem/getPathMatcher(java.lang.String)|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///java/net/URI/toString()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///java/net/URI/getAuthority()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/LoadDataCommand$$anonfun$3/LoadDataCommand$$anonfun$3(org.apache.spark.sql.execution.command.LoadDataCommand,java.nio.file.FileSystem,java.nio.file.PathMatcher)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/partition()|",
      "|java+method:///scala/Option/nonEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/isDatasourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/table()|",
      "|java+method:///java/net/URI/getQuery()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/loadPartition(org.apache.spark.sql.catalyst.TableIdentifier,java.lang.String,scala.collection.immutable.Map,boolean,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///java/net/URI/getPath()|",
      "|java+method:///java/net/URI/getScheme()|",
      "|java+method:///java/lang/System/getProperty(java.lang.String)|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///scala/collection/MapLike/keys()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///java/nio/file/FileSystems/getDefault()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/quotedString()|",
      "|java+method:///java/lang/String/startsWith(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/path()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/loadTable(org.apache.spark.sql.catalyst.TableIdentifier,java.lang.String,boolean,boolean)|",
      "|java+method:///scala/collection/mutable/ArrayOps/exists(scala.Function1)|",
      "|java+method:///java/lang/String/contains(java.lang.CharSequence)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/hadoop/conf/Configuration/get(java.lang.String)|",
      "|java+method:///java/nio/file/Path/getParent()|",
      "|java+method:///java/io/File/listFiles()|",
      "|java+method:///java/nio/file/FileSystem/getPath(java.lang.String,java.lang.String%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///java/net/URI/getFragment()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/analysis/Analyzer/resolver()|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/sql/Dataset/select(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///scala/collection/Seq/isEmpty()|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/analyzer()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/Analyzer/resolver()|",
      "|java+constructor:///org/apache/spark/sql/DataFrameNaFunctions$$anonfun$4/DataFrameNaFunctions$$anonfun$4(org.apache.spark.sql.DataFrameNaFunctions)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/immutable/Map/head()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/immutable/Map$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/types/StructType/fields()|",
      "|java+constructor:///org/apache/spark/sql/DataFrameNaFunctions$$anonfun$5/DataFrameNaFunctions$$anonfun$5(org.apache.spark.sql.DataFrameNaFunctions,scala.collection.Seq,scala.collection.immutable.Map,org.apache.spark.sql.types.AtomicType,scala.Function2)|",
      "|java+method:///scala/collection/immutable/Map/isEmpty()|",
      "|java+method:///org/apache/spark/sql/Dataset/sparkSession()|",
      "|java+method:///org/apache/spark/sql/Dataset/schema()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/sql/Dataset/select(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/Analyzer/resolver()|",
      "|java+method:///scala/collection/Seq/isEmpty()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/analyzer()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/DataFrameNaFunctions$$anonfun$3/DataFrameNaFunctions$$anonfun$3(org.apache.spark.sql.DataFrameNaFunctions,scala.collection.Seq,scala.collection.immutable.Map,org.apache.spark.sql.types.AtomicType,scala.Function2)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/immutable/Map/head()|",
      "|java+constructor:///org/apache/spark/sql/DataFrameNaFunctions$$anonfun$2/DataFrameNaFunctions$$anonfun$2(org.apache.spark.sql.DataFrameNaFunctions)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/immutable/Map$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/types/StructType/fields()|",
      "|java+method:///scala/collection/immutable/Map/isEmpty()|",
      "|java+method:///org/apache/spark/sql/Dataset/sparkSession()|",
      "|java+method:///org/apache/spark/sql/Dataset/schema()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/streaming/DataStreamWriter/start()|",
    "called": "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$6()|",
      "|java+constructor:///scala/Tuple2$mcZZ$sp/Tuple2$mcZZ$sp(boolean,boolean)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryManager/startQuery$default$7()|",
      "|java+method:///org/apache/spark/sql/streaming/DataStreamWriter/source()|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryManager/startQuery$default$9()|",
      "|java+method:///org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/streaming/DataStreamWriter/outputMode()|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryManager/startQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)|",
      "|java+method:///scala/collection/mutable/HashMap/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///org/apache/spark/sql/Dataset/exprEnc()|",
      "|java+constructor:///org/apache/spark/sql/streaming/DataStreamWriter$$anonfun$1/DataStreamWriter$$anonfun$1(org.apache.spark.sql.streaming.DataStreamWriter)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/Dataset/createOrReplaceTempView(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$3()|",
      "|java+method:///org/apache/spark/sql/streaming/DataStreamWriter/extraOptions()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$8()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/sql/streaming/OutputMode/Complete()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/ForeachSink/ForeachSink(org.apache.spark.sql.ForeachWriter,org.apache.spark.sql.Encoder)|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///scala/Tuple2/_2$mcZ$sp()|",
      "|java+method:///scala/Tuple2/_1$mcZ$sp()|",
      "|java+method:///org/apache/spark/sql/streaming/DataStreamWriter/assertNotPartitioned(java.lang.String)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/MemoryPlan/MemoryPlan(org.apache.spark.sql.execution.streaming.MemorySink)|",
      "|java+method:///org/apache/spark/sql/streaming/DataStreamWriter/org$apache$spark$sql$streaming$DataStreamWriter$$df()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///org/apache/spark/sql/Dataset/sparkSession()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/streamingQueryManager()|",
      "|java+method:///scala/Predef$/$conforms()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQuery/name()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/streaming/DataStreamWriter/trigger()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/MemorySink/MemorySink(org.apache.spark.sql.types.StructType,org.apache.spark.sql.streaming.OutputMode)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/streaming/DataStreamWriter/foreachWriter()|",
      "|java+method:///org/apache/spark/sql/streaming/DataStreamWriter/normalizedParCols()|",
      "|java+method:///org/apache/spark/sql/Dataset/schema()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource/DataSource(org.apache.spark.sql.SparkSession,java.lang.String,scala.collection.Seq,scala.Option,scala.collection.Seq,scala.Option,scala.collection.immutable.Map,scala.Option)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$6()|",
      "|java+constructor:///scala/Tuple2$mcZZ$sp/Tuple2$mcZZ$sp(boolean,boolean)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryManager/startQuery$default$7()|",
      "|java+method:///org/apache/spark/sql/streaming/DataStreamWriter/source()|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryManager/startQuery$default$9()|",
      "|java+method:///org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/streaming/DataStreamWriter/outputMode()|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryManager/startQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)|",
      "|java+method:///scala/collection/mutable/HashMap/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///org/apache/spark/sql/Dataset/exprEnc()|",
      "|java+constructor:///org/apache/spark/sql/streaming/DataStreamWriter$$anonfun$1/DataStreamWriter$$anonfun$1(org.apache.spark.sql.streaming.DataStreamWriter)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/Dataset/createOrReplaceTempView(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$3()|",
      "|java+method:///org/apache/spark/sql/streaming/DataStreamWriter/extraOptions()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$8()|",
      "|java+method:///org/apache/spark/sql/streaming/OutputMode/Complete()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/ForeachSink/ForeachSink(org.apache.spark.sql.ForeachWriter,org.apache.spark.sql.Encoder)|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///scala/Tuple2/_2$mcZ$sp()|",
      "|java+method:///scala/Tuple2/_1$mcZ$sp()|",
      "|java+method:///org/apache/spark/sql/streaming/DataStreamWriter/assertNotPartitioned(java.lang.String)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/MemoryPlan/MemoryPlan(org.apache.spark.sql.execution.streaming.MemorySink)|",
      "|java+method:///org/apache/spark/sql/streaming/DataStreamWriter/org$apache$spark$sql$streaming$DataStreamWriter$$df()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///org/apache/spark/sql/Dataset/sparkSession()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/streamingQueryManager()|",
      "|java+method:///scala/Predef$/$conforms()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQuery/name()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/streaming/DataStreamWriter/trigger()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/MemorySink/MemorySink(org.apache.spark.sql.types.StructType,org.apache.spark.sql.streaming.OutputMode)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/streaming/DataStreamWriter/foreachWriter()|",
      "|java+method:///org/apache/spark/sql/streaming/DataStreamWriter/normalizedParCols()|",
      "|java+method:///org/apache/spark/sql/Dataset/schema()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource/DataSource(org.apache.spark.sql.SparkSession,java.lang.String,scala.collection.Seq,scala.Option,scala.collection.Seq,scala.Option,scala.collection.immutable.Map,scala.Option)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableCommand$$anonfun$2/CreateDataSourceTableCommand$$anonfun$2(org.apache.spark.sql.execution.command.CreateDataSourceTableCommand,org.apache.spark.sql.internal.SessionState)|",
      "|java+method:///org/apache/spark/sql/types/StructType/fieldNames()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/FileIndex/rootPaths()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/copy(java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/unquotedString()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/tableExists(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableCommand$$anonfun$1/CreateDataSourceTableCommand$$anonfun$1(org.apache.spark.sql.execution.command.CreateDataSourceTableCommand)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/isEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/resolveRelation$default$1()|",
      "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/manageFilesourcePartitions()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/HadoopFsRelation/partitionSchema()|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/HadoopFsRelation/location()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/createTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/ignoreIfExists()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/database()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/spark/sql/types/StructType/nonEmpty()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/table()|",
      "|java+method:///org/apache/spark/sql/sources/BaseRelation/schema()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource/DataSource(org.apache.spark.sql.SparkSession,java.lang.String,scala.collection.Seq,scala.Option,scala.collection.Seq,scala.Option,scala.collection.immutable.Map,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()|",
      "|java+method:///org/apache/spark/sql/types/StructType/isEmpty()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/copy$default$1()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableCommand$$anonfun$2/CreateDataSourceTableCommand$$anonfun$2(org.apache.spark.sql.execution.command.CreateDataSourceTableCommand,org.apache.spark.sql.internal.SessionState)|",
      "|java+method:///org/apache/spark/sql/types/StructType/fieldNames()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/FileIndex/rootPaths()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/copy(java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/unquotedString()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/tableExists(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource$/apply$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/CreateDataSourceTableCommand$$anonfun$1/CreateDataSourceTableCommand$$anonfun$1(org.apache.spark.sql.execution.command.CreateDataSourceTableCommand)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/isEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/DataSource/resolveRelation$default$1()|",
      "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/manageFilesourcePartitions()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/HadoopFsRelation/partitionSchema()|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/HadoopFsRelation/location()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/createTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/ignoreIfExists()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/database()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/spark/sql/types/StructType/nonEmpty()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$18()|",
      "|java+method:///scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|",
      "|java+method:///org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/table()|",
      "|java+method:///org/apache/spark/sql/sources/BaseRelation/schema()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/DataSource/DataSource(org.apache.spark.sql.SparkSession,java.lang.String,scala.collection.Seq,scala.Option,scala.collection.Seq,scala.Option,scala.collection.immutable.Map,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()|",
      "|java+method:///org/apache/spark/sql/types/StructType/isEmpty()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/copy$default$1()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/internal/CatalogImpl$$anonfun$dropTempView$1/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/dropTempView(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$sessionCatalog()|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/uncacheQuery(org.apache.spark.sql.Dataset,boolean)|",
      "|java+method:///org/apache/spark/sql/internal/SharedState/cacheManager()|",
      "|java+method:///org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sharedState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/dropTempView(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/uncacheQuery$default$2()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$sessionCatalog()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/dropTempView(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/internal/SharedState/cacheManager()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sharedState()|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/uncacheQuery(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,boolean)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/internal/CatalogImpl$$anonfun$dropGlobalTempView$1/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/dropGlobalTempView(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$sessionCatalog()|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/uncacheQuery$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/dropGlobalTempView(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/uncacheQuery(org.apache.spark.sql.Dataset,boolean)|",
      "|java+method:///org/apache/spark/sql/internal/SharedState/cacheManager()|",
      "|java+method:///org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sharedState()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$sessionCatalog()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/dropGlobalTempView(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/internal/SharedState/cacheManager()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sharedState()|",
      "|java+method:///org/apache/spark/sql/execution/CacheManager/uncacheQuery(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,boolean)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/expressions/UnsafeRow/copy()|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anon$1/StateStoreSaveExec$$anonfun$doExecute$3$$anon$1(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Iterator/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$4/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$4(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.catalyst.expressions.LessThanOrEqual)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/keyExpressions()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/metric/SQLMetric/$plus$eq(long)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/outputMode()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/numKeys()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$1/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$1(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$6/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$6(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/remove(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$5/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$5(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.catalyst.expressions.codegen.Predicate)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/generate(java.lang.Object,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/commit()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/longMetric(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/SparkPlan/output()|",
      "|java+method:///scala/collection/Iterator/next()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/newPredicate(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/LessThanOrEqual/LessThanOrEqual(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression)|",
      "|java+method:///scala/collection/Seq/find(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/UnsafeRow/copy()|",
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$3/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$3(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.execution.metric.SQLMetric)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$7/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$7(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.execution.metric.SQLMetric)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Attribute/dataType()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/updates()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/put(org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.sql.catalyst.expressions.UnsafeRow)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/GetStructField/GetStructField(org.apache.spark.sql.catalyst.expressions.Expression,int,scala.Option)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/eventTimeWatermark()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/GetStructField$/apply$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Literal$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/iterator()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anon$1/StateStoreSaveExec$$anonfun$doExecute$3$$anon$1(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/child()|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$4/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$4(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.execution.streaming.state.StateStore)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/keyExpressions()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/iterator()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/sql/execution/metric/SQLMetric/$plus$eq(long)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/outputMode()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/numKeys()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$7/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$7(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$6/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$6(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.catalyst.expressions.codegen.Predicate)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/remove(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/generate(java.lang.Object,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/commit()|",
      "|java+method:///org/apache/spark/TaskContext$/get()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/longMetric(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/SparkPlan/output()|",
      "|java+method:///scala/collection/Iterator/next()|",
      "|java+method:///scala/collection/Iterator/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/org$apache$spark$sql$execution$streaming$StateStoreSaveExec$$watermarkPredicate()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/UnsafeRow/copy()|",
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$8/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$8(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.execution.metric.SQLMetric)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$5/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$5(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.execution.metric.SQLMetric)|",
      "|java+method:///org/apache/spark/TaskContext/addTaskCompletionListener(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/updates()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/put(org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.sql.catalyst.expressions.UnsafeRow)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$5/UDFRegistration$$anonfun$register$5(org.apache.spark.sql.UDFRegistration,scala.Function3,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$4/UDFRegistration$$anonfun$4(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$5/UDFRegistration$$anonfun$register$5(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function3,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$4/UDFRegistration$$anonfun$4(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/PartitioningUtils$/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set)|",
    "called": "|java+method:///org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitionSpec/PartitionSpec(org.apache.spark.sql.types.StructType,scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq/isEmpty()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq/head()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/SeqLike/size()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/Predef$/assert(boolean,scala.Function0)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$5/PartitioningUtils$$anonfun$5()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$4/PartitioningUtils$$anonfun$4()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitioningUtils$PartitionValues/literals()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$3/PartitioningUtils$$anonfun$3()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$7/PartitioningUtils$$anonfun$7()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$6/PartitioningUtils$$anonfun$6()|",
      "|java+method:///scala/collection/TraversableLike/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/generic/GenericTraversableTemplate/unzip(scala.Function1)|",
      "|java+method:///scala/collection/Seq/distinct()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionSpec$/emptySpec()|",
      "|java+method:///scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/$conforms()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitioningUtils$/resolvePartitions(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$parsePartitions$1/PartitioningUtils$$anonfun$parsePartitions$1(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$2/PartitioningUtils$$anonfun$2(boolean,scala.collection.immutable.Set)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitioningUtils$PartitionValues/columnNames()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitionSpec/PartitionSpec(org.apache.spark.sql.types.StructType,scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq/isEmpty()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq/head()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionSpec$/emptySpec()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/SeqLike/size()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/Predef$/assert(boolean,scala.Function0)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$5/PartitioningUtils$$anonfun$5()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$4/PartitioningUtils$$anonfun$4()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitioningUtils$PartitionValues/literals()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$3/PartitioningUtils$$anonfun$3()|",
      "|java+method:///scala/collection/Seq/flatten(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$7/PartitioningUtils$$anonfun$7()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$6/PartitioningUtils$$anonfun$6()|",
      "|java+method:///scala/collection/TraversableLike/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/generic/GenericTraversableTemplate/unzip(scala.Function1)|",
      "|java+method:///scala/collection/Seq/distinct()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/$conforms()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitioningUtils$/resolvePartitions(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$parsePartitions$1/PartitioningUtils$$anonfun$parsePartitions$1(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$2/PartitioningUtils$$anonfun$2(boolean,scala.collection.immutable.Set)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitioningUtils$PartitionValues/columnNames()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/Dataset$$anonfun$22/apply(org.apache.spark.sql.catalyst.expressions.Attribute)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/expressions/RowOrdering$/isOrderable(org.apache.spark.sql.types.DataType)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Attribute/dataType()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/catalyst/expressions/RowOrdering$/isOrderable(org.apache.spark.sql.types.DataType)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Attribute/dataType()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$26/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$26$$anonfun$apply$2/UDFRegistration$$anonfun$register$26$$anonfun$apply$2(org.apache.spark.sql.UDFRegistration$$anonfun$register$26)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration/register(java.lang.String,scala.Function15,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$16/UDFRegistration$$anonfun$16(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$17/UDFRegistration$$anonfun$register$17(org.apache.spark.sql.UDFRegistration,scala.Function15,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/Try/toOption()|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$17/UDFRegistration$$anonfun$register$17(org.apache.spark.sql.UDFRegistration,java.lang.String,scala.Function15,org.apache.spark.sql.types.DataType,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$16/UDFRegistration$$anonfun$16(org.apache.spark.sql.UDFRegistration,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/Dataset$$anonfun$repartition$2/apply()|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/RepartitionByExpression/RepartitionByExpression(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Option)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/Dataset$$anonfun$repartition$2/apply()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/RepartitionByExpression/RepartitionByExpression(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$repartition$2$$anonfun$apply$15/Dataset$$anonfun$repartition$2$$anonfun$apply$15(org.apache.spark.sql.Dataset$$anonfun$repartition$2)|",
      "|java+method:///org/apache/spark/sql/Dataset/logicalPlan()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/Dataset$$anonfun$repartition$2/apply()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/RepartitionByExpression/RepartitionByExpression(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Option)|",
      "|java+constructor:///org/apache/spark/sql/Dataset$$anonfun$repartition$2$$anonfun$apply$16/Dataset$$anonfun$repartition$2$$anonfun$apply$16(org.apache.spark.sql.Dataset$$anonfun$repartition$2)|",
      "|java+method:///org/apache/spark/sql/Dataset/logicalPlan()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/UDFRegistration$$anonfun$register$35/apply(scala.collection.Seq)|",
    "called": "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/UDFRegistration$$anonfun$register$35$$anonfun$apply$11/UDFRegistration$$anonfun$register$35$$anonfun$apply$11(org.apache.spark.sql.UDFRegistration$$anonfun$register$35)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()|"
    ],
    "affectedLib": "org.apache.spark:spark-catalyst_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+constructor:///org/apache/spark/sql/execution/datasources/SharedInMemoryCache/SharedInMemoryCache(long)|",
    "called": "|java+method:///org/apache/spark/internal/Logging$class/$init$(org.apache.spark.internal.Logging)|",
    "v1Body": [
      "|java+constructor:///java/util/concurrent/atomic/AtomicBoolean/AtomicBoolean(boolean)|",
      "|java+method:///org/spark_project/guava/cache/CacheBuilder/build()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/SharedInMemoryCache$$anon$1/SharedInMemoryCache$$anon$1(org.apache.spark.sql.execution.datasources.SharedInMemoryCache)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/SharedInMemoryCache$$anon$2/SharedInMemoryCache$$anon$2(org.apache.spark.sql.execution.datasources.SharedInMemoryCache)|",
      "|java+method:///org/apache/spark/internal/Logging$class/$init$(org.apache.spark.internal.Logging)|",
      "|java+method:///org/spark_project/guava/cache/CacheBuilder/maximumWeight(long)|",
      "|java+method:///org/spark_project/guava/cache/CacheBuilder/newBuilder()|",
      "|java+method:///org/spark_project/guava/cache/CacheBuilder/weigher(org.spark_project.guava.cache.Weigher)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/spark_project/guava/cache/CacheBuilder/removalListener(org.spark_project.guava.cache.RemovalListener)|"
    ],
    "v2Body": [
      "|java+constructor:///java/util/concurrent/atomic/AtomicBoolean/AtomicBoolean(boolean)|",
      "|java+method:///org/spark_project/guava/cache/CacheBuilder/build()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/SharedInMemoryCache$$anon$1/SharedInMemoryCache$$anon$1(org.apache.spark.sql.execution.datasources.SharedInMemoryCache,int)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/SharedInMemoryCache$$anon$2/SharedInMemoryCache$$anon$2(org.apache.spark.sql.execution.datasources.SharedInMemoryCache)|",
      "|java+method:///org/apache/spark/internal/Logging$class/$init$(org.apache.spark.internal.Logging)|",
      "|java+method:///org/spark_project/guava/cache/CacheBuilder/maximumWeight(long)|",
      "|java+method:///org/spark_project/guava/cache/CacheBuilder/newBuilder()|",
      "|java+method:///org/spark_project/guava/cache/CacheBuilder/weigher(org.spark_project.guava.cache.Weigher)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/spark_project/guava/cache/CacheBuilder/removalListener(org.spark_project.guava.cache.RemovalListener)|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)|",
    "called": "|java+method:///org/apache/spark/SparkContext/defaultParallelism()|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions$default$2()|",
      "|java+method:///java/lang/Math/max(int,int)|",
      "|java+method:///org/apache/spark/SparkContext/defaultParallelism()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///scala/collection/mutable/ArrayOps/tail()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$mergeSchemasInParallel$1/ParquetFileFormat$$anonfun$mergeSchemasInParallel$1(scala.runtime.ObjectRef)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sparkContext()|",
      "|java+method:///scala/collection/mutable/ArrayOps/isEmpty()|",
      "|java+method:///java/lang/Math/min(int,int)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/head()|",
      "|java+method:///org/apache/spark/rdd/RDD/collect()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/writeLegacyParquetFormat()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/isParquetBinaryAsString()|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$18/ParquetFileFormat$$anonfun$18()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/isParquetINT96AsTimestamp()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$19/ParquetFileFormat$$anonfun$19(boolean,boolean,boolean,org.apache.spark.util.SerializableConfiguration)|",
      "|java+method:///scala/runtime/ObjectRef/create(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$11/ParquetFileFormat$$anonfun$11()|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions$default$2()|",
      "|java+method:///java/lang/Math/max(int,int)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///scala/collection/mutable/ArrayOps/tail()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$mergeSchemasInParallel$1/ParquetFileFormat$$anonfun$mergeSchemasInParallel$1(scala.runtime.ObjectRef)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sparkContext()|",
      "|java+method:///scala/collection/mutable/ArrayOps/isEmpty()|",
      "|java+method:///java/lang/Math/min(int,int)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/head()|",
      "|java+method:///org/apache/spark/rdd/RDD/collect()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/writeLegacyParquetFormat()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/isParquetBinaryAsString()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$12/ParquetFileFormat$$anonfun$12(boolean,boolean,boolean,org.apache.spark.util.SerializableConfiguration,boolean)|",
      "|java+method:///org/apache/spark/SparkContext/defaultParallelism()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/isParquetINT96AsTimestamp()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/ignoreCorruptFiles()|",
      "|java+method:///scala/runtime/ObjectRef/create(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/org$apache$spark$sql$execution$streaming$state$StateStore$$doMaintenance()|",
    "called": "|java+method:///org/apache/spark/SparkEnv$/get()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/stop()|",
      "|java+method:///scala/collection/mutable/HashMap/toSeq()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/loadedProviders()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$org$apache$spark$sql$execution$streaming$state$StateStore$$doMaintenance$2/StateStore$$anonfun$org$apache$spark$sql$execution$streaming$state$StateStore$$doMaintenance$2()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$org$apache$spark$sql$execution$streaming$state$StateStore$$doMaintenance$1/StateStore$$anonfun$org$apache$spark$sql$execution$streaming$state$StateStore$$doMaintenance$1()|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|"
    ],
    "v2Body": [
      "|java+constructor:///java/lang/IllegalStateException/IllegalStateException(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/toSeq()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/org$apache$spark$sql$execution$streaming$state$StateStore$$loadedProviders()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$org$apache$spark$sql$execution$streaming$state$StateStore$$doMaintenance$2/StateStore$$anonfun$org$apache$spark$sql$execution$streaming$state$StateStore$$doMaintenance$2()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$org$apache$spark$sql$execution$streaming$state$StateStore$$doMaintenance$1/StateStore$$anonfun$org$apache$spark$sql$execution$streaming$state$StateStore$$doMaintenance$1()|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/SharedInMemoryCache$$anon$1/weigh(scala.Tuple2,org.apache.hadoop.fs.FileStatus%5B%5D)|",
    "called": "|java+method:///org/apache/spark/util/SizeEstimator$/estimate(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/SizeEstimator$/estimate(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/execution/datasources/SharedInMemoryCache/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/estimate(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/SharedInMemoryCache$$anon$1$$anonfun$weigh$1/SharedInMemoryCache$$anon$1$$anonfun$weigh$1(org.apache.spark.sql.execution.datasources.SharedInMemoryCache$$anon$1)|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/FileScanRDD$$anon$1/hasNext()|",
    "called": "|java+constructor:///org/apache/spark/TaskKilledException/TaskKilledException()|",
    "v1Body": [
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/FileScanRDD$$anon$1/nextIterator()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/TaskKilledException/TaskKilledException()|",
      "|java+method:///org/apache/spark/TaskContext/isInterrupted()|",
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/FileScanRDD$$anon$1/nextIterator()|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()|",
    "called": "|java+method:///org/apache/spark/metrics/MetricsSystem/removeSource(org.apache.spark.metrics.source.Source)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/terminationLatch()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/triggerExecutor()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/TERMINATED()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/exception()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/ACTIVE()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/logicalPlan()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$2/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$2(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/OffsetSeq/toString()|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryStatus/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/id()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/updateStatusMessage(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/SparkSession/streams()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/startLatch()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/committedOffsets()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamProgress/toOffsetSeq(scala.collection.Seq,org.apache.spark.sql.execution.streaming.OffsetSeqMetadata)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/state()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/currentStatus_$eq(org.apache.spark.sql.streaming.StreamingQueryStatus)|",
      "|java+method:///java/util/concurrent/CountDownLatch/countDown()|",
      "|java+method:///scala/util/control/NonFatal$/apply(java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/name()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/sources()|",
      "|java+method:///java/lang/Throwable/getMessage()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$3/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$3(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryManager/notifyQueryTermination(org.apache.spark.sql.streaming.StreamingQuery)|",
      "|java+method:///org/apache/spark/sql/SparkSession$/setActiveSession(org.apache.spark.sql.SparkSession)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/registerSource(org.apache.spark.metrics.source.Source)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$4/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$4(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/streamDeathCause_$eq(org.apache.spark.sql.streaming.StreamingQueryException)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/removeSource(org.apache.spark.metrics.source.Source)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/offsetSeqMetadata()|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryListener$QueryTerminatedEvent/StreamingQueryListener$QueryTerminatedEvent(java.util.UUID,java.util.UUID,scala.Option)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/status()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/postEvent(org.apache.spark.sql.streaming.StreamingQueryListener$Event)|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryStatus/copy(java.lang.String,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/ProcessingTimeExecutor/execute(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryListener$QueryStartedEvent/StreamingQueryListener$QueryStartedEvent(java.util.UUID,java.util.UUID,java.lang.String)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryException/StreamingQueryException(org.apache.spark.sql.streaming.StreamingQuery,java.lang.String,java.lang.Throwable,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/streamingMetricsEnabled()|",
      "|java+method:///org/apache/spark/SparkEnv/metricsSystem()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/availableOffsets()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sparkContext()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$prettyIdString()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/sparkSession()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/streamMetrics()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/runId()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/state_$eq(org.apache.spark.sql.execution.streaming.StreamExecution$State)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/triggerExecutor()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/exception()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/ACTIVE()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/logicalPlan()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/INITIALIZING()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/id()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/terminationLatch()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/isInitialized()|",
      "|java+method:///java/util/concurrent/locks/Condition/signalAll()|",
      "|java+method:///org/apache/spark/sql/SparkSession/streams()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/startLatch()|",
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/awaitBatchLockCondition()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/TERMINATED()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/committedOffsets()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamProgress/toOffsetSeq(scala.collection.Seq,org.apache.spark.sql.execution.streaming.OffsetSeqMetadata)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/state()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/currentStatus_$eq(org.apache.spark.sql.streaming.StreamingQueryStatus)|",
      "|java+method:///java/util/concurrent/CountDownLatch/countDown()|",
      "|java+method:///java/util/concurrent/atomic/AtomicReference/compareAndSet(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/util/control/NonFatal$/apply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/checkpointRoot()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$5/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$5(org.apache.spark.sql.execution.streaming.StreamExecution,org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/name()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/stopSources()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$2/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$2(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/sources()|",
      "|java+method:///java/lang/Throwable/getMessage()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$3/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$3(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryManager/notifyQueryTermination(org.apache.spark.sql.streaming.StreamingQuery)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/awaitBatchLock()|",
      "|java+method:///org/apache/spark/sql/SparkSession$/setActiveSession(org.apache.spark.sql.SparkSession)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/registerSource(org.apache.spark.metrics.source.Source)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$4/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$4(org.apache.spark.sql.execution.streaming.StreamExecution)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/streamDeathCause_$eq(org.apache.spark.sql.streaming.StreamingQueryException)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///java/util/concurrent/atomic/AtomicReference/set(java.lang.Object)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/removeSource(org.apache.spark.metrics.source.Source)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/offsetSeqMetadata()|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryListener$QueryTerminatedEvent/StreamingQueryListener$QueryTerminatedEvent(java.util.UUID,java.util.UUID,scala.Option)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/status()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/postEvent(org.apache.spark.sql.streaming.StreamingQueryListener$Event)|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryStatus/copy(java.lang.String,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/ProcessingTimeExecutor/execute(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryListener$QueryStartedEvent/StreamingQueryListener$QueryStartedEvent(java.util.UUID,java.util.UUID,java.lang.String)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/streamingMetricsEnabled()|",
      "|java+method:///java/util/concurrent/locks/ReentrantLock/unlock()|",
      "|java+method:///org/apache/spark/SparkEnv/metricsSystem()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/availableOffsets()|",
      "|java+method:///java/util/concurrent/atomic/AtomicReference/get()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/OffsetSeq/toString()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sparkContext()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$prettyIdString()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/sparkSession()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/streamMetrics()|",
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryStatus/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/initializationLatch()|",
      "|java+constructor:///org/apache/spark/sql/streaming/StreamingQueryException/StreamingQueryException(java.lang.String,java.lang.String,java.lang.Throwable,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/runId()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/delete(org.apache.hadoop.fs.Path,boolean)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/updateStatusMessage(java.lang.String)|",
      "|java+method:///java/util/concurrent/locks/ReentrantLock/lock()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamExecution/toDebugString(boolean)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)|",
    "called": "|java+method:///org/apache/spark/broadcast/Broadcast/value()|",
    "v1Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///java/net/URI/URI(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/RecordReaderIterator/map(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initBatch(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.InternalRow)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/enableReturningBatches()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/parquet/hadoop/ParquetRecordReader/ParquetRecordReader(org.apache.parquet.hadoop.api.ReadSupport,org.apache.parquet.filter2.compat.FilterCompat$Filter)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/ParquetReadSupport()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/JoinedRow/JoinedRow()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getPath()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionedFile/partitionValues()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/VectorizedParquetRecordReader()|",
      "|java+constructor:///org/apache/hadoop/mapreduce/lib/input/FileSplit/FileSplit(org.apache.hadoop.fs.Path,long,long,java.lang.String%5B%5D)|",
      "|java+method:///scala/Some/x()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$apply$4/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$apply$4(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1,org.apache.spark.sql.execution.datasources.RecordReaderIterator)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)|",
      "|java+method:///org/apache/spark/sql/types/StructType/length()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$8/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$8(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1,org.apache.spark.sql.execution.datasources.PartitionedFile)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/parquet/filter2/compat/FilterCompat/get(org.apache.parquet.filter2.predicate.FilterPredicate,org.apache.parquet.filter.UnboundRecordFilter)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/JobID/JobID()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getLocations()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/generate(java.lang.Object,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/TaskContext$/get()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getStart()|",
      "|java+method:///org/apache/spark/broadcast/Broadcast/value()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionedFile/filePath()|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionedFile/length()|",
      "|java+method:///org/apache/spark/sql/types/StructType/toAttributes()|",
      "|java+method:///org/apache/spark/util/SerializableConfiguration/value()|",
      "|java+constructor:///org/apache/hadoop/mapreduce/task/TaskAttemptContextImpl/TaskAttemptContextImpl(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Array$/empty(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionedFile/start()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.net.URI)|",
      "|java+method:///org/apache/parquet/hadoop/ParquetInputFormat/setFilterPredicate(org.apache.hadoop.conf.Configuration,org.apache.parquet.filter2.predicate.FilterPredicate)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/TaskAttemptID/TaskAttemptID(org.apache.hadoop.mapreduce.TaskID,int)|",
      "|java+constructor:///org/apache/parquet/hadoop/ParquetInputSplit/ParquetInputSplit(org.apache.hadoop.fs.Path,long,long,long,java.lang.String%5B%5D,long%5B%5D)|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getLength()|",
      "|java+method:///org/apache/hadoop/mapreduce/task/TaskAttemptContextImpl/getConfiguration()|",
      "|java+method:///org/apache/parquet/hadoop/ParquetRecordReader/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$apply$6/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$apply$6(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1,org.apache.spark.sql.catalyst.expressions.JoinedRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,org.apache.spark.sql.execution.datasources.PartitionedFile)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/RecordReaderIterator/RecordReaderIterator(org.apache.hadoop.mapreduce.RecordReader)|",
      "|java+method:///org/apache/spark/sql/types/StructType/size()|",
      "|java+method:///org/apache/spark/sql/catalyst/InternalRow/numFields()|",
      "|java+constructor:///org/apache/hadoop/mapreduce/TaskID/TaskID(org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.mapreduce.TaskType,int)|",
      "|java+constructor:///org/apache/parquet/hadoop/ParquetRecordReader/ParquetRecordReader(org.apache.parquet.hadoop.api.ReadSupport)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$9/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$9(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///java/net/URI/URI(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/RecordReaderIterator/map(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initBatch(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.InternalRow)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/enableReturningBatches()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/parquet/hadoop/ParquetRecordReader/ParquetRecordReader(org.apache.parquet.hadoop.api.ReadSupport,org.apache.parquet.filter2.compat.FilterCompat$Filter)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/ParquetReadSupport()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/JoinedRow/JoinedRow()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getPath()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/VectorizedParquetRecordReader()|",
      "|java+method:///scala/Some/x()|",
      "|java+constructor:///org/apache/hadoop/mapreduce/lib/input/FileSplit/FileSplit(org.apache.hadoop.fs.Path,long,long,java.lang.String%5B%5D)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)|",
      "|java+method:///org/apache/spark/sql/types/StructType/length()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$8/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$8(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1,org.apache.spark.sql.execution.datasources.PartitionedFile)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$apply$3/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$apply$3(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1,org.apache.spark.sql.execution.datasources.RecordReaderIterator)|",
      "|java+method:///org/apache/parquet/filter2/compat/FilterCompat/get(org.apache.parquet.filter2.predicate.FilterPredicate,org.apache.parquet.filter.UnboundRecordFilter)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/JobID/JobID()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getLocations()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/generate(java.lang.Object,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/TaskContext$/get()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getStart()|",
      "|java+method:///org/apache/spark/broadcast/Broadcast/value()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionedFile/filePath()|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionedFile/length()|",
      "|java+method:///org/apache/spark/sql/types/StructType/toAttributes()|",
      "|java+method:///org/apache/spark/util/SerializableConfiguration/value()|",
      "|java+constructor:///org/apache/hadoop/mapreduce/task/TaskAttemptContextImpl/TaskAttemptContextImpl(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Array$/empty(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionedFile/start()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.net.URI)|",
      "|java+method:///org/apache/parquet/hadoop/ParquetInputFormat/setFilterPredicate(org.apache.hadoop.conf.Configuration,org.apache.parquet.filter2.predicate.FilterPredicate)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/TaskAttemptID/TaskAttemptID(org.apache.hadoop.mapreduce.TaskID,int)|",
      "|java+constructor:///org/apache/parquet/hadoop/ParquetInputSplit/ParquetInputSplit(org.apache.hadoop.fs.Path,long,long,long,java.lang.String%5B%5D,long%5B%5D)|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getLength()|",
      "|java+method:///org/apache/hadoop/mapreduce/task/TaskAttemptContextImpl/getConfiguration()|",
      "|java+method:///org/apache/parquet/hadoop/ParquetRecordReader/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$apply$5/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$apply$5(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1,org.apache.spark.sql.catalyst.expressions.JoinedRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,org.apache.spark.sql.execution.datasources.PartitionedFile)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitionedFile/partitionValues()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/RecordReaderIterator/RecordReaderIterator(org.apache.hadoop.mapreduce.RecordReader)|",
      "|java+method:///org/apache/spark/sql/types/StructType/size()|",
      "|java+method:///org/apache/spark/sql/catalyst/InternalRow/numFields()|",
      "|java+constructor:///org/apache/hadoop/mapreduce/TaskID/TaskID(org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.mapreduce.TaskType,int)|",
      "|java+constructor:///org/apache/parquet/hadoop/ParquetRecordReader/ParquetRecordReader(org.apache.parquet.hadoop.api.ReadSupport)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$9/ParquetFileFormat$$anonfun$buildReader$1$$anonfun$9(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReader$1)|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+constructor:///org/apache/spark/sql/execution/streaming/HDFSMetadataLog/HDFSMetadataLog(org.apache.spark.sql.SparkSession,java.lang.String,scala.reflect.ClassTag)|",
    "called": "|java+method:///org/apache/spark/internal/Logging$class/$init$(org.apache.spark.internal.Logging)|",
    "v1Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anon$1/HDFSMetadataLog$$anon$1(org.apache.spark.sql.execution.streaming.HDFSMetadataLog)|",
      "|java+method:///scala/reflect/ClassTag/runtimeClass()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/HDFSMetadataLog$FileManager/mkdirs(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/HDFSMetadataLog$FileManager/exists(org.apache.hadoop.fs.Path)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anonfun$2/HDFSMetadataLog$$anonfun$2(org.apache.spark.sql.execution.streaming.HDFSMetadataLog)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/HDFSMetadataLog/metadataPath()|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///org/apache/spark/internal/Logging$class/$init$(org.apache.spark.internal.Logging)|",
      "|java+method:///org/json4s/jackson/Serialization$/formats(org.json4s.TypeHints)|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/Predef$/implicitly(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/HDFSMetadataLog/fileManager()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/HDFSMetadataLog/manifest()|",
      "|java+method:///scala/Predef$/Manifest()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/HDFSMetadataLog/createFileManager()|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anon$1/HDFSMetadataLog$$anon$1(org.apache.spark.sql.execution.streaming.HDFSMetadataLog)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/HDFSMetadataLog/manifest()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anonfun$3/HDFSMetadataLog$$anonfun$3(org.apache.spark.sql.execution.streaming.HDFSMetadataLog)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anonfun$1/HDFSMetadataLog$$anonfun$1(org.apache.spark.sql.execution.streaming.HDFSMetadataLog)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///org/apache/spark/internal/Logging$class/$init$(org.apache.spark.internal.Logging)|",
      "|java+method:///org/json4s/jackson/Serialization$/formats(org.json4s.TypeHints)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/HDFSMetadataLog/org$apache$spark$sql$execution$streaming$HDFSMetadataLog$$runUninterruptiblyIfLocal(scala.Function0)|",
      "|java+method:///scala/reflect/ClassTag/runtimeClass()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/Predef$/implicitly(java.lang.Object)|",
      "|java+method:///scala/Predef$/Manifest()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/HDFSMetadataLog/createFileManager()|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+constructor:///org/apache/spark/sql/execution/CacheManager/CacheManager()|",
    "called": "|java+method:///org/apache/spark/internal/Logging$class/$init$(org.apache.spark.internal.Logging)|",
    "v1Body": [
      "|java+method:///org/apache/spark/internal/Logging$class/$init$(org.apache.spark.internal.Logging)|",
      "|java+constructor:///java/util/concurrent/locks/ReentrantReadWriteLock/ReentrantReadWriteLock()|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/internal/Logging$class/$init$(org.apache.spark.internal.Logging)|",
      "|java+constructor:///java/util/concurrent/locks/ReentrantReadWriteLock/ReentrantReadWriteLock()|",
      "|java+constructor:///java/util/LinkedList/LinkedList()|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/streaming/StreamingQueryListenerBus/post(org.apache.spark.sql.streaming.StreamingQueryListener$Event)|",
    "called": "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(org.apache.spark.scheduler.SparkListenerEvent)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryListener$QueryStartedEvent/runId()|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamingQueryListenerBus/activeQueryRunIds()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamingQueryListenerBus/postToAll(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(org.apache.spark.scheduler.SparkListenerEvent)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/streaming/StreamingQueryListener$QueryStartedEvent/runId()|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamingQueryListenerBus/activeQueryRunIds()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StreamingQueryListenerBus/postToAll(org.apache.spark.sql.streaming.StreamingQueryListener$Event)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(org.apache.spark.scheduler.SparkListenerEvent)|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)|",
    "called": "|java+method:///org/apache/spark/util/CompletionIterator$/apply(scala.collection.Iterator,scala.Function0)|",
    "v1Body": [
      "|java+method:///scala/runtime/BooleanRef/create(boolean)|",
      "|java+method:///scala/collection/JavaConverters$/propertiesAsScalaMapConverter(java.util.Properties)|",
      "|java+method:///scala/Function0/apply()|",
      "|java+method:///org/apache/spark/sql/jdbc/JdbcDialects$/get(java.lang.String)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/TaskContext/addTaskCompletionListener(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/columnList()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/getWhereClause(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartition)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/resultSetToSparkInternalRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType,org.apache.spark.executor.InputMetrics)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/TaskContext/taskMetrics()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/table()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$compute$2/JDBCRDD$$anonfun$compute$2(org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD,scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$compute$1/JDBCRDD$$anonfun$compute$1(org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD,scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/Predef$/$conforms()|",
      "|java+method:///java/sql/PreparedStatement/setFetchSize(int)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/fetchSize()|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/inputMetrics()|",
      "|java+method:///java/sql/Connection/prepareStatement(java.lang.String,int,int)|",
      "|java+method:///java/sql/PreparedStatement/executeQuery()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/asConnectionProperties()|",
      "|java+method:///org/apache/spark/util/CompletionIterator$/apply(scala.collection.Iterator,scala.Function0)|",
      "|java+method:///scala/runtime/ObjectRef/create(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/jdbc/JdbcDialect/beforeFetch(java.sql.Connection,scala.collection.immutable.Map)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BooleanRef/create(boolean)|",
      "|java+constructor:///org/apache/spark/InterruptibleIterator/InterruptibleIterator(org.apache.spark.TaskContext,scala.collection.Iterator)|",
      "|java+method:///scala/collection/JavaConverters$/propertiesAsScalaMapConverter(java.util.Properties)|",
      "|java+method:///scala/Function0/apply()|",
      "|java+method:///org/apache/spark/sql/jdbc/JdbcDialects$/get(java.lang.String)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/TaskContext/addTaskCompletionListener(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/columnList()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/getWhereClause(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartition)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/resultSetToSparkInternalRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType,org.apache.spark.executor.InputMetrics)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/TaskContext/taskMetrics()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/table()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$compute$2/JDBCRDD$$anonfun$compute$2(org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD,scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$compute$1/JDBCRDD$$anonfun$compute$1(org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD,scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/Predef$/$conforms()|",
      "|java+method:///java/sql/PreparedStatement/setFetchSize(int)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/fetchSize()|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/inputMetrics()|",
      "|java+method:///java/sql/Connection/prepareStatement(java.lang.String,int,int)|",
      "|java+method:///java/sql/PreparedStatement/executeQuery()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/asConnectionProperties()|",
      "|java+method:///org/apache/spark/util/CompletionIterator$/apply(scala.collection.Iterator,scala.Function0)|",
      "|java+method:///scala/runtime/ObjectRef/create(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/jdbc/JdbcDialect/beforeFetch(java.sql.Connection,scala.collection.immutable.Map)|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSource/FileStreamSource(org.apache.spark.sql.SparkSession,java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.Seq,java.lang.String,scala.collection.immutable.Map)|",
    "called": "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSource$SeenFilesMap/FileStreamSource$SeenFilesMap(long)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamOptions/maxFilesPerTrigger()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/Source$class/$init$(org.apache.spark.sql.execution.streaming.Source)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSource/metadataLog()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/isGlobPath(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSourceLog/getLatest()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSource/seenFiles()|",
      "|java+method:///scala/Predef$/implicitly(java.lang.Object)|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$5/FileStreamSource$$anonfun$5(org.apache.spark.sql.execution.streaming.FileStreamSource)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSource$SeenFilesMap/purge()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$4/FileStreamSource$$anonfun$4(org.apache.spark.sql.execution.streaming.FileStreamSource)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/makeQualified(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSourceLog/allFiles()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamOptions/FileStreamOptions(scala.collection.immutable.Map)|",
      "|java+method:///scala/math/Ordering/reverse()|",
      "|java+method:///org/apache/spark/internal/Logging$class/$init$(org.apache.spark.internal.Logging)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///scala/Predef$/ArrowAssoc(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamOptions/latestFirst()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSourceLog$/VERSION()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSource/org$apache$spark$sql$execution$streaming$FileStreamSource$$sourceOptions()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/collection/immutable/Map/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSource/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$6/FileStreamSource$$anonfun$6(org.apache.spark.sql.execution.streaming.FileStreamSource)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSource/logWarning(scala.Function0)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$3/FileStreamSource$$anonfun$3(org.apache.spark.sql.execution.streaming.FileStreamSource)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamOptions/maxFileAgeMs()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamOptions/optionMapWithoutPath()|",
      "|java+method:///scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$1/FileStreamSource$$anonfun$1(org.apache.spark.sql.execution.streaming.FileStreamSource)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSourceLog/FileStreamSourceLog(java.lang.String,org.apache.spark.sql.SparkSession,java.lang.String)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSource$SeenFilesMap/FileStreamSource$SeenFilesMap(long)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamOptions/maxFilesPerTrigger()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/Source$class/$init$(org.apache.spark.sql.execution.streaming.Source)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSource/metadataLog()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/isGlobPath(org.apache.hadoop.fs.Path)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSource/seenFiles()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSource/org$apache$spark$sql$execution$streaming$FileStreamSource$$maxFileAgeMs()|",
      "|java+method:///scala/Predef$/implicitly(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$5/FileStreamSource$$anonfun$5(org.apache.spark.sql.execution.streaming.FileStreamSource)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSource$SeenFilesMap/purge()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$4/FileStreamSource$$anonfun$4(org.apache.spark.sql.execution.streaming.FileStreamSource)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/makeQualified(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSourceLog/allFiles()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamOptions/FileStreamOptions(scala.collection.immutable.Map)|",
      "|java+method:///scala/math/Ordering/reverse()|",
      "|java+method:///org/apache/spark/internal/Logging$class/$init$(org.apache.spark.internal.Logging)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSourceLog/getLatest()|",
      "|java+method:///scala/Predef$/ArrowAssoc(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamOptions/latestFirst()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSourceLog$/VERSION()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/collection/immutable/Map/contains(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSourceLog/FileStreamSourceLog(int,org.apache.spark.sql.SparkSession,java.lang.String)|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSource/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$6/FileStreamSource$$anonfun$6(org.apache.spark.sql.execution.streaming.FileStreamSource)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSource/sourceOptions()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSource/logWarning(scala.Function0)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$3/FileStreamSource$$anonfun$3(org.apache.spark.sql.execution.streaming.FileStreamSource)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSource/org$apache$spark$sql$execution$streaming$FileStreamSource$$maxFilesPerBatch()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamOptions/maxFileAgeMs()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamOptions/optionMapWithoutPath()|",
      "|java+method:///scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$1/FileStreamSource$$anonfun$1(org.apache.spark.sql.execution.streaming.FileStreamSource)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+constructor:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/EventTimeWatermarkExec(org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.unsafe.types.CalendarInterval,org.apache.spark.sql.execution.SparkPlan)|",
    "called": "|java+method:///org/apache/spark/SparkContext/register(org.apache.spark.util.AccumulatorV2)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/sparkContext()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/EventTimeStatsAccum$/$lessinit$greater$default$1()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/EventTimeStatsAccum/EventTimeStatsAccum(org.apache.spark.sql.execution.streaming.EventTimeStats)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec$$anonfun$2/EventTimeWatermarkExec$$anonfun$2(org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec)|",
      "|java+method:///org/apache/spark/SparkContext/register(org.apache.spark.util.AccumulatorV2)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkPlan/SparkPlan()|",
      "|java+method:///org/apache/spark/sql/execution/SparkPlan/output()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/eventTimeStats()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/sparkContext()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/EventTimeStatsAccum$/$lessinit$greater$default$1()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/EventTimeStatsAccum/EventTimeStatsAccum(org.apache.spark.sql.execution.streaming.EventTimeStats)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec$$anonfun$2/EventTimeWatermarkExec$$anonfun$2(org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec)|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/logical/EventTimeWatermark$/getDelayMs(org.apache.spark.unsafe.types.CalendarInterval)|",
      "|java+method:///org/apache/spark/SparkContext/register(org.apache.spark.util.AccumulatorV2)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/sql/execution/SparkPlan/SparkPlan()|",
      "|java+method:///org/apache/spark/sql/execution/SparkPlan/output()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/eventTimeStats()|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$/org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)|",
    "called": "|java+method:///org/apache/spark/metrics/source/HiveCatalogMetrics$/incrementParallelListingJobCount(int)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions$default$2()|",
      "|java+method:///scala/Predef$DummyImplicit$/dummyImplicit()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sparkContext()|",
      "|java+method:///java/lang/Math/min(int,int)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/rdd/RDD/collect()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$13/PartitioningAwareFileIndex$$anonfun$13()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$3/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$3()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$11/PartitioningAwareFileIndex$$anonfun$11()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$12/PartitioningAwareFileIndex$$anonfun$12(org.apache.hadoop.fs.PathFilter,org.apache.spark.util.SerializableConfiguration)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/parallelPartitionDiscoveryThreshold()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$2/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$2(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/metrics/source/HiveCatalogMetrics$/incrementParallelListingJobCount(int)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$1/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$1(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions$default$2()|",
      "|java+method:///scala/Predef$DummyImplicit$/dummyImplicit()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sparkContext()|",
      "|java+method:///java/lang/Math/min(int,int)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/rdd/RDD/collect()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/parallelPartitionDiscoveryParallelism()|",
      "|java+method:///org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$13/PartitioningAwareFileIndex$$anonfun$13()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$3/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$3()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$11/PartitioningAwareFileIndex$$anonfun$11()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$12/PartitioningAwareFileIndex$$anonfun$12(org.apache.hadoop.fs.PathFilter,org.apache.spark.util.SerializableConfiguration)|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/parallelPartitionDiscoveryThreshold()|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$2/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$2(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/metrics/source/HiveCatalogMetrics$/incrementParallelListingJobCount(int)|",
      "|java+constructor:///org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$1/PartitioningAwareFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles$1(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/coordinatorRef()|",
    "called": "|java+method:///org/apache/spark/SparkEnv$/get()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/org$apache$spark$sql$execution$streaming$state$StateStore$$_coordRef_$eq(org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef$/forExecutor(org.apache.spark.SparkEnv)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/org$apache$spark$sql$execution$streaming$state$StateStore$$_coordRef()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$coordinatorRef$1/StateStore$$anonfun$coordinatorRef$1()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/org$apache$spark$sql$execution$streaming$state$StateStore$$_coordRef_$eq(org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef$/forExecutor(org.apache.spark.SparkEnv)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$coordinatorRef$1/StateStore$$anonfun$coordinatorRef$1()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/org$apache$spark$sql$execution$streaming$state$StateStore$$loadedProviders()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/org$apache$spark$sql$execution$streaming$state$StateStore$$_coordRef()|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)|",
    "called": "|java+method:///org/apache/spark/SparkContext/hadoopConfiguration()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/internal/SQLConf/resolver()|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions(org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/tableName()|",
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/cmd()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$4/AlterTableRecoverPartitionsCommand$$anonfun$run$4(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,org.apache.hadoop.fs.Path)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$6/AlterTableRecoverPartitionsCommand$$anonfun$run$6(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,int)|",
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///org/apache/spark/sql/SQLContext/conf()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/Seq/isEmpty()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/RuntimeConfig/get(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/location()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/collection/GenMap$/empty()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sqlContext()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/SparkSession/conf()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$getPathFilter(org.apache.hadoop.conf.Configuration)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/quotedString()|",
      "|java+method:///org/apache/spark/SparkContext/hadoopConfiguration()|",
      "|java+method:///scala/collection/GenSeq/length()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sparkContext()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/addPartitions(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.GenSeq,scala.collection.GenMap)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$5/AlterTableRecoverPartitionsCommand$$anonfun$run$5(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,org.apache.hadoop.fs.Path,int)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/gatherFastStats()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$7/AlterTableRecoverPartitionsCommand$$anonfun$run$7(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,int)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/internal/SQLConf/resolver()|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$7/AlterTableRecoverPartitionsCommand$$anonfun$run$7(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,int)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions(org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/tableName()|",
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/cmd()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$4/AlterTableRecoverPartitionsCommand$$anonfun$run$4(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,org.apache.hadoop.fs.Path)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$6/AlterTableRecoverPartitionsCommand$$anonfun$run$6(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,int)|",
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///org/apache/spark/sql/SQLContext/conf()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/Seq/isEmpty()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/RuntimeConfig/get(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/location()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///scala/collection/GenMap$/empty()|",
      "|java+method:///org/apache/spark/sql/SparkSession/sqlContext()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/SparkSession/conf()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/conf()|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$getPathFilter(org.apache.hadoop.conf.Configuration)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/quotedString()|",
      "|java+method:///org/apache/spark/SparkContext/hadoopConfiguration()|",
      "|java+method:///scala/collection/GenSeq/length()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/SparkSession/sparkContext()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/addPartitions(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.GenSeq,scala.collection.GenMap)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$18()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$5/AlterTableRecoverPartitionsCommand$$anonfun$run$5(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,org.apache.hadoop.fs.Path,int)|",
      "|java+method:///org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf/gatherFastStats()|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)|",
    "called": "|java+method:///org/apache/spark/util/Utils$/resolveURI(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///org/apache/spark/util/Utils$/resolveURI(java.lang.String)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///java/nio/file/Path/toAbsolutePath()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/isLocal()|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/isOverwrite()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/LoadDataCommand$$anonfun$run$1/LoadDataCommand$$anonfun$run$1(org.apache.spark.sql.execution.command.LoadDataCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)|",
      "|java+constructor:///java/net/URI/URI(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/TraversableOnce/size()|",
      "|java+constructor:///java/net/URI/URI(java.lang.String)|",
      "|java+method:///java/nio/file/FileSystem/getPathMatcher(java.lang.String)|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///java/net/URI/toString()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///java/net/URI/getAuthority()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/LoadDataCommand$$anonfun$3/LoadDataCommand$$anonfun$3(org.apache.spark.sql.execution.command.LoadDataCommand,java.nio.file.FileSystem,java.nio.file.PathMatcher)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/partition()|",
      "|java+method:///scala/Option/nonEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/isDatasourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/table()|",
      "|java+method:///java/net/URI/getQuery()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/loadPartition(org.apache.spark.sql.catalyst.TableIdentifier,java.lang.String,scala.collection.immutable.Map,boolean,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///java/net/URI/getPath()|",
      "|java+method:///java/net/URI/getScheme()|",
      "|java+method:///java/lang/System/getProperty(java.lang.String)|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///scala/collection/MapLike/keys()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///java/nio/file/FileSystems/getDefault()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/quotedString()|",
      "|java+method:///java/lang/String/startsWith(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/path()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/loadTable(org.apache.spark.sql.catalyst.TableIdentifier,java.lang.String,boolean,boolean)|",
      "|java+method:///scala/collection/mutable/ArrayOps/exists(scala.Function1)|",
      "|java+method:///java/lang/String/contains(java.lang.CharSequence)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/hadoop/conf/Configuration/get(java.lang.String)|",
      "|java+method:///java/nio/file/Path/getParent()|",
      "|java+method:///java/io/File/listFiles()|",
      "|java+method:///java/nio/file/FileSystem/getPath(java.lang.String,java.lang.String%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///java/net/URI/getFragment()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/catalog()|",
      "|java+method:///org/apache/spark/util/Utils$/resolveURI(java.lang.String)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///java/nio/file/Path/toAbsolutePath()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/isLocal()|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/isOverwrite()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/command/LoadDataCommand$$anonfun$run$1/LoadDataCommand$$anonfun$run$1(org.apache.spark.sql.execution.command.LoadDataCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)|",
      "|java+constructor:///java/net/URI/URI(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/TraversableOnce/size()|",
      "|java+constructor:///java/net/URI/URI(java.lang.String)|",
      "|java+method:///java/nio/file/FileSystem/getPathMatcher(java.lang.String)|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///java/net/URI/toString()|",
      "|java+constructor:///org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///java/net/URI/getAuthority()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()|",
      "|java+constructor:///org/apache/spark/sql/execution/command/LoadDataCommand$$anonfun$3/LoadDataCommand$$anonfun$3(org.apache.spark.sql.execution.command.LoadDataCommand,java.nio.file.FileSystem,java.nio.file.PathMatcher)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/partition()|",
      "|java+method:///scala/Option/nonEmpty()|",
      "|java+method:///org/apache/spark/sql/execution/command/DDLUtils$/isDatasourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/table()|",
      "|java+method:///java/net/URI/getQuery()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/loadPartition(org.apache.spark.sql.catalyst.TableIdentifier,java.lang.String,scala.collection.immutable.Map,boolean,boolean,boolean)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///java/net/URI/getPath()|",
      "|java+method:///java/net/URI/getScheme()|",
      "|java+method:///java/lang/System/getProperty(java.lang.String)|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///scala/collection/MapLike/keys()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()|",
      "|java+method:///java/nio/file/FileSystems/getDefault()|",
      "|java+method:///org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+method:///org/apache/spark/sql/catalyst/TableIdentifier/quotedString()|",
      "|java+method:///java/lang/String/startsWith(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/command/LoadDataCommand/path()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/SessionCatalog/loadTable(org.apache.spark.sql.catalyst.TableIdentifier,java.lang.String,boolean,boolean)|",
      "|java+method:///scala/collection/mutable/ArrayOps/exists(scala.Function1)|",
      "|java+method:///java/lang/String/contains(java.lang.CharSequence)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()|",
      "|java+method:///org/apache/hadoop/conf/Configuration/get(java.lang.String)|",
      "|java+method:///java/nio/file/Path/getParent()|",
      "|java+method:///java/io/File/listFiles()|",
      "|java+method:///java/nio/file/FileSystem/getPath(java.lang.String,java.lang.String%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()|",
      "|java+method:///org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///java/net/URI/getFragment()|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/api/r/SQLUtils$/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)|",
    "called": "|java+method:///org/apache/spark/SparkContext/conf()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/api/r/SQLUtils$/withHiveExternalCatalog(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/sql/SparkSession$/builder()|",
      "|java+method:///org/apache/spark/sql/SparkSession$/hiveClassesArePresent()|",
      "|java+constructor:///org/apache/spark/sql/api/r/SQLUtils$$anonfun$3/SQLUtils$$anonfun$3()|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext/sc()|",
      "|java+method:///org/apache/spark/sql/api/r/SQLUtils$/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/SparkSession$Builder/getOrCreate()|",
      "|java+method:///org/apache/spark/sql/api/r/SQLUtils$/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)|",
      "|java+method:///org/apache/spark/sql/SparkSession$Builder/sparkContext(org.apache.spark.SparkContext)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///java/lang/String/toLowerCase()|",
      "|java+method:///org/apache/spark/sql/SparkSession$/builder()|",
      "|java+method:///org/apache/spark/sql/SparkSession$/hiveClassesArePresent()|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+constructor:///org/apache/spark/sql/api/r/SQLUtils$$anonfun$3/SQLUtils$$anonfun$3()|",
      "|java+method:///org/apache/spark/sql/api/r/SQLUtils$/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/internal/StaticSQLConf$/CATALOG_IMPLEMENTATION()|",
      "|java+method:///org/apache/spark/sql/SparkSession$Builder/getOrCreate()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/api/r/SQLUtils$/withHiveExternalCatalog(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/sql/api/r/SQLUtils$/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)|",
      "|java+method:///org/apache/spark/sql/SparkSession$Builder/sparkContext(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext/sc()|",
      "|java+method:///org/apache/spark/internal/config/ConfigEntry/key()|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)|",
    "called": "|java+method:///org/apache/spark/TaskContext/addTaskCompletionListener(scala.Function1)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anon$1/StateStoreSaveExec$$anonfun$doExecute$3$$anon$1(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Iterator/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$4/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$4(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.catalyst.expressions.LessThanOrEqual)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/keyExpressions()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/metric/SQLMetric/$plus$eq(long)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/outputMode()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/numKeys()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$1/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$1(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$6/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$6(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/remove(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$5/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$5(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.catalyst.expressions.codegen.Predicate)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/generate(java.lang.Object,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/commit()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/longMetric(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/SparkPlan/output()|",
      "|java+method:///scala/collection/Iterator/next()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/newPredicate(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/LessThanOrEqual/LessThanOrEqual(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression)|",
      "|java+method:///scala/collection/Seq/find(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/UnsafeRow/copy()|",
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$3/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$3(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.execution.metric.SQLMetric)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$7/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$7(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.execution.metric.SQLMetric)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Attribute/dataType()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/updates()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/put(org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.sql.catalyst.expressions.UnsafeRow)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/catalyst/expressions/GetStructField/GetStructField(org.apache.spark.sql.catalyst.expressions.Expression,int,scala.Option)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/eventTimeWatermark()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/GetStructField$/apply$default$3()|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/Literal$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/iterator()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anon$1/StateStoreSaveExec$$anonfun$doExecute$3$$anon$1(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/child()|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$4/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$4(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.execution.streaming.state.StateStore)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/keyExpressions()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/iterator()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/sql/execution/metric/SQLMetric/$plus$eq(long)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/outputMode()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/numKeys()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$7/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$7(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$6/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$6(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.catalyst.expressions.codegen.Predicate)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/remove(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/generate(java.lang.Object,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/commit()|",
      "|java+method:///org/apache/spark/TaskContext$/get()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/longMetric(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/SparkPlan/output()|",
      "|java+method:///scala/collection/Iterator/next()|",
      "|java+method:///scala/collection/Iterator/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec/org$apache$spark$sql$execution$streaming$StateStoreSaveExec$$watermarkPredicate()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/catalyst/expressions/UnsafeRow/copy()|",
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$8/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$8(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.execution.metric.SQLMetric)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$5/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$5(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.execution.metric.SQLMetric)|",
      "|java+method:///org/apache/spark/TaskContext/addTaskCompletionListener(scala.Function1)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/updates()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore/put(org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.sql.catalyst.expressions.UnsafeRow)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+constructor:///org/apache/spark/sql/execution/datasources/FileScanRDD$$anon$1$$anon$2/FileScanRDD$$anon$1$$anon$2(org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1)|",
    "called": "|java+constructor:///org/apache/spark/util/NextIterator/NextIterator()|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/execution/datasources/FileScanRDD$$anon$1/org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$$outer()|",
      "|java+constructor:///org/apache/spark/util/NextIterator/NextIterator()|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/execution/datasources/FileScanRDD$$anon$1$$anon$2/liftedTree1$1()|",
      "|java+constructor:///org/apache/spark/util/NextIterator/NextIterator()|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/startMaintenanceIfNeeded()|",
    "called": "|java+method:///org/apache/spark/SparkConf/getTimeAsMs(java.lang.String,java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/MAINTENANCE_INTERVAL_DEFAULT_SECS()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/maintenanceTask()|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/loadedProviders()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///java/util/concurrent/ScheduledExecutorService/scheduleAtFixedRate(java.lang.Runnable,long,long,java.util.concurrent.TimeUnit)|",
      "|java+method:///org/apache/spark/SparkConf/getTimeAsMs(java.lang.String,java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/maintenanceTaskExecutor()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/MAINTENANCE_INTERVAL_CONFIG()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/maintenanceTask_$eq(java.util.concurrent.ScheduledFuture)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/state/StateStore$$anon$1/StateStore$$anon$1()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$startMaintenanceIfNeeded$1/StateStore$$anonfun$startMaintenanceIfNeeded$1()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/logInfo(scala.Function0)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkEnv/conf()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/sql/execution/streaming/state/StateStore$MaintenanceTask/StateStore$MaintenanceTask(long,scala.Function0,scala.Function0)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/MAINTENANCE_INTERVAL_DEFAULT_SECS()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/MAINTENANCE_INTERVAL_CONFIG()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/isMaintenanceRunning()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/maintenanceTask_$eq(org.apache.spark.sql.execution.streaming.state.StateStore$MaintenanceTask)|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$startMaintenanceIfNeeded$2/StateStore$$anonfun$startMaintenanceIfNeeded$2()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$startMaintenanceIfNeeded$3/StateStore$$anonfun$startMaintenanceIfNeeded$3()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$startMaintenanceIfNeeded$1/StateStore$$anonfun$startMaintenanceIfNeeded$1()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/logInfo(scala.Function0)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkEnv/conf()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/state/StateStore$/org$apache$spark$sql$execution$streaming$state$StateStore$$loadedProviders()|",
      "|java+method:///org/apache/spark/SparkConf/getTimeAsMs(java.lang.String,java.lang.String)|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+method:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1/apply()|",
    "called": "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/immutable/Map/values()|",
      "|java+method:///org/apache/spark/SparkContext/broadcast(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/immutable/StringOps/toLong()|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(org.apache.spark.scheduler.SparkListenerEvent)|",
      "|java+constructor:///org/apache/spark/sql/execution/ui/SparkListenerDriverAccumUpdates/SparkListenerDriverAccumUpdates(long,scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ClassTag$/Any()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/physical/BroadcastMode/transform(org.apache.spark.sql.catalyst.InternalRow%5B%5D)|",
      "|java+method:///org/apache/spark/sql/execution/metric/SQLMetric/$plus$eq(long)|",
      "|java+method:///java/lang/OutOfMemoryError/initCause(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/mode()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/longMetric(java.lang.String)|",
      "|java+method:///scala/Predef$/longArrayOps(long%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1$$anonfun$apply$2/BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1$$anonfun$apply$2(org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1)|",
      "|java+method:///java/lang/OutOfMemoryError/getCause()|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///java/lang/System/nanoTime()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1$$anonfun$1/BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1$$anonfun$1(org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1)|",
      "|java+method:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/sparkContext()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/mutable/ArrayOps/sum(scala.math.Numeric)|",
      "|java+method:///org/apache/spark/SparkContext/listenerBus()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/TraversableOnce/toSeq()|",
      "|java+method:///scala/collection/Iterable/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/internal/config/ConfigEntry/key()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf$/AUTO_BROADCASTJOIN_THRESHOLD()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/OutOfMemoryError/OutOfMemoryError(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/metrics()|",
      "|java+method:///org/apache/spark/sql/execution/SparkPlan/executeCollect()|",
      "|java+method:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec$$anonfun$relationFuture$1/org$apache$spark$sql$execution$exchange$BroadcastExchangeExec$$anonfun$$$outer()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1/apply()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/ClassTag$/Long()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/immutable/Map/values()|",
      "|java+method:///java/lang/OutOfMemoryError/getCause()|",
      "|java+method:///java/lang/System/nanoTime()|",
      "|java+method:///org/apache/spark/SparkContext/broadcast(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/sql/execution/metric/SQLMetrics$/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ClassTag$/Any()|",
      "|java+method:///org/apache/spark/sql/catalyst/plans/physical/BroadcastMode/transform(org.apache.spark.sql.catalyst.InternalRow%5B%5D)|",
      "|java+method:///org/apache/spark/sql/execution/metric/SQLMetric/$plus$eq(long)|",
      "|java+method:///java/lang/OutOfMemoryError/initCause(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/mode()|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/longMetric(java.lang.String)|",
      "|java+method:///scala/Predef$/longArrayOps(long%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/Iterable/toSeq()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/child()|",
      "|java+constructor:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1$$anonfun$1/BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1$$anonfun$1(org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1)|",
      "|java+method:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/sparkContext()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/mutable/ArrayOps/sum(scala.math.Numeric)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/internal/config/ConfigEntry/key()|",
      "|java+method:///org/apache/spark/sql/internal/SQLConf$/AUTO_BROADCASTJOIN_THRESHOLD()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/OutOfMemoryError/OutOfMemoryError(java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/metrics()|",
      "|java+method:///org/apache/spark/sql/execution/SparkPlan/executeCollect()|",
      "|java+method:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec$$anonfun$relationFuture$1/org$apache$spark$sql$execution$exchange$BroadcastExchangeExec$$anonfun$$$outer()|",
      "|java+method:///org/apache/spark/sql/execution/exchange/BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1/apply()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/ClassTag$/Long()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+constructor:///org/apache/spark/sql/execution/streaming/state/StateStore$/StateStore$()|",
    "called": "|java+method:///org/apache/spark/internal/Logging$class/$init$(org.apache.spark.internal.Logging)|",
    "v1Body": [
      "|java+method:///org/apache/spark/internal/Logging$class/$init$(org.apache.spark.internal.Logging)|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonSingleThreadScheduledExecutor(java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/internal/Logging$class/$init$(org.apache.spark.internal.Logging)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-sql_2.11:2.1.0",
    "coordinatesV2": "org.apache.spark:spark-sql_2.11:2.1.1",
    "caller": "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSink/FileStreamSink(org.apache.spark.sql.SparkSession,java.lang.String,org.apache.spark.sql.execution.datasources.FileFormat,scala.collection.Seq,scala.collection.immutable.Map)|",
    "called": "|java+method:///org/apache/spark/internal/Logging$class/$init$(org.apache.spark.internal.Logging)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSink/basePath()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(org.apache.hadoop.fs.Path,java.lang.String)|",
      "|java+method:///java/net/URI/toString()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSinkLog/FileStreamSinkLog(java.lang.String,org.apache.spark.sql.SparkSession,java.lang.String)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSink/logPath()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/apache/spark/internal/Logging$class/$init$(org.apache.spark.internal.Logging)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSinkLog$/VERSION()|",
      "|java+method:///org/apache/hadoop/fs/Path/toUri()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSink$/metadataDir()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/SparkSession/sessionState()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSink/basePath()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(org.apache.hadoop.fs.Path,java.lang.String)|",
      "|java+method:///java/net/URI/toString()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSink/logPath()|",
      "|java+method:///org/apache/spark/sql/internal/SessionState/newHadoopConf()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+constructor:///org/apache/spark/sql/execution/streaming/FileStreamSinkLog/FileStreamSinkLog(int,org.apache.spark.sql.SparkSession,java.lang.String)|",
      "|java+method:///org/apache/spark/internal/Logging$class/$init$(org.apache.spark.internal.Logging)|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSinkLog$/VERSION()|",
      "|java+method:///org/apache/hadoop/fs/Path/toUri()|",
      "|java+method:///org/apache/spark/sql/execution/streaming/FileStreamSink$/metadataDir()|"
    ],
    "affectedLib": "org.apache.spark:spark-core_2.11:2.1.1",
    "change": "UPDATED"
  }
]