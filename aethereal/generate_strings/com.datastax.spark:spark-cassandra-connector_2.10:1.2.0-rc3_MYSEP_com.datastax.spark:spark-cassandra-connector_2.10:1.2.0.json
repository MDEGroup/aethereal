[
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/metrics/OutputMetricsUpdater$TaskMetricsSupport$class/$init$(com.datastax.spark.connector.metrics.OutputMetricsUpdater$TaskMetricsSupport)|",
    "called": "|java+constructor:///com/twitter/jsr166e/LongAdder/LongAdder()|",
    "v1Body": [
      "|java+constructor:///com/google/common/cache/LongAdderBuilder$LongAdderWrapper/LongAdderBuilder$LongAdderWrapper()|",
      "|java+method:///com/datastax/spark/connector/metrics/OutputMetricsUpdater$TaskMetricsSupport/com$datastax$spark$connector$metrics$OutputMetricsUpdater$TaskMetricsSupport$_setter_$atomicCounter_$eq(com.google.common.cache.LongAdderBuilder$LongAdderWrapper)|",
      "|java+method:///com/google/common/cache/LongAdderBuilder$LongAdderWrapper/add(long)|",
      "|java+method:///org/apache/spark/executor/OutputMetrics/bytesWritten()|",
      "|java+method:///com/datastax/spark/connector/metrics/OutputMetricsUpdater$TaskMetricsSupport/atomicCounter()|",
      "|java+method:///com/datastax/spark/connector/metrics/OutputMetricsUpdater$TaskMetricsSupport/outputMetrics()|"
    ],
    "v2Body": [
      "|java+method:///com/datastax/spark/connector/metrics/OutputMetricsUpdater$TaskMetricsSupport/com$datastax$spark$connector$metrics$OutputMetricsUpdater$TaskMetricsSupport$_setter_$atomicCounter_$eq(com.twitter.jsr166e.LongAdder)|",
      "|java+method:///org/apache/spark/executor/OutputMetrics/bytesWritten()|",
      "|java+method:///com/twitter/jsr166e/LongAdder/add(long)|",
      "|java+method:///com/datastax/spark/connector/metrics/OutputMetricsUpdater$TaskMetricsSupport/atomicCounter()|",
      "|java+constructor:///com/twitter/jsr166e/LongAdder/LongAdder()|",
      "|java+method:///com/datastax/spark/connector/metrics/OutputMetricsUpdater$TaskMetricsSupport/outputMetrics()|"
    ],
    "affectedLib": "com.twitter:jsr166e:1.1.0",
    "change": "ADDED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/metrics/OutputMetricsUpdater$TaskMetricsSupport$class/updateTaskMetrics(com.datastax.spark.connector.metrics.OutputMetricsUpdater$TaskMetricsSupport,boolean,int)|",
    "called": "|java+method:///com/twitter/jsr166e/LongAdder/longValue()|",
    "v1Body": [
      "|java+method:///com/google/common/cache/LongAdderBuilder$LongAdderWrapper/add(long)|",
      "|java+method:///com/datastax/spark/connector/metrics/OutputMetricsUpdater$TaskMetricsSupport/atomicCounter()|",
      "|java+method:///com/google/common/cache/LongAdderBuilder$LongAdderWrapper/longValue()|",
      "|java+method:///com/datastax/spark/connector/metrics/OutputMetricsUpdater$TaskMetricsSupport/outputMetrics()|",
      "|java+method:///org/apache/spark/executor/OutputMetrics/bytesWritten_$eq(long)|"
    ],
    "v2Body": [
      "|java+method:///com/twitter/jsr166e/LongAdder/add(long)|",
      "|java+method:///com/datastax/spark/connector/metrics/OutputMetricsUpdater$TaskMetricsSupport/atomicCounter()|",
      "|java+method:///com/twitter/jsr166e/LongAdder/longValue()|",
      "|java+method:///com/datastax/spark/connector/metrics/OutputMetricsUpdater$TaskMetricsSupport/outputMetrics()|",
      "|java+method:///org/apache/spark/executor/OutputMetrics/bytesWritten_$eq(long)|"
    ],
    "affectedLib": "com.twitter:jsr166e:1.1.0",
    "change": "ADDED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/rdd/partitioner/CassandraPartitionedRDD/getPreferredLocations(org.apache.spark.Partition)|",
    "called": "|java+method:///scala/collection/immutable/Map$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///scala/collection/SetLike/toSeq()|",
      "|java+method:///com/datastax/spark/connector/rdd/partitioner/ReplicaPartition/endpoints()|",
      "|java+method:///scala/collection/immutable/Set/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/collection/immutable/Set$/canBuildFrom()|",
      "|java+constructor:///com/datastax/spark/connector/rdd/partitioner/CassandraPartitionedRDD$$anonfun$getPreferredLocations$1/CassandraPartitionedRDD$$anonfun$getPreferredLocations$1(com.datastax.spark.connector.rdd.partitioner.CassandraPartitionedRDD)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/SetLike/toSeq()|",
      "|java+method:///com/datastax/spark/connector/rdd/partitioner/ReplicaPartition/endpoints()|",
      "|java+method:///scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq/distinct()|",
      "|java+method:///scala/collection/immutable/Map$/canBuildFrom()|",
      "|java+method:///com/datastax/spark/connector/rdd/partitioner/CassandraPartitionedRDD/rpcToLocalAddressMap()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/collection/immutable/Set$/canBuildFrom()|",
      "|java+method:///scala/collection/immutable/Set/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/partitioner/CassandraPartitionedRDD$$anonfun$getPreferredLocations$1/CassandraPartitionedRDD$$anonfun$getPreferredLocations$1(com.datastax.spark.connector.rdd.partitioner.CassandraPartitionedRDD,scala.collection.immutable.Map,scala.collection.immutable.Map)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/partitioner/CassandraPartitionedRDD$$anonfun$1/CassandraPartitionedRDD$$anonfun$1(com.datastax.spark.connector.rdd.partitioner.CassandraPartitionedRDD)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+constructor:///com/datastax/spark/connector/writer/RateLimiter/RateLimiter(long,long,scala.Function0,scala.Function1)|",
    "called": "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
    "v1Body": [
      "|java+constructor:///java/util/concurrent/atomic/AtomicLong/AtomicLong(long)|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "v2Body": [
      "|java+constructor:///java/util/concurrent/atomic/AtomicLong/AtomicLong(long)|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/Function0/apply$mcJ$sp()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+constructor:///com/datastax/spark/connector/writer/RateLimiter$$anonfun$1/RateLimiter$$anonfun$1(com.datastax.spark.connector.writer.RateLimiter)|",
      "|java+constructor:///com/datastax/spark/connector/writer/RateLimiter$$anonfun$2/RateLimiter$$anonfun$2(com.datastax.spark.connector.writer.RateLimiter)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+constructor:///com/datastax/spark/connector/writer/GroupingBatchBuilder/GroupingBatchBuilder(com.datastax.spark.connector.writer.BoundStatementBuilder,com.datastax.spark.connector.writer.BatchStatementBuilder,scala.Function1,com.datastax.spark.connector.BatchSize,int,scala.collection.Iterator)|",
    "called": "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
    "v1Body": [
      "|java+method:///com/datastax/spark/connector/writer/Batch$/batchOrdering()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+constructor:///com/datastax/spark/connector/writer/GroupingBatchBuilder$$anonfun$1/GroupingBatchBuilder$$anonfun$1(com.datastax.spark.connector.writer.GroupingBatchBuilder)|",
      "|java+method:///scala/collection/TraversableOnce$class/$init$(scala.collection.TraversableOnce)|",
      "|java+method:///scala/collection/GenTraversableOnce$class/$init$(scala.collection.GenTraversableOnce)|",
      "|java+method:///scala/collection/Iterator$class/$init$(scala.collection.Iterator)|",
      "|java+constructor:///com/google/common/collect/AbstractIterator/AbstractIterator()|",
      "|java+constructor:///com/datastax/spark/connector/util/PriorityHashMap/PriorityHashMap(int,scala.math.Ordering)|"
    ],
    "v2Body": [
      "|java+method:///com/datastax/spark/connector/writer/Batch$/batchOrdering()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+constructor:///com/datastax/spark/connector/util/PriorityHashMap/PriorityHashMap(int,scala.math.Ordering)|",
      "|java+constructor:///com/datastax/spark/connector/writer/GroupingBatchBuilder$$anonfun$1/GroupingBatchBuilder$$anonfun$1(com.datastax.spark.connector.writer.GroupingBatchBuilder)|",
      "|java+method:///scala/collection/TraversableOnce$class/$init$(scala.collection.TraversableOnce)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///scala/collection/GenTraversableOnce$class/$init$(scala.collection.GenTraversableOnce)|",
      "|java+method:///scala/collection/Iterator$class/$init$(scala.collection.Iterator)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/mapper/TupleColumnMapper/newTable(java.lang.String,java.lang.String)|",
    "called": "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
    "v1Body": [
      "|java+method:///scala/reflect/api/Symbols$SymbolApi/typeSignatureIn(scala.reflect.api.Types$TypeApi)|",
      "|java+method:///scala/collection/immutable/List/tail()|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///scala/collection/TraversableLike/withFilter(scala.Function1)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/reflect/api/Symbols$SymbolApi/asMethod()|",
      "|java+constructor:///com/datastax/spark/connector/cql/TableDef/TableDef(java.lang.String,java.lang.String,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///com/datastax/spark/connector/util/Reflect$/constructor(scala.reflect.api.Types$TypeApi)|",
      "|java+method:///scala/reflect/api/TypeTags$WeakTypeTag/tpe()|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$newTable$1/TupleColumnMapper$$anonfun$newTable$1(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+method:///scala/collection/generic/FilterMonadic/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/api/Types$MethodTypeApi/params()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/Predef$/implicitly(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/List/head()|",
      "|java+method:///scala/collection/immutable/List/nonEmpty()|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$4/TupleColumnMapper$$anonfun$4(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$5/TupleColumnMapper$$anonfun$5(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$6/TupleColumnMapper$$anonfun$6(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$7/TupleColumnMapper$$anonfun$7(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+method:///scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/List/zipWithIndex(scala.collection.generic.CanBuildFrom)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/runtime/package$/universe()|",
      "|java+method:///scala/reflect/api/Symbols$SymbolApi/typeSignatureIn(scala.reflect.api.Types$TypeApi)|",
      "|java+method:///scala/collection/immutable/List/tail()|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///scala/collection/TraversableLike/withFilter(scala.Function1)|",
      "|java+method:///scala/reflect/api/TypeTags/TypeTag()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/reflect/api/Symbols$SymbolApi/asMethod()|",
      "|java+constructor:///com/datastax/spark/connector/cql/TableDef/TableDef(java.lang.String,java.lang.String,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///com/datastax/spark/connector/util/Reflect$/constructor(scala.reflect.api.Types$TypeApi)|",
      "|java+method:///scala/reflect/api/TypeTags$WeakTypeTag/tpe()|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$newTable$1/TupleColumnMapper$$anonfun$newTable$1(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+method:///scala/collection/generic/FilterMonadic/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/api/Types$MethodTypeApi/params()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/Predef$/implicitly(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/List/head()|",
      "|java+method:///scala/collection/immutable/List/nonEmpty()|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$4/TupleColumnMapper$$anonfun$4(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$5/TupleColumnMapper$$anonfun$5(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$6/TupleColumnMapper$$anonfun$6(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$7/TupleColumnMapper$$anonfun$7(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+method:///scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/List/zipWithIndex(scala.collection.generic.CanBuildFrom)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/AbstractGettableData$/convert(java.lang.Object,com.datastax.driver.core.ProtocolVersion)|",
    "called": "|java+method:///scala/collection/SeqView$/canBuildFrom()|",
    "v1Body": [
      "|java+constructor:///com/datastax/spark/connector/AbstractGettableData$$anonfun$convert$3/AbstractGettableData$$anonfun$convert$3(com.datastax.driver.core.ProtocolVersion)|",
      "|java+method:///scala/collection/mutable/Map/view()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///scala/collection/SeqView$/canBuildFrom()|",
      "|java+method:///scala/collection/IterableView$/canBuildFrom()|",
      "|java+method:///org/apache/cassandra/utils/ByteBufferUtil/getArray(java.nio.ByteBuffer)|",
      "|java+method:///scala/collection/IterableView/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///com/datastax/spark/connector/UDTValue$/fromJavaDriverUDTValue(com.datastax.driver.core.UDTValue,com.datastax.driver.core.ProtocolVersion)|",
      "|java+method:///scala/collection/SeqView/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/TraversableOnce/toSet()|",
      "|java+method:///scala/collection/mutable/Buffer/view()|",
      "|java+constructor:///com/datastax/spark/connector/AbstractGettableData$$anonfun$convert$1/AbstractGettableData$$anonfun$convert$1(com.datastax.driver.core.ProtocolVersion)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/collection/TraversableOnce/toList()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaSet(java.util.Set)|",
      "|java+method:///scala/collection/mutable/Set/view()|",
      "|java+constructor:///com/datastax/spark/connector/AbstractGettableData$$anonfun$convert$2/AbstractGettableData$$anonfun$convert$2(com.datastax.driver.core.ProtocolVersion)|"
    ],
    "v2Body": [
      "|java+constructor:///com/datastax/spark/connector/AbstractGettableData$$anonfun$convert$3/AbstractGettableData$$anonfun$convert$3(com.datastax.driver.core.ProtocolVersion)|",
      "|java+method:///scala/collection/mutable/Map/view()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///scala/collection/SeqView$/canBuildFrom()|",
      "|java+method:///com/datastax/spark/connector/util/ByteBufferUtil$/toArray(java.nio.ByteBuffer)|",
      "|java+method:///scala/collection/IterableView$/canBuildFrom()|",
      "|java+method:///scala/collection/IterableView/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///com/datastax/spark/connector/UDTValue$/fromJavaDriverUDTValue(com.datastax.driver.core.UDTValue,com.datastax.driver.core.ProtocolVersion)|",
      "|java+method:///scala/collection/SeqView/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/TraversableOnce/toSet()|",
      "|java+method:///scala/collection/mutable/Buffer/view()|",
      "|java+constructor:///com/datastax/spark/connector/AbstractGettableData$$anonfun$convert$1/AbstractGettableData$$anonfun$convert$1(com.datastax.driver.core.ProtocolVersion)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/collection/TraversableOnce/toList()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaSet(java.util.Set)|",
      "|java+method:///scala/collection/mutable/Set/view()|",
      "|java+constructor:///com/datastax/spark/connector/AbstractGettableData$$anonfun$convert$2/AbstractGettableData$$anonfun$convert$2(com.datastax.driver.core.ProtocolVersion)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/writer/BoundStatementBuilder$$anonfun$1/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
    "v1Body": [
      "|java+method:///com/datastax/spark/connector/writer/BoundStatementBuilder$$anonfun$1/apply(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///com/datastax/spark/connector/writer/BoundStatementBuilder$$anonfun$1/apply(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/rdd/partitioner/CassandraPartitionedRDD$$anonfun$getPreferredLocations$1/apply(java.net.InetAddress)|",
    "called": "|java+constructor:///scala/Some/Some(java.lang.Object)|",
    "v1Body": [
      "|java+method:///java/net/InetAddress/getHostAddress()|"
    ],
    "v2Body": [
      "|java+constructor:///com/datastax/spark/connector/rdd/partitioner/CassandraPartitionedRDD$$anonfun$getPreferredLocations$1$$anonfun$2/CassandraPartitionedRDD$$anonfun$getPreferredLocations$1$$anonfun$2(com.datastax.spark.connector.rdd.partitioner.CassandraPartitionedRDD$$anonfun$getPreferredLocations$1)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/partitioner/CassandraPartitionedRDD$$anonfun$getPreferredLocations$1$$anonfun$apply$1/CassandraPartitionedRDD$$anonfun$getPreferredLocations$1$$anonfun$apply$1(com.datastax.spark.connector.rdd.partitioner.CassandraPartitionedRDD$$anonfun$getPreferredLocations$1)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///scala/collection/GenTraversable/flatten(scala.Function1)|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/writer/BoundStatementBuilder/bind(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
    "v1Body": [
      "|java+method:///com/datastax/spark/connector/writer/RowWriter/readColumnValues(java.lang.Object,java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/immutable/IndexedSeq/size()|",
      "|java+constructor:///scala/runtime/IntRef/IntRef(int)|",
      "|java+constructor:///com/datastax/spark/connector/writer/BoundStatementBuilder$$anonfun$bind$1/BoundStatementBuilder$$anonfun$bind$1(com.datastax.spark.connector.writer.BoundStatementBuilder,com.datastax.spark.connector.writer.RichBoundStatement,scala.runtime.IntRef)|",
      "|java+method:///scala/collection/immutable/Range/foreach(scala.Function1)|",
      "|java+method:///com/datastax/spark/connector/writer/BoundStatementBuilder/rowWriter()|",
      "|java+constructor:///com/datastax/spark/connector/writer/RichBoundStatement/RichBoundStatement(com.datastax.driver.core.PreparedStatement)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+method:///com/datastax/spark/connector/writer/BoundStatementBuilder/com$datastax$spark$connector$writer$BoundStatementBuilder$$columnNames()|",
      "|java+method:///com/datastax/spark/connector/writer/BoundStatementBuilder/com$datastax$spark$connector$writer$BoundStatementBuilder$$buffer()|",
      "|java+method:///com/datastax/spark/connector/writer/BoundStatementBuilder/preparedStmt()|",
      "|java+method:///com/datastax/spark/connector/writer/RichBoundStatement/bytesCount_$eq(int)|"
    ],
    "v2Body": [
      "|java+method:///com/datastax/spark/connector/writer/RowWriter/readColumnValues(java.lang.Object,java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/immutable/IndexedSeq/size()|",
      "|java+method:///scala/reflect/ClassTag$/AnyRef()|",
      "|java+method:///com/datastax/spark/connector/writer/BoundStatementBuilder/prefixConverted()|",
      "|java+constructor:///com/datastax/spark/connector/writer/BoundStatementBuilder$$anonfun$bind$1/BoundStatementBuilder$$anonfun$bind$1(com.datastax.spark.connector.writer.BoundStatementBuilder,com.datastax.spark.connector.writer.RichBoundStatement,scala.runtime.IntRef)|",
      "|java+method:///scala/collection/immutable/Range/foreach(scala.Function1)|",
      "|java+method:///com/datastax/spark/connector/writer/BoundStatementBuilder/rowWriter()|",
      "|java+method:///com/datastax/spark/connector/writer/RichBoundStatement/bind(java.lang.Object%5B%5D)|",
      "|java+constructor:///com/datastax/spark/connector/writer/RichBoundStatement/RichBoundStatement(com.datastax.driver.core.PreparedStatement)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+method:///com/datastax/spark/connector/writer/BoundStatementBuilder/com$datastax$spark$connector$writer$BoundStatementBuilder$$columnNames()|",
      "|java+method:///com/datastax/spark/connector/writer/BoundStatementBuilder/com$datastax$spark$connector$writer$BoundStatementBuilder$$buffer()|",
      "|java+method:///com/datastax/spark/connector/writer/BoundStatementBuilder/preparedStmt()|",
      "|java+method:///com/datastax/spark/connector/writer/RichBoundStatement/bytesCount_$eq(int)|",
      "|java+method:///scala/collection/immutable/IndexedSeq/toArray(scala.reflect.ClassTag)|",
      "|java+constructor:///scala/runtime/IntRef/IntRef(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$1/apply()|",
    "called": "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$1/apply()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$1/apply()|",
      "|java+method:///scala/collection/immutable/Set/mkString(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+constructor:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader/ClassBasedRowReader(com.datastax.spark.connector.cql.TableDef,int,scala.collection.immutable.Map,scala.reflect.api.TypeTags$TypeTag,com.datastax.spark.connector.mapper.ColumnMapper)|",
    "called": "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/RowReader$class/$init$(com.datastax.spark.connector.rdd.reader.RowReader)|",
      "|java+method:///scala/collection/immutable/Map/values()|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader/setterTypes()|",
      "|java+method:///scala/Predef$/implicitly(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///com/datastax/spark/connector/mapper/ColumnMap/constructor()|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/collection/immutable/Iterable$/canBuildFrom()|",
      "|java+method:///scala/collection/Iterable/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/Map/keys()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader/com$datastax$spark$connector$rdd$reader$ClassBasedRowReader$$columnMap()|",
      "|java+method:///scala/reflect/api/TypeTags$WeakTypeTag/tpe()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///com/datastax/spark/connector/mapper/ColumnMapper/columnMap(com.datastax.spark.connector.cql.TableDef,scala.collection.immutable.Map)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/constructorParamTypes()|",
      "|java+method:///com/datastax/spark/connector/mapper/ColumnMap/setters()|",
      "|java+method:///scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader$$anonfun$1/ClassBasedRowReader$$anonfun$1(com.datastax.spark.connector.rdd.reader.ClassBasedRowReader)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader$$anonfun$2/ClassBasedRowReader$$anonfun$2(com.datastax.spark.connector.rdd.reader.ClassBasedRowReader)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader$$anonfun$3/ClassBasedRowReader$$anonfun$3(com.datastax.spark.connector.rdd.reader.ClassBasedRowReader)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/AnyObjectFactory(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader/factory()|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///com/datastax/spark/connector/rdd/reader/RowReader$class/$init$(com.datastax.spark.connector.rdd.reader.RowReader)|",
      "|java+method:///scala/collection/immutable/Map/values()|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader/setterTypes()|",
      "|java+method:///scala/reflect/api/TypeTags/TypeTag()|",
      "|java+method:///scala/Predef$/implicitly(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///com/datastax/spark/connector/mapper/ColumnMap/constructor()|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/collection/immutable/Iterable$/canBuildFrom()|",
      "|java+method:///scala/collection/Iterable/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/Map/keys()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader/com$datastax$spark$connector$rdd$reader$ClassBasedRowReader$$columnMap()|",
      "|java+method:///scala/reflect/api/TypeTags$WeakTypeTag/tpe()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/reflect/runtime/package$/universe()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///com/datastax/spark/connector/mapper/ColumnMapper/columnMap(com.datastax.spark.connector.cql.TableDef,scala.collection.immutable.Map)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/constructorParamTypes()|",
      "|java+method:///com/datastax/spark/connector/mapper/ColumnMap/setters()|",
      "|java+method:///scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader$$anonfun$1/ClassBasedRowReader$$anonfun$1(com.datastax.spark.connector.rdd.reader.ClassBasedRowReader)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader$$anonfun$2/ClassBasedRowReader$$anonfun$2(com.datastax.spark.connector.rdd.reader.ClassBasedRowReader)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader$$anonfun$3/ClassBasedRowReader$$anonfun$3(com.datastax.spark.connector.rdd.reader.ClassBasedRowReader)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/AnyObjectFactory(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader/factory()|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/rdd/CassandraTableScanRDD/getPreferredLocations(org.apache.spark.Partition)|",
    "called": "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///scala/collection/Iterable/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraTableScanRDD$$anonfun$getPreferredLocations$1/CassandraTableScanRDD$$anonfun$getPreferredLocations$1(com.datastax.spark.connector.rdd.CassandraTableScanRDD)|",
      "|java+method:///com/datastax/spark/connector/rdd/partitioner/CassandraPartition/endpoints()|",
      "|java+method:///scala/collection/TraversableOnce/toSeq()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraTableScanRDD$$anonfun$getPreferredLocations$1/CassandraTableScanRDD$$anonfun$getPreferredLocations$1(com.datastax.spark.connector.rdd.CassandraTableScanRDD)|",
      "|java+method:///scala/collection/Iterable/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq/distinct()|",
      "|java+method:///com/datastax/spark/connector/rdd/partitioner/CassandraPartition/endpoints()|",
      "|java+method:///scala/collection/TraversableOnce/toSeq()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/writer/TableWriter$$anonfun$write$1/apply(com.datastax.driver.core.Session)|",
    "called": "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///com/datastax/spark/connector/writer/RateLimiter$/$lessinit$greater$default$4()|",
      "|java+method:///com/datastax/spark/connector/writer/WriteConf/batchSize()|",
      "|java+method:///com/datastax/spark/connector/writer/TableWriter/keyspaceName()|",
      "|java+method:///com/datastax/spark/connector/writer/WriteConf/throughputMiBPS()|",
      "|java+constructor:///com/datastax/spark/connector/writer/TableWriter$$anonfun$write$1$$anonfun$apply$1/TableWriter$$anonfun$write$1$$anonfun$apply$1(com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1)|",
      "|java+constructor:///com/datastax/spark/connector/writer/BatchStatementBuilder/BatchStatementBuilder(com.datastax.driver.core.BatchStatement$Type,com.datastax.spark.connector.writer.RoutingKeyGenerator,com.datastax.driver.core.ConsistencyLevel)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///com/datastax/spark/connector/writer/TableWriter/com$datastax$spark$connector$writer$TableWriter$$prepareStatement(com.datastax.driver.core.Session)|",
      "|java+method:///com/datastax/driver/core/PreparedStatement/setConsistencyLevel(com.datastax.driver.core.ConsistencyLevel)|",
      "|java+constructor:///com/datastax/spark/connector/writer/TableWriter$$anonfun$write$1$$anonfun$apply$2/TableWriter$$anonfun$write$1$$anonfun$apply$2(com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1,com.datastax.spark.connector.writer.QueryExecutor,com.datastax.spark.connector.writer.RateLimiter)|",
      "|java+method:///com/datastax/spark/connector/writer/GroupingBatchBuilder/foreach(scala.Function1)|",
      "|java+constructor:///com/datastax/spark/connector/writer/RoutingKeyGenerator/RoutingKeyGenerator(com.datastax.spark.connector.cql.TableDef,scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///com/datastax/spark/connector/writer/RateLimiter/RateLimiter(long,long,scala.Function0,scala.Function1)|",
      "|java+method:///com/datastax/spark/connector/util/CountingIterator$/$lessinit$greater$default$2()|",
      "|java+constructor:///com/datastax/spark/connector/writer/BoundStatementBuilder/BoundStatementBuilder(com.datastax.spark.connector.writer.RowWriter,com.datastax.driver.core.PreparedStatement,com.datastax.driver.core.ProtocolVersion)|",
      "|java+method:///com/datastax/spark/connector/writer/WriteConf/batchGroupingBufferSize()|",
      "|java+constructor:///com/datastax/spark/connector/util/CountingIterator/CountingIterator(scala.collection.Iterator,scala.Option)|",
      "|java+constructor:///java/io/IOException/IOException(java.lang.String)|",
      "|java+constructor:///com/datastax/spark/connector/writer/TableWriter$$anonfun$write$1$$anonfun$12/TableWriter$$anonfun$write$1$$anonfun$12(com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1)|",
      "|java+method:///com/datastax/spark/connector/writer/QueryExecutor/waitForCurrentlyExecutingTasks()|",
      "|java+method:///com/datastax/spark/connector/writer/TableWriter/protocolVersion()|",
      "|java+constructor:///com/datastax/spark/connector/writer/TableWriter$$anonfun$write$1$$anonfun$11/TableWriter$$anonfun$write$1$$anonfun$11(com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1)|",
      "|java+constructor:///com/datastax/spark/connector/writer/QueryExecutor/QueryExecutor(com.datastax.driver.core.Session,int,scala.Option,scala.Option)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///com/datastax/spark/connector/writer/TableWriter$$anonfun$write$1$$anonfun$13/TableWriter$$anonfun$write$1$$anonfun$13(com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1,com.datastax.driver.core.Session,com.datastax.spark.connector.writer.RoutingKeyGenerator)|",
      "|java+method:///com/datastax/spark/connector/writer/QueryExecutor/successful()|",
      "|java+constructor:///com/datastax/spark/connector/writer/TableWriter$$anonfun$write$1$$anonfun$apply$3/TableWriter$$anonfun$write$1$$anonfun$apply$3(com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1,com.datastax.spark.connector.util.CountingIterator,double)|",
      "|java+method:///com/datastax/spark/connector/metrics/OutputMetricsUpdater/finish()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///com/datastax/spark/connector/writer/TableWriter/logDebug(scala.Function0)|",
      "|java+method:///com/datastax/spark/connector/writer/TableWriter/columnNames()|",
      "|java+method:///com/datastax/spark/connector/writer/WriteConf/parallelismLevel()|",
      "|java+constructor:///com/datastax/spark/connector/writer/GroupingBatchBuilder/GroupingBatchBuilder(com.datastax.spark.connector.writer.BoundStatementBuilder,com.datastax.spark.connector.writer.BatchStatementBuilder,scala.Function1,com.datastax.spark.connector.BatchSize,int,scala.collection.Iterator)|",
      "|java+method:///com/datastax/spark/connector/writer/WriteConf/consistencyLevel()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///com/datastax/spark/connector/writer/TableWriter/tableName()|",
      "|java+method:///com/datastax/spark/connector/writer/RateLimiter$/$lessinit$greater$default$3()|",
      "|java+method:///com/datastax/spark/connector/writer/TableWriter/com$datastax$spark$connector$writer$TableWriter$$isCounterUpdate()|",
      "|java+method:///com/datastax/spark/connector/writer/TableWriter/logInfo(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///com/datastax/spark/connector/writer/RateLimiter$/$lessinit$greater$default$4()|",
      "|java+method:///com/datastax/spark/connector/writer/WriteConf/batchSize()|",
      "|java+method:///com/datastax/spark/connector/writer/TableWriter/keyspaceName()|",
      "|java+method:///com/datastax/spark/connector/writer/WriteConf/throughputMiBPS()|",
      "|java+constructor:///com/datastax/spark/connector/writer/TableWriter$$anonfun$write$1$$anonfun$apply$1/TableWriter$$anonfun$write$1$$anonfun$apply$1(com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1)|",
      "|java+constructor:///com/datastax/spark/connector/writer/BatchStatementBuilder/BatchStatementBuilder(com.datastax.driver.core.BatchStatement$Type,com.datastax.spark.connector.writer.RoutingKeyGenerator,com.datastax.driver.core.ConsistencyLevel)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///com/datastax/spark/connector/writer/TableWriter/com$datastax$spark$connector$writer$TableWriter$$prepareStatement(com.datastax.driver.core.Session)|",
      "|java+constructor:///com/datastax/spark/connector/writer/BoundStatementBuilder/BoundStatementBuilder(com.datastax.spark.connector.writer.RowWriter,com.datastax.driver.core.PreparedStatement,com.datastax.driver.core.ProtocolVersion,scala.collection.Seq)|",
      "|java+method:///com/datastax/driver/core/PreparedStatement/setConsistencyLevel(com.datastax.driver.core.ConsistencyLevel)|",
      "|java+constructor:///com/datastax/spark/connector/writer/TableWriter$$anonfun$write$1$$anonfun$apply$2/TableWriter$$anonfun$write$1$$anonfun$apply$2(com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1,com.datastax.spark.connector.writer.QueryExecutor,com.datastax.spark.connector.writer.RateLimiter)|",
      "|java+method:///com/datastax/spark/connector/writer/GroupingBatchBuilder/foreach(scala.Function1)|",
      "|java+constructor:///com/datastax/spark/connector/writer/RoutingKeyGenerator/RoutingKeyGenerator(com.datastax.spark.connector.cql.TableDef,scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///com/datastax/spark/connector/writer/RateLimiter/RateLimiter(long,long,scala.Function0,scala.Function1)|",
      "|java+method:///com/datastax/spark/connector/util/CountingIterator$/$lessinit$greater$default$2()|",
      "|java+method:///com/datastax/spark/connector/writer/WriteConf/batchGroupingBufferSize()|",
      "|java+constructor:///com/datastax/spark/connector/util/CountingIterator/CountingIterator(scala.collection.Iterator,scala.Option)|",
      "|java+constructor:///java/io/IOException/IOException(java.lang.String)|",
      "|java+constructor:///com/datastax/spark/connector/writer/TableWriter$$anonfun$write$1$$anonfun$12/TableWriter$$anonfun$write$1$$anonfun$12(com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1)|",
      "|java+method:///com/datastax/spark/connector/writer/QueryExecutor/waitForCurrentlyExecutingTasks()|",
      "|java+method:///com/datastax/spark/connector/writer/TableWriter/protocolVersion()|",
      "|java+constructor:///com/datastax/spark/connector/writer/TableWriter$$anonfun$write$1$$anonfun$11/TableWriter$$anonfun$write$1$$anonfun$11(com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1)|",
      "|java+constructor:///com/datastax/spark/connector/writer/QueryExecutor/QueryExecutor(com.datastax.driver.core.Session,int,scala.Option,scala.Option)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///com/datastax/spark/connector/writer/TableWriter$$anonfun$write$1$$anonfun$13/TableWriter$$anonfun$write$1$$anonfun$13(com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1,com.datastax.driver.core.Session,com.datastax.spark.connector.writer.RoutingKeyGenerator)|",
      "|java+method:///com/datastax/spark/connector/writer/QueryExecutor/successful()|",
      "|java+constructor:///com/datastax/spark/connector/writer/TableWriter$$anonfun$write$1$$anonfun$apply$3/TableWriter$$anonfun$write$1$$anonfun$apply$3(com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1,com.datastax.spark.connector.util.CountingIterator,double)|",
      "|java+method:///com/datastax/spark/connector/metrics/OutputMetricsUpdater/finish()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///com/datastax/spark/connector/writer/TableWriter/logDebug(scala.Function0)|",
      "|java+method:///com/datastax/spark/connector/writer/TableWriter/columnNames()|",
      "|java+method:///com/datastax/spark/connector/writer/WriteConf/parallelismLevel()|",
      "|java+constructor:///com/datastax/spark/connector/writer/GroupingBatchBuilder/GroupingBatchBuilder(com.datastax.spark.connector.writer.BoundStatementBuilder,com.datastax.spark.connector.writer.BatchStatementBuilder,scala.Function1,com.datastax.spark.connector.BatchSize,int,scala.collection.Iterator)|",
      "|java+method:///com/datastax/spark/connector/writer/WriteConf/consistencyLevel()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///com/datastax/spark/connector/writer/TableWriter/tableName()|",
      "|java+method:///com/datastax/spark/connector/writer/BoundStatementBuilder$/$lessinit$greater$default$4()|",
      "|java+method:///com/datastax/spark/connector/writer/RateLimiter$/$lessinit$greater$default$3()|",
      "|java+method:///com/datastax/spark/connector/writer/TableWriter/com$datastax$spark$connector$writer$TableWriter$$isCounterUpdate()|",
      "|java+method:///com/datastax/spark/connector/writer/TableWriter/logInfo(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///org/apache/spark/sql/cassandra/CassandraRelation/statistics$lzycompute()|",
    "called": "|java+method:///scala/math/BigInt$/apply(long)|",
    "v1Body": [
      "|java+method:///org/apache/spark/sql/cassandra/CassandraRelation/keyspaceName()|",
      "|java+method:///scala/package$/BigInt()|",
      "|java+method:///scala/math/BigInt$/apply(long)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/sql/cassandra/CassandraRelation/tableName()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/Statistics/Statistics(scala.math.BigInt)|",
      "|java+method:///org/apache/spark/SparkConf/getLong(java.lang.String,long)|",
      "|java+method:///org/apache/spark/sql/cassandra/CassandraSQLContext/conf()|",
      "|java+method:///org/apache/spark/sql/cassandra/CassandraSQLContext/defaultSizeInBytes()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/sql/cassandra/CassandraRelation/keyspaceName()|",
      "|java+method:///scala/package$/BigInt()|",
      "|java+method:///scala/math/BigInt$/apply(long)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/sql/cassandra/CassandraRelation/cc()|",
      "|java+method:///org/apache/spark/sql/cassandra/CassandraRelation/tableName()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/sql/catalyst/plans/logical/Statistics/Statistics(scala.math.BigInt)|",
      "|java+method:///org/apache/spark/SparkConf/getLong(java.lang.String,long)|",
      "|java+method:///org/apache/spark/sql/cassandra/CassandraSQLContext/conf()|",
      "|java+method:///org/apache/spark/sql/cassandra/CassandraSQLContext/defaultSizeInBytes()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/singleKeyCqlQuery$lzycompute()|",
    "called": "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/joinColumnNames()|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$2/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$2(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/selectedColumnRefs()|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$1/CassandraJoinRDD$$anonfun$1(com.datastax.spark.connector.rdd.CassandraJoinRDD,scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/quote(java.lang.String)|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/tableName()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///com/datastax/spark/connector/rdd/CqlWhereClause/predicates()|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/clusteringOrder()|",
      "|java+method:///com/datastax/spark/connector/cql/TableDef/partitionKey()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/where()|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$10/CassandraJoinRDD$$anonfun$10(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$11/CassandraJoinRDD$$anonfun$11(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$12/CassandraJoinRDD$$anonfun$12(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$13/CassandraJoinRDD$$anonfun$13(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$14/CassandraJoinRDD$$anonfun$14(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$15/CassandraJoinRDD$$anonfun$15(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$3/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$3(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/keyspaceName()|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/logDebug(scala.Function0)|",
      "|java+method:///scala/collection/Seq/collect(scala.PartialFunction,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$1/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$1(com.datastax.spark.connector.rdd.CassandraJoinRDD,scala.collection.immutable.Set)|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/tableDef()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$9/CassandraJoinRDD$$anonfun$9(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$8/CassandraJoinRDD$$anonfun$8(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/limit()|",
      "|java+method:///scala/collection/TraversableOnce/toSet()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$7/CassandraJoinRDD$$anonfun$7(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+method:///scala/collection/immutable/Set/isEmpty()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$4/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$4(com.datastax.spark.connector.rdd.CassandraJoinRDD,java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/clusteringOrder()|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/joinColumnNames()|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$2/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$2(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/selectedColumnRefs()|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$1/CassandraJoinRDD$$anonfun$1(com.datastax.spark.connector.rdd.CassandraJoinRDD,scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/quote(java.lang.String)|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/tableName()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///com/datastax/spark/connector/rdd/CqlWhereClause/predicates()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/where()|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$10/CassandraJoinRDD$$anonfun$10(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$11/CassandraJoinRDD$$anonfun$11(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$12/CassandraJoinRDD$$anonfun$12(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$13/CassandraJoinRDD$$anonfun$13(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$14/CassandraJoinRDD$$anonfun$14(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$15/CassandraJoinRDD$$anonfun$15(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$3/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$3(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/keyspaceName()|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/logDebug(scala.Function0)|",
      "|java+method:///scala/collection/Seq/collect(scala.PartialFunction,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$1/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$1(com.datastax.spark.connector.rdd.CassandraJoinRDD,scala.collection.immutable.Set)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$9/CassandraJoinRDD$$anonfun$9(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$8/CassandraJoinRDD$$anonfun$8(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+method:///com/datastax/spark/connector/rdd/CassandraJoinRDD/limit()|",
      "|java+method:///scala/collection/TraversableOnce/toSet()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$7/CassandraJoinRDD$$anonfun$7(com.datastax.spark.connector.rdd.CassandraJoinRDD)|",
      "|java+method:///scala/collection/immutable/Set/isEmpty()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$4/CassandraJoinRDD$$anonfun$singleKeyCqlQuery$4(com.datastax.spark.connector.rdd.CassandraJoinRDD,java.lang.String)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/writer/ObjectSizeEstimator$/com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable(java.lang.Object)|",
    "called": "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
    "v1Body": [
      "|java+method:///scala/collection/immutable/Set/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/cassandra/utils/ByteBufferUtil/getArray(java.nio.ByteBuffer)|",
      "|java+method:///scala/collection/mutable/Map$/canBuildFrom()|",
      "|java+method:///scala/collection/immutable/Set$/canBuildFrom()|",
      "|java+method:///scala/collection/mutable/Buffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///scala/collection/mutable/Set/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///scala/collection/mutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///com/datastax/spark/connector/writer/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$1/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$1()|",
      "|java+constructor:///com/datastax/spark/connector/writer/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$5/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$5()|",
      "|java+constructor:///com/datastax/spark/connector/writer/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$2/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$2()|",
      "|java+constructor:///com/datastax/spark/connector/writer/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$6/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$6()|",
      "|java+constructor:///com/datastax/spark/connector/writer/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$3/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$3()|",
      "|java+constructor:///com/datastax/spark/connector/writer/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$4/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$4()|",
      "|java+method:///scala/collection/immutable/Map$/canBuildFrom()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/collection/mutable/Set$/canBuildFrom()|",
      "|java+method:///scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaSet(java.util.Set)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/immutable/Set/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///com/datastax/spark/connector/util/ByteBufferUtil$/toArray(java.nio.ByteBuffer)|",
      "|java+method:///scala/collection/mutable/Map$/canBuildFrom()|",
      "|java+method:///scala/collection/immutable/Set$/canBuildFrom()|",
      "|java+method:///scala/collection/mutable/Buffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///scala/collection/mutable/Set/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///scala/collection/mutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///com/datastax/spark/connector/writer/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$1/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$1()|",
      "|java+constructor:///com/datastax/spark/connector/writer/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$5/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$5()|",
      "|java+constructor:///com/datastax/spark/connector/writer/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$2/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$2()|",
      "|java+constructor:///com/datastax/spark/connector/writer/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$6/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$6()|",
      "|java+constructor:///com/datastax/spark/connector/writer/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$3/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$3()|",
      "|java+constructor:///com/datastax/spark/connector/writer/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$4/ObjectSizeEstimator$$anonfun$com$datastax$spark$connector$writer$ObjectSizeEstimator$$makeSerializable$4()|",
      "|java+method:///scala/collection/immutable/Map$/canBuildFrom()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/collection/mutable/Set$/canBuildFrom()|",
      "|java+method:///scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaSet(java.util.Set)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/constructorParamTypes$lzycompute()|",
    "called": "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
    "v1Body": [
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$$anonfun$constructorParamTypes$1/AnyObjectFactory$$anonfun$constructorParamTypes$1(com.datastax.spark.connector.rdd.reader.AnyObjectFactory)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///scala/collection/LinearSeqOptimized/find(scala.Function1)|",
      "|java+method:///scala/reflect/api/Symbols$TermSymbolApi/alternatives()|",
      "|java+method:///com/datastax/spark/connector/util/Reflect$/constructor(scala.reflect.api.Types$TypeApi)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/javaClass()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$/oneIfMemberClass(java.lang.Class)|",
      "|java+method:///java/lang/reflect/Constructor/getParameterTypes()|",
      "|java+method:///scala/collection/mutable/ArrayOps/drop(int)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$$anonfun$constructorParamTypes$2/AnyObjectFactory$$anonfun$constructorParamTypes$2(com.datastax.spark.connector.rdd.reader.AnyObjectFactory,java.lang.Class%5B%5D)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/javaConstructor()|",
      "|java+method:///scala/reflect/api/Symbols$SymbolApi/asTerm()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/com$datastax$spark$connector$rdd$reader$AnyObjectFactory$$tpe()|",
      "|java+method:///scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/runtime/package$/universe()|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$$anonfun$constructorParamTypes$1/AnyObjectFactory$$anonfun$constructorParamTypes$1(com.datastax.spark.connector.rdd.reader.AnyObjectFactory)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///scala/collection/LinearSeqOptimized/find(scala.Function1)|",
      "|java+method:///scala/reflect/api/TypeTags/TypeTag()|",
      "|java+method:///scala/reflect/api/Symbols$TermSymbolApi/alternatives()|",
      "|java+method:///com/datastax/spark/connector/util/Reflect$/constructor(scala.reflect.api.Types$TypeApi)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/javaClass()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$/oneIfMemberClass(java.lang.Class)|",
      "|java+method:///java/lang/reflect/Constructor/getParameterTypes()|",
      "|java+method:///scala/collection/mutable/ArrayOps/drop(int)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$$anonfun$constructorParamTypes$2/AnyObjectFactory$$anonfun$constructorParamTypes$2(com.datastax.spark.connector.rdd.reader.AnyObjectFactory,java.lang.Class%5B%5D)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/javaConstructor()|",
      "|java+method:///scala/reflect/api/Symbols$SymbolApi/asTerm()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/com$datastax$spark$connector$rdd$reader$AnyObjectFactory$$tpe()|",
      "|java+method:///scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/writer/RateLimiter/leak()|",
    "called": "|java+method:///scala/math/package$/max(long,long)|",
    "v1Body": [
      "|java+method:///java/util/concurrent/atomic/AtomicLong/getAndSet(long)|",
      "|java+method:///com/datastax/spark/connector/writer/RateLimiter/lastTime()|",
      "|java+method:///com/datastax/spark/connector/writer/RateLimiter/leak(long)|",
      "|java+method:///scala/Function0/apply$mcJ$sp()|"
    ],
    "v2Body": [
      "|java+method:///java/util/concurrent/atomic/AtomicLong/getAndSet(long)|",
      "|java+method:///com/datastax/spark/connector/writer/RateLimiter/leak(long)|",
      "|java+method:///scala/Function0/apply$mcJ$sp()|",
      "|java+method:///scala/math/package$/max(long,long)|",
      "|java+method:///com/datastax/spark/connector/writer/RateLimiter/lastTime()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/RDDFunctions/repartitionByCassandraReplica(java.lang.String,java.lang.String,int,com.datastax.spark.connector.cql.CassandraConnector,scala.reflect.ClassTag,com.datastax.spark.connector.writer.RowWriterFactory)|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///com/datastax/spark/connector/RDDFunctions/keyByCassandraReplica(java.lang.String,java.lang.String,com.datastax.spark.connector.cql.CassandraConnector,scala.reflect.ClassTag,com.datastax.spark.connector.writer.RowWriterFactory)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/partitionBy(org.apache.spark.Partitioner)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkContext$/rddToPairRDDFunctions$default$4(org.apache.spark.rdd.RDD)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/partitioner/ReplicaPartitioner/ReplicaPartitioner(int,com.datastax.spark.connector.cql.CassandraConnector)|",
      "|java+constructor:///com/datastax/spark/connector/RDDFunctions$$anonfun$1/RDDFunctions$$anonfun$1(com.datastax.spark.connector.RDDFunctions)|",
      "|java+method:///com/datastax/spark/connector/package$/toRDDFunctions(org.apache.spark.rdd.RDD)|",
      "|java+method:///org/apache/spark/SparkContext$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/partitioner/CassandraPartitionedRDD/CassandraPartitionedRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///com/datastax/spark/connector/RDDFunctions/keyByCassandraReplica(java.lang.String,java.lang.String,com.datastax.spark.connector.cql.CassandraConnector,scala.reflect.ClassTag,com.datastax.spark.connector.writer.RowWriterFactory)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/partitionBy(org.apache.spark.Partitioner)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkContext$/rddToPairRDDFunctions$default$4(org.apache.spark.rdd.RDD)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/partitioner/ReplicaPartitioner/ReplicaPartitioner(int,com.datastax.spark.connector.cql.CassandraConnector)|",
      "|java+constructor:///com/datastax/spark/connector/RDDFunctions$$anonfun$1/RDDFunctions$$anonfun$1(com.datastax.spark.connector.RDDFunctions)|",
      "|java+method:///com/datastax/spark/connector/package$/toRDDFunctions(org.apache.spark.rdd.RDD)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/partitioner/CassandraPartitionedRDD/CassandraPartitionedRDD(org.apache.spark.rdd.RDD,java.lang.String,java.lang.String,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkContext$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/mapper/TupleColumnMapper/newTable(java.lang.String,java.lang.String)|",
    "called": "|java+method:///scala/reflect/runtime/package$/universe()|",
    "v1Body": [
      "|java+method:///scala/reflect/api/Symbols$SymbolApi/typeSignatureIn(scala.reflect.api.Types$TypeApi)|",
      "|java+method:///scala/collection/immutable/List/tail()|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///scala/collection/TraversableLike/withFilter(scala.Function1)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/reflect/api/Symbols$SymbolApi/asMethod()|",
      "|java+constructor:///com/datastax/spark/connector/cql/TableDef/TableDef(java.lang.String,java.lang.String,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///com/datastax/spark/connector/util/Reflect$/constructor(scala.reflect.api.Types$TypeApi)|",
      "|java+method:///scala/reflect/api/TypeTags$WeakTypeTag/tpe()|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$newTable$1/TupleColumnMapper$$anonfun$newTable$1(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+method:///scala/collection/generic/FilterMonadic/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/api/Types$MethodTypeApi/params()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/Predef$/implicitly(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/List/head()|",
      "|java+method:///scala/collection/immutable/List/nonEmpty()|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$4/TupleColumnMapper$$anonfun$4(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$5/TupleColumnMapper$$anonfun$5(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$6/TupleColumnMapper$$anonfun$6(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$7/TupleColumnMapper$$anonfun$7(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+method:///scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/List/zipWithIndex(scala.collection.generic.CanBuildFrom)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/runtime/package$/universe()|",
      "|java+method:///scala/reflect/api/Symbols$SymbolApi/typeSignatureIn(scala.reflect.api.Types$TypeApi)|",
      "|java+method:///scala/collection/immutable/List/tail()|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///scala/collection/TraversableLike/withFilter(scala.Function1)|",
      "|java+method:///scala/reflect/api/TypeTags/TypeTag()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/reflect/api/Symbols$SymbolApi/asMethod()|",
      "|java+constructor:///com/datastax/spark/connector/cql/TableDef/TableDef(java.lang.String,java.lang.String,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///com/datastax/spark/connector/util/Reflect$/constructor(scala.reflect.api.Types$TypeApi)|",
      "|java+method:///scala/reflect/api/TypeTags$WeakTypeTag/tpe()|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$newTable$1/TupleColumnMapper$$anonfun$newTable$1(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+method:///scala/collection/generic/FilterMonadic/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/api/Types$MethodTypeApi/params()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/Predef$/implicitly(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/List/head()|",
      "|java+method:///scala/collection/immutable/List/nonEmpty()|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$4/TupleColumnMapper$$anonfun$4(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$5/TupleColumnMapper$$anonfun$5(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$6/TupleColumnMapper$$anonfun$6(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+constructor:///com/datastax/spark/connector/mapper/TupleColumnMapper$$anonfun$7/TupleColumnMapper$$anonfun$7(com.datastax.spark.connector.mapper.TupleColumnMapper)|",
      "|java+method:///scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/List/zipWithIndex(scala.collection.generic.CanBuildFrom)|"
    ],
    "affectedLib": "org.scala-lang:scala-reflect:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/util/JavaApiHelper$/getRuntimeClass(scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///scala/reflect/runtime/package$/universe()|",
    "v1Body": [
      "|java+method:///com/datastax/spark/connector/util/JavaApiHelper$/mirror()|",
      "|java+method:///scala/reflect/api/TypeTags$TypeTag/tpe()|",
      "|java+method:///scala/reflect/api/JavaMirrors$JavaMirror/runtimeClass(scala.reflect.api.Types$TypeApi)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/runtime/package$/universe()|",
      "|java+method:///scala/reflect/api/TypeTags/TypeTag()|",
      "|java+method:///com/datastax/spark/connector/util/JavaApiHelper$/mirror()|",
      "|java+method:///scala/reflect/api/TypeTags$TypeTag/tpe()|",
      "|java+method:///scala/reflect/api/JavaMirrors$JavaMirror/runtimeClass(scala.reflect.api.Types$TypeApi)|"
    ],
    "affectedLib": "org.scala-lang:scala-reflect:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+constructor:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/AnyObjectFactory(scala.reflect.api.TypeTags$TypeTag)|",
    "called": "|java+method:///scala/reflect/runtime/package$/universe()|",
    "v1Body": [
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$/oneIfMemberClass(java.lang.Class)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/rm()|",
      "|java+method:///scala/Predef$/implicitly(java.lang.Object)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$/resolveConstructor(java.lang.Class)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$/toParamTypeNames(java.lang.reflect.Constructor)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+method:///scala/reflect/api/TypeTags$WeakTypeTag/tpe()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/javaClass()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/constructorParamTypes()|",
      "|java+method:///scala/reflect/api/Mirrors$RuntimeMirror/runtimeClass(scala.reflect.api.Types$TypeApi)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/com$datastax$spark$connector$rdd$reader$AnyObjectFactory$$tpe()|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/runtime/package$/universe()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$/oneIfMemberClass(java.lang.Class)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/rm()|",
      "|java+method:///scala/reflect/api/TypeTags/TypeTag()|",
      "|java+method:///scala/Predef$/implicitly(java.lang.Object)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$/resolveConstructor(java.lang.Class)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$/toParamTypeNames(java.lang.reflect.Constructor)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+method:///scala/reflect/api/TypeTags$WeakTypeTag/tpe()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/javaClass()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/constructorParamTypes()|",
      "|java+method:///scala/reflect/api/Mirrors$RuntimeMirror/runtimeClass(scala.reflect.api.Types$TypeApi)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/com$datastax$spark$connector$rdd$reader$AnyObjectFactory$$tpe()|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "affectedLib": "org.scala-lang:scala-reflect:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/types/TypeConverter$class/targetTypeName(com.datastax.spark.connector.types.TypeConverter)|",
    "called": "|java+method:///scala/reflect/runtime/package$/universe()|",
    "v1Body": [
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///scala/reflect/api/TypeTags$TypeTag/tpe()|",
      "|java+method:///com/datastax/spark/connector/types/TypeConverter/targetTypeTag()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/runtime/package$/universe()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///scala/reflect/api/TypeTags/TypeTag()|",
      "|java+method:///scala/reflect/api/TypeTags$TypeTag/tpe()|",
      "|java+method:///com/datastax/spark/connector/types/TypeConverter/targetTypeTag()|"
    ],
    "affectedLib": "org.scala-lang:scala-reflect:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/types/ColumnType$class/scalaTypeName(com.datastax.spark.connector.types.ColumnType)|",
    "called": "|java+method:///scala/reflect/runtime/package$/universe()|",
    "v1Body": [
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///com/datastax/spark/connector/types/ColumnType/scalaTypeTag()|",
      "|java+method:///scala/reflect/api/TypeTags$TypeTag/tpe()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/runtime/package$/universe()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///com/datastax/spark/connector/types/ColumnType/scalaTypeTag()|",
      "|java+method:///scala/reflect/api/TypeTags/TypeTag()|",
      "|java+method:///scala/reflect/api/TypeTags$TypeTag/tpe()|"
    ],
    "affectedLib": "org.scala-lang:scala-reflect:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/constructorParamTypes$lzycompute()|",
    "called": "|java+method:///scala/reflect/runtime/package$/universe()|",
    "v1Body": [
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$$anonfun$constructorParamTypes$1/AnyObjectFactory$$anonfun$constructorParamTypes$1(com.datastax.spark.connector.rdd.reader.AnyObjectFactory)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///scala/collection/LinearSeqOptimized/find(scala.Function1)|",
      "|java+method:///scala/reflect/api/Symbols$TermSymbolApi/alternatives()|",
      "|java+method:///com/datastax/spark/connector/util/Reflect$/constructor(scala.reflect.api.Types$TypeApi)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/javaClass()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$/oneIfMemberClass(java.lang.Class)|",
      "|java+method:///java/lang/reflect/Constructor/getParameterTypes()|",
      "|java+method:///scala/collection/mutable/ArrayOps/drop(int)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$$anonfun$constructorParamTypes$2/AnyObjectFactory$$anonfun$constructorParamTypes$2(com.datastax.spark.connector.rdd.reader.AnyObjectFactory,java.lang.Class%5B%5D)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/javaConstructor()|",
      "|java+method:///scala/reflect/api/Symbols$SymbolApi/asTerm()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/com$datastax$spark$connector$rdd$reader$AnyObjectFactory$$tpe()|",
      "|java+method:///scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/runtime/package$/universe()|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$$anonfun$constructorParamTypes$1/AnyObjectFactory$$anonfun$constructorParamTypes$1(com.datastax.spark.connector.rdd.reader.AnyObjectFactory)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///scala/collection/LinearSeqOptimized/find(scala.Function1)|",
      "|java+method:///scala/reflect/api/TypeTags/TypeTag()|",
      "|java+method:///scala/reflect/api/Symbols$TermSymbolApi/alternatives()|",
      "|java+method:///com/datastax/spark/connector/util/Reflect$/constructor(scala.reflect.api.Types$TypeApi)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/javaClass()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$/oneIfMemberClass(java.lang.Class)|",
      "|java+method:///java/lang/reflect/Constructor/getParameterTypes()|",
      "|java+method:///scala/collection/mutable/ArrayOps/drop(int)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory$$anonfun$constructorParamTypes$2/AnyObjectFactory$$anonfun$constructorParamTypes$2(com.datastax.spark.connector.rdd.reader.AnyObjectFactory,java.lang.Class%5B%5D)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/javaConstructor()|",
      "|java+method:///scala/reflect/api/Symbols$SymbolApi/asTerm()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/com$datastax$spark$connector$rdd$reader$AnyObjectFactory$$tpe()|",
      "|java+method:///scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "affectedLib": "org.scala-lang:scala-reflect:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0-rc3",
    "coordinatesV2": "com.datastax.spark:spark-cassandra-connector_2.10:1.2.0",
    "caller": "|java+constructor:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader/ClassBasedRowReader(com.datastax.spark.connector.cql.TableDef,int,scala.collection.immutable.Map,scala.reflect.api.TypeTags$TypeTag,com.datastax.spark.connector.mapper.ColumnMapper)|",
    "called": "|java+method:///scala/reflect/runtime/package$/universe()|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/RowReader$class/$init$(com.datastax.spark.connector.rdd.reader.RowReader)|",
      "|java+method:///scala/collection/immutable/Map/values()|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader/setterTypes()|",
      "|java+method:///scala/Predef$/implicitly(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///com/datastax/spark/connector/mapper/ColumnMap/constructor()|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/collection/immutable/Iterable$/canBuildFrom()|",
      "|java+method:///scala/collection/Iterable/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/Map/keys()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader/com$datastax$spark$connector$rdd$reader$ClassBasedRowReader$$columnMap()|",
      "|java+method:///scala/reflect/api/TypeTags$WeakTypeTag/tpe()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///com/datastax/spark/connector/mapper/ColumnMapper/columnMap(com.datastax.spark.connector.cql.TableDef,scala.collection.immutable.Map)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/constructorParamTypes()|",
      "|java+method:///com/datastax/spark/connector/mapper/ColumnMap/setters()|",
      "|java+method:///scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader$$anonfun$1/ClassBasedRowReader$$anonfun$1(com.datastax.spark.connector.rdd.reader.ClassBasedRowReader)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader$$anonfun$2/ClassBasedRowReader$$anonfun$2(com.datastax.spark.connector.rdd.reader.ClassBasedRowReader)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader$$anonfun$3/ClassBasedRowReader$$anonfun$3(com.datastax.spark.connector.rdd.reader.ClassBasedRowReader)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/AnyObjectFactory(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader/factory()|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///com/datastax/spark/connector/rdd/reader/RowReader$class/$init$(com.datastax.spark.connector.rdd.reader.RowReader)|",
      "|java+method:///scala/collection/immutable/Map/values()|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader/setterTypes()|",
      "|java+method:///scala/reflect/api/TypeTags/TypeTag()|",
      "|java+method:///scala/Predef$/implicitly(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///com/datastax/spark/connector/mapper/ColumnMap/constructor()|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/collection/immutable/Iterable$/canBuildFrom()|",
      "|java+method:///scala/collection/Iterable/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/Map/keys()|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader/com$datastax$spark$connector$rdd$reader$ClassBasedRowReader$$columnMap()|",
      "|java+method:///scala/reflect/api/TypeTags$WeakTypeTag/tpe()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/reflect/runtime/package$/universe()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///com/datastax/spark/connector/mapper/ColumnMapper/columnMap(com.datastax.spark.connector.cql.TableDef,scala.collection.immutable.Map)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/constructorParamTypes()|",
      "|java+method:///com/datastax/spark/connector/mapper/ColumnMap/setters()|",
      "|java+method:///scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader$$anonfun$1/ClassBasedRowReader$$anonfun$1(com.datastax.spark.connector.rdd.reader.ClassBasedRowReader)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader$$anonfun$2/ClassBasedRowReader$$anonfun$2(com.datastax.spark.connector.rdd.reader.ClassBasedRowReader)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader$$anonfun$3/ClassBasedRowReader$$anonfun$3(com.datastax.spark.connector.rdd.reader.ClassBasedRowReader)|",
      "|java+constructor:///com/datastax/spark/connector/rdd/reader/AnyObjectFactory/AnyObjectFactory(scala.reflect.api.TypeTags$TypeTag)|",
      "|java+method:///com/datastax/spark/connector/rdd/reader/ClassBasedRowReader/factory()|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-reflect:2.10.5",
    "change": "UPDATED"
  }
]