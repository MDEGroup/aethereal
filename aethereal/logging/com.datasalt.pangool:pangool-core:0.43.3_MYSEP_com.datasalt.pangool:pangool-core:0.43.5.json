[
  {
    "coordinatesV1": "com.datasalt.pangool:pangool-core:0.43.5",
    "coordinatesV2": "com.datasalt.pangool:pangool-core:0.43.3",
    "caller": "|java+method:///com/datasalt/pangool/tuplemr/TupleMRBuilder/createJob()|",
    "called": "|java+method:///org/apache/hadoop/conf/Configuration/set(java.lang.String,java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/hadoop/mapreduce/Job/setMapOutputKeyClass(java.lang.Class)|",
      "|java+method:///com/datasalt/pangool/tuplemr/mapred/lib/input/PangoolMultipleInputs/addInputPath(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.InputFormat,org.apache.hadoop.mapreduce.Mapper)|",
      "|java+method:///java/util/Iterator/hasNext()|",
      "|java+method:///com/datasalt/pangool/tuplemr/mapred/lib/output/PangoolMultipleOutputs/addNamedOutputContext(org.apache.hadoop.mapreduce.Job,java.lang.String,java.lang.String,java.lang.String)|",
      "|java+constructor:///java/lang/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setPartitionerClass(java.lang.Class)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setGroupingComparatorClass(java.lang.Class)|",
      "|java+method:///java/util/List/iterator()|",
      "|java+method:///java/util/UUID/toString()|",
      "|java+method:///java/lang/StringBuilder/toString()|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRConfig/getRollupFrom()|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRException/failIfEmpty(java.util.Collection,java.lang.String)|",
      "|java+constructor:///com/datasalt/pangool/tuplemr/TupleMRException/TupleMRException(java.lang.Throwable)|",
      "|java+method:///java/lang/StringBuilder/append(java.lang.String)|",
      "|java+method:///java/util/Map$Entry/getKey()|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setReducerClass(java.lang.Class)|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRBuilder$Input/access$400(com.datasalt.pangool.tuplemr.TupleMRBuilder$Input)|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRBuilder$Input/access$300(com.datasalt.pangool.tuplemr.TupleMRBuilder$Input)|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRBuilder$Output/access$100(com.datasalt.pangool.tuplemr.TupleMRBuilder$Output)|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRBuilder$Input/access$200(com.datasalt.pangool.tuplemr.TupleMRBuilder$Input)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/getConfiguration()|",
      "|java+constructor:///org/apache/hadoop/mapreduce/Job/Job(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/conf/Configuration/set(java.lang.String,java.lang.String)|",
      "|java+method:///java/util/UUID/randomUUID()|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRException/failIfNull(java.lang.Object,java.lang.String)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setOutputValueClass(java.lang.Class)|",
      "|java+method:///java/util/Iterator/next()|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRConfig/set(com.datasalt.pangool.tuplemr.TupleMRConfig,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRBuilder$Output/access$500(com.datasalt.pangool.tuplemr.TupleMRBuilder$Output)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setJarByClass(java.lang.Class)|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRBuilder$Output/access$600(com.datasalt.pangool.tuplemr.TupleMRBuilder$Output)|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRBuilder$Output/access$700(com.datasalt.pangool.tuplemr.TupleMRBuilder$Output)|",
      "|java+method:///com/datasalt/pangool/tuplemr/serialization/TupleSerialization/enableSerialization(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setCombinerClass(java.lang.Class)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setSortComparatorClass(java.lang.Class)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setMapOutputValueClass(java.lang.Class)|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRBuilder$Output/access$800(com.datasalt.pangool.tuplemr.TupleMRBuilder$Output)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/Job/Job(org.apache.hadoop.conf.Configuration,java.lang.String)|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/output/FileOutputFormat/setOutputPath(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path)|",
      "|java+method:///java/util/Set/iterator()|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setOutputFormatClass(java.lang.Class)|",
      "|java+method:///java/lang/StringBuilder/append(char)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+method:///java/util/Map$Entry/getValue()|",
      "|java+method:///java/util/Map/entrySet()|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRBuilder/buildConf()|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setOutputKeyClass(java.lang.Class)|",
      "|java+method:///com/datasalt/pangool/tuplemr/mapred/lib/output/PangoolMultipleOutputs/addNamedOutput(org.apache.hadoop.mapreduce.Job,java.lang.String,org.apache.hadoop.mapreduce.OutputFormat,java.lang.Class,java.lang.Class)|",
      "|java+method:///com/datasalt/pangool/utils/DCUtils/serializeToDC(java.lang.Object,java.lang.String,org.apache.hadoop.conf.Configuration)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/hadoop/mapreduce/Job/setMapOutputKeyClass(java.lang.Class)|",
      "|java+method:///com/datasalt/pangool/tuplemr/MultipleInputsInterface/getMultiInputs()|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setReducerClass(java.lang.Class)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/getConfiguration()|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRConfig/set(com.datasalt.pangool.tuplemr.TupleMRConfig,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setJarByClass(java.lang.Class)|",
      "|java+constructor:///java/lang/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setGroupingComparatorClass(java.lang.Class)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+method:///java/util/UUID/toString()|",
      "|java+method:///com/datasalt/pangool/utils/DCUtils/serializeToDC(java.lang.Object,java.lang.String,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///java/lang/StringBuilder/toString()|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRConfig/getRollupFrom()|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRException/failIfEmpty(java.util.Collection,java.lang.String)|",
      "|java+constructor:///com/datasalt/pangool/tuplemr/TupleMRException/TupleMRException(java.lang.Throwable)|",
      "|java+method:///java/lang/StringBuilder/append(java.lang.String)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/Job/Job(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/conf/Configuration/set(java.lang.String,java.lang.String)|",
      "|java+method:///java/util/UUID/randomUUID()|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRException/failIfNull(java.lang.Object,java.lang.String)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setOutputValueClass(java.lang.Class)|",
      "|java+method:///com/datasalt/pangool/tuplemr/serialization/TupleSerialization/enableSerialization(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setCombinerClass(java.lang.Class)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setSortComparatorClass(java.lang.Class)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setMapOutputValueClass(java.lang.Class)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/Job/Job(org.apache.hadoop.conf.Configuration,java.lang.String)|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/output/FileOutputFormat/setOutputPath(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path)|",
      "|java+method:///com/datasalt/pangool/tuplemr/MultipleInputsInterface/configureJob(org.apache.hadoop.mapreduce.Job)|",
      "|java+method:///com/datasalt/pangool/tuplemr/NamedOutputsInterface/configureJob(org.apache.hadoop.mapreduce.Job)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setPartitionerClass(java.lang.Class)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setOutputFormatClass(java.lang.Class)|",
      "|java+method:///java/lang/StringBuilder/append(char)|",
      "|java+method:///com/datasalt/pangool/tuplemr/TupleMRBuilder/buildConf()|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setOutputKeyClass(java.lang.Class)|"
    ],
    "affectedLib": "org.apache.hadoop:hadoop-core:0.20.2",
    "change": "REMOVED"
  },
  {
    "coordinatesV1": "com.datasalt.pangool:pangool-core:0.43.5",
    "coordinatesV2": "com.datasalt.pangool:pangool-core:0.43.3",
    "caller": "|java+method:///com/datasalt/pangool/tuplemr/MapOnlyJobBuilder/createJob()|",
    "called": "|java+method:///org/apache/hadoop/conf/Configuration/set(java.lang.String,java.lang.String)|",
    "v1Body": [
      "|java+method:///com/datasalt/pangool/tuplemr/mapred/lib/input/PangoolMultipleInputs/addInputPath(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.InputFormat,org.apache.hadoop.mapreduce.Mapper)|",
      "|java+method:///java/util/Iterator/hasNext()|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setNumReduceTasks(int)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/getConfiguration()|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setOutputValueClass(java.lang.Class)|",
      "|java+method:///java/util/Iterator/next()|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setJarByClass(java.lang.Class)|",
      "|java+constructor:///java/lang/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/output/FileOutputFormat/setOutputPath(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+method:///java/util/List/iterator()|",
      "|java+method:///java/util/UUID/toString()|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setOutputKeyClass(java.lang.Class)|",
      "|java+method:///com/datasalt/pangool/utils/DCUtils/serializeToDC(java.lang.Object,java.lang.String,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///java/lang/StringBuilder/toString()|",
      "|java+constructor:///com/datasalt/pangool/tuplemr/TupleMRException/TupleMRException(java.lang.Throwable)|",
      "|java+method:///java/lang/StringBuilder/append(java.lang.String)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/Job/Job(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/conf/Configuration/set(java.lang.String,java.lang.String)|",
      "|java+method:///java/util/UUID/randomUUID()|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setOutputFormatClass(java.lang.Class)|",
      "|java+method:///java/lang/StringBuilder/append(char)|"
    ],
    "v2Body": [
      "|java+method:///java/util/Iterator/hasNext()|",
      "|java+method:///com/datasalt/pangool/tuplemr/MultipleInputsInterface/getMultiInputs()|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setNumReduceTasks(int)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/getConfiguration()|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setOutputValueClass(java.lang.Class)|",
      "|java+method:///java/util/Iterator/next()|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setJarByClass(java.lang.Class)|",
      "|java+constructor:///java/lang/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/output/FileOutputFormat/setOutputPath(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+method:///java/util/UUID/toString()|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setOutputKeyClass(java.lang.Class)|",
      "|java+method:///com/datasalt/pangool/utils/DCUtils/serializeToDC(java.lang.Object,java.lang.String,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///java/lang/StringBuilder/toString()|",
      "|java+constructor:///com/datasalt/pangool/tuplemr/TupleMRException/TupleMRException(java.lang.Throwable)|",
      "|java+method:///java/lang/StringBuilder/append(java.lang.String)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/Job/Job(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/conf/Configuration/set(java.lang.String,java.lang.String)|",
      "|java+method:///java/util/UUID/randomUUID()|",
      "|java+method:///com/datasalt/pangool/tuplemr/MultipleInputsInterface/configureJob(org.apache.hadoop.mapreduce.Job)|",
      "|java+method:///com/datasalt/pangool/tuplemr/NamedOutputsInterface/configureJob(org.apache.hadoop.mapreduce.Job)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/setOutputFormatClass(java.lang.Class)|",
      "|java+method:///java/lang/StringBuilder/append(char)|",
      "|java+method:///java/util/List/iterator()|",
      "|java+constructor:///com/datasalt/pangool/tuplemr/TupleMRException/TupleMRException(java.lang.String)|"
    ],
    "affectedLib": "org.apache.hadoop:hadoop-core:0.20.2",
    "change": "REMOVED"
  },
  {
    "coordinatesV1": "com.datasalt.pangool:pangool-core:0.43.5",
    "coordinatesV2": "com.datasalt.pangool:pangool-core:0.43.3",
    "caller": "|java+method:///com/datasalt/pangool/solr/TupleSolrOutputFormat/setupSolrHomeCache(java.io.File,org.apache.hadoop.conf.Configuration)|",
    "called": "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String,java.lang.String)|",
    "v1Body": [
      "|java+method:///java/lang/StringBuilder/append(java.lang.String)|",
      "|java+method:///java/util/Arrays/asList(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/hadoop/filecache/DistributedCache/getCacheArchives(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///java/util/UUID/randomUUID()|",
      "|java+constructor:///java/lang/StringBuilder/StringBuilder()|",
      "|java+method:///java/io/File/getAbsolutePath()|",
      "|java+method:///java/lang/StringBuilder/append(char)|",
      "|java+method:///java/lang/StringBuilder/append(java.lang.Object)|",
      "|java+method:///java/io/File/createTempFile(java.lang.String,java.lang.String)|",
      "|java+method:///java/util/UUID/toString()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/hadoop/filecache/DistributedCache/addCacheArchive(java.net.URI,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/fs/Path/toString()|",
      "|java+constructor:///java/io/IOException/IOException(java.lang.String)|",
      "|java+method:///com/datasalt/pangool/solr/TupleSolrOutputFormat/createZip(java.io.File,java.io.File)|",
      "|java+method:///java/io/File/isDirectory()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/getUri()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String,java.lang.String)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///org/apache/commons/logging/Log/info(java.lang.Object)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/get(org.apache.hadoop.conf.Configuration)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///java/lang/StringBuilder/toString()|",
      "|java+method:///java/io/File/toString()|",
      "|java+method:///java/net/URI/resolve(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///java/lang/StringBuilder/append(java.lang.String)|",
      "|java+method:///java/util/Arrays/asList(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/hadoop/filecache/DistributedCache/getCacheArchives(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///java/util/UUID/randomUUID()|",
      "|java+method:///java/io/File/exists()|",
      "|java+constructor:///java/lang/StringBuilder/StringBuilder()|",
      "|java+method:///java/io/File/getAbsolutePath()|",
      "|java+method:///java/lang/StringBuilder/append(char)|",
      "|java+method:///java/lang/StringBuilder/append(java.lang.Object)|",
      "|java+method:///java/io/File/createTempFile(java.lang.String,java.lang.String)|",
      "|java+method:///java/util/UUID/toString()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/hadoop/filecache/DistributedCache/addCacheArchive(java.net.URI,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/fs/Path/toString()|",
      "|java+method:///org/apache/commons/logging/Log/debug(java.lang.Object)|",
      "|java+constructor:///java/io/IOException/IOException(java.lang.String)|",
      "|java+method:///com/datasalt/pangool/solr/TupleSolrOutputFormat/createZip(java.io.File,java.io.File)|",
      "|java+method:///java/io/File/isDirectory()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/getUri()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/get(org.apache.hadoop.conf.Configuration)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///java/lang/StringBuilder/toString()|",
      "|java+method:///java/io/File/toString()|",
      "|java+method:///java/net/URI/resolve(java.lang.String)|"
    ],
    "affectedLib": "org.apache.hadoop:hadoop-core:0.20.2",
    "change": "REMOVED"
  },
  {
    "coordinatesV1": "com.datasalt.pangool:pangool-core:0.43.3",
    "coordinatesV2": "com.datasalt.pangool:pangool-core:0.43.5",
    "caller": "|java+method:///com/datasalt/pangool/tuplemr/mapred/lib/input/TupleTextInputFormat$TupleTextInputReader/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)|",
    "called": "|java+constructor:///com/googlecode/jcsv/CSVStrategy/CSVStrategy(char,char,char,boolean,boolean)|",
    "v1Body": [
      "|java+method:///org/apache/hadoop/mapreduce/TaskAttemptContext/getConfiguration()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getStart()|",
      "|java+method:///com/datasalt/pangool/tuplemr/mapred/lib/input/TupleTextInputFormat$TupleTextInputReader/init(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getPath()|",
      "|java+method:///org/apache/hadoop/mapreduce/InputSplit/getLength()|"
    ],
    "v2Body": [
      "|java+constructor:///com/googlecode/jcsv/CSVStrategy/CSVStrategy(char,char,char,boolean,boolean)|",
      "|java+method:///java/lang/Math/min(long,long)|",
      "|java+method:///org/apache/hadoop/mapreduce/TaskAttemptContext/getConfiguration()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getStart()|",
      "|java+method:///org/apache/hadoop/io/compress/CompressionCodecFactory/getCodec(org.apache.hadoop.fs.Path)|",
      "|java+constructor:///org/apache/hadoop/io/compress/CompressionCodecFactory/CompressionCodecFactory(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getPath()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/open(org.apache.hadoop.fs.Path)|",
      "|java+constructor:///org/apache/hadoop/util/LineReader/LineReader(java.io.InputStream,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/fs/FSDataInputStream/seek(long)|",
      "|java+method:///org/apache/hadoop/io/compress/CompressionCodec/createInputStream(java.io.InputStream)|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileSplit/getLength()|",
      "|java+constructor:///org/apache/hadoop/io/Text/Text()|",
      "|java+method:///org/apache/hadoop/util/LineReader/readLine(org.apache.hadoop.io.Text,int,int)|",
      "|java+method:///org/apache/hadoop/conf/Configuration/getInt(java.lang.String,int)|",
      "|java+method:///java/lang/Character/charValue()|"
    ],
    "affectedLib": "com.googlecode.jcsv:jcsv:1.4.0",
    "change": "ADDED"
  }
]