[
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2/apply(java.lang.Class)|",
    "called": "|java+method:///org/apache/xbean/asm5/ClassReader/accept(org.apache.xbean.asm5.ClassVisitor,int)|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/SetLike/$plus$eq(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/MethodIdentifier/MethodIdentifier(java.lang.Class,java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/mutable/Set/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/FieldAccessFinder$$anon$3/org$apache$spark$util$FieldAccessFinder$$anon$$$outer()|",
      "|java+method:///scala/collection/mutable/Map/apply(java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Set/$plus$eq(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/FieldAccessFinder/FieldAccessFinder(scala.collection.mutable.Map,boolean,scala.Option,scala.collection.mutable.Set)|",
      "|java+method:///java/lang/String/endsWith(java.lang.String)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getClassReader(java.lang.Class)|",
      "|java+method:///com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/ClassReader/accept(com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassVisitor,int)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/SetLike/$plus$eq(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/MethodIdentifier/MethodIdentifier(java.lang.Class,java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/mutable/Set/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/FieldAccessFinder$$anon$3/org$apache$spark$util$FieldAccessFinder$$anon$$$outer()|",
      "|java+method:///scala/collection/mutable/Map/apply(java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getClassReader(java.lang.Class)|",
      "|java+method:///scala/collection/mutable/Set/$plus$eq(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/FieldAccessFinder/FieldAccessFinder(scala.collection.mutable.Map,boolean,scala.Option,scala.collection.mutable.Set)|",
      "|java+method:///org/apache/xbean/asm5/ClassReader/accept(org.apache.xbean.asm5.ClassVisitor,int)|",
      "|java+method:///java/lang/String/endsWith(java.lang.String)|"
    ],
    "affectedLib": "org.apache.xbean:xbean-asm5-shaded:4.4",
    "change": "ADDED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$15/apply(java.lang.Class)|",
    "called": "|java+method:///org/apache/xbean/asm5/ClassReader/accept(org.apache.xbean.asm5.ClassVisitor,int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/FieldAccessFinder$/$lessinit$greater$default$4()|",
      "|java+constructor:///org/apache/spark/util/FieldAccessFinder/FieldAccessFinder(scala.collection.mutable.Map,boolean,scala.Option,scala.collection.mutable.Set)|",
      "|java+method:///org/apache/spark/util/FieldAccessFinder$/$lessinit$greater$default$3()|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getClassReader(java.lang.Class)|",
      "|java+method:///com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/ClassReader/accept(com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassVisitor,int)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/FieldAccessFinder$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getClassReader(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/util/FieldAccessFinder/FieldAccessFinder(scala.collection.mutable.Map,boolean,scala.Option,scala.collection.mutable.Set)|",
      "|java+method:///org/apache/xbean/asm5/ClassReader/accept(org.apache.xbean.asm5.ClassVisitor,int)|",
      "|java+method:///org/apache/spark/util/FieldAccessFinder$/$lessinit$greater$default$3()|"
    ],
    "affectedLib": "org.apache.xbean:xbean-asm5-shaded:4.4",
    "change": "ADDED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/ReturnStatementFinder$$anon$2/ReturnStatementFinder$$anon$2(org.apache.spark.util.ReturnStatementFinder)|",
    "called": "|java+constructor:///org/apache/xbean/asm5/MethodVisitor/MethodVisitor(int)|",
    "v1Body": [
      "|java+constructor:///com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/MethodVisitor/MethodVisitor(int)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/xbean/asm5/MethodVisitor/MethodVisitor(int)|"
    ],
    "affectedLib": "org.apache.xbean:xbean-asm5-shaded:4.4",
    "change": "ADDED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/ReturnStatementFinder/ReturnStatementFinder()|",
    "called": "|java+constructor:///org/apache/xbean/asm5/ClassVisitor/ClassVisitor(int)|",
    "v1Body": [
      "|java+constructor:///com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/ClassVisitor/ClassVisitor(int)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/xbean/asm5/ClassVisitor/ClassVisitor(int)|"
    ],
    "affectedLib": "org.apache.xbean:xbean-asm5-shaded:4.4",
    "change": "ADDED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/InnerClosureFinder$$anon$4/InnerClosureFinder$$anon$4(org.apache.spark.util.InnerClosureFinder)|",
    "called": "|java+constructor:///org/apache/xbean/asm5/MethodVisitor/MethodVisitor(int)|",
    "v1Body": [
      "|java+constructor:///com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/MethodVisitor/MethodVisitor(int)|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/xbean/asm5/MethodVisitor/MethodVisitor(int)|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.apache.xbean:xbean-asm5-shaded:4.4",
    "change": "ADDED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/ClosureCleaner$/org$apache$spark$util$ClosureCleaner$$clean(java.lang.Object,boolean,boolean,scala.collection.mutable.Map)|",
    "called": "|java+method:///org/apache/xbean/asm5/ClassReader/accept(org.apache.xbean.asm5.ClassVisitor,int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/ClosureCleaner$/org$apache$spark$util$ClosureCleaner$$isClosure(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$5/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$5(java.lang.reflect.Method%5B%5D)|",
      "|java+method:///java/lang/reflect/Field/setAccessible(boolean)|",
      "|java+method:///java/lang/Class/getDeclaredMethods()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$1/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$1(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$2/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$2(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/org$apache$spark$util$ClosureCleaner$$getOuterClassesAndObjects(java.lang.Object)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14(scala.collection.mutable.Map)|",
      "|java+method:///scala/collection/immutable/List/withFilter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$16/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$16(scala.collection.mutable.Map)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/logDebug(scala.Function0)|",
      "|java+method:///scala/collection/immutable/List/reverse()|",
      "|java+method:///scala/collection/SetLike/contains(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/List/tail()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$4/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$4()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///java/lang/Class/getDeclaredField(java.lang.String)|",
      "|java+method:///scala/collection/immutable/List/foreach(scala.Function1)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/immutable/List/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/Map/contains(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$12/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$12()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$17/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$17()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$13/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$13()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$20/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$20()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$6/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$6()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$21/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$21()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$10/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$10()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$9/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$9(scala.collection.immutable.List)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/ensureSerializable(java.lang.Object)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$23/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$23(scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$8/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$8()|",
      "|java+method:///scala/collection/mutable/Map/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$19/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$19(scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$18/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$18(scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$7/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$7(scala.collection.immutable.List)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$22/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$22(boolean,scala.collection.mutable.Map,scala.runtime.ObjectRef)|",
      "|java+method:///scala/collection/mutable/Map/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$11/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$11(scala.collection.immutable.List)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$3/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$3(java.lang.reflect.Field%5B%5D)|",
      "|java+method:///scala/collection/immutable/List/head()|",
      "|java+method:///java/lang/reflect/Field/set(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/immutable/List/size()|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getClassReader(java.lang.Class)|",
      "|java+method:///com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/ClassReader/accept(com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassVisitor,int)|",
      "|java+method:///scala/collection/immutable/List/$colon$colon(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ReturnStatementFinder/ReturnStatementFinder()|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/logWarning(scala.Function0)|",
      "|java+method:///scala/collection/mutable/Map/isEmpty()|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$24/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$24(java.lang.Object)|",
      "|java+method:///java/lang/Class/getDeclaredFields()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$15/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$15(boolean,scala.collection.mutable.Map)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getInnerClosureClasses(java.lang.Object)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/ClosureCleaner$/org$apache$spark$util$ClosureCleaner$$isClosure(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$5/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$5(java.lang.reflect.Method%5B%5D)|",
      "|java+method:///java/lang/reflect/Field/setAccessible(boolean)|",
      "|java+method:///java/lang/Class/getDeclaredMethods()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$1/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$1(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$2/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$2(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getClassReader(java.lang.Class)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/org$apache$spark$util$ClosureCleaner$$getOuterClassesAndObjects(java.lang.Object)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14(scala.collection.mutable.Map)|",
      "|java+method:///scala/collection/immutable/List/withFilter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$16/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$16(scala.collection.mutable.Map)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/logDebug(scala.Function0)|",
      "|java+method:///scala/collection/immutable/List/reverse()|",
      "|java+method:///scala/collection/SetLike/contains(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/List/tail()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$4/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$4()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///java/lang/Class/getDeclaredField(java.lang.String)|",
      "|java+method:///scala/collection/immutable/List/foreach(scala.Function1)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/immutable/List/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/Map/contains(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$12/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$12()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$17/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$17()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$13/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$13()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$20/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$20()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$6/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$6()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$21/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$21()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$10/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$10()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$9/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$9(scala.collection.immutable.List)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/ensureSerializable(java.lang.Object)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$23/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$23(scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$8/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$8()|",
      "|java+method:///scala/collection/mutable/Map/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$19/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$19(scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$18/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$18(scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$7/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$7(scala.collection.immutable.List)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$22/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$22(boolean,scala.collection.mutable.Map,scala.runtime.ObjectRef)|",
      "|java+method:///scala/collection/mutable/Map/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$11/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$11(scala.collection.immutable.List)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$3/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$3(java.lang.reflect.Field%5B%5D)|",
      "|java+method:///scala/collection/immutable/List/head()|",
      "|java+method:///java/lang/reflect/Field/set(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/immutable/List/size()|",
      "|java+method:///scala/collection/immutable/List/$colon$colon(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ReturnStatementFinder/ReturnStatementFinder()|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/logWarning(scala.Function0)|",
      "|java+method:///scala/collection/mutable/Map/isEmpty()|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$24/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$24(java.lang.Object)|",
      "|java+method:///java/lang/Class/getDeclaredFields()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$15/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$15(boolean,scala.collection.mutable.Map)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getInnerClosureClasses(java.lang.Object)|",
      "|java+method:///org/apache/xbean/asm5/ClassReader/accept(org.apache.xbean.asm5.ClassVisitor,int)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.apache.xbean:xbean-asm5-shaded:4.4",
    "change": "ADDED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/ClosureCleaner$/getInnerClosureClasses(java.lang.Object)|",
    "called": "|java+method:///org/apache/xbean/asm5/ClassReader/accept(org.apache.xbean.asm5.ClassVisitor,int)|",
    "v1Body": [
      "|java+method:///scala/collection/immutable/List/isEmpty()|",
      "|java+method:///scala/collection/immutable/List/tail()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/Set/$minus$minus(scala.collection.GenTraversableOnce)|",
      "|java+method:///scala/collection/mutable/Set/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$getInnerClosureClasses$1/ClosureCleaner$$anonfun$getInnerClosureClasses$1(scala.collection.mutable.Set,scala.runtime.ObjectRef)|",
      "|java+method:///scala/collection/immutable/List/head()|",
      "|java+method:///scala/collection/mutable/Set$/apply(scala.collection.Seq)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+constructor:///org/apache/spark/util/InnerClosureFinder/InnerClosureFinder(scala.collection.mutable.Set)|",
      "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/Set/$minus(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Set/toList()|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getClassReader(java.lang.Class)|",
      "|java+method:///com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/ClassReader/accept(com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassVisitor,int)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/immutable/List/isEmpty()|",
      "|java+method:///scala/collection/immutable/List/tail()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/Set/$minus$minus(scala.collection.GenTraversableOnce)|",
      "|java+method:///scala/collection/mutable/Set/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$getInnerClosureClasses$1/ClosureCleaner$$anonfun$getInnerClosureClasses$1(scala.collection.mutable.Set,scala.runtime.ObjectRef)|",
      "|java+method:///scala/collection/immutable/List/head()|",
      "|java+method:///scala/collection/mutable/Set$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getClassReader(java.lang.Class)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+constructor:///org/apache/spark/util/InnerClosureFinder/InnerClosureFinder(scala.collection.mutable.Set)|",
      "|java+method:///org/apache/xbean/asm5/ClassReader/accept(org.apache.xbean.asm5.ClassVisitor,int)|",
      "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/Set/$minus(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Set/toList()|"
    ],
    "affectedLib": "org.apache.xbean:xbean-asm5-shaded:4.4",
    "change": "ADDED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/InnerClosureFinder/InnerClosureFinder(scala.collection.mutable.Set)|",
    "called": "|java+constructor:///org/apache/xbean/asm5/ClassVisitor/ClassVisitor(int)|",
    "v1Body": [
      "|java+constructor:///com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/ClassVisitor/ClassVisitor(int)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/xbean/asm5/ClassVisitor/ClassVisitor(int)|"
    ],
    "affectedLib": "org.apache.xbean:xbean-asm5-shaded:4.4",
    "change": "ADDED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/FieldAccessFinder/FieldAccessFinder(scala.collection.mutable.Map,boolean,scala.Option,scala.collection.mutable.Set)|",
    "called": "|java+constructor:///org/apache/xbean/asm5/ClassVisitor/ClassVisitor(int)|",
    "v1Body": [
      "|java+constructor:///com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/ClassVisitor/ClassVisitor(int)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/xbean/asm5/ClassVisitor/ClassVisitor(int)|"
    ],
    "affectedLib": "org.apache.xbean:xbean-asm5-shaded:4.4",
    "change": "ADDED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/ReturnStatementFinder$$anon$1/ReturnStatementFinder$$anon$1(org.apache.spark.util.ReturnStatementFinder)|",
    "called": "|java+constructor:///org/apache/xbean/asm5/MethodVisitor/MethodVisitor(int)|",
    "v1Body": [
      "|java+constructor:///com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/MethodVisitor/MethodVisitor(int)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/xbean/asm5/MethodVisitor/MethodVisitor(int)|"
    ],
    "affectedLib": "org.apache.xbean:xbean-asm5-shaded:4.4",
    "change": "ADDED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/FieldAccessFinder$$anon$3/FieldAccessFinder$$anon$3(org.apache.spark.util.FieldAccessFinder)|",
    "called": "|java+constructor:///org/apache/xbean/asm5/MethodVisitor/MethodVisitor(int)|",
    "v1Body": [
      "|java+constructor:///com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/MethodVisitor/MethodVisitor(int)|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/xbean/asm5/MethodVisitor/MethodVisitor(int)|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.apache.xbean:xbean-asm5-shaded:4.4",
    "change": "ADDED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/ClosureCleaner$/getClassReader(java.lang.Class)|",
    "called": "|java+constructor:///org/apache/xbean/asm5/ClassReader/ClassReader(java.io.InputStream)|",
    "v1Body": [
      "|java+method:///java/lang/Class/getResourceAsStream(java.lang.String)|",
      "|java+constructor:///java/io/ByteArrayOutputStream/ByteArrayOutputStream(int)|",
      "|java+constructor:///java/io/ByteArrayInputStream/ByteArrayInputStream(byte%5B%5D)|",
      "|java+method:///java/lang/Class/getName()|",
      "|java+method:///java/lang/String/replaceFirst(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/copyStream(java.io.InputStream,java.io.OutputStream,boolean,boolean)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/util/Utils$/copyStream$default$4()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///java/io/ByteArrayOutputStream/toByteArray()|",
      "|java+constructor:///com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/ClassReader/ClassReader(java.io.InputStream)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Class/getResourceAsStream(java.lang.String)|",
      "|java+constructor:///java/io/ByteArrayOutputStream/ByteArrayOutputStream(int)|",
      "|java+constructor:///java/io/ByteArrayInputStream/ByteArrayInputStream(byte%5B%5D)|",
      "|java+method:///java/lang/Class/getName()|",
      "|java+constructor:///org/apache/xbean/asm5/ClassReader/ClassReader(java.io.InputStream)|",
      "|java+method:///java/lang/String/replaceFirst(java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/util/Utils$/copyStream(java.io.InputStream,java.io.OutputStream,boolean,boolean)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/util/Utils$/copyStream$default$4()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///java/io/ByteArrayOutputStream/toByteArray()|"
    ],
    "affectedLib": "org.apache.xbean:xbean-asm5-shaded:4.4",
    "change": "ADDED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/PythonRunner$/main(java.lang.String%5B%5D)|",
    "called": "|java+method:///py4j/GatewayServer/getListeningPort()|",
    "v1Body": [
      "|java+constructor:///java/lang/ProcessBuilder/ProcessBuilder(java.util.List)|",
      "|java+method:///scala/sys/package$/env()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///java/lang/String/valueOf(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/python/PythonUtils$/mergePythonPaths(scala.collection.Seq)|",
      "|java+method:///py4j/GatewayServer/shutdown()|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPath(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/api/python/PythonUtils$/sparkPythonPath()|",
      "|java+constructor:///org/apache/spark/deploy/PythonRunner$$anonfun$main$1/PythonRunner$$anonfun$main$1()|",
      "|java+constructor:///org/apache/spark/util/RedirectThread/RedirectThread(java.io.InputStream,java.io.OutputStream,java.lang.String,boolean)|",
      "|java+method:///java/lang/Thread/join()|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPaths(java.lang.String,boolean)|",
      "|java+method:///java/lang/ProcessBuilder/environment()|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/lang/ProcessBuilder/redirectErrorStream(boolean)|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPaths$default$2()|",
      "|java+method:///java/lang/Thread/start()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/slice(int,int)|",
      "|java+constructor:///java/lang/Thread/Thread(java.lang.Runnable)|",
      "|java+method:///scala/collection/immutable/Map/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+method:///java/lang/Thread/setDaemon(boolean)|",
      "|java+constructor:///org/apache/spark/SparkUserAppException/SparkUserAppException(int)|",
      "|java+method:///java/lang/Thread/setName(java.lang.String)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/util/RedirectThread$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/util/RedirectThread/start()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPath$default$2()|",
      "|java+constructor:///py4j/GatewayServer/GatewayServer(java.lang.Object,int)|",
      "|java+constructor:///org/apache/spark/deploy/PythonRunner$$anon$1/PythonRunner$$anon$1(py4j.GatewayServer)|",
      "|java+method:///py4j/GatewayServer/getListeningPort()|",
      "|java+constructor:///org/apache/spark/deploy/PythonRunner$$anonfun$1/PythonRunner$$anonfun$1()|",
      "|java+method:///java/lang/Process/waitFor()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$plus$eq(scala.collection.TraversableOnce)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+constructor:///java/lang/ProcessBuilder/ProcessBuilder(java.util.List)|",
      "|java+method:///scala/sys/package$/env()|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///java/lang/String/valueOf(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/python/PythonUtils$/mergePythonPaths(scala.collection.Seq)|",
      "|java+method:///py4j/GatewayServer/shutdown()|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPath(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/api/python/PythonUtils$/sparkPythonPath()|",
      "|java+constructor:///org/apache/spark/deploy/PythonRunner$$anonfun$main$1/PythonRunner$$anonfun$main$1()|",
      "|java+constructor:///org/apache/spark/util/RedirectThread/RedirectThread(java.io.InputStream,java.io.OutputStream,java.lang.String,boolean)|",
      "|java+method:///java/lang/Thread/join()|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPaths(java.lang.String,boolean)|",
      "|java+method:///java/lang/ProcessBuilder/environment()|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/lang/ProcessBuilder/redirectErrorStream(boolean)|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPaths$default$2()|",
      "|java+method:///java/lang/Thread/start()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/slice(int,int)|",
      "|java+constructor:///java/lang/Thread/Thread(java.lang.Runnable)|",
      "|java+method:///scala/collection/immutable/Map/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+method:///java/lang/Thread/setDaemon(boolean)|",
      "|java+constructor:///org/apache/spark/SparkUserAppException/SparkUserAppException(int)|",
      "|java+method:///java/lang/Thread/setName(java.lang.String)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/util/RedirectThread$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/util/RedirectThread/start()|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPath$default$2()|",
      "|java+constructor:///py4j/GatewayServer/GatewayServer(java.lang.Object,int)|",
      "|java+constructor:///org/apache/spark/deploy/PythonRunner$$anon$1/PythonRunner$$anon$1(py4j.GatewayServer)|",
      "|java+method:///py4j/GatewayServer/getListeningPort()|",
      "|java+constructor:///org/apache/spark/deploy/PythonRunner$$anonfun$1/PythonRunner$$anonfun$1()|",
      "|java+method:///java/lang/Process/waitFor()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$plus$eq(scala.collection.TraversableOnce)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "net.sf.py4j:py4j:0.9",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/MesosExternalShuffleBlockHandler(org.apache.spark.network.util.TransportConf)|",
    "called": "|java+constructor:///org/apache/spark/network/shuffle/ExternalShuffleBlockHandler/ExternalShuffleBlockHandler(org.apache.spark.network.util.TransportConf,java.io.File)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/network/shuffle/ExternalShuffleBlockHandler/ExternalShuffleBlockHandler(org.apache.spark.network.util.TransportConf)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/network/shuffle/ExternalShuffleBlockHandler/ExternalShuffleBlockHandler(org.apache.spark.network.util.TransportConf,java.io.File)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|"
    ],
    "affectedLib": "org.apache.spark:spark-network-shuffle_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/CoarseMesosSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.SparkContext,java.lang.String,org.apache.spark.SecurityManager)|",
    "called": "|java+constructor:///org/apache/spark/network/shuffle/mesos/MesosExternalShuffleClient/MesosExternalShuffleClient(org.apache.spark.network.util.TransportConf,org.apache.spark.network.sasl.SecretKeyHolder,boolean,boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/CoarseGrainedSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.rpc.RpcEnv)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/create()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/conf()|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/shuffleServiceEnabled()|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$class/$init$(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|",
      "|java+constructor:///java/util/concurrent/locks/ReentrantLock/ReentrantLock()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/network/netty/SparkTransportConf$/fromSparkConf(org.apache.spark.SparkConf,int)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+constructor:///org/apache/spark/network/shuffle/mesos/MesosExternalShuffleClient/MesosExternalShuffleClient(org.apache.spark.network.util.TransportConf,org.apache.spark.network.sasl.SecretKeyHolder,boolean,boolean)|",
      "|java+method:///org/apache/spark/network/netty/SparkTransportConf$/fromSparkConf$default$2()|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/SecurityManager/isAuthenticationEnabled()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/parseConstraintString(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/SecurityManager/isSaslEncryptionEnabled()|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/CoarseGrainedSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.rpc.RpcEnv)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/create()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/conf()|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/shuffleServiceEnabled()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$class/$init$(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|",
      "|java+constructor:///java/util/concurrent/locks/ReentrantLock/ReentrantLock()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+constructor:///org/apache/spark/network/shuffle/mesos/MesosExternalShuffleClient/MesosExternalShuffleClient(org.apache.spark.network.util.TransportConf,org.apache.spark.network.sasl.SecretKeyHolder,boolean,boolean)|",
      "|java+method:///org/apache/spark/network/netty/SparkTransportConf$/fromSparkConf$default$3()|",
      "|java+method:///org/apache/spark/network/netty/SparkTransportConf$/fromSparkConf(org.apache.spark.SparkConf,java.lang.String,int)|",
      "|java+method:///org/apache/spark/SecurityManager/isAuthenticationEnabled()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/parseConstraintString(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/getRejectOfferDurationForUnmetConstraints(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///org/apache/spark/SecurityManager/isSaslEncryptionEnabled()|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|"
    ],
    "affectedLib": "org.apache.spark:spark-network-shuffle_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/ExternalShuffleService/newShuffleBlockHandler(org.apache.spark.network.util.TransportConf)|",
    "called": "|java+constructor:///org/apache/spark/network/shuffle/ExternalShuffleBlockHandler/ExternalShuffleBlockHandler(org.apache.spark.network.util.TransportConf,java.io.File)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/network/shuffle/ExternalShuffleBlockHandler/ExternalShuffleBlockHandler(org.apache.spark.network.util.TransportConf)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/network/shuffle/ExternalShuffleBlockHandler/ExternalShuffleBlockHandler(org.apache.spark.network.util.TransportConf,java.io.File)|"
    ],
    "affectedLib": "org.apache.spark:spark-network-shuffle_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/handleMessage(org.apache.spark.network.shuffle.protocol.BlockTransferMessage,org.apache.spark.network.client.TransportClient,org.apache.spark.network.client.RpcResponseCallback)|",
    "called": "|java+method:///org/apache/spark/network/shuffle/ExternalShuffleBlockHandler/handleMessage(org.apache.spark.network.shuffle.protocol.BlockTransferMessage,org.apache.spark.network.client.TransportClient,org.apache.spark.network.client.RpcResponseCallback)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/connectedApps()|",
      "|java+constructor:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$1/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$1(org.apache.spark.deploy.mesos.MesosExternalShuffleBlockHandler,java.lang.String,java.net.SocketAddress)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/RegisterDriverParam()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///java/lang/String/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/network/client/RpcResponseCallback/onSuccess(byte%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$2/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$2(org.apache.spark.deploy.mesos.MesosExternalShuffleBlockHandler,java.lang.String,java.net.SocketAddress,java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/network/client/TransportClient/getSocketAddress()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$RegisterDriverParam$/unapply(org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver)|",
      "|java+method:///org/apache/spark/network/shuffle/ExternalShuffleBlockHandler/handleMessage(org.apache.spark.network.shuffle.protocol.BlockTransferMessage,org.apache.spark.network.client.TransportClient,org.apache.spark.network.client.RpcResponseCallback)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/applicationRemoved(java.lang.String,boolean)|"
    ],
    "v2Body": [
      "|java+method:///java/nio/ByteBuffer/allocate(int)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/connectedApps()|",
      "|java+constructor:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$1/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$1(org.apache.spark.deploy.mesos.MesosExternalShuffleBlockHandler,java.lang.String,java.net.SocketAddress)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/RegisterDriverParam()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///java/lang/String/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/network/client/RpcResponseCallback/onSuccess(java.nio.ByteBuffer)|",
      "|java+constructor:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$2/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$2(org.apache.spark.deploy.mesos.MesosExternalShuffleBlockHandler,java.lang.String,java.net.SocketAddress,java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/network/client/TransportClient/getSocketAddress()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$RegisterDriverParam$/unapply(org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver)|",
      "|java+method:///org/apache/spark/network/shuffle/ExternalShuffleBlockHandler/handleMessage(org.apache.spark.network.shuffle.protocol.BlockTransferMessage,org.apache.spark.network.client.TransportClient,org.apache.spark.network.client.RpcResponseCallback)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/applicationRemoved(java.lang.String,boolean)|"
    ],
    "affectedLib": "org.apache.spark:spark-network-shuffle_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/statusUpdate(org.apache.mesos.SchedulerDriver,org.apache.mesos.Protos$TaskStatus)|",
    "called": "|java+method:///org/apache/spark/network/shuffle/mesos/MesosExternalShuffleClient/registerDriverWithShuffleService(java.lang.String,int)|",
    "v1Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$1/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$1(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$3/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$3(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$5/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$5(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,int)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$4/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$4(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,java.lang.String,int,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$slaveIdToHost()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/shuffleServiceEnabled()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///org/apache/spark/network/shuffle/mesos/MesosExternalShuffleClient/registerDriverWithShuffleService(java.lang.String,int)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID/getValue()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/failuresBySlaveId()|",
      "|java+method:///scala/collection/mutable/Map/$minus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/conf()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/taskIdToSlaveId()|",
      "|java+method:///org/apache/spark/TaskState$/isFailed(scala.Enumeration$Value)|",
      "|java+method:///scala/Enumeration$Value/equals(java.lang.Object)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getTaskId()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/TaskState$/fromMesos(org.apache.mesos.Protos$TaskState)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/slaveIdsWithExecutors()|",
      "|java+method:///org/apache/spark/TaskState$/RUNNING()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/MAX_SLAVE_FAILURES()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/stateLock()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$6/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$6(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,java.lang.String)|",
      "|java+method:///scala/collection/mutable/Map/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logInfo(scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/mesos/Protos$TaskID/getValue()|",
      "|java+method:///org/apache/spark/TaskState$/isFinished(scala.Enumeration$Value)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/mesos/SchedulerDriver/reviveOffers()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/executorTerminated(org.apache.mesos.SchedulerDriver,java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$2/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$2(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,int,org.apache.mesos.Protos$TaskState)|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getState()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/mesosExternalShuffleClient()|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getSlaveId()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/assume(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/coresByTaskId()|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/remove(java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/conf()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$1/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$1(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$3/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$3(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$5/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$5(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$4/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$4(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,java.lang.String,int,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$slaveIdToHost()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/shuffleServiceEnabled()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///org/apache/spark/network/shuffle/mesos/MesosExternalShuffleClient/registerDriverWithShuffleService(java.lang.String,int)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID/getValue()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/failuresBySlaveId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/taskIdToSlaveId()|",
      "|java+method:///org/apache/spark/TaskState$/isFailed(scala.Enumeration$Value)|",
      "|java+method:///scala/Enumeration$Value/equals(java.lang.Object)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getTaskId()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/TaskState$/fromMesos(org.apache.mesos.Protos$TaskState)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/slaveIdsWithExecutors()|",
      "|java+method:///org/apache/spark/TaskState$/RUNNING()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/MAX_SLAVE_FAILURES()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/stateLock()|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/get(java.lang.Object)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$6/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$6(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/mesos/Protos$TaskID/getValue()|",
      "|java+method:///org/apache/spark/TaskState$/isFinished(scala.Enumeration$Value)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/remove(java.lang.Object)|",
      "|java+method:///org/apache/mesos/SchedulerDriver/reviveOffers()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/executorTerminated(org.apache.mesos.SchedulerDriver,java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$2/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$2(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,int,org.apache.mesos.Protos$TaskState)|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getState()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/mesosExternalShuffleClient()|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getSlaveId()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/assume(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/coresByTaskId()|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/remove(java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.apache.spark:spark-network-shuffle_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/uploadBlock(java.lang.String,int,java.lang.String,org.apache.spark.storage.BlockId,org.apache.spark.network.buffer.ManagedBuffer,org.apache.spark.storage.StorageLevel)|",
    "called": "|java+constructor:///org/apache/spark/network/shuffle/protocol/UploadBlock/UploadBlock(java.lang.String,java.lang.String,java.lang.String,byte%5B%5D,byte%5B%5D)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/network/netty/NettyBlockTransferService$$anon$2/NettyBlockTransferService$$anon$2(org.apache.spark.network.netty.NettyBlockTransferService,org.apache.spark.storage.BlockId,scala.concurrent.Promise)|",
      "|java+constructor:///org/apache/spark/network/shuffle/protocol/UploadBlock/UploadBlock(java.lang.String,java.lang.String,java.lang.String,byte%5B%5D,byte%5B%5D)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/serializer()|",
      "|java+method:///org/apache/spark/network/shuffle/protocol/UploadBlock/toByteArray()|",
      "|java+method:///java/nio/ByteBuffer/hasArray()|",
      "|java+method:///org/apache/spark/network/client/TransportClientFactory/createClient(java.lang.String,int)|",
      "|java+method:///org/apache/spark/network/buffer/ManagedBuffer/nioByteBuffer()|",
      "|java+method:///scala/concurrent/Promise/future()|",
      "|java+method:///org/apache/spark/serializer/JavaSerializer/newInstance()|",
      "|java+method:///java/nio/ByteBuffer/get(byte%5B%5D)|",
      "|java+method:///org/apache/spark/storage/BlockId/toString()|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/serialize(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///scala/concurrent/Promise$/apply()|",
      "|java+method:///java/nio/ByteBuffer/remaining()|",
      "|java+method:///org/apache/spark/network/client/TransportClient/sendRpc(byte%5B%5D,org.apache.spark.network.client.RpcResponseCallback)|",
      "|java+method:///java/nio/ByteBuffer/array()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/network/netty/NettyBlockTransferService$$anon$2/NettyBlockTransferService$$anon$2(org.apache.spark.network.netty.NettyBlockTransferService,org.apache.spark.storage.BlockId,scala.concurrent.Promise)|",
      "|java+constructor:///org/apache/spark/network/shuffle/protocol/UploadBlock/UploadBlock(java.lang.String,java.lang.String,java.lang.String,byte%5B%5D,byte%5B%5D)|",
      "|java+method:///org/apache/spark/network/client/TransportClient/sendRpc(java.nio.ByteBuffer,org.apache.spark.network.client.RpcResponseCallback)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/serializer()|",
      "|java+method:///java/nio/ByteBuffer/array()|",
      "|java+method:///org/apache/spark/network/client/TransportClientFactory/createClient(java.lang.String,int)|",
      "|java+method:///org/apache/spark/network/buffer/ManagedBuffer/nioByteBuffer()|",
      "|java+method:///scala/concurrent/Promise/future()|",
      "|java+method:///org/apache/spark/serializer/JavaSerializer/newInstance()|",
      "|java+method:///java/nio/ByteBuffer/get(byte%5B%5D)|",
      "|java+method:///org/apache/spark/storage/BlockId/toString()|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/serialize(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///scala/concurrent/Promise$/apply()|",
      "|java+method:///java/nio/ByteBuffer/remaining()|",
      "|java+method:///org/apache/spark/network/shuffle/protocol/UploadBlock/toByteBuffer()|",
      "|java+method:///java/nio/ByteBuffer/hasArray()|"
    ],
    "affectedLib": "org.apache.spark:spark-network-shuffle_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader/UnsafeSorterSpillReader(org.apache.spark.storage.BlockManager,java.io.File,org.apache.spark.storage.BlockId)|",
    "called": "|java+method:///org/spark-project/guava/io/Closeables/close(java.io.Closeable,boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/BlockManager/wrapForCompression(org.apache.spark.storage.BlockId,java.io.InputStream)|",
      "|java+method:///java/io/DataInputStream/readInt()|",
      "|java+constructor:///java/io/DataInputStream/DataInputStream(java.io.InputStream)|",
      "|java+constructor:///java/io/BufferedInputStream/BufferedInputStream(java.io.InputStream)|",
      "|java+method:///java/io/File/length()|",
      "|java+constructor:///java/lang/AssertionError/AssertionError()|",
      "|java+constructor:///java/io/FileInputStream/FileInputStream(java.io.File)|",
      "|java+constructor:///org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator/UnsafeSorterIterator()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/BlockManager/wrapForCompression(org.apache.spark.storage.BlockId,java.io.InputStream)|",
      "|java+method:///org/spark-project/guava/io/Closeables/close(java.io.Closeable,boolean)|",
      "|java+method:///java/io/DataInputStream/readInt()|",
      "|java+constructor:///java/io/DataInputStream/DataInputStream(java.io.InputStream)|",
      "|java+constructor:///java/io/BufferedInputStream/BufferedInputStream(java.io.InputStream)|",
      "|java+method:///java/io/File/length()|",
      "|java+constructor:///java/lang/AssertionError/AssertionError()|",
      "|java+constructor:///java/io/FileInputStream/FileInputStream(java.io.File)|",
      "|java+constructor:///org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator/UnsafeSorterIterator()|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/executorTerminated(org.apache.mesos.SchedulerDriver,java.lang.String,java.lang.String)|",
    "called": "|java+method:///org/spark-project/guava/collect/HashBiMap/inverse()|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/removeExecutor(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$pendingRemovedSlaveIds()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/taskIdToSlaveId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/stateLock()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/remove(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/sparkExecutorId(java.lang.String,java.lang.String)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/inverse()|",
      "|java+method:///org/spark-project/guava/collect/BiMap/get(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Map/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/slaveIdsWithExecutors()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$pendingRemovedSlaveIds()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+constructor:///org/apache/spark/scheduler/SlaveLost/SlaveLost(java.lang.String)|",
      "|java+method:///org/spark-project/guava/collect/BiMap/containsKey(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/taskIdToSlaveId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/slaveIdsWithExecutors()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/removeExecutor(java.lang.String,org.apache.spark.scheduler.ExecutorLossReason)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/stateLock()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/remove(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/sparkExecutorId(java.lang.String,java.lang.String)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/inverse()|",
      "|java+method:///org/spark-project/guava/collect/BiMap/get(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/DriverRunner/org$apache$spark$deploy$worker$DriverRunner$$initialize$1(java.lang.Process,java.lang.ProcessBuilder,java.io.File)|",
    "called": "|java+method:///org/spark-project/guava/io/Files/append(java.lang.CharSequence,java.io.File,java.nio.charset.Charset)|",
    "v1Body": [
      "|java+method:///java/lang/Process/getErrorStream()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/CommandUtils$/redirectStream(java.io.InputStream,java.io.File)|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/spark-project/guava/io/Files/append(java.lang.CharSequence,java.io.File,java.nio.charset.Charset)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///scala/collection/mutable/Buffer/mkString(java.lang.String,java.lang.String,java.lang.String)|",
      "|java+method:///java/lang/ProcessBuilder/command()|",
      "|java+method:///scala/collection/immutable/StringOps/$times(int)|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Process/getErrorStream()|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/CommandUtils$/redirectStream(java.io.InputStream,java.io.File)|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String,java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/spark-project/guava/io/Files/append(java.lang.CharSequence,java.io.File,java.nio.charset.Charset)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///java/lang/ProcessBuilder/command()|",
      "|java+method:///scala/collection/immutable/StringOps/$times(int)|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$class/parseConstraintString(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils,java.lang.String)|",
    "called": "|java+method:///org/spark-project/guava/base/Splitter/withKeyValueSeparator(char)|",
    "v1Body": [
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/spark-project/guava/base/Splitter/trimResults()|",
      "|java+method:///org/spark-project/guava/base/Splitter/withKeyValueSeparator(char)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/spark-project/guava/base/Splitter/on(char)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$parseConstraintString$1/MesosSchedulerUtils$$anonfun$parseConstraintString$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|",
      "|java+method:///scala/collection/mutable/Map$/canBuildFrom()|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///scala/collection/mutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/MapLike/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+method:///java/lang/String/isEmpty()|",
      "|java+method:///org/spark-project/guava/base/Splitter$MapSplitter/split(java.lang.CharSequence)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/spark-project/guava/base/Splitter/trimResults()|",
      "|java+method:///scala/collection/immutable/Map/mapValues(scala.Function1)|",
      "|java+method:///org/spark-project/guava/base/Splitter/withKeyValueSeparator(char)|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///org/spark-project/guava/base/Splitter/on(char)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///java/lang/String/isEmpty()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$parseConstraintString$1/MesosSchedulerUtils$$anonfun$parseConstraintString$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/spark-project/guava/base/Splitter$MapSplitter/split(java.lang.CharSequence)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/CoarseMesosSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.SparkContext,java.lang.String,org.apache.spark.SecurityManager)|",
    "called": "|java+method:///org/spark-project/guava/collect/HashBiMap/create()|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/CoarseGrainedSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.rpc.RpcEnv)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/create()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/conf()|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/shuffleServiceEnabled()|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$class/$init$(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|",
      "|java+constructor:///java/util/concurrent/locks/ReentrantLock/ReentrantLock()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/network/netty/SparkTransportConf$/fromSparkConf(org.apache.spark.SparkConf,int)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+constructor:///org/apache/spark/network/shuffle/mesos/MesosExternalShuffleClient/MesosExternalShuffleClient(org.apache.spark.network.util.TransportConf,org.apache.spark.network.sasl.SecretKeyHolder,boolean,boolean)|",
      "|java+method:///org/apache/spark/network/netty/SparkTransportConf$/fromSparkConf$default$2()|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/SecurityManager/isAuthenticationEnabled()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/parseConstraintString(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/SecurityManager/isSaslEncryptionEnabled()|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/CoarseGrainedSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.rpc.RpcEnv)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/create()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/conf()|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/shuffleServiceEnabled()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$class/$init$(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|",
      "|java+constructor:///java/util/concurrent/locks/ReentrantLock/ReentrantLock()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+constructor:///org/apache/spark/network/shuffle/mesos/MesosExternalShuffleClient/MesosExternalShuffleClient(org.apache.spark.network.util.TransportConf,org.apache.spark.network.sasl.SecretKeyHolder,boolean,boolean)|",
      "|java+method:///org/apache/spark/network/netty/SparkTransportConf$/fromSparkConf$default$3()|",
      "|java+method:///org/apache/spark/network/netty/SparkTransportConf$/fromSparkConf(org.apache.spark.SparkConf,java.lang.String,int)|",
      "|java+method:///org/apache/spark/SecurityManager/isAuthenticationEnabled()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/parseConstraintString(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/getRejectOfferDurationForUnmetConstraints(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///org/apache/spark/SecurityManager/isSaslEncryptionEnabled()|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/init(org.apache.spark.network.BlockDataManager)|",
    "called": "|java+constructor:///org/apache/spark/network/sasl/SaslClientBootstrap/SaslClientBootstrap(org.apache.spark.network.util.TransportConf,java.lang.String,org.apache.spark.network.sasl.SecretKeyHolder,boolean)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/network/sasl/SaslServerBootstrap/SaslServerBootstrap(org.apache.spark.network.util.TransportConf,org.apache.spark.network.sasl.SecretKeyHolder)|",
      "|java+method:///scala/Option/toList()|",
      "|java+constructor:///org/apache/spark/network/netty/NettyBlockTransferService$$anonfun$init$1/NettyBlockTransferService$$anonfun$init$1(org.apache.spark.network.netty.NettyBlockTransferService)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/authEnabled()|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/createServer(scala.collection.immutable.List)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/serializer()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/network/sasl/SaslClientBootstrap/SaslClientBootstrap(org.apache.spark.network.util.TransportConf,java.lang.String,org.apache.spark.network.sasl.SecretKeyHolder,boolean)|",
      "|java+constructor:///org/apache/spark/network/TransportContext/TransportContext(org.apache.spark.network.util.TransportConf,org.apache.spark.network.server.RpcHandler)|",
      "|java+method:///org/apache/spark/SecurityManager/isSaslEncryptionEnabled()|",
      "|java+constructor:///org/apache/spark/network/netty/NettyBlockRpcServer/NettyBlockRpcServer(org.apache.spark.serializer.Serializer,org.apache.spark.network.BlockDataManager)|",
      "|java+method:///org/apache/spark/network/TransportContext/createClientFactory(java.util.List)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/transportConf()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkConf/getAppId()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/network/sasl/SaslServerBootstrap/SaslServerBootstrap(org.apache.spark.network.util.TransportConf,org.apache.spark.network.sasl.SecretKeyHolder)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
      "|java+method:///scala/Option/toList()|",
      "|java+constructor:///org/apache/spark/network/netty/NettyBlockTransferService$$anonfun$init$1/NettyBlockTransferService$$anonfun$init$1(org.apache.spark.network.netty.NettyBlockTransferService)|",
      "|java+method:///scala/collection/Iterable/toSeq()|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/authEnabled()|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/createServer(scala.collection.immutable.List)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/serializer()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkConf/getAppId()|",
      "|java+constructor:///org/apache/spark/network/sasl/SaslClientBootstrap/SaslClientBootstrap(org.apache.spark.network.util.TransportConf,java.lang.String,org.apache.spark.network.sasl.SecretKeyHolder,boolean)|",
      "|java+constructor:///org/apache/spark/network/TransportContext/TransportContext(org.apache.spark.network.util.TransportConf,org.apache.spark.network.server.RpcHandler)|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SecurityManager/isSaslEncryptionEnabled()|",
      "|java+constructor:///org/apache/spark/network/netty/NettyBlockRpcServer/NettyBlockRpcServer(java.lang.String,org.apache.spark.serializer.Serializer,org.apache.spark.network.BlockDataManager)|",
      "|java+method:///org/apache/spark/network/TransportContext/createClientFactory(java.util.List)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/transportConf()|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/TachyonBlockManager/getBytes(org.apache.spark.storage.BlockId)|",
    "called": "|java+method:///org/spark-project/guava/io/ByteStreams/readFully(java.io.InputStream,byte%5B%5D)|",
    "v1Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///tachyon/client/TachyonFile/getLocationHosts()|",
      "|java+method:///tachyon/client/InStream/close()|",
      "|java+method:///tachyon/client/TachyonFile/length()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/spark-project/guava/io/ByteStreams/readFully(java.io.InputStream,byte%5B%5D)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/storage/TachyonBlockManager$$anonfun$getBytes$1/TachyonBlockManager$$anonfun$getBytes$1(org.apache.spark.storage.TachyonBlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///java/util/List/size()|",
      "|java+method:///tachyon/client/TachyonFile/getInStream(tachyon.client.ReadType)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/getFile(org.apache.spark.storage.BlockId)|",
      "|java+method:///java/nio/ByteBuffer/wrap(byte%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///tachyon/client/TachyonFile/getLocationHosts()|",
      "|java+method:///tachyon/client/TachyonFile/length()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/spark-project/guava/io/ByteStreams/readFully(java.io.InputStream,byte%5B%5D)|",
      "|java+method:///tachyon/client/file/FileInStream/close()|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/storage/TachyonBlockManager$$anonfun$getBytes$1/TachyonBlockManager$$anonfun$getBytes$1(org.apache.spark.storage.TachyonBlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///java/util/List/size()|",
      "|java+method:///tachyon/client/TachyonFile/getInStream(tachyon.client.ReadType)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/getFile(org.apache.spark.storage.BlockId)|",
      "|java+method:///java/nio/ByteBuffer/wrap(byte%5B%5D)|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/org$apache$spark$network$netty$NettyBlockTransferService$$startService$1(int,scala.collection.immutable.List)|",
    "called": "|java+method:///org/apache/spark/network/TransportContext/createServer(int,java.util.List)|",
    "v1Body": [
      "|java+method:///org/apache/spark/network/server/TransportServer/getPort()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/network/TransportContext/createServer(int,java.util.List)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///org/apache/spark/network/server/TransportServer/getPort()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/network/TransportContext/createServer(int,java.util.List)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/TestUtils$/createCompiledClass(java.lang.String,java.io.File,org.apache.spark.TestUtils$JavaSourceFromString,scala.collection.Seq)|",
    "called": "|java+method:///org/spark-project/guava/io/Files/move(java.io.File,java.io.File)|",
    "v1Body": [
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///javax/tools/ToolProvider/getSystemJavaCompiler()|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$5/TestUtils$$anonfun$5()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///javax/tools/JavaCompiler/getTask(java.io.Writer,javax.tools.JavaFileManager,javax.tools.DiagnosticListener,java.lang.Iterable,java.lang.Iterable,java.lang.Iterable)|",
      "|java+method:///org/spark-project/guava/io/Files/move(java.io.File,java.io.File)|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///javax/tools/JavaCompiler$CompilationTask/call()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/Predef$/assert(boolean,scala.Function0)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$createCompiledClass$1/TestUtils$$anonfun$createCompiledClass$1(java.io.File)|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$createCompiledClass$2/TestUtils$$anonfun$createCompiledClass$2(java.io.File)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///javax/tools/ToolProvider/getSystemJavaCompiler()|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$6/TestUtils$$anonfun$6()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///java/util/Arrays/asList(java.lang.Object%5B%5D)|",
      "|java+method:///javax/tools/JavaCompiler/getTask(java.io.Writer,javax.tools.JavaFileManager,javax.tools.DiagnosticListener,java.lang.Iterable,java.lang.Iterable,java.lang.Iterable)|",
      "|java+method:///org/spark-project/guava/io/Files/move(java.io.File,java.io.File)|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///javax/tools/JavaCompiler$CompilationTask/call()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/Predef$/assert(boolean,scala.Function0)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$createCompiledClass$1/TestUtils$$anonfun$createCompiledClass$1(java.io.File)|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$createCompiledClass$2/TestUtils$$anonfun$createCompiledClass$2(java.io.File)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SecurityManager/generateSecretKey()|",
    "called": "|java+method:///org/spark-project/guava/hash/HashCode/toString()|",
    "v1Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///akka/util/Crypt$/generateSecureCookie()|",
      "|java+method:///org/apache/spark/SecurityManager$/SPARK_AUTH_SECRET_CONF()|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///org/apache/spark/SecurityManager/isAuthenticationEnabled()|",
      "|java+constructor:///org/apache/hadoop/io/Text/Text(byte%5B%5D)|",
      "|java+method:///org/apache/hadoop/io/Text/toString()|",
      "|java+method:///org/apache/spark/SecurityManager/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/getSecretKeyFromUserCredentials(java.lang.String)|",
      "|java+method:///scala/sys/package$/env()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/addSecretKeyToUserCredentials(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SecurityManager/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/SecurityManager$/ENV_AUTH_SECRET()|",
      "|java+constructor:///org/apache/spark/SecurityManager$$anonfun$9/SecurityManager$$anonfun$9(org.apache.spark.SecurityManager)|",
      "|java+constructor:///org/apache/spark/SecurityManager$$anonfun$8/SecurityManager$$anonfun$8(org.apache.spark.SecurityManager)|",
      "|java+constructor:///org/apache/spark/SecurityManager$$anonfun$7/SecurityManager$$anonfun$7(org.apache.spark.SecurityManager)|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/isYarnMode()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/Exception/Exception(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/SecurityManager/sparkSecretLookupKey()|",
      "|java+constructor:///org/apache/spark/SecurityManager$$anonfun$10/SecurityManager$$anonfun$10(org.apache.spark.SecurityManager)|"
    ],
    "v2Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///org/apache/spark/SecurityManager$/SPARK_AUTH_SECRET_CONF()|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/SecurityManager/isAuthenticationEnabled()|",
      "|java+constructor:///java/security/SecureRandom/SecureRandom()|",
      "|java+constructor:///org/apache/hadoop/io/Text/Text(byte%5B%5D)|",
      "|java+method:///org/apache/hadoop/io/Text/toString()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/getSecretKeyFromUserCredentials(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/addSecretKeyToUserCredentials(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SecurityManager/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/SecurityManager$/ENV_AUTH_SECRET()|",
      "|java+constructor:///org/apache/spark/SecurityManager$$anonfun$7/SecurityManager$$anonfun$7(org.apache.spark.SecurityManager)|",
      "|java+constructor:///org/apache/spark/SecurityManager$$anonfun$generateSecretKey$1/SecurityManager$$anonfun$generateSecretKey$1(org.apache.spark.SecurityManager)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkConf/getenv(java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/SecurityManager$/SECRET_LOOKUP_KEY()|",
      "|java+method:///java/security/SecureRandom/nextBytes(byte%5B%5D)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/isYarnMode()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///org/spark-project/guava/hash/HashCode/toString()|",
      "|java+method:///org/spark-project/guava/hash/HashCodes/fromBytes(byte%5B%5D)|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver/getBlockData(org.apache.spark.storage.ShuffleBlockId)|",
    "called": "|java+constructor:///org/apache/spark/network/buffer/FileSegmentManagedBuffer/FileSegmentManagedBuffer(org.apache.spark.network.util.TransportConf,java.io.File,long,long)|",
    "v1Body": [
      "|java+method:///java/util/Iterator/hasNext()|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$ShuffleFileGroup/getFileSegmentFor(int,int)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver/transportConf()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockId/mapId()|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver/org$apache$spark$shuffle$FileShuffleBlockResolver$$shuffleStates()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/storage/FileSegment/length()|",
      "|java+constructor:///java/lang/IllegalStateException/IllegalStateException(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/FileSegment/offset()|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$ShuffleState/allFileGroups()|",
      "|java+method:///org/apache/spark/storage/BlockManager/diskBlockManager()|",
      "|java+method:///org/apache/spark/storage/DiskBlockManager/getFile(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockId/shuffleId()|",
      "|java+method:///org/apache/spark/storage/FileSegment/file()|",
      "|java+constructor:///org/apache/spark/network/buffer/FileSegmentManagedBuffer/FileSegmentManagedBuffer(org.apache.spark.network.util.TransportConf,java.io.File,long,long)|",
      "|java+method:///java/util/Iterator/next()|",
      "|java+method:///java/util/concurrent/ConcurrentLinkedQueue/iterator()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///java/io/File/length()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver/org$apache$spark$shuffle$FileShuffleBlockResolver$$consolidateShuffleFiles()|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver/org$apache$spark$shuffle$FileShuffleBlockResolver$$blockManager()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockId/reduceId()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver/transportConf()|",
      "|java+constructor:///org/apache/spark/network/buffer/FileSegmentManagedBuffer/FileSegmentManagedBuffer(org.apache.spark.network.util.TransportConf,java.io.File,long,long)|",
      "|java+method:///java/io/File/length()|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver/org$apache$spark$shuffle$FileShuffleBlockResolver$$blockManager()|",
      "|java+method:///org/apache/spark/storage/BlockManager/diskBlockManager()|",
      "|java+method:///org/apache/spark/storage/DiskBlockManager/getFile(org.apache.spark.storage.BlockId)|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/statusUpdate(org.apache.mesos.SchedulerDriver,org.apache.mesos.Protos$TaskStatus)|",
    "called": "|java+method:///org/spark-project/guava/collect/HashBiMap/get(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$1/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$1(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$3/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$3(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$5/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$5(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,int)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$4/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$4(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,java.lang.String,int,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$slaveIdToHost()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/shuffleServiceEnabled()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///org/apache/spark/network/shuffle/mesos/MesosExternalShuffleClient/registerDriverWithShuffleService(java.lang.String,int)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID/getValue()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/failuresBySlaveId()|",
      "|java+method:///scala/collection/mutable/Map/$minus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/conf()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/taskIdToSlaveId()|",
      "|java+method:///org/apache/spark/TaskState$/isFailed(scala.Enumeration$Value)|",
      "|java+method:///scala/Enumeration$Value/equals(java.lang.Object)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getTaskId()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/TaskState$/fromMesos(org.apache.mesos.Protos$TaskState)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/slaveIdsWithExecutors()|",
      "|java+method:///org/apache/spark/TaskState$/RUNNING()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/MAX_SLAVE_FAILURES()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/stateLock()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$6/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$6(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,java.lang.String)|",
      "|java+method:///scala/collection/mutable/Map/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logInfo(scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/mesos/Protos$TaskID/getValue()|",
      "|java+method:///org/apache/spark/TaskState$/isFinished(scala.Enumeration$Value)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/mesos/SchedulerDriver/reviveOffers()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/executorTerminated(org.apache.mesos.SchedulerDriver,java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$2/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$2(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,int,org.apache.mesos.Protos$TaskState)|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getState()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/mesosExternalShuffleClient()|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getSlaveId()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/assume(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/coresByTaskId()|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/remove(java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/conf()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$1/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$1(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$3/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$3(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$5/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$5(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$4/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$4(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,java.lang.String,int,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$slaveIdToHost()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/shuffleServiceEnabled()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///org/apache/spark/network/shuffle/mesos/MesosExternalShuffleClient/registerDriverWithShuffleService(java.lang.String,int)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID/getValue()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/failuresBySlaveId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/taskIdToSlaveId()|",
      "|java+method:///org/apache/spark/TaskState$/isFailed(scala.Enumeration$Value)|",
      "|java+method:///scala/Enumeration$Value/equals(java.lang.Object)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getTaskId()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/TaskState$/fromMesos(org.apache.mesos.Protos$TaskState)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/slaveIdsWithExecutors()|",
      "|java+method:///org/apache/spark/TaskState$/RUNNING()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/MAX_SLAVE_FAILURES()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/stateLock()|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/get(java.lang.Object)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$6/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$6(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/mesos/Protos$TaskID/getValue()|",
      "|java+method:///org/apache/spark/TaskState$/isFinished(scala.Enumeration$Value)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/remove(java.lang.Object)|",
      "|java+method:///org/apache/mesos/SchedulerDriver/reviveOffers()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/executorTerminated(org.apache.mesos.SchedulerDriver,java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$2/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$2(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,int,org.apache.mesos.Protos$TaskState)|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getState()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/mesosExternalShuffleClient()|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getSlaveId()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/assume(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/coresByTaskId()|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/remove(java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/HttpFileServer/addFileToDir(java.io.File,java.io.File)|",
    "called": "|java+method:///org/spark-project/guava/io/Files/copy(java.io.File,java.io.File)|",
    "v1Body": [
      "|java+method:///java/io/File/getName()|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///java/io/File/isDirectory()|",
      "|java+method:///org/spark-project/guava/io/Files/copy(java.io.File,java.io.File)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Predef$/any2stringadd(java.lang.Object)|",
      "|java+method:///scala/runtime/StringAdd$/$plus$extension(java.lang.Object,java.lang.String)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///java/io/File/getName()|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///java/io/File/isDirectory()|",
      "|java+method:///org/apache/spark/util/Utils$/encodeFileNameToURIRawPath(java.lang.String)|",
      "|java+method:///org/spark-project/guava/io/Files/copy(java.io.File,java.io.File)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Predef$/any2stringadd(java.lang.Object)|",
      "|java+method:///scala/runtime/StringAdd$/$plus$extension(java.lang.Object,java.lang.String)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/SizeEstimator$/SizeEstimator$()|",
    "called": "|java+constructor:///org/spark-project/guava/collect/MapMaker/MapMaker()|",
    "v1Body": [
      "|java+constructor:///java/util/concurrent/ConcurrentHashMap/ConcurrentHashMap()|",
      "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/initialize()|",
      "|java+method:///scala/Predef$/wrapIntArray(int%5B%5D)|"
    ],
    "v2Body": [
      "|java+constructor:///org/spark-project/guava/collect/MapMaker/MapMaker()|",
      "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/spark-project/guava/collect/MapMaker/weakKeys()|",
      "|java+method:///org/spark-project/guava/collect/MapMaker/makeMap()|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/initialize()|",
      "|java+method:///scala/Predef$/wrapIntArray(int%5B%5D)|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/input/WholeTextFileRecordReader/nextKeyValue()|",
    "called": "|java+method:///org/spark-project/guava/io/Closeables/close(java.io.Closeable,boolean)|",
    "v1Body": [
      "|java+method:///org/apache/hadoop/io/compress/CompressionCodecFactory/getCodec(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/hadoop/io/compress/CompressionCodec/createInputStream(java.io.InputStream)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/open(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/spark-project/guava/io/Closeables/close(java.io.Closeable,boolean)|",
      "|java+constructor:///org/apache/hadoop/conf/Configuration/Configuration()|",
      "|java+constructor:///org/apache/hadoop/io/compress/CompressionCodecFactory/CompressionCodecFactory(org.apache.hadoop.conf.Configuration)|",
      "|java+constructor:///org/apache/hadoop/io/Text/Text(byte%5B%5D)|",
      "|java+method:///org/spark-project/guava/io/ByteStreams/toByteArray(java.io.InputStream)|",
      "|java+method:///org/apache/hadoop/io/Text/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/hadoop/io/compress/CompressionCodecFactory/getCodec(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/spark-project/guava/io/ByteStreams/toByteArray(java.io.InputStream)|",
      "|java+method:///org/apache/hadoop/io/compress/CompressionCodec/createInputStream(java.io.InputStream)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/open(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/spark-project/guava/io/Closeables/close(java.io.Closeable,boolean)|",
      "|java+constructor:///org/apache/hadoop/conf/Configuration/Configuration()|",
      "|java+constructor:///org/apache/hadoop/io/compress/CompressionCodecFactory/CompressionCodecFactory(org.apache.hadoop.conf.Configuration)|",
      "|java+constructor:///org/apache/hadoop/io/Text/Text(byte%5B%5D)|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/TestUtils$$anonfun$createJar$1/apply(java.io.File)|",
    "called": "|java+method:///org/spark-project/guava/io/ByteStreams/copy(java.io.InputStream,java.io.OutputStream)|",
    "v1Body": [
      "|java+method:///org/spark-project/guava/io/ByteStreams/copy(java.io.InputStream,java.io.OutputStream)|",
      "|java+method:///java/io/File/getName()|",
      "|java+method:///java/util/jar/JarOutputStream/putNextEntry(java.util.zip.ZipEntry)|",
      "|java+method:///java/io/FileInputStream/close()|",
      "|java+constructor:///java/util/jar/JarEntry/JarEntry(java.lang.String)|",
      "|java+constructor:///java/io/FileInputStream/FileInputStream(java.io.File)|"
    ],
    "v2Body": [
      "|java+method:///org/spark-project/guava/io/ByteStreams/copy(java.io.InputStream,java.io.OutputStream)|",
      "|java+method:///java/nio/file/Paths/get(java.lang.String,java.lang.String%5B%5D)|",
      "|java+method:///java/util/jar/JarOutputStream/putNextEntry(java.util.zip.ZipEntry)|",
      "|java+constructor:///java/util/jar/JarEntry/JarEntry(java.lang.String)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///java/io/FileInputStream/FileInputStream(java.io.File)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/io/File/getName()|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$createJar$1$$anonfun$5/TestUtils$$anonfun$createJar$1$$anonfun$5(org.apache.spark.TestUtils$$anonfun$createJar$1)|",
      "|java+method:///java/io/FileInputStream/close()|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/uploadBlock(java.lang.String,int,java.lang.String,org.apache.spark.storage.BlockId,org.apache.spark.network.buffer.ManagedBuffer,org.apache.spark.storage.StorageLevel)|",
    "called": "|java+method:///org/apache/spark/network/client/TransportClient/sendRpc(java.nio.ByteBuffer,org.apache.spark.network.client.RpcResponseCallback)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/network/netty/NettyBlockTransferService$$anon$2/NettyBlockTransferService$$anon$2(org.apache.spark.network.netty.NettyBlockTransferService,org.apache.spark.storage.BlockId,scala.concurrent.Promise)|",
      "|java+constructor:///org/apache/spark/network/shuffle/protocol/UploadBlock/UploadBlock(java.lang.String,java.lang.String,java.lang.String,byte%5B%5D,byte%5B%5D)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/serializer()|",
      "|java+method:///org/apache/spark/network/shuffle/protocol/UploadBlock/toByteArray()|",
      "|java+method:///java/nio/ByteBuffer/hasArray()|",
      "|java+method:///org/apache/spark/network/client/TransportClientFactory/createClient(java.lang.String,int)|",
      "|java+method:///org/apache/spark/network/buffer/ManagedBuffer/nioByteBuffer()|",
      "|java+method:///scala/concurrent/Promise/future()|",
      "|java+method:///org/apache/spark/serializer/JavaSerializer/newInstance()|",
      "|java+method:///java/nio/ByteBuffer/get(byte%5B%5D)|",
      "|java+method:///org/apache/spark/storage/BlockId/toString()|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/serialize(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///scala/concurrent/Promise$/apply()|",
      "|java+method:///java/nio/ByteBuffer/remaining()|",
      "|java+method:///org/apache/spark/network/client/TransportClient/sendRpc(byte%5B%5D,org.apache.spark.network.client.RpcResponseCallback)|",
      "|java+method:///java/nio/ByteBuffer/array()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/network/netty/NettyBlockTransferService$$anon$2/NettyBlockTransferService$$anon$2(org.apache.spark.network.netty.NettyBlockTransferService,org.apache.spark.storage.BlockId,scala.concurrent.Promise)|",
      "|java+constructor:///org/apache/spark/network/shuffle/protocol/UploadBlock/UploadBlock(java.lang.String,java.lang.String,java.lang.String,byte%5B%5D,byte%5B%5D)|",
      "|java+method:///org/apache/spark/network/client/TransportClient/sendRpc(java.nio.ByteBuffer,org.apache.spark.network.client.RpcResponseCallback)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/serializer()|",
      "|java+method:///java/nio/ByteBuffer/array()|",
      "|java+method:///org/apache/spark/network/client/TransportClientFactory/createClient(java.lang.String,int)|",
      "|java+method:///org/apache/spark/network/buffer/ManagedBuffer/nioByteBuffer()|",
      "|java+method:///scala/concurrent/Promise/future()|",
      "|java+method:///org/apache/spark/serializer/JavaSerializer/newInstance()|",
      "|java+method:///java/nio/ByteBuffer/get(byte%5B%5D)|",
      "|java+method:///org/apache/spark/storage/BlockId/toString()|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/serialize(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///scala/concurrent/Promise$/apply()|",
      "|java+method:///java/nio/ByteBuffer/remaining()|",
      "|java+method:///org/apache/spark/network/shuffle/protocol/UploadBlock/toByteBuffer()|",
      "|java+method:///java/nio/ByteBuffer/hasArray()|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/deploy/ExternalShuffleService/ExternalShuffleService(org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
    "called": "|java+constructor:///org/apache/spark/network/TransportContext/TransportContext(org.apache.spark.network.util.TransportConf,org.apache.spark.network.server.RpcHandler,boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/blockHandler()|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/newShuffleBlockHandler(org.apache.spark.network.util.TransportConf)|",
      "|java+method:///org/apache/spark/SecurityManager/isAuthenticationEnabled()|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+constructor:///org/apache/spark/network/TransportContext/TransportContext(org.apache.spark.network.util.TransportConf,org.apache.spark.network.server.RpcHandler)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/transportConf()|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/network/netty/SparkTransportConf$/fromSparkConf(org.apache.spark.SparkConf,int)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/blockHandler()|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/newShuffleBlockHandler(org.apache.spark.network.util.TransportConf)|",
      "|java+method:///org/apache/spark/network/netty/SparkTransportConf$/fromSparkConf(org.apache.spark.SparkConf,java.lang.String,int)|",
      "|java+method:///org/apache/spark/SecurityManager/isAuthenticationEnabled()|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/transportConf()|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+constructor:///org/apache/spark/network/TransportContext/TransportContext(org.apache.spark.network.util.TransportConf,org.apache.spark.network.server.RpcHandler,boolean)|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1/apply(org.apache.mesos.Protos$Offer)|",
    "called": "|java+method:///org/spark-project/guava/collect/HashBiMap/put(java.lang.Object,java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/slaveIdsWithExecutors()|",
      "|java+method:///org/apache/mesos/Protos$Offer/getResourcesList()|",
      "|java+method:///scala/collection/mutable/Map/update(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///java/util/Collections/singleton(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$1/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$1(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/toAttributeMap(java.util.List)|",
      "|java+method:///org/apache/mesos/Protos$OfferID/getValue()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo/newBuilder()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/totalCoresAcquired()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/extraCoresPerSlave()|",
      "|java+method:///org/apache/mesos/Protos$Offer/getAttributesList()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/newMesosTaskId()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/build()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/totalCoresAcquired_$eq(int)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID/getValue()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/failuresBySlaveId()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/addAllResources(java.lang.Iterable)|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/taskIdToSlaveId()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setSlaveId(org.apache.mesos.Protos$SlaveID)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/size()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setCommand(org.apache.mesos.Protos$CommandInfo)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$2/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$2(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1,org.apache.mesos.Protos$TaskInfo$Builder)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/calculateTotalMemory(org.apache.spark.SparkContext)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/setValue(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$3/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$3(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.immutable.Map,double,int,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/MAX_SLAVE_FAILURES()|",
      "|java+method:///org/apache/mesos/SchedulerDriver/declineOffer(org.apache.mesos.Protos$OfferID)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$4/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$4(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.immutable.Map,double,int,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$slaveOfferConstraints()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setName(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/mesos/Protos$TaskID/newBuilder()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/maxCores()|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$slaveIdToHost()|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/partitionResources(java.util.List,java.lang.String,double)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/build()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setTaskId(org.apache.mesos.Protos$TaskID)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/createCommand(org.apache.mesos.Protos$Offer,int,int)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/math/package$/min(int,int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/executorLimit()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/mesos/SchedulerDriver/launchTasks(java.util.Collection,java.util.Collection,org.apache.mesos.Protos$Filters)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getSlaveId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/getResource(java.util.List,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/matchesAttributeRequirements(scala.collection.immutable.Map,scala.collection.immutable.Map)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/coresByTaskId()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getHostname()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/slaveIdsWithExecutors()|",
      "|java+method:///org/apache/mesos/Protos$Offer/getResourcesList()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$1/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$1(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/toAttributeMap(java.util.List)|",
      "|java+method:///org/apache/mesos/Protos$OfferID/getValue()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo/newBuilder()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/totalCoresAcquired()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/extraCoresPerSlave()|",
      "|java+method:///org/apache/mesos/Protos$Offer/getAttributesList()|",
      "|java+method:///org/apache/mesos/Protos$Filters$Builder/setRefuseSeconds(double)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/newMesosTaskId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$rejectOfferDurationForUnmetConstraints()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/build()|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/totalCoresAcquired_$eq(int)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID/getValue()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/failuresBySlaveId()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/addAllResources(java.lang.Iterable)|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/taskIdToSlaveId()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setSlaveId(org.apache.mesos.Protos$SlaveID)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/size()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setCommand(org.apache.mesos.Protos$CommandInfo)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$2/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$2(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1,org.apache.mesos.Protos$TaskInfo$Builder)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/calculateTotalMemory(org.apache.spark.SparkContext)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/setValue(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$3/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$3(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.immutable.Map,double,int,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/MAX_SLAVE_FAILURES()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$5/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$5(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.immutable.Map,double,int,java.lang.String)|",
      "|java+method:///org/apache/mesos/SchedulerDriver/declineOffer(org.apache.mesos.Protos$OfferID)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$4/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$4(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.immutable.Map,double,int,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$slaveOfferConstraints()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setName(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/mesos/Protos$TaskID/newBuilder()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/maxCores()|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$slaveIdToHost()|",
      "|java+method:///org/apache/mesos/SchedulerDriver/declineOffer(org.apache.mesos.Protos$OfferID,org.apache.mesos.Protos$Filters)|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/partitionResources(java.util.List,java.lang.String,double)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/build()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setTaskId(org.apache.mesos.Protos$TaskID)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/createCommand(org.apache.mesos.Protos$Offer,int,int)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/math/package$/min(int,int)|",
      "|java+method:///java/util/Collections/singleton(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$Filters$Builder/build()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/executorLimit()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$Filters/newBuilder()|",
      "|java+method:///org/apache/mesos/SchedulerDriver/launchTasks(java.util.Collection,java.util.Collection,org.apache.mesos.Protos$Filters)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getSlaveId()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/getResource(java.util.List,java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/coresByTaskId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/matchesAttributeRequirements(scala.collection.immutable.Map,scala.collection.immutable.Map)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/put(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getHostname()|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/handleMessage(org.apache.spark.network.shuffle.protocol.BlockTransferMessage,org.apache.spark.network.client.TransportClient,org.apache.spark.network.client.RpcResponseCallback)|",
    "called": "|java+method:///org/apache/spark/network/client/TransportClient/getSocketAddress()|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/connectedApps()|",
      "|java+constructor:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$1/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$1(org.apache.spark.deploy.mesos.MesosExternalShuffleBlockHandler,java.lang.String,java.net.SocketAddress)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/RegisterDriverParam()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///java/lang/String/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/network/client/RpcResponseCallback/onSuccess(byte%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$2/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$2(org.apache.spark.deploy.mesos.MesosExternalShuffleBlockHandler,java.lang.String,java.net.SocketAddress,java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/network/client/TransportClient/getSocketAddress()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$RegisterDriverParam$/unapply(org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver)|",
      "|java+method:///org/apache/spark/network/shuffle/ExternalShuffleBlockHandler/handleMessage(org.apache.spark.network.shuffle.protocol.BlockTransferMessage,org.apache.spark.network.client.TransportClient,org.apache.spark.network.client.RpcResponseCallback)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/applicationRemoved(java.lang.String,boolean)|"
    ],
    "v2Body": [
      "|java+method:///java/nio/ByteBuffer/allocate(int)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/connectedApps()|",
      "|java+constructor:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$1/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$1(org.apache.spark.deploy.mesos.MesosExternalShuffleBlockHandler,java.lang.String,java.net.SocketAddress)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/RegisterDriverParam()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///java/lang/String/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/network/client/RpcResponseCallback/onSuccess(java.nio.ByteBuffer)|",
      "|java+constructor:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$2/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$2(org.apache.spark.deploy.mesos.MesosExternalShuffleBlockHandler,java.lang.String,java.net.SocketAddress,java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/network/client/TransportClient/getSocketAddress()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$RegisterDriverParam$/unapply(org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver)|",
      "|java+method:///org/apache/spark/network/shuffle/ExternalShuffleBlockHandler/handleMessage(org.apache.spark.network.shuffle.protocol.BlockTransferMessage,org.apache.spark.network.client.TransportClient,org.apache.spark.network.client.RpcResponseCallback)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/applicationRemoved(java.lang.String,boolean)|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/input/PortableDataStream/toArray()|",
    "called": "|java+method:///org/spark-project/guava/io/Closeables/close(java.io.Closeable,boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/input/PortableDataStream/open()|",
      "|java+method:///org/spark-project/guava/io/ByteStreams/toByteArray(java.io.InputStream)|",
      "|java+method:///org/apache/spark/input/PortableDataStream/close()|",
      "|java+method:///org/apache/spark/input/PortableDataStream/fileIn()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/input/PortableDataStream/open()|",
      "|java+method:///org/spark-project/guava/io/Closeables/close(java.io.Closeable,boolean)|",
      "|java+method:///org/spark-project/guava/io/ByteStreams/toByteArray(java.io.InputStream)|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonSingleThreadScheduledExecutor(java.lang.String)|",
    "called": "|java+constructor:///org/spark-project/guava/util/concurrent/ThreadFactoryBuilder/ThreadFactoryBuilder()|",
    "v1Body": [
      "|java+method:///org/spark-project/guava/util/concurrent/ThreadFactoryBuilder/build()|",
      "|java+method:///java/util/concurrent/Executors/newSingleThreadScheduledExecutor(java.util.concurrent.ThreadFactory)|",
      "|java+method:///org/spark-project/guava/util/concurrent/ThreadFactoryBuilder/setNameFormat(java.lang.String)|",
      "|java+method:///org/spark-project/guava/util/concurrent/ThreadFactoryBuilder/setDaemon(boolean)|",
      "|java+constructor:///org/spark-project/guava/util/concurrent/ThreadFactoryBuilder/ThreadFactoryBuilder()|"
    ],
    "v2Body": [
      "|java+constructor:///java/util/concurrent/ScheduledThreadPoolExecutor/ScheduledThreadPoolExecutor(int,java.util.concurrent.ThreadFactory)|",
      "|java+method:///org/spark-project/guava/util/concurrent/ThreadFactoryBuilder/build()|",
      "|java+method:///org/spark-project/guava/util/concurrent/ThreadFactoryBuilder/setNameFormat(java.lang.String)|",
      "|java+method:///java/util/concurrent/ScheduledThreadPoolExecutor/setRemoveOnCancelPolicy(boolean)|",
      "|java+method:///org/spark-project/guava/util/concurrent/ThreadFactoryBuilder/setDaemon(boolean)|",
      "|java+constructor:///org/spark-project/guava/util/concurrent/ThreadFactoryBuilder/ThreadFactoryBuilder()|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor()|",
    "called": "|java+method:///org/spark-project/guava/io/Files/write(java.lang.CharSequence,java.io.File,java.nio.charset.Charset)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$1/ExecutorRunner$$anonfun$1(org.apache.spark.deploy.worker.ExecutorRunner)|",
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$1/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$1(org.apache.spark.deploy.worker.ExecutorRunner,java.util.List)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/publicAddress()|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+method:///org/apache/spark/util/logging/FileAppender$/apply(java.io.InputStream,java.io.File,org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/state()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/state_$eq(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/stderrAppender_$eq(org.apache.spark.util.logging.FileAppender)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/executorDir()|",
      "|java+method:///java/lang/Process/waitFor()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///java/lang/ProcessBuilder/directory(java.io.File)|",
      "|java+method:///java/lang/Process/getErrorStream()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/org$apache$spark$deploy$worker$ExecutorRunner$$killProcess(scala.Option)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/execId()|",
      "|java+method:///org/apache/spark/deploy/worker/CommandUtils$/buildProcessBuilder$default$6()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/stdoutAppender_$eq(org.apache.spark.util.logging.FileAppender)|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/EXITED()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/command()|",
      "|java+method:///scala/collection/mutable/Buffer/mkString(java.lang.String,java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$2/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$2(org.apache.spark.deploy.worker.ExecutorRunner)|",
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/FAILED()|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/KILLED()|",
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$3/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$3(org.apache.spark.deploy.worker.ExecutorRunner)|",
      "|java+method:///org/apache/spark/deploy/worker/CommandUtils$/buildProcessBuilder$default$7()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/spark-project/guava/io/Files/write(java.lang.CharSequence,java.io.File,java.nio.charset.Charset)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/logInfo(scala.Function0)|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/appLocalDirs()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/webUiPort()|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/process_$eq(java.lang.Process)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/worker/CommandUtils$/buildProcessBuilder(org.apache.spark.deploy.Command,org.apache.spark.SecurityManager,int,java.lang.String,scala.Function1,scala.collection.Seq,scala.collection.Map)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/process()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/worker()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/appDesc()|",
      "|java+method:///java/io/File/getAbsolutePath()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///java/lang/ProcessBuilder/command()|",
      "|java+method:///scala/collection/immutable/StringOps/$times(int)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/memory()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///java/lang/Exception/toString()|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/DeployMessages$ExecutorStateChanged(java.lang.String,int,scala.Enumeration$Value,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/appId()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/sparkHome()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///java/lang/ProcessBuilder/environment()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$1/ExecutorRunner$$anonfun$1(org.apache.spark.deploy.worker.ExecutorRunner)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/publicAddress()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/state_$eq(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/stderrAppender_$eq(org.apache.spark.util.logging.FileAppender)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/executorDir()|",
      "|java+method:///java/lang/Process/waitFor()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///java/lang/ProcessBuilder/directory(java.io.File)|",
      "|java+method:///java/lang/Process/getErrorStream()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/org$apache$spark$deploy$worker$ExecutorRunner$$killProcess(scala.Option)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/execId()|",
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$1/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$1(org.apache.spark.deploy.worker.ExecutorRunner,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/CommandUtils$/buildProcessBuilder$default$6()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/stdoutAppender_$eq(org.apache.spark.util.logging.FileAppender)|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/EXITED()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/command()|",
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$2/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$2(org.apache.spark.deploy.worker.ExecutorRunner)|",
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/FAILED()|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/KILLED()|",
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$3/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$3(org.apache.spark.deploy.worker.ExecutorRunner)|",
      "|java+method:///org/apache/spark/deploy/worker/CommandUtils$/buildProcessBuilder$default$7()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/spark-project/guava/io/Files/write(java.lang.CharSequence,java.io.File,java.nio.charset.Charset)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/logInfo(scala.Function0)|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/appLocalDirs()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/webUiPort()|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/process_$eq(java.lang.Process)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/worker/CommandUtils$/buildProcessBuilder(org.apache.spark.deploy.Command,org.apache.spark.SecurityManager,int,java.lang.String,scala.Function1,scala.collection.Seq,scala.collection.Map)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+method:///org/apache/spark/util/logging/FileAppender$/apply(java.io.InputStream,java.io.File,org.apache.spark.SparkConf)|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/process()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/state()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/worker()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/appDesc()|",
      "|java+method:///java/io/File/getAbsolutePath()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///java/lang/ProcessBuilder/command()|",
      "|java+method:///scala/collection/immutable/StringOps/$times(int)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/memory()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///java/lang/Exception/toString()|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/DeployMessages$ExecutorStateChanged(java.lang.String,int,scala.Enumeration$Value,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/appId()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/sparkHome()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///java/lang/ProcessBuilder/environment()|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/collection/Utils$/takeOrdered(scala.collection.Iterator,int,scala.math.Ordering)|",
    "called": "|java+method:///org/spark-project/guava/collect/Ordering/leastOf(java.util.Iterator,int)|",
    "v1Body": [
      "|java+method:///org/spark-project/guava/collect/Ordering/leastOf(java.util.Iterator,int)|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterator(scala.collection.Iterator)|",
      "|java+method:///scala/collection/Iterable/iterator()|",
      "|java+method:///scala/collection/JavaConversions$/collectionAsScalaIterable(java.util.Collection)|",
      "|java+constructor:///org/apache/spark/util/collection/Utils$$anon$1/Utils$$anon$1(scala.math.Ordering)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+constructor:///org/apache/spark/util/collection/Utils$$anon$1/Utils$$anon$1(scala.math.Ordering)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaIteratorConverter(java.util.Iterator)|",
      "|java+method:///org/spark-project/guava/collect/Ordering/leastOf(java.util.Iterator,int)|",
      "|java+method:///java/util/List/iterator()|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/ExternalShuffleService/start()|",
    "called": "|java+constructor:///org/apache/spark/network/sasl/SaslServerBootstrap/SaslServerBootstrap(org.apache.spark.network.util.TransportConf,org.apache.spark.network.sasl.SecretKeyHolder)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/network/sasl/SaslServerBootstrap/SaslServerBootstrap(org.apache.spark.network.util.TransportConf,org.apache.spark.network.sasl.SecretKeyHolder)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/server_$eq(org.apache.spark.network.server.TransportServer)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/server()|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/transportContext()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/org$apache$spark$deploy$ExternalShuffleService$$port()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/org$apache$spark$deploy$ExternalShuffleService$$useSasl()|",
      "|java+constructor:///org/apache/spark/deploy/ExternalShuffleService$$anonfun$start$1/ExternalShuffleService$$anonfun$start$1(org.apache.spark.deploy.ExternalShuffleService)|",
      "|java+constructor:///org/apache/spark/deploy/ExternalShuffleService$$anonfun$start$2/ExternalShuffleService$$anonfun$start$2(org.apache.spark.deploy.ExternalShuffleService)|",
      "|java+method:///org/apache/spark/network/TransportContext/createServer(int,java.util.List)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/transportConf()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/network/sasl/SaslServerBootstrap/SaslServerBootstrap(org.apache.spark.network.util.TransportConf,org.apache.spark.network.sasl.SecretKeyHolder)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/server_$eq(org.apache.spark.network.server.TransportServer)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/server()|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/transportContext()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/org$apache$spark$deploy$ExternalShuffleService$$port()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/org$apache$spark$deploy$ExternalShuffleService$$useSasl()|",
      "|java+constructor:///org/apache/spark/deploy/ExternalShuffleService$$anonfun$start$1/ExternalShuffleService$$anonfun$start$1(org.apache.spark.deploy.ExternalShuffleService)|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/deploy/ExternalShuffleService$$anonfun$start$2/ExternalShuffleService$$anonfun$start$2(org.apache.spark.deploy.ExternalShuffleService)|",
      "|java+method:///org/apache/spark/network/TransportContext/createServer(int,java.util.List)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/transportConf()|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader/loadNext()|",
    "called": "|java+method:///org/spark-project/guava/io/ByteStreams/readFully(java.io.InputStream,byte%5B%5D,int,int)|",
    "v1Body": [
      "|java+method:///java/io/DataInputStream/readInt()|",
      "|java+method:///java/io/InputStream/close()|",
      "|java+method:///org/spark-project/guava/io/ByteStreams/readFully(java.io.InputStream,byte%5B%5D,int,int)|",
      "|java+method:///java/io/DataInputStream/readLong()|",
      "|java+method:///java/io/File/delete()|"
    ],
    "v2Body": [
      "|java+method:///java/io/DataInputStream/readInt()|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader/close()|",
      "|java+method:///org/spark-project/guava/io/ByteStreams/readFully(java.io.InputStream,byte%5B%5D,int,int)|",
      "|java+method:///java/io/DataInputStream/readLong()|"
    ],
    "affectedLib": "org.apache.spark:spark-network-common_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/allocate(int)|",
    "called": "|java+method:///org/apache/spark/unsafe/array/LongArray/zeroOut()|",
    "v1Body": [
      "|java+method:///org/apache/spark/unsafe/array/ByteArrayMethods/nextPowerOf2(long)|",
      "|java+method:///java/lang/Math/min(long,long)|",
      "|java+method:///java/lang/Math/max(int,int)|",
      "|java+constructor:///java/lang/AssertionError/AssertionError()|",
      "|java+method:///org/apache/spark/unsafe/memory/MemoryBlock/fromLongArray(long%5B%5D)|",
      "|java+constructor:///org/apache/spark/unsafe/array/LongArray/LongArray(org.apache.spark.unsafe.memory.MemoryBlock)|",
      "|java+constructor:///org/apache/spark/unsafe/bitset/BitSet/BitSet(org.apache.spark.unsafe.memory.MemoryBlock)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/unsafe/array/ByteArrayMethods/nextPowerOf2(long)|",
      "|java+method:///java/lang/Math/min(long,long)|",
      "|java+method:///java/lang/Math/max(int,int)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/allocateArray(long)|",
      "|java+method:///org/apache/spark/unsafe/array/LongArray/zeroOut()|",
      "|java+constructor:///java/lang/AssertionError/AssertionError()|"
    ],
    "affectedLib": "org.apache.spark:spark-unsafe_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter$SortedIterator/loadNext()|",
    "called": "|java+method:///org/apache/spark/unsafe/Platform/getInt(java.lang.Object,long)|",
    "v1Body": [
      "|java+method:///org/apache/spark/unsafe/Platform/getInt(java.lang.Object,long)|",
      "|java+method:///org/apache/spark/unsafe/memory/TaskMemoryManager/getOffsetInPage(long)|",
      "|java+method:///org/apache/spark/unsafe/memory/TaskMemoryManager/getPage(long)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/unsafe/Platform/getInt(java.lang.Object,long)|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter/access$100(org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter)|",
      "|java+method:///org/apache/spark/memory/TaskMemoryManager/getOffsetInPage(long)|",
      "|java+method:///org/apache/spark/memory/TaskMemoryManager/getPage(long)|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter/access$000(org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter)|",
      "|java+method:///org/apache/spark/unsafe/array/LongArray/get(int)|"
    ],
    "affectedLib": "org.apache.spark:spark-unsafe_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/safeLookup(java.lang.Object,long,int,org.apache.spark.unsafe.map.BytesToBytesMap$Location)|",
    "called": "|java+method:///org/apache/spark/unsafe/array/ByteArrayMethods/arrayEquals(java.lang.Object,long,java.lang.Object,long,long)|",
    "v1Body": [
      "|java+method:///org/apache/spark/unsafe/memory/MemoryLocation/getBaseOffset()|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap$Location/getKeyAddress()|",
      "|java+method:///org/apache/spark/unsafe/array/LongArray/get(int)|",
      "|java+method:///org/apache/spark/unsafe/array/ByteArrayMethods/arrayEquals(java.lang.Object,long,java.lang.Object,long,long)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap$Location/access$400(org.apache.spark.unsafe.map.BytesToBytesMap$Location,int,int,boolean)|",
      "|java+method:///org/apache/spark/unsafe/memory/MemoryLocation/getBaseObject()|",
      "|java+method:///org/apache/spark/unsafe/hash/Murmur3_x86_32/hashUnsafeWords(java.lang.Object,long,int)|",
      "|java+constructor:///java/lang/AssertionError/AssertionError()|",
      "|java+method:///org/apache/spark/unsafe/bitset/BitSet/isSet(int)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap$Location/getKeyLength()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap$Location/getKeyLength()|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap$Location/access$1000(org.apache.spark.unsafe.map.BytesToBytesMap$Location,int,int,boolean)|",
      "|java+method:///org/apache/spark/unsafe/memory/MemoryLocation/getBaseOffset()|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap$Location/getKeyAddress()|",
      "|java+method:///org/apache/spark/unsafe/array/LongArray/get(int)|",
      "|java+method:///org/apache/spark/unsafe/array/ByteArrayMethods/arrayEquals(java.lang.Object,long,java.lang.Object,long,long)|",
      "|java+method:///org/apache/spark/unsafe/memory/MemoryLocation/getBaseObject()|",
      "|java+method:///org/apache/spark/unsafe/hash/Murmur3_x86_32/hashUnsafeWords(java.lang.Object,long,int)|",
      "|java+constructor:///java/lang/AssertionError/AssertionError()|"
    ],
    "affectedLib": "org.apache.spark:spark-unsafe_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter/insertRecord(long,long)|",
    "called": "|java+method:///org/apache/spark/unsafe/array/LongArray/set(int,long)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter/expandPointerArray()|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter/hasSpaceForAnotherRecord()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/unsafe/array/LongArray/size()|",
      "|java+method:///org/apache/spark/memory/MemoryConsumer/allocateArray(long)|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter/expandPointerArray(org.apache.spark.unsafe.array.LongArray)|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter/hasSpaceForAnotherRecord()|",
      "|java+method:///org/apache/spark/unsafe/array/LongArray/set(int,long)|"
    ],
    "affectedLib": "org.apache.spark:spark-unsafe_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap$Location/putNewKey(java.lang.Object,long,int,java.lang.Object,long,int)|",
    "called": "|java+method:///org/apache/spark/unsafe/Platform/copyMemory(java.lang.Object,long,java.lang.Object,long,long)|",
    "v1Body": [
      "|java+method:///org/apache/spark/unsafe/memory/TaskMemoryManager/allocatePage(long)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$700(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$600(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/shuffle/ShuffleMemoryManager/release(long)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$000(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1400(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1200(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1100(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1000(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/growAndRehash()|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap$Location/updateAddressesAndSizes(long)|",
      "|java+method:///org/apache/spark/unsafe/bitset/BitSet/set(int)|",
      "|java+constructor:///java/lang/AssertionError/AssertionError()|",
      "|java+constructor:///java/lang/IllegalStateException/IllegalStateException(java.lang.String)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$900(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/Platform/copyMemory(java.lang.Object,long,java.lang.Object,long,long)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$800(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/memory/MemoryBlock/getBaseOffset()|",
      "|java+method:///org/apache/spark/unsafe/array/LongArray/size()|",
      "|java+method:///java/lang/Long/valueOf(long)|",
      "|java+method:///org/slf4j/Logger/debug(java.lang.String,java.lang.Object)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$500(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+constructor:///java/lang/AssertionError/AssertionError(java.lang.Object)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$100(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1214(org.apache.spark.unsafe.map.BytesToBytesMap,long)|",
      "|java+method:///org/apache/spark/unsafe/memory/MemoryBlock/getBaseObject()|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1300(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/Platform/putInt(java.lang.Object,long,int)|",
      "|java+method:///java/util/List/add(java.lang.Object)|",
      "|java+method:///org/apache/spark/shuffle/ShuffleMemoryManager/tryToAcquire(long)|",
      "|java+method:///org/apache/spark/unsafe/array/LongArray/set(int,long)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$708(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/memory/TaskMemoryManager/encodePageNumberAndOffset(org.apache.spark.unsafe.memory.MemoryBlock,long)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/unsafe/Platform/getInt(java.lang.Object,long)|",
      "|java+method:///org/apache/spark/unsafe/Platform/copyMemory(java.lang.Object,long,java.lang.Object,long,long)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1308(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+constructor:///java/lang/AssertionError/AssertionError(java.lang.Object)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1700(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1500(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1400(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1100(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1800(org.apache.spark.unsafe.map.BytesToBytesMap,long)|",
      "|java+method:///org/apache/spark/memory/TaskMemoryManager/encodePageNumberAndOffset(org.apache.spark.unsafe.memory.MemoryBlock,long)|",
      "|java+method:///org/apache/spark/unsafe/Platform/putInt(java.lang.Object,long,int)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap$Location/updateAddressesAndSizes(long)|",
      "|java+method:///org/apache/spark/unsafe/array/LongArray/set(int,long)|",
      "|java+constructor:///java/lang/AssertionError/AssertionError()|",
      "|java+method:///org/apache/spark/unsafe/memory/MemoryBlock/getBaseOffset()|",
      "|java+method:///org/apache/spark/unsafe/array/LongArray/size()|",
      "|java+method:///org/apache/spark/unsafe/memory/MemoryBlock/size()|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1600(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/memory/MemoryBlock/getBaseObject()|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1300(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1402(org.apache.spark.unsafe.map.BytesToBytesMap,boolean)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1200(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1714(org.apache.spark.unsafe.map.BytesToBytesMap,long)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/growAndRehash()|"
    ],
    "affectedLib": "org.apache.spark:spark-unsafe_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap$Location/with(int,int,boolean)|",
    "called": "|java+method:///org/apache/spark/unsafe/array/LongArray/get(int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$500(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/array/LongArray/get(int)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap$Location/updateAddressesAndSizes(long)|",
      "|java+constructor:///java/lang/AssertionError/AssertionError()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/unsafe/array/LongArray/get(int)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/access$1200(org.apache.spark.unsafe.map.BytesToBytesMap)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap$Location/updateAddressesAndSizes(long)|",
      "|java+constructor:///java/lang/AssertionError/AssertionError()|"
    ],
    "affectedLib": "org.apache.spark:spark-unsafe_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeSortDataFormat/allocate(int)|",
    "called": "|java+constructor:///org/apache/spark/unsafe/array/LongArray/LongArray(org.apache.spark.unsafe.memory.MemoryBlock)|",
    "v1Body": [
      "|java+method:///java/lang/StringBuilder/append(java.lang.String)|",
      "|java+constructor:///java/lang/AssertionError/AssertionError(java.lang.Object)|",
      "|java+constructor:///java/lang/StringBuilder/StringBuilder()|",
      "|java+method:///java/lang/StringBuilder/toString()|",
      "|java+method:///java/lang/StringBuilder/append(int)|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeSortDataFormat/allocate(int)|"
    ],
    "v2Body": [
      "|java+method:///java/lang/StringBuilder/append(java.lang.String)|",
      "|java+constructor:///java/lang/AssertionError/AssertionError(java.lang.Object)|",
      "|java+constructor:///java/lang/StringBuilder/StringBuilder()|",
      "|java+constructor:///org/apache/spark/unsafe/array/LongArray/LongArray(org.apache.spark.unsafe.memory.MemoryBlock)|",
      "|java+method:///java/lang/StringBuilder/append(int)|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeSortDataFormat/allocate(int)|",
      "|java+method:///java/lang/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/unsafe/memory/MemoryBlock/fromLongArray(long%5B%5D)|"
    ],
    "affectedLib": "org.apache.spark:spark-unsafe_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter/insertRecord(java.lang.Object,long,int,long)|",
    "called": "|java+method:///org/apache/spark/unsafe/Platform/copyMemory(java.lang.Object,long,java.lang.Object,long,long)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter/growPointerArrayIfNecessary()|",
      "|java+method:///org/apache/spark/unsafe/Platform/copyMemory(java.lang.Object,long,java.lang.Object,long,long)|",
      "|java+constructor:///java/io/IOException/IOException(java.lang.String)|",
      "|java+method:///java/lang/StringBuilder/append(java.lang.String)|",
      "|java+method:///java/lang/StringBuilder/append(long)|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter/insertRecord(long,long)|",
      "|java+constructor:///java/lang/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/unsafe/Platform/putInt(java.lang.Object,long,int)|",
      "|java+method:///org/apache/spark/shuffle/ShuffleMemoryManager/tryToAcquire(long)|",
      "|java+method:///org/apache/spark/unsafe/memory/TaskMemoryManager/encodePageNumberAndOffset(org.apache.spark.unsafe.memory.MemoryBlock,long)|",
      "|java+constructor:///java/lang/AssertionError/AssertionError()|",
      "|java+method:///java/lang/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter/acquireNewPageIfNecessary(int)|",
      "|java+method:///org/apache/spark/unsafe/memory/TaskMemoryManager/allocatePage(long)|",
      "|java+method:///org/apache/spark/unsafe/array/ByteArrayMethods/roundNumberOfBytesToNearestWord(int)|",
      "|java+method:///org/apache/spark/unsafe/memory/MemoryBlock/getBaseOffset()|",
      "|java+method:///org/apache/spark/shuffle/ShuffleMemoryManager/release(long)|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter/spill()|",
      "|java+method:///org/apache/spark/unsafe/memory/MemoryBlock/getBaseObject()|",
      "|java+method:///java/util/LinkedList/add(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter/growPointerArrayIfNecessary()|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter/acquireNewPageIfNecessary(int)|",
      "|java+method:///org/apache/spark/unsafe/Platform/copyMemory(java.lang.Object,long,java.lang.Object,long,long)|",
      "|java+method:///org/apache/spark/unsafe/memory/MemoryBlock/getBaseObject()|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter/insertRecord(long,long)|",
      "|java+method:///org/apache/spark/memory/TaskMemoryManager/encodePageNumberAndOffset(org.apache.spark.unsafe.memory.MemoryBlock,long)|",
      "|java+method:///org/apache/spark/unsafe/Platform/putInt(java.lang.Object,long,int)|",
      "|java+constructor:///java/lang/AssertionError/AssertionError()|"
    ],
    "affectedLib": "org.apache.spark:spark-unsafe_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/growAndRehash()|",
    "called": "|java+method:///org/apache/spark/unsafe/array/LongArray/set(int,long)|",
    "v1Body": [
      "|java+method:///org/apache/spark/unsafe/bitset/BitSet/isSet(int)|",
      "|java+method:///java/lang/System/nanoTime()|",
      "|java+method:///org/apache/spark/unsafe/array/LongArray/get(int)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/allocate(int)|",
      "|java+method:///org/apache/spark/unsafe/bitset/BitSet/set(int)|",
      "|java+method:///java/lang/Math/min(int,int)|",
      "|java+method:///org/apache/spark/unsafe/array/LongArray/set(int,long)|",
      "|java+method:///org/apache/spark/unsafe/map/HashMapGrowthStrategy/nextCapacity(int)|",
      "|java+method:///org/apache/spark/unsafe/bitset/BitSet/capacity()|",
      "|java+constructor:///java/lang/AssertionError/AssertionError()|",
      "|java+method:///org/apache/spark/unsafe/bitset/BitSet/nextSetBit(int)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/freeArray(org.apache.spark.unsafe.array.LongArray)|",
      "|java+method:///org/apache/spark/unsafe/array/LongArray/size()|",
      "|java+method:///java/lang/System/nanoTime()|",
      "|java+method:///org/apache/spark/unsafe/array/LongArray/get(int)|",
      "|java+method:///org/apache/spark/unsafe/map/BytesToBytesMap/allocate(int)|",
      "|java+method:///org/apache/spark/unsafe/map/HashMapGrowthStrategy/nextCapacity(int)|",
      "|java+method:///java/lang/Math/min(int,int)|",
      "|java+method:///org/apache/spark/unsafe/array/LongArray/set(int,long)|",
      "|java+constructor:///java/lang/AssertionError/AssertionError()|"
    ],
    "affectedLib": "org.apache.spark:spark-unsafe_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/collection/unsafe/sort/PrefixComparators$BinaryPrefixComparator/computePrefix(byte%5B%5D)|",
    "called": "|java+method:///org/apache/spark/unsafe/types/ByteArray/getPrefix(byte%5B%5D)|",
    "v1Body": [
      "|java+method:///org/apache/spark/unsafe/Platform/getByte(java.lang.Object,long)|",
      "|java+method:///java/lang/Math/min(int,int)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/unsafe/types/ByteArray/getPrefix(byte%5B%5D)|"
    ],
    "affectedLib": "org.apache.spark:spark-unsafe_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter/insertKVRecord(java.lang.Object,long,int,java.lang.Object,long,int,long)|",
    "called": "|java+method:///org/apache/spark/unsafe/Platform/copyMemory(java.lang.Object,long,java.lang.Object,long,long)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter/growPointerArrayIfNecessary()|",
      "|java+method:///org/apache/spark/unsafe/Platform/copyMemory(java.lang.Object,long,java.lang.Object,long,long)|",
      "|java+constructor:///java/io/IOException/IOException(java.lang.String)|",
      "|java+method:///java/lang/StringBuilder/append(java.lang.String)|",
      "|java+method:///java/lang/StringBuilder/append(long)|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter/insertRecord(long,long)|",
      "|java+constructor:///java/lang/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/unsafe/Platform/putInt(java.lang.Object,long,int)|",
      "|java+method:///org/apache/spark/shuffle/ShuffleMemoryManager/tryToAcquire(long)|",
      "|java+method:///org/apache/spark/unsafe/memory/TaskMemoryManager/encodePageNumberAndOffset(org.apache.spark.unsafe.memory.MemoryBlock,long)|",
      "|java+constructor:///java/lang/AssertionError/AssertionError()|",
      "|java+method:///java/lang/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter/acquireNewPageIfNecessary(int)|",
      "|java+method:///org/apache/spark/unsafe/memory/TaskMemoryManager/allocatePage(long)|",
      "|java+method:///org/apache/spark/unsafe/array/ByteArrayMethods/roundNumberOfBytesToNearestWord(int)|",
      "|java+method:///org/apache/spark/unsafe/memory/MemoryBlock/getBaseOffset()|",
      "|java+method:///org/apache/spark/shuffle/ShuffleMemoryManager/release(long)|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter/spill()|",
      "|java+method:///org/apache/spark/unsafe/memory/MemoryBlock/getBaseObject()|",
      "|java+method:///java/util/LinkedList/add(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter/growPointerArrayIfNecessary()|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter/acquireNewPageIfNecessary(int)|",
      "|java+method:///org/apache/spark/unsafe/Platform/copyMemory(java.lang.Object,long,java.lang.Object,long,long)|",
      "|java+method:///org/apache/spark/unsafe/memory/MemoryBlock/getBaseObject()|",
      "|java+method:///org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter/insertRecord(long,long)|",
      "|java+method:///org/apache/spark/memory/TaskMemoryManager/encodePageNumberAndOffset(org.apache.spark.unsafe.memory.MemoryBlock,long)|",
      "|java+method:///org/apache/spark/unsafe/Platform/putInt(java.lang.Object,long,int)|",
      "|java+constructor:///java/lang/AssertionError/AssertionError()|"
    ],
    "affectedLib": "org.apache.spark:spark-unsafe_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$resourceOffers$4/apply(scala.Tuple2)|",
    "called": "|java+method:///scala/collection/JavaConverters$/bufferAsJavaListConverter(scala.collection.mutable.Buffer)|",
    "v1Body": [
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/JavaConversions$/bufferAsJavaList(scala.collection.mutable.Buffer)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/mesos/SchedulerDriver/launchTasks(java.util.Collection,java.util.Collection)|",
      "|java+method:///java/util/Collections/singleton(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/mesos/SchedulerDriver/launchTasks(java.util.Collection,java.util.Collection)|",
      "|java+method:///java/util/Collections/singleton(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/JavaConverters$/bufferAsJavaListConverter(scala.collection.mutable.Buffer)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/ExecutorCacheTaskLocation/toString()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/runtime/ScalaRunTime$/_toString(scala.Product)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/ExecutorCacheTaskLocation/host()|",
      "|java+method:///org/apache/spark/scheduler/ExecutorCacheTaskLocation/executorId()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/scheduler/TaskLocation$/executorLocationTag()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anon$1/releaseWriters(boolean)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
    "v1Body": [
      "|java+method:///java/util/concurrent/ConcurrentLinkedQueue/add(java.lang.Object)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$ShuffleFileGroup/recordMapOutput(int,long%5B%5D,long%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anon$1$$anonfun$5/FileShuffleBlockResolver$$anon$1$$anonfun$5(org.apache.spark.shuffle.FileShuffleBlockResolver$$anon$1)|",
      "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anon$1$$anonfun$4/FileShuffleBlockResolver$$anon$1$$anonfun$4(org.apache.spark.shuffle.FileShuffleBlockResolver$$anon$1)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anon$1/shuffleState()|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anon$1/org$apache$spark$shuffle$FileShuffleBlockResolver$$anon$$fileGroup()|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anon$1/writers()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anon$1/recycleFileGroup(org.apache.spark.shuffle.FileShuffleBlockResolver$ShuffleFileGroup)|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$ShuffleState/completedMapTasks()|",
      "|java+method:///scala/reflect/ClassTag$/Long()|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver/org$apache$spark$shuffle$FileShuffleBlockResolver$$consolidateShuffleFiles()|"
    ],
    "v2Body": [
      "|java+method:///java/util/concurrent/ConcurrentLinkedQueue/add(java.lang.Object)|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$ShuffleState/completedMapTasks()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anon$1/shuffleState()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/rpc/RpcEnvConfig$/RpcEnvConfig$()|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction6/AbstractFunction6()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction5/AbstractFunction5()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction6/AbstractFunction6()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleExecutorLost$3/apply(scala.Tuple2)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/removeOutputsOnExecutor(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/MapOutputTrackerMaster/registerMapOutputs(int,org.apache.spark.scheduler.MapStatus%5B%5D,boolean)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_1$mcI$sp()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/outputLocs()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleExecutorLost$3$$anonfun$19/DAGScheduler$$anonfun$handleExecutorLost$3$$anonfun$19(org.apache.spark.scheduler.DAGScheduler$$anonfun$handleExecutorLost$3)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/outputLocInMapOutputTrackerFormat()|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/removeOutputsOnExecutor(java.lang.String)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/MapOutputTrackerMaster/registerMapOutputs(int,org.apache.spark.scheduler.MapStatus%5B%5D,boolean)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/Tuple2/_1$mcI$sp()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/Utils$/getThreadDump()|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///java/lang/management/ThreadMXBean/dumpAllThreads(boolean,boolean)|",
      "|java+method:///scala/collection/mutable/ArrayOps/sortBy(scala.Function1,scala.math.Ordering)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/filter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$getThreadDump$2/Utils$$anonfun$getThreadDump$2()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$getThreadDump$1/Utils$$anonfun$getThreadDump$1()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$12/Utils$$anonfun$12()|",
      "|java+method:///java/lang/management/ManagementFactory/getThreadMXBean()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$11/Utils$$anonfun$11()|",
      "|java+method:///java/lang/management/ManagementFactory/getThreadMXBean()|",
      "|java+method:///java/lang/management/ThreadMXBean/dumpAllThreads(boolean,boolean)|",
      "|java+method:///scala/collection/mutable/ArrayOps/sortBy(scala.Function1,scala.math.Ordering)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/filter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$getThreadDump$2/Utils$$anonfun$getThreadDump$2()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$getThreadDump$1/Utils$$anonfun$getThreadDump$1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$39/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$39/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$39/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$class/takeOrdered(org.apache.spark.api.java.JavaRDDLike,int,java.util.Comparator)|",
    "called": "|java+method:///scala/math/Ordering$/comparatorToOrdering(java.util.Comparator)|",
    "v1Body": [
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+constructor:///java/util/ArrayList/ArrayList(java.util.Collection)|",
      "|java+method:///scala/math/Ordering$/comparatorToOrdering(java.util.Comparator)|",
      "|java+method:///org/apache/spark/rdd/RDD/takeOrdered(int,scala.math.Ordering)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/package$/Ordering()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///scala/math/Ordering$/comparatorToOrdering(java.util.Comparator)|",
      "|java+method:///org/apache/spark/rdd/RDD/takeOrdered(int,scala.math.Ordering)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///scala/package$/Ordering()|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/getConfig()|",
    "called": "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Predef$/any2ArrowAssoc(java.lang.Object)|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/lang/String/toString()|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$logDir()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Predef$/any2ArrowAssoc(java.lang.Object)|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/lang/String/toString()|",
      "|java+method:///scala/collection/immutable/MapLike/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/isFsInSafeMode()|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$logDir()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2/apply(java.lang.Class)|",
    "called": "|java+constructor:///scala/Some/Some(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/SetLike/$plus$eq(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/MethodIdentifier/MethodIdentifier(java.lang.Class,java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/mutable/Set/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/FieldAccessFinder$$anon$3/org$apache$spark$util$FieldAccessFinder$$anon$$$outer()|",
      "|java+method:///scala/collection/mutable/Map/apply(java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Set/$plus$eq(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/FieldAccessFinder/FieldAccessFinder(scala.collection.mutable.Map,boolean,scala.Option,scala.collection.mutable.Set)|",
      "|java+method:///java/lang/String/endsWith(java.lang.String)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getClassReader(java.lang.Class)|",
      "|java+method:///com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/ClassReader/accept(com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassVisitor,int)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/SetLike/$plus$eq(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/MethodIdentifier/MethodIdentifier(java.lang.Class,java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/mutable/Set/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/FieldAccessFinder$$anon$3/org$apache$spark$util$FieldAccessFinder$$anon$$$outer()|",
      "|java+method:///scala/collection/mutable/Map/apply(java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getClassReader(java.lang.Class)|",
      "|java+method:///scala/collection/mutable/Set/$plus$eq(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/FieldAccessFinder/FieldAccessFinder(scala.collection.mutable.Map,boolean,scala.Option,scala.collection.mutable.Set)|",
      "|java+method:///org/apache/xbean/asm5/ClassReader/accept(org.apache.xbean.asm5.ClassVisitor,int)|",
      "|java+method:///java/lang/String/endsWith(java.lang.String)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$stackTraceToJson$1/apply(java.lang.StackTraceElement)|",
    "called": "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/json4s/JsonDSL$/pair2jvalue(scala.Tuple2,scala.Function1)|",
      "|java+method:///org/json4s/JsonDSL$JsonAssoc/$tilde(scala.Tuple2,scala.Function1)|",
      "|java+method:///scala/Predef$/any2ArrowAssoc(java.lang.Object)|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$stackTraceToJson$1$$anonfun$apply$8/JsonProtocol$$anonfun$stackTraceToJson$1$$anonfun$apply$8(org.apache.spark.util.JsonProtocol$$anonfun$stackTraceToJson$1)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$stackTraceToJson$1$$anonfun$apply$9/JsonProtocol$$anonfun$stackTraceToJson$1$$anonfun$apply$9(org.apache.spark.util.JsonProtocol$$anonfun$stackTraceToJson$1)|",
      "|java+method:///java/lang/StackTraceElement/getFileName()|",
      "|java+method:///org/json4s/JsonDSL$/pair2Assoc(scala.Tuple2,scala.Function1)|",
      "|java+method:///org/json4s/JsonDSL$/jobject2assoc(org.json4s.JsonAST$JObject)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$stackTraceToJson$1$$anonfun$apply$11/JsonProtocol$$anonfun$stackTraceToJson$1$$anonfun$apply$11(org.apache.spark.util.JsonProtocol$$anonfun$stackTraceToJson$1)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$stackTraceToJson$1$$anonfun$apply$10/JsonProtocol$$anonfun$stackTraceToJson$1$$anonfun$apply$10(org.apache.spark.util.JsonProtocol$$anonfun$stackTraceToJson$1)|",
      "|java+method:///java/lang/StackTraceElement/getLineNumber()|",
      "|java+method:///java/lang/StackTraceElement/getClassName()|",
      "|java+method:///org/json4s/JsonDSL$JsonListAssoc/$tilde(org.json4s.JsonAST$JObject)|",
      "|java+method:///java/lang/StackTraceElement/getMethodName()|"
    ],
    "v2Body": [
      "|java+method:///org/json4s/JsonDSL$/pair2jvalue(scala.Tuple2,scala.Function1)|",
      "|java+method:///org/json4s/JsonDSL$JsonAssoc/$tilde(scala.Tuple2,scala.Function1)|",
      "|java+method:///scala/Predef$/any2ArrowAssoc(java.lang.Object)|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$stackTraceToJson$1$$anonfun$apply$9/JsonProtocol$$anonfun$stackTraceToJson$1$$anonfun$apply$9(org.apache.spark.util.JsonProtocol$$anonfun$stackTraceToJson$1)|",
      "|java+method:///java/lang/StackTraceElement/getFileName()|",
      "|java+method:///org/json4s/JsonDSL$/pair2Assoc(scala.Tuple2,scala.Function1)|",
      "|java+method:///org/json4s/JsonDSL$/jobject2assoc(org.json4s.JsonAST$JObject)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$stackTraceToJson$1$$anonfun$apply$12/JsonProtocol$$anonfun$stackTraceToJson$1$$anonfun$apply$12(org.apache.spark.util.JsonProtocol$$anonfun$stackTraceToJson$1)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$stackTraceToJson$1$$anonfun$apply$11/JsonProtocol$$anonfun$stackTraceToJson$1$$anonfun$apply$11(org.apache.spark.util.JsonProtocol$$anonfun$stackTraceToJson$1)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$stackTraceToJson$1$$anonfun$apply$10/JsonProtocol$$anonfun$stackTraceToJson$1$$anonfun$apply$10(org.apache.spark.util.JsonProtocol$$anonfun$stackTraceToJson$1)|",
      "|java+method:///java/lang/StackTraceElement/getLineNumber()|",
      "|java+method:///java/lang/StackTraceElement/getClassName()|",
      "|java+method:///org/json4s/JsonDSL$JsonListAssoc/$tilde(org.json4s.JsonAST$JObject)|",
      "|java+method:///java/lang/StackTraceElement/getMethodName()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3/apply()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3/apply()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$53/apply(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$removeExecutor$1/apply(java.lang.String)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$removeExecutor$1$$anonfun$apply$11/TaskSchedulerImpl$$anonfun$removeExecutor$1$$anonfun$apply$11(org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$removeExecutor$1,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/hostsByRack()|"
    ],
    "v2Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$removeExecutor$1$$anonfun$apply$12/TaskSchedulerImpl$$anonfun$removeExecutor$1$$anonfun$apply$12(org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$removeExecutor$1,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/hostsByRack()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$9/apply(java.lang.String)|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/Utils$/splitCommandString(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage/makeTimeline(scala.collection.Seq,long)|",
    "called": "|java+method:///scala/xml/Unparsed$/apply(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+method:///scala/xml/Elem/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/IterableLike/take(int)|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/TIMELINE_LEGEND()|",
      "|java+method:///scala/Predef$/$scope()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$66/StagePage$$anonfun$66(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$59/StagePage$$anonfun$59(org.apache.spark.ui.jobs.StagePage,long,scala.collection.mutable.HashSet,scala.runtime.LongRef,scala.runtime.LongRef)|",
      "|java+constructor:///scala/runtime/LongRef/LongRef(long)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$58/StagePage$$anonfun$58(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///scala/collection/Seq/sortBy(scala.Function1,scala.math.Ordering)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/xml/NodeSeq$/canBuildFrom()|",
      "|java+method:///scala/xml/Unparsed$/apply(java.lang.String)|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/MAX_TIMELINE_TASKS()|",
      "|java+method:///scala/collection/mutable/HashSet/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/HashSet$/canBuildFrom()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+method:///scala/xml/Elem/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$70/StagePage$$anonfun$70(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/TIMELINE_LEGEND()|",
      "|java+constructor:///scala/runtime/LongRef/LongRef(long)|",
      "|java+method:///scala/collection/Seq/sortBy(scala.Function1,scala.math.Ordering)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/xml/NodeSeq$/canBuildFrom()|",
      "|java+method:///scala/xml/Unparsed$/apply(java.lang.String)|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/MAX_TIMELINE_TASKS()|",
      "|java+method:///scala/collection/mutable/HashSet/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/HashSet$/canBuildFrom()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+method:///scala/collection/IterableLike/take(int)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$62/StagePage$$anonfun$62(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$63/StagePage$$anonfun$63(org.apache.spark.ui.jobs.StagePage,long,scala.collection.mutable.HashSet,scala.runtime.LongRef,scala.runtime.LongRef)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/Utils$/findLocalInetAddress()|",
    "called": "|java+method:///scala/collection/JavaConverters$/enumerationAsScalaIteratorConverter(java.util.Enumeration)|",
    "v1Body": [
      "|java+method:///scala/collection/immutable/List/reverse()|",
      "|java+method:///java/net/InetAddress/getLocalHost()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$findLocalInetAddress$2/Utils$$anonfun$findLocalInetAddress$2(java.net.InetAddress)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$findLocalInetAddress$1/Utils$$anonfun$findLocalInetAddress$1(java.net.InetAddress,java.lang.Object)|",
      "|java+method:///java/net/NetworkInterface/getNetworkInterfaces()|",
      "|java+method:///java/lang/System/getenv(java.lang.String)|",
      "|java+method:///scala/runtime/NonLocalReturnControl/key()|",
      "|java+method:///org/apache/spark/util/Utils$/isWindows()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///java/net/InetAddress/getByName(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/logWarning(scala.Function0)|",
      "|java+method:///scala/collection/immutable/List/foreach(scala.Function1)|",
      "|java+method:///java/net/InetAddress/isLoopbackAddress()|",
      "|java+method:///scala/collection/Iterator/toList()|",
      "|java+method:///scala/collection/JavaConversions$/enumerationAsScalaIterator(java.util.Enumeration)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$findLocalInetAddress$3/Utils$$anonfun$findLocalInetAddress$3()|",
      "|java+method:///scala/runtime/NonLocalReturnControl/value()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Seq/reverse()|",
      "|java+method:///java/net/InetAddress/getLocalHost()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$findLocalInetAddress$1/Utils$$anonfun$findLocalInetAddress$1(java.net.InetAddress,java.lang.Object)|",
      "|java+method:///java/net/NetworkInterface/getNetworkInterfaces()|",
      "|java+method:///scala/collection/TraversableOnce/toSeq()|",
      "|java+method:///java/lang/System/getenv(java.lang.String)|",
      "|java+method:///scala/runtime/NonLocalReturnControl/key()|",
      "|java+method:///org/apache/spark/util/Utils$/isWindows()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///java/net/InetAddress/getByName(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$findLocalInetAddress$2/Utils$$anonfun$findLocalInetAddress$2(java.net.InetAddress)|",
      "|java+method:///scala/collection/JavaConverters$/enumerationAsScalaIteratorConverter(java.util.Enumeration)|",
      "|java+method:///org/apache/spark/util/Utils$/logWarning(scala.Function0)|",
      "|java+method:///java/net/InetAddress/isLoopbackAddress()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$findLocalInetAddress$3/Utils$$anonfun$findLocalInetAddress$3()|",
      "|java+method:///scala/runtime/NonLocalReturnControl/value()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ExecutorAllocationManager/org$apache$spark$ExecutorAllocationManager$$onExecutorBusy(java.lang.String)|",
    "called": "|java+method:///scala/collection/mutable/HashSet/remove(java.lang.Object)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/ExecutorAllocationManager$$anonfun$org$apache$spark$ExecutorAllocationManager$$onExecutorBusy$1/ExecutorAllocationManager$$anonfun$org$apache$spark$ExecutorAllocationManager$$onExecutorBusy$1(org.apache.spark.ExecutorAllocationManager,java.lang.String)|",
      "|java+method:///org/apache/spark/ExecutorAllocationManager/logDebug(scala.Function0)|",
      "|java+method:///scala/collection/mutable/HashMap/remove(java.lang.Object)|",
      "|java+method:///org/apache/spark/ExecutorAllocationManager/removeTimes()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ExecutorAllocationManager/logDebug(scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/collection/mutable/HashSet/remove(java.lang.Object)|",
      "|java+method:///org/apache/spark/ExecutorAllocationManager/org$apache$spark$ExecutorAllocationManager$$executorsPendingToRemove()|",
      "|java+constructor:///org/apache/spark/ExecutorAllocationManager$$anonfun$org$apache$spark$ExecutorAllocationManager$$onExecutorBusy$1/ExecutorAllocationManager$$anonfun$org$apache$spark$ExecutorAllocationManager$$onExecutorBusy$1(org.apache.spark.ExecutorAllocationManager,java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/remove(java.lang.Object)|",
      "|java+method:///org/apache/spark/ExecutorAllocationManager/removeTimes()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$102/TaskDataSource$$anonfun$102(org.apache.spark.ui.jobs.TaskDataSource)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/Utils$$anonfun$getPropertiesFromFile$3/apply(java.lang.String)|",
    "called": "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/propertiesAsScalaMap(java.util.Properties)|",
      "|java+method:///java/lang/String/trim()|",
      "|java+method:///scala/collection/mutable/Map/apply(java.lang.Object)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///java/util/Properties/getProperty(java.lang.String)|",
      "|java+method:///java/lang/String/trim()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$68/apply(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$/accumulableInfoFromJson(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$65/JsonProtocol$$anonfun$65()|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator$$anonfun$6/ExternalAppendOnlyMap$DiskMapIterator$$anonfun$6(org.apache.spark.util.collection.ExternalAppendOnlyMap$DiskMapIterator)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore/currentUnrollMemoryForThisTask()|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+constructor:///org/apache/spark/storage/MemoryStore$$anonfun$currentUnrollMemoryForThisTask$1/MemoryStore$$anonfun$currentUnrollMemoryForThisTask$1(org.apache.spark.storage.MemoryStore)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/accountingLock()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/currentTaskAttemptId()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/unrollMemoryMap()|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+constructor:///org/apache/spark/storage/MemoryStore$$anonfun$currentUnrollMemoryForThisTask$1/MemoryStore$$anonfun$currentUnrollMemoryForThisTask$1(org.apache.spark.storage.MemoryStore)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/currentTaskAttemptId()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/unrollMemoryMap()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$50/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$50/apply(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$50/apply(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/history/HistoryServer$/getAttemptURI(java.lang.String,scala.Option)|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/history/HistoryServer$/UI_PATH_PREFIX()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer$$anonfun$5/HistoryServer$$anonfun$5()|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer$$anonfun$4/HistoryServer$$anonfun$4()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/history/HistoryServer$/UI_PATH_PREFIX()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer$$anonfun$7/HistoryServer$$anonfun$7()|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer$$anonfun$6/HistoryServer$$anonfun$6()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/RPackageUtils$$anonfun$checkAndBuildRPackage$1/apply(java.lang.String)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/org$apache$spark$deploy$RPackageUtils$$print$default$3()|",
      "|java+constructor:///java/io/File/File(java.net.URI)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/util/Utils$/resolveURI(java.lang.String)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/org$apache$spark$deploy$RPackageUtils$$extractRFolder(java.util.jar.JarFile,java.io.PrintStream,boolean)|",
      "|java+constructor:///java/util/jar/JarFile/JarFile(java.io.File)|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/RJarDoc()|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/org$apache$spark$deploy$RPackageUtils$$print(java.lang.String,java.io.PrintStream,java.util.logging.Level,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/org$apache$spark$deploy$RPackageUtils$$print$default$4()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/checkManifestForR(java.util.jar.JarFile)|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/org$apache$spark$deploy$RPackageUtils$$rPackageBuilder(java.io.File,java.io.PrintStream,boolean)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///java/io/File/delete()|"
    ],
    "v2Body": [
      "|java+constructor:///java/io/File/File(java.net.URI)|",
      "|java+method:///org/apache/spark/api/r/RUtils$/rPackages_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/util/Utils$/createTempDir$default$1()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/util/Utils$/resolveURI(java.lang.String)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/org$apache$spark$deploy$RPackageUtils$$extractRFolder(java.util.jar.JarFile,java.io.PrintStream,boolean)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/org$apache$spark$deploy$RPackageUtils$$print$default$3()|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/org$apache$spark$deploy$RPackageUtils$$rPackageBuilder(java.io.File,java.io.PrintStream,boolean,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/RJarDoc()|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/org$apache$spark$deploy$RPackageUtils$$print(java.lang.String,java.io.PrintStream,java.util.logging.Level,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/org$apache$spark$deploy$RPackageUtils$$print$default$4()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/util/Utils$/createTempDir$default$2()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/checkManifestForR(java.util.jar.JarFile)|",
      "|java+method:///org/apache/spark/api/r/RUtils$/rPackages()|",
      "|java+method:///java/io/File/getAbsolutePath()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///java/util/jar/JarFile/JarFile(java.io.File)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/util/Utils$/createTempDir(java.lang.String,java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/deploy/RPackageUtils$$anonfun$checkAndBuildRPackage$1$$anonfun$apply$1/RPackageUtils$$anonfun$checkAndBuildRPackage$1$$anonfun$apply$1(org.apache.spark.deploy.RPackageUtils$$anonfun$checkAndBuildRPackage$1,java.io.File)|",
      "|java+method:///java/io/File/delete()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaSparkContext/union(org.apache.spark.api.java.JavaRDD,java.util.List)|",
    "called": "|java+method:///scala/collection/Seq$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkContext/union(scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/api/java/JavaRDD/classTag()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/api/java/JavaSparkContext$$anonfun$1/JavaSparkContext$$anonfun$1(org.apache.spark.api.java.JavaSparkContext)|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext/sc()|",
      "|java+method:///org/apache/spark/api/java/JavaRDD$/fromRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkContext/union(scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/api/java/JavaRDD/classTag()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/api/java/JavaSparkContext$$anonfun$1/JavaSparkContext$$anonfun$1(org.apache.spark.api.java.JavaSparkContext)|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext/sc()|",
      "|java+method:///org/apache/spark/api/java/JavaRDD$/fromRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$1$$anonfun$apply$mcVI$sp$1/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$1$$anonfun$apply$mcVI$sp$1/apply(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$1$$anonfun$apply$mcVI$sp$1/apply(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/SizeEstimator$/getClassInfo(java.lang.Class)|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/Int()|",
      "|java+method:///java/lang/Class/getSuperclass()|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/org$apache$spark$util$SizeEstimator$$pointerSize()|",
      "|java+constructor:///org/apache/spark/util/SizeEstimator$$anonfun$getClassInfo$3/SizeEstimator$$anonfun$getClassInfo$3(scala.runtime.ObjectRef,int%5B%5D)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/classInfos()|",
      "|java+method:///scala/collection/immutable/List/max(scala.math.Ordering)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/getClassInfo(java.lang.Class)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/fieldSizes()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/util/SizeEstimator$$anonfun$getClassInfo$2/SizeEstimator$$anonfun$getClassInfo$2(scala.runtime.LongRef,int%5B%5D,scala.runtime.LongRef)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$ClassInfo/pointerFields()|",
      "|java+method:///scala/collection/immutable/List/withFilter(scala.Function1)|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Array$/fill(int,scala.Function0,scala.reflect.ClassTag)|",
      "|java+constructor:///scala/runtime/LongRef/LongRef(long)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/org$apache$spark$util$SizeEstimator$$alignSizeUp(long,int)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/get(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/SizeEstimator$$anonfun$1/SizeEstimator$$anonfun$1()|",
      "|java+constructor:///org/apache/spark/util/SizeEstimator$ClassInfo/SizeEstimator$ClassInfo(long,scala.collection.immutable.List)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/SizeEstimator$$anonfun$getClassInfo$1/SizeEstimator$$anonfun$getClassInfo$1(int%5B%5D)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$ClassInfo/shellSize()|",
      "|java+method:///java/lang/Class/getDeclaredFields()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/immutable/List/withFilter(scala.Function1)|",
      "|java+method:///scala/reflect/ClassTag$/Int()|",
      "|java+method:///java/lang/Class/getSuperclass()|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/org$apache$spark$util$SizeEstimator$$pointerSize()|",
      "|java+constructor:///org/apache/spark/util/SizeEstimator$$anonfun$getClassInfo$3/SizeEstimator$$anonfun$getClassInfo$3(scala.runtime.ObjectRef,int%5B%5D)|",
      "|java+constructor:///org/apache/spark/util/SizeEstimator$$anonfun$1/SizeEstimator$$anonfun$1()|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/classInfos()|",
      "|java+method:///scala/collection/immutable/List/max(scala.math.Ordering)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/getClassInfo(java.lang.Class)|",
      "|java+method:///java/util/concurrent/ConcurrentMap/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/fieldSizes()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/util/SizeEstimator$$anonfun$getClassInfo$2/SizeEstimator$$anonfun$getClassInfo$2(scala.runtime.LongRef,int%5B%5D,scala.runtime.LongRef)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$ClassInfo/pointerFields()|",
      "|java+method:///scala/Array$/fill(int,scala.Function0,scala.reflect.ClassTag)|",
      "|java+constructor:///scala/runtime/LongRef/LongRef(long)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/org$apache$spark$util$SizeEstimator$$alignSizeUp(long,int)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/SizeEstimator$ClassInfo/SizeEstimator$ClassInfo(long,scala.collection.immutable.List)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/SizeEstimator$$anonfun$getClassInfo$1/SizeEstimator$$anonfun$getClassInfo$1(int%5B%5D)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$ClassInfo/shellSize()|",
      "|java+method:///java/lang/Class/getDeclaredFields()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///java/util/concurrent/ConcurrentMap/get(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/createExecutorInfo(java.util.List,java.lang.String)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo$Builder/setCommand(org.apache.mesos.Protos$CommandInfo$Builder)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable/newBuilder()|",
      "|java+method:///org/apache/mesos/Protos$Environment$Builder/addVariables(org.apache.mesos.Protos$Environment$Variable)|",
      "|java+method:///org/apache/spark/SparkContext/executorEnvs()|",
      "|java+method:///org/apache/mesos/protobuf/ByteString/copyFrom(byte%5B%5D)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$URI$Builder/setValue(java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$ExecutorID$Builder/build()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/foreach(scala.Function1)|",
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo$Builder/setExecutorId(org.apache.mesos.Protos$ExecutorID)|",
      "|java+method:///org/apache/mesos/Protos$Environment/newBuilder()|",
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo$Builder/build()|",
      "|java+method:///org/apache/mesos/Protos$ExecutorID/newBuilder()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/partitionResources(java.util.List,java.lang.String,double)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable$Builder/setName(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/createExecArg()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo$Builder/setData(org.apache.mesos.protobuf.ByteString)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/addUris(org.apache.mesos.Protos$CommandInfo$URI$Builder)|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable$Builder/build()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$createExecutorInfo$3/MesosSchedulerBackend$$anonfun$createExecutorInfo$3(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend,org.apache.mesos.Protos$CommandInfo$Builder)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$createExecutorInfo$1/MesosSchedulerBackend$$anonfun$createExecutorInfo$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend,org.apache.mesos.Protos$Environment$Builder)|",
      "|java+constructor:///java/io/File/File(java.lang.String,java.lang.String)|",
      "|java+method:///java/lang/Class/getName()|",
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo/newBuilder()|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/setEnvironment(org.apache.mesos.Protos$Environment$Builder)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$createExecutorInfo$2/MesosSchedulerBackend$$anonfun$createExecutorInfo$2(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend,org.apache.mesos.Protos$Environment$Builder)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$1/MesosSchedulerBackend$$anonfun$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$2/MesosSchedulerBackend$$anonfun$2(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$3/MesosSchedulerBackend$$anonfun$3(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$4/MesosSchedulerBackend$$anonfun$4(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$5/MesosSchedulerBackend$$anonfun$5(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$6/MesosSchedulerBackend$$anonfun$6(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend)|",
      "|java+method:///scala/collection/mutable/ArrayOps/last()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo$Builder/addAllResources(java.lang.Iterable)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/setValue(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/mesosExecutorCores()|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$URI/newBuilder()|",
      "|java+method:///java/io/File/getCanonicalPath()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$createExecutorInfo$4/MesosSchedulerBackend$$anonfun$createExecutorInfo$4(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend,org.apache.mesos.Protos$ExecutorInfo$Builder)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/calculateTotalMemory(org.apache.spark.SparkContext)|",
      "|java+method:///scala/collection/immutable/StringOps/split(char)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/mesos/Protos$ExecutorID$Builder/setValue(java.lang.String)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo/newBuilder()|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable$Builder/setValue(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/head()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo$Builder/setCommand(org.apache.mesos.Protos$CommandInfo$Builder)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable/newBuilder()|",
      "|java+method:///org/apache/mesos/Protos$Environment$Builder/addVariables(org.apache.mesos.Protos$Environment$Variable)|",
      "|java+method:///org/apache/spark/SparkContext/executorEnvs()|",
      "|java+method:///org/apache/mesos/protobuf/ByteString/copyFrom(byte%5B%5D)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$URI$Builder/setValue(java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$ExecutorID$Builder/build()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/foreach(scala.Function1)|",
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo$Builder/setExecutorId(org.apache.mesos.Protos$ExecutorID)|",
      "|java+method:///org/apache/mesos/Protos$Environment/newBuilder()|",
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo$Builder/build()|",
      "|java+method:///org/apache/mesos/Protos$ExecutorID/newBuilder()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/partitionResources(java.util.List,java.lang.String,double)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable$Builder/setName(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/createExecArg()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo$Builder/setData(org.apache.mesos.protobuf.ByteString)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/addUris(org.apache.mesos.Protos$CommandInfo$URI$Builder)|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable$Builder/build()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$createExecutorInfo$3/MesosSchedulerBackend$$anonfun$createExecutorInfo$3(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend,org.apache.mesos.Protos$CommandInfo$Builder)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$createExecutorInfo$1/MesosSchedulerBackend$$anonfun$createExecutorInfo$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend,org.apache.mesos.Protos$Environment$Builder)|",
      "|java+constructor:///java/io/File/File(java.lang.String,java.lang.String)|",
      "|java+method:///java/lang/Class/getName()|",
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo/newBuilder()|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/setEnvironment(org.apache.mesos.Protos$Environment$Builder)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$7/MesosSchedulerBackend$$anonfun$7(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$createExecutorInfo$2/MesosSchedulerBackend$$anonfun$createExecutorInfo$2(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend,org.apache.mesos.Protos$Environment$Builder)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$2/MesosSchedulerBackend$$anonfun$2(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$3/MesosSchedulerBackend$$anonfun$3(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$4/MesosSchedulerBackend$$anonfun$4(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$5/MesosSchedulerBackend$$anonfun$5(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$6/MesosSchedulerBackend$$anonfun$6(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend)|",
      "|java+method:///scala/collection/mutable/ArrayOps/last()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo$Builder/addAllResources(java.lang.Iterable)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/setValue(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/mesosExecutorCores()|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$URI/newBuilder()|",
      "|java+method:///java/io/File/getCanonicalPath()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$createExecutorInfo$4/MesosSchedulerBackend$$anonfun$createExecutorInfo$4(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend,org.apache.mesos.Protos$ExecutorInfo$Builder)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/calculateTotalMemory(org.apache.spark.SparkContext)|",
      "|java+method:///scala/collection/immutable/StringOps/split(char)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/mesos/Protos$ExecutorID$Builder/setValue(java.lang.String)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo/newBuilder()|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable$Builder/setValue(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/head()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore/unrollSafely(org.apache.spark.storage.BlockId,scala.collection.Iterator,scala.collection.mutable.ArrayBuffer)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/Any()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/accountingLock()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/maxUnrollMemory()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/reservePendingUnrollMemoryForThisTask(long)|",
      "|java+method:///scala/package$/Right()|",
      "|java+method:///org/apache/spark/util/collection/SizeTrackingVector/toArray()|",
      "|java+method:///scala/util/Right$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/storage/ResultWithDroppedBlocks/droppedBlocks()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/releaseUnrollMemoryForThisTask(long)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$plus$eq(scala.collection.TraversableOnce)|",
      "|java+constructor:///org/apache/spark/storage/MemoryStore$$anonfun$unrollSafely$1/MemoryStore$$anonfun$unrollSafely$1(org.apache.spark.storage.MemoryStore,org.apache.spark.storage.BlockId,long)|",
      "|java+method:///org/apache/spark/util/collection/SizeTrackingVector/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/ensureFreeSpace(org.apache.spark.storage.BlockId,long)|",
      "|java+method:///scala/collection/Iterator/$plus$plus(scala.Function0)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/currentUnrollMemoryForThisTask()|",
      "|java+constructor:///org/apache/spark/storage/MemoryStore$$anonfun$unrollSafely$2/MemoryStore$$anonfun$unrollSafely$2(org.apache.spark.storage.MemoryStore,scala.collection.Iterator)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/reserveUnrollMemoryForThisTask(long)|",
      "|java+method:///scala/collection/Iterator/next()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/logUnrollFailureMessage(org.apache.spark.storage.BlockId,long)|",
      "|java+constructor:///org/apache/spark/util/collection/SizeTrackingVector/SizeTrackingVector(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$unrollMemoryThreshold()|",
      "|java+method:///org/apache/spark/util/collection/SizeTrackingVector/estimateSize()|",
      "|java+method:///scala/package$/Left()|",
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+method:///org/apache/spark/util/collection/SizeTrackingVector/iterator()|",
      "|java+method:///scala/util/Left$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/currentUnrollMemory()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/collection/SizeTrackingVector/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/reserveUnrollMemoryForThisTask(org.apache.spark.storage.BlockId,long,scala.collection.mutable.Buffer)|",
      "|java+constructor:///org/apache/spark/storage/MemoryStore$$anonfun$unrollSafely$1/MemoryStore$$anonfun$unrollSafely$1(org.apache.spark.storage.MemoryStore)|",
      "|java+method:///scala/package$/Right()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/currentTaskAttemptId()|",
      "|java+method:///org/apache/spark/util/collection/SizeTrackingVector/toArray()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/pendingUnrollMemoryMap()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/logWarning(scala.Function0)|",
      "|java+method:///scala/util/Left$/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/storage/MemoryStore$$anonfun$unrollSafely$2/MemoryStore$$anonfun$unrollSafely$2(org.apache.spark.storage.MemoryStore,org.apache.spark.storage.BlockId,long)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ClassTag$/Any()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///scala/collection/Iterator/$plus$plus(scala.Function0)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/currentUnrollMemoryForThisTask()|",
      "|java+constructor:///org/apache/spark/storage/MemoryStore$$anonfun$unrollSafely$3/MemoryStore$$anonfun$unrollSafely$3(org.apache.spark.storage.MemoryStore,scala.collection.Iterator)|",
      "|java+method:///scala/collection/Iterator/next()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/logUnrollFailureMessage(org.apache.spark.storage.BlockId,long)|",
      "|java+constructor:///org/apache/spark/util/collection/SizeTrackingVector/SizeTrackingVector(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$unrollMemoryThreshold()|",
      "|java+method:///org/apache/spark/util/collection/SizeTrackingVector/estimateSize()|",
      "|java+method:///scala/package$/Left()|",
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/util/collection/SizeTrackingVector/iterator()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/unrollMemoryMap()|",
      "|java+method:///scala/util/Right$/apply(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaSparkContext/jars()|",
    "called": "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkContext/jars()|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext/sc()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///org/apache/spark/SparkContext/jars()|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext/sc()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaHadoopRDD$$anonfun$mapPartitionsWithInputSplit$1/apply(org.apache.hadoop.mapred.InputSplit,scala.collection.Iterator)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/java/function/Function2/call(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaIterator(java.util.Iterator)|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterator(scala.collection.Iterator)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///org/apache/spark/api/java/function/Function2/call(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaIteratorConverter(java.util.Iterator)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/ReliableCheckpointRDD/getPartitions()|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Array$/tabulate(int,scala.Function1,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/listStatus(org.apache.hadoop.fs.Path)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayOps/zipWithIndex(scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/ArrayOps/sortBy(scala.Function1,scala.math.Ordering)|",
      "|java+method:///scala/collection/mutable/ArrayOps/filter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/rdd/ReliableCheckpointRDD$$anonfun$getPartitions$2/ReliableCheckpointRDD$$anonfun$getPartitions$2(org.apache.spark.rdd.ReliableCheckpointRDD)|",
      "|java+constructor:///org/apache/spark/rdd/ReliableCheckpointRDD$$anonfun$2/ReliableCheckpointRDD$$anonfun$2(org.apache.spark.rdd.ReliableCheckpointRDD)|",
      "|java+constructor:///org/apache/spark/rdd/ReliableCheckpointRDD$$anonfun$3/ReliableCheckpointRDD$$anonfun$3(org.apache.spark.rdd.ReliableCheckpointRDD)|",
      "|java+constructor:///org/apache/spark/rdd/ReliableCheckpointRDD$$anonfun$4/ReliableCheckpointRDD$$anonfun$4(org.apache.spark.rdd.ReliableCheckpointRDD)|",
      "|java+constructor:///org/apache/spark/rdd/ReliableCheckpointRDD$$anonfun$getPartitions$1/ReliableCheckpointRDD$$anonfun$getPartitions$1(org.apache.spark.rdd.ReliableCheckpointRDD)|",
      "|java+method:///org/apache/spark/rdd/ReliableCheckpointRDD/cpath()|",
      "|java+method:///org/apache/spark/rdd/ReliableCheckpointRDD/fs()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Array$/tabulate(int,scala.Function1,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/listStatus(org.apache.hadoop.fs.Path)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayOps/zipWithIndex(scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/ArrayOps/sortBy(scala.Function1,scala.math.Ordering)|",
      "|java+method:///scala/collection/mutable/ArrayOps/filter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/rdd/ReliableCheckpointRDD$$anonfun$getPartitions$2/ReliableCheckpointRDD$$anonfun$getPartitions$2(org.apache.spark.rdd.ReliableCheckpointRDD)|",
      "|java+constructor:///org/apache/spark/rdd/ReliableCheckpointRDD$$anonfun$4/ReliableCheckpointRDD$$anonfun$4(org.apache.spark.rdd.ReliableCheckpointRDD)|",
      "|java+constructor:///org/apache/spark/rdd/ReliableCheckpointRDD$$anonfun$5/ReliableCheckpointRDD$$anonfun$5(org.apache.spark.rdd.ReliableCheckpointRDD)|",
      "|java+constructor:///org/apache/spark/rdd/ReliableCheckpointRDD$$anonfun$6/ReliableCheckpointRDD$$anonfun$6(org.apache.spark.rdd.ReliableCheckpointRDD)|",
      "|java+constructor:///org/apache/spark/rdd/ReliableCheckpointRDD$$anonfun$getPartitions$1/ReliableCheckpointRDD$$anonfun$getPartitions$1(org.apache.spark.rdd.ReliableCheckpointRDD)|",
      "|java+method:///org/apache/spark/rdd/ReliableCheckpointRDD/cpath()|",
      "|java+method:///org/apache/spark/rdd/ReliableCheckpointRDD/fs()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/serializer/JavaIterableWrapperSerializer/read(com.esotericsoftware.kryo.Kryo,com.esotericsoftware.kryo.io.Input,java.lang.Class)|",
    "called": "|java+method:///scala/collection/JavaConverters$/asJavaIterableConverter(scala.collection.Iterable)|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/asJavaIterable(scala.collection.Iterable)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/serializer/JavaIterableWrapperSerializer/read(com.esotericsoftware.kryo.Kryo,com.esotericsoftware.kryo.io.Input,java.lang.Class)|",
      "|java+method:///com/esotericsoftware/kryo/Kryo/readClassAndObject(com.esotericsoftware.kryo.io.Input)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIterableConverter(scala.collection.Iterable)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/serializer/JavaIterableWrapperSerializer/read(com.esotericsoftware.kryo.Kryo,com.esotericsoftware.kryo.io.Input,java.lang.Class)|",
      "|java+method:///com/esotericsoftware/kryo/Kryo/readClassAndObject(com.esotericsoftware.kryo.io.Input)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anon$2$$anonfun$run$1/apply$mcV$sp()|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$registerMasterThreadPool()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$registerMasterFutures()|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anon$2$$anonfun$run$1$$anonfun$apply$mcV$sp$1/AppClient$ClientEndpoint$$anon$2$$anonfun$run$1$$anonfun$apply$mcV$sp$1(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$2$$anonfun$run$1)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$$outer()|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/org$apache$spark$deploy$client$AppClient$$REGISTRATION_RETRIES()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anon$2/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$anon$$$outer()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/org$apache$spark$deploy$client$AppClient$$registered()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/markDead(java.lang.String)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///java/util/concurrent/ThreadPoolExecutor/shutdownNow()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$registerWithMaster(int)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anon$2$$anonfun$run$1$$anonfun$apply$mcV$sp$2/AppClient$ClientEndpoint$$anon$2$$anonfun$run$1$$anonfun$apply$mcV$sp$2(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$2$$anonfun$run$1)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$registerMasterThreadPool()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$registerMasterFutures()|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anon$2$$anonfun$run$1$$anonfun$apply$mcV$sp$1/AppClient$ClientEndpoint$$anon$2$$anonfun$run$1$$anonfun$apply$mcV$sp$1(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$2$$anonfun$run$1)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$$outer()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/org$apache$spark$deploy$client$AppClient$$REGISTRATION_RETRIES()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/org$apache$spark$deploy$client$AppClient$$registered()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/markDead(java.lang.String)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///java/util/concurrent/ThreadPoolExecutor/shutdownNow()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$registerWithMaster(int)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anon$2$$anonfun$run$1$$anonfun$apply$mcV$sp$2/AppClient$ClientEndpoint$$anon$2$$anonfun$run$1$$anonfun$apply$mcV$sp$2(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$2$$anonfun$run$1)|",
      "|java+method:///java/util/concurrent/atomic/AtomicBoolean/get()|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anon$2/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$anon$$$outer()|",
      "|java+method:///java/util/concurrent/atomic/AtomicReference/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$getDependencies$1/apply(org.apache.spark.rdd.RDD)|",
    "called": "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
    "v1Body": [
      "|java+method:///org/apache/spark/rdd/CoGroupedRDD/logDebug(scala.Function0)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/ShuffleDependency$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/ShuffleDependency$/$lessinit$greater$default$6()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitioner()|",
      "|java+method:///org/apache/spark/rdd/CoGroupedRDD/org$apache$spark$rdd$CoGroupedRDD$$serializer()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/OneToOneDependency/OneToOneDependency(org.apache.spark.rdd.RDD)|",
      "|java+constructor:///org/apache/spark/ShuffleDependency/ShuffleDependency(org.apache.spark.rdd.RDD,org.apache.spark.Partitioner,scala.Option,scala.Option,scala.Option,boolean)|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$getDependencies$1$$anonfun$apply$1/CoGroupedRDD$$anonfun$getDependencies$1$$anonfun$apply$1(org.apache.spark.rdd.CoGroupedRDD$$anonfun$getDependencies$1,org.apache.spark.rdd.RDD)|",
      "|java+method:///org/apache/spark/ShuffleDependency$/$lessinit$greater$default$5()|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$getDependencies$1$$anonfun$apply$2/CoGroupedRDD$$anonfun$getDependencies$1$$anonfun$apply$2(org.apache.spark.rdd.CoGroupedRDD$$anonfun$getDependencies$1,org.apache.spark.rdd.RDD)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/CoGroupedRDD/logDebug(scala.Function0)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/ShuffleDependency$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/ShuffleDependency$/$lessinit$greater$default$6()|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD/partitioner()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/OneToOneDependency/OneToOneDependency(org.apache.spark.rdd.RDD)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/reflect/ClassTag$/Any()|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$getDependencies$1$$anonfun$apply$1/CoGroupedRDD$$anonfun$getDependencies$1$$anonfun$apply$1(org.apache.spark.rdd.CoGroupedRDD$$anonfun$getDependencies$1,org.apache.spark.rdd.RDD)|",
      "|java+method:///org/apache/spark/ShuffleDependency$/$lessinit$greater$default$5()|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$getDependencies$1$$anonfun$apply$2/CoGroupedRDD$$anonfun$getDependencies$1$$anonfun$apply$2(org.apache.spark.rdd.CoGroupedRDD$$anonfun$getDependencies$1,org.apache.spark.rdd.RDD)|",
      "|java+method:///org/apache/spark/rdd/CoGroupedRDD/org$apache$spark$rdd$CoGroupedRDD$$serializer()|",
      "|java+constructor:///org/apache/spark/ShuffleDependency/ShuffleDependency(org.apache.spark.rdd.RDD,org.apache.spark.Partitioner,scala.Option,scala.Option,scala.Option,boolean,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.reflect.ClassTag)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$reportHeartBeat()|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$computeTotalGcTime()|",
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/executor/Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$2/Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$2(org.apache.spark.executor.Executor)|",
      "|java+method:///org/apache/spark/executor/Executor/logInfo(scala.Function0)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/toArray(scala.reflect.ClassTag)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/storage/BlockManager/reregister()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/storage/BlockManager/blockManagerId()|",
      "|java+method:///scala/collection/JavaConversions$/collectionAsScalaIterable(java.util.Collection)|",
      "|java+method:///org/apache/spark/HeartbeatResponse/reregisterBlockManager()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/executor/Executor/heartbeatReceiverRef()|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/values()|",
      "|java+constructor:///org/apache/spark/Heartbeat/Heartbeat(java.lang.String,scala.Tuple2%5B%5D,org.apache.spark.storage.BlockManagerId)|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|",
      "|java+constructor:///org/apache/spark/executor/Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$3/Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$3(org.apache.spark.executor.Executor)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$runningTasks()|",
      "|java+constructor:///org/apache/spark/executor/Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$1/Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$1(org.apache.spark.executor.Executor,scala.collection.mutable.ArrayBuffer,long)|",
      "|java+method:///org/apache/spark/executor/Executor/logWarning(scala.Function0,java.lang.Throwable)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$computeTotalGcTime()|",
      "|java+method:///org/apache/spark/storage/BlockManager/blockManagerId()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/executor/Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$2/Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$2(org.apache.spark.executor.Executor)|",
      "|java+method:///org/apache/spark/executor/Executor/logInfo(scala.Function0)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/toArray(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/JavaConverters$/collectionAsScalaIterableConverter(java.util.Collection)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/storage/BlockManager/reregister()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/HeartbeatResponse/reregisterBlockManager()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/executor/Executor/heartbeatReceiverRef()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/values()|",
      "|java+constructor:///org/apache/spark/Heartbeat/Heartbeat(java.lang.String,scala.Tuple2%5B%5D,org.apache.spark.storage.BlockManagerId)|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|",
      "|java+constructor:///org/apache/spark/executor/Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$3/Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$3(org.apache.spark.executor.Executor)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$runningTasks()|",
      "|java+constructor:///org/apache/spark/executor/Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$1/Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$1(org.apache.spark.executor.Executor,scala.collection.mutable.ArrayBuffer,long)|",
      "|java+method:///org/apache/spark/executor/Executor/logWarning(scala.Function0,java.lang.Throwable)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$18/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$18/apply(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$18/apply(org.apache.spark.scheduler.ActiveJob)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaPairRDD$$anonfun$cogroupResult2ToJava$1/apply(scala.Tuple3)|",
    "called": "|java+method:///scala/collection/JavaConverters$/asJavaIterableConverter(scala.collection.Iterable)|",
    "v1Body": [
      "|java+method:///scala/Tuple3/_1()|",
      "|java+method:///scala/Tuple3/_2()|",
      "|java+method:///scala/Tuple3/_3()|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterable(scala.collection.Iterable)|",
      "|java+constructor:///scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIterableConverter(scala.collection.Iterable)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/Tuple3/_1()|",
      "|java+method:///scala/Tuple3/_2()|",
      "|java+method:///scala/Tuple3/_3()|",
      "|java+constructor:///scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/Utils$/loadDefaultSparkProperties(org.apache.spark.SparkConf,java.lang.String)|",
    "called": "|java+method:///scala/Option/getOrElse(scala.Function0)|",
    "v1Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$11/Utils$$anonfun$11()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$loadDefaultSparkProperties$1/Utils$$anonfun$loadDefaultSparkProperties$1(org.apache.spark.SparkConf)|"
    ],
    "v2Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$10/Utils$$anonfun$10()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$loadDefaultSparkProperties$1/Utils$$anonfun$loadDefaultSparkProperties$1(org.apache.spark.SparkConf)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/ui/ApplicationPage/render(javax.servlet.http.HttpServletRequest)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/deploy/master/ui/ApplicationPage$$anonfun$2/ApplicationPage$$anonfun$2(org.apache.spark.deploy.master.ui.ApplicationPage,java.lang.String,org.apache.spark.deploy.DeployMessages$MasterStateResponse)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$4()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/state()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/basicSparkPage(scala.Function0,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/maxCores()|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/ApplicationPage$$anonfun$render$2/ApplicationPage$$anonfun$render$2(org.apache.spark.deploy.master.ui.ApplicationPage,scala.xml.NodeBuffer)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/coresLeft()|",
      "|java+method:///scala/collection/TraversableOnce/toSet()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/submitDate()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/coresGranted()|",
      "|java+method:///javax/servlet/http/HttpServletRequest/getParameter(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/ApplicationPage$$anonfun$render$1/ApplicationPage$$anonfun$render$1(org.apache.spark.deploy.master.ui.ApplicationPage,scala.xml.Elem)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable(scala.collection.Seq,scala.Function1,scala.collection.Iterable,boolean,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/ApplicationPage$$anonfun$1/ApplicationPage$$anonfun$1(org.apache.spark.deploy.master.ui.ApplicationPage,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterStateResponse/activeApps()|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///scala/collection/Seq/filter(scala.Function1)|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$5()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$6()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/name()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/user()|",
      "|java+method:///scala/xml/Elem/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/xml/NodeSeq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$7()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/desc()|",
      "|java+constructor:///scala/xml/Comment/Comment(java.lang.String)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/megabytesToString(long)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$8()|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/id()|",
      "|java+method:///scala/collection/Iterable/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq/diff(scala.collection.GenSeq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/appUiUrl()|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/removedExecutors()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/ui/ApplicationPage/master()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/memoryPerExecutorMB()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/executors()|",
      "|java+method:///scala/collection/immutable/Set/toSeq()|",
      "|java+method:///scala/collection/mutable/ArrayOps/find(scala.Function1)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/ApplicationPage$$anonfun$4/ApplicationPage$$anonfun$4(org.apache.spark.deploy.master.ui.ApplicationPage)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/ApplicationPage$$anonfun$5/ApplicationPage$$anonfun$5(org.apache.spark.deploy.master.ui.ApplicationPage)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/ApplicationPage$$anonfun$3/ApplicationPage$$anonfun$3(org.apache.spark.deploy.master.ui.ApplicationPage)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/deploy/master/ui/ApplicationPage$$anonfun$2/ApplicationPage$$anonfun$2(org.apache.spark.deploy.master.ui.ApplicationPage,java.lang.String,org.apache.spark.deploy.DeployMessages$MasterStateResponse)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$4()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/state()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/basicSparkPage(scala.Function0,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/maxCores()|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/ApplicationPage$$anonfun$render$2/ApplicationPage$$anonfun$render$2(org.apache.spark.deploy.master.ui.ApplicationPage,scala.xml.NodeBuffer)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/coresLeft()|",
      "|java+method:///scala/collection/TraversableOnce/toSet()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/submitDate()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/coresGranted()|",
      "|java+method:///javax/servlet/http/HttpServletRequest/getParameter(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/ApplicationPage$$anonfun$render$1/ApplicationPage$$anonfun$render$1(org.apache.spark.deploy.master.ui.ApplicationPage,scala.xml.Elem)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable(scala.collection.Seq,scala.Function1,scala.collection.Iterable,boolean,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/ApplicationPage$$anonfun$1/ApplicationPage$$anonfun$1(org.apache.spark.deploy.master.ui.ApplicationPage,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterStateResponse/activeApps()|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///scala/collection/Seq/filter(scala.Function1)|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$5()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$6()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/name()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/user()|",
      "|java+method:///scala/xml/Elem/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/xml/NodeSeq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$7()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/desc()|",
      "|java+constructor:///scala/xml/Comment/Comment(java.lang.String)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/megabytesToString(long)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$8()|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/id()|",
      "|java+method:///scala/collection/Iterable/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq/diff(scala.collection.GenSeq)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/curAppUIUrl()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/removedExecutors()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/ui/ApplicationPage/master()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/memoryPerExecutorMB()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/executors()|",
      "|java+method:///scala/collection/immutable/Set/toSeq()|",
      "|java+method:///scala/collection/mutable/ArrayOps/find(scala.Function1)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/ApplicationPage$$anonfun$4/ApplicationPage$$anonfun$4(org.apache.spark.deploy.master.ui.ApplicationPage)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/ApplicationPage$$anonfun$5/ApplicationPage$$anonfun$5(org.apache.spark.deploy.master.ui.ApplicationPage)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/ApplicationPage$$anonfun$3/ApplicationPage$$anonfun$3(org.apache.spark.deploy.master.ui.ApplicationPage)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient/org$apache$spark$deploy$rest$RestSubmissionClient$$reportSubmissionStatus(org.apache.spark.deploy.rest.CreateSubmissionResponse)|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$org$apache$spark$deploy$rest$RestSubmissionClient$$reportSubmissionStatus$1/RestSubmissionClient$$anonfun$org$apache$spark$deploy$rest$RestSubmissionClient$$reportSubmissionStatus$1(org.apache.spark.deploy.rest.RestSubmissionClient,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$3/RestSubmissionClient$$anonfun$3(org.apache.spark.deploy.rest.RestSubmissionClient)|",
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$org$apache$spark$deploy$rest$RestSubmissionClient$$reportSubmissionStatus$3/RestSubmissionClient$$anonfun$org$apache$spark$deploy$rest$RestSubmissionClient$$reportSubmissionStatus$3(org.apache.spark.deploy.rest.RestSubmissionClient,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient/logError(scala.Function0)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/rest/CreateSubmissionResponse/success()|",
      "|java+method:///org/apache/spark/deploy/rest/CreateSubmissionResponse/message()|",
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$org$apache$spark$deploy$rest$RestSubmissionClient$$reportSubmissionStatus$2/RestSubmissionClient$$anonfun$org$apache$spark$deploy$rest$RestSubmissionClient$$reportSubmissionStatus$2(org.apache.spark.deploy.rest.RestSubmissionClient)|",
      "|java+method:///org/apache/spark/deploy/rest/CreateSubmissionResponse/submissionId()|",
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$2/RestSubmissionClient$$anonfun$2(org.apache.spark.deploy.rest.RestSubmissionClient)|",
      "|java+method:///scala/Predef$/Boolean2boolean(java.lang.Boolean)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient/pollSubmissionStatus(java.lang.String)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$org$apache$spark$deploy$rest$RestSubmissionClient$$reportSubmissionStatus$1/RestSubmissionClient$$anonfun$org$apache$spark$deploy$rest$RestSubmissionClient$$reportSubmissionStatus$1(org.apache.spark.deploy.rest.RestSubmissionClient,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$4/RestSubmissionClient$$anonfun$4(org.apache.spark.deploy.rest.RestSubmissionClient)|",
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$3/RestSubmissionClient$$anonfun$3(org.apache.spark.deploy.rest.RestSubmissionClient)|",
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$org$apache$spark$deploy$rest$RestSubmissionClient$$reportSubmissionStatus$3/RestSubmissionClient$$anonfun$org$apache$spark$deploy$rest$RestSubmissionClient$$reportSubmissionStatus$3(org.apache.spark.deploy.rest.RestSubmissionClient,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/rest/CreateSubmissionResponse/submissionId()|",
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient/logError(scala.Function0)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/rest/CreateSubmissionResponse/success()|",
      "|java+method:///org/apache/spark/deploy/rest/CreateSubmissionResponse/message()|",
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$org$apache$spark$deploy$rest$RestSubmissionClient$$reportSubmissionStatus$2/RestSubmissionClient$$anonfun$org$apache$spark$deploy$rest$RestSubmissionClient$$reportSubmissionStatus$2(org.apache.spark.deploy.rest.RestSubmissionClient)|",
      "|java+method:///scala/Predef$/Boolean2boolean(java.lang.Boolean)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient/pollSubmissionStatus(java.lang.String)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$foldByKey$1/apply()|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkEnv/serializer()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKey(scala.Function1,scala.Function2,scala.Function2,org.apache.spark.Partitioner,boolean,org.apache.spark.serializer.Serializer)|",
      "|java+constructor:///scala/runtime/VolatileByteRef/VolatileByteRef(byte)|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/serialize(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKey$default$6()|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$foldByKey$1$$anonfun$2/PairRDDFunctions$$anonfun$foldByKey$1$$anonfun$2(org.apache.spark.rdd.PairRDDFunctions$$anonfun$foldByKey$1,byte%5B%5D,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)|",
      "|java+method:///org/apache/spark/rdd/RDD/context()|",
      "|java+method:///org/apache/spark/serializer/Serializer/newInstance()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///java/nio/ByteBuffer/get(byte%5B%5D)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$foldByKey$1$$anonfun$apply$11/PairRDDFunctions$$anonfun$foldByKey$1$$anonfun$apply$11(org.apache.spark.rdd.PairRDDFunctions$$anonfun$foldByKey$1,scala.Function0,scala.Function2)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$foldByKey$1/apply()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKey$default$5()|",
      "|java+method:///java/nio/ByteBuffer/limit()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKeyWithClassTag$default$5()|",
      "|java+constructor:///scala/runtime/VolatileByteRef/VolatileByteRef(byte)|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/serialize(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///java/nio/ByteBuffer/limit()|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$foldByKey$1$$anonfun$2/PairRDDFunctions$$anonfun$foldByKey$1$$anonfun$2(org.apache.spark.rdd.PairRDDFunctions$$anonfun$foldByKey$1,byte%5B%5D,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKeyWithClassTag$default$6()|",
      "|java+method:///org/apache/spark/SparkEnv/serializer()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKeyWithClassTag(scala.Function1,scala.Function2,scala.Function2,org.apache.spark.Partitioner,boolean,org.apache.spark.serializer.Serializer,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD/context()|",
      "|java+method:///org/apache/spark/serializer/Serializer/newInstance()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///java/nio/ByteBuffer/get(byte%5B%5D)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$foldByKey$1$$anonfun$apply$11/PairRDDFunctions$$anonfun$foldByKey$1$$anonfun$apply$11(org.apache.spark.rdd.PairRDDFunctions$$anonfun$foldByKey$1,scala.Function0,scala.Function2)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$foldByKey$1/apply()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$class/collectPartitions(org.apache.spark.api.java.JavaRDDLike,int%5B%5D)|",
    "called": "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function1,scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///scala/reflect/ClassTag/runtimeClass()|",
      "|java+method:///scala/Predef$/wrapIntArray(int%5B%5D)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/context()|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/classTag()|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///scala/collection/mutable/ArrayOps/toArray(scala.reflect.ClassTag)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/api/java/JavaRDDLike$$anonfun$1/JavaRDDLike$$anonfun$1(org.apache.spark.api.java.JavaRDDLike)|",
      "|java+constructor:///org/apache/spark/api/java/JavaRDDLike$$anonfun$collectPartitions$1/JavaRDDLike$$anonfun$collectPartitions$1(org.apache.spark.api.java.JavaRDDLike)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function1,scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///scala/reflect/ClassTag/runtimeClass()|",
      "|java+method:///scala/Predef$/wrapIntArray(int%5B%5D)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/context()|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/classTag()|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/api/java/JavaRDDLike$$anonfun$1/JavaRDDLike$$anonfun$1(org.apache.spark.api.java.JavaRDDLike)|",
      "|java+constructor:///org/apache/spark/api/java/JavaRDDLike$$anonfun$collectPartitions$1/JavaRDDLike$$anonfun$collectPartitions$1(org.apache.spark.api.java.JavaRDDLike)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore/getBytes(org.apache.spark.storage.BlockId)|",
    "called": "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///java/nio/ByteBuffer/duplicate()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataSerialize$default$3()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataSerialize(org.apache.spark.storage.BlockId,scala.collection.Iterator,org.apache.spark.serializer.Serializer)|",
      "|java+method:///scala/collection/mutable/ArrayOps/iterator()|",
      "|java+method:///org/apache/spark/storage/MemoryEntry/value()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockStore/blockManager()|",
      "|java+method:///org/apache/spark/storage/MemoryEntry/deserialized()|",
      "|java+method:///java/util/LinkedHashMap/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$entries()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataSerialize(org.apache.spark.storage.BlockId,scala.collection.Iterator)|",
      "|java+method:///java/nio/ByteBuffer/duplicate()|",
      "|java+method:///scala/collection/mutable/ArrayOps/iterator()|",
      "|java+method:///org/apache/spark/storage/MemoryEntry/value()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockStore/blockManager()|",
      "|java+method:///org/apache/spark/storage/MemoryEntry/deserialized()|",
      "|java+method:///java/util/LinkedHashMap/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$entries()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/PythonHadoopUtil$/mergeConfs(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)|",
    "called": "|java+method:///scala/collection/JavaConverters$/iterableAsScalaIterableConverter(java.lang.Iterable)|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/asScalaIterator(java.util.Iterator)|",
      "|java+method:///org/apache/hadoop/conf/Configuration/iterator()|",
      "|java+constructor:///org/apache/spark/api/python/PythonHadoopUtil$$anonfun$mergeConfs$1/PythonHadoopUtil$$anonfun$mergeConfs$1(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///scala/collection/Iterator/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/hadoop/conf/Configuration/Configuration(org.apache.hadoop.conf.Configuration)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/hadoop/conf/Configuration/Configuration(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///scala/collection/JavaConverters$/iterableAsScalaIterableConverter(java.lang.Iterable)|",
      "|java+constructor:///org/apache/spark/api/python/PythonHadoopUtil$$anonfun$mergeConfs$1/PythonHadoopUtil$$anonfun$mergeConfs$1(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3/apply(scala.Tuple2)|",
    "called": "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/Stage/jobIds()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3/removeStage$1(int)|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashSet/isEmpty()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$apply$4/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$apply$4(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3,int)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/jobId()|",
      "|java+method:///scala/Tuple2/_1$mcI$sp()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/Stage/jobIds()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/mutable/HashSet/isEmpty()|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3/removeStage$1(int)|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$apply$3/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$apply$3(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3,int)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/jobId()|",
      "|java+method:///scala/Tuple2/_1$mcI$sp()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/statusUpdate(org.apache.mesos.SchedulerDriver,org.apache.mesos.Protos$TaskStatus)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$1()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$6()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$9()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/retainedDrivers()|",
      "|java+constructor:///java/util/Date/Date()|",
      "|java+method:///org/apache/mesos/Protos$TaskID/getValue()|",
      "|java+method:///org/apache/spark/TaskState$/isFinished(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterSubmissionState/finishDate_$eq(scala.Option)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$statusUpdate$1/MesosClusterScheduler$$anonfun$statusUpdate$1(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/pendingRetryDriversState()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/size()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/trimStart(int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$pendingRecover()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy(java.lang.String,java.lang.String,int,double,boolean,org.apache.spark.deploy.Command,scala.collection.immutable.Map,java.lang.String,java.util.Date,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/stateLock()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$2()|",
      "|java+method:///scala/math/package$/max(int,int)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$3()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterSubmissionState/driverDescription()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterPersistenceEngine/persist(java.lang.String,java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$4()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/shouldRelaunch(org.apache.mesos.Protos$TaskState)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$5()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$7()|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getTaskId()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$8()|",
      "|java+method:///org/apache/spark/TaskState$/fromMesos(org.apache.mesos.Protos$TaskState)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/retryState()|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getReason()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/supervise()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/removeFromLaunchedDrivers(java.lang.String)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterRetryState/MesosClusterRetryState(org.apache.mesos.Protos$TaskStatus,int,java.util.Date,int)|",
      "|java+constructor:///java/util/Date/Date(long)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$14/MesosClusterScheduler$$anonfun$14(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$launchedDrivers()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$15/MesosClusterScheduler$$anonfun$15(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$finishedDrivers()|",
      "|java+constructor:///scala/Tuple2$mcII$sp/Tuple2$mcII$sp(int,int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$pendingRetryDrivers()|",
      "|java+method:///java/util/Date/getTime()|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getState()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterSubmissionState/mesosTaskStatus_$eq(scala.Option)|",
      "|java+method:///scala/Tuple2/_1$mcI$sp()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$1()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$6()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$9()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/retainedDrivers()|",
      "|java+constructor:///java/util/Date/Date()|",
      "|java+method:///org/apache/mesos/Protos$TaskID/getValue()|",
      "|java+method:///org/apache/spark/TaskState$/isFinished(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterSubmissionState/finishDate_$eq(scala.Option)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$statusUpdate$1/MesosClusterScheduler$$anonfun$statusUpdate$1(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/pendingRetryDriversState()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/size()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/trimStart(int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$pendingRecover()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy(java.lang.String,java.lang.String,int,double,boolean,org.apache.spark.deploy.Command,scala.collection.immutable.Map,java.lang.String,java.util.Date,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/stateLock()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$2()|",
      "|java+method:///scala/math/package$/max(int,int)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$3()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterSubmissionState/driverDescription()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterPersistenceEngine/persist(java.lang.String,java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$4()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/shouldRelaunch(org.apache.mesos.Protos$TaskState)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$5()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$7()|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getTaskId()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/copy$default$8()|",
      "|java+method:///org/apache/spark/TaskState$/fromMesos(org.apache.mesos.Protos$TaskState)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/retryState()|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getReason()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/supervise()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/removeFromLaunchedDrivers(java.lang.String)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterRetryState/MesosClusterRetryState(org.apache.mesos.Protos$TaskStatus,int,java.util.Date,int)|",
      "|java+constructor:///java/util/Date/Date(long)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$launchedDrivers()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$15/MesosClusterScheduler$$anonfun$15(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$finishedDrivers()|",
      "|java+constructor:///scala/Tuple2$mcII$sp/Tuple2$mcII$sp(int,int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$pendingRetryDrivers()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$16/MesosClusterScheduler$$anonfun$16(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+method:///java/util/Date/getTime()|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getState()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterSubmissionState/mesosTaskStatus_$eq(scala.Option)|",
      "|java+method:///scala/Tuple2/_1$mcI$sp()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$subtract$3/apply()|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$subtract$3$$anonfun$apply$32/RDD$$anonfun$subtract$3$$anonfun$apply$32(org.apache.spark.rdd.RDD$$anonfun$subtract$3)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$subtract$3$$anonfun$apply$34/RDD$$anonfun$subtract$3$$anonfun$apply$34(org.apache.spark.rdd.RDD$$anonfun$subtract$3)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$subtract$3$$anonfun$apply$35/RDD$$anonfun$subtract$3$$anonfun$apply$35(org.apache.spark.rdd.RDD$$anonfun$subtract$3)|",
      "|java+method:///scala/reflect/ClassTag$/Null()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$subtract$3$$anon$2/RDD$$anonfun$subtract$3$$anon$2(org.apache.spark.rdd.RDD$$anonfun$subtract$3)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/keys()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$subtract$3/apply()|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$subtract$3$$anonfun$apply$33/RDD$$anonfun$subtract$3$$anonfun$apply$33(org.apache.spark.rdd.RDD$$anonfun$subtract$3)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/RDD/partitioner()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/subtractByKey(org.apache.spark.rdd.RDD,org.apache.spark.Partitioner,scala.reflect.ClassTag)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/reflect/ClassTag$/Null()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$subtract$3$$anonfun$apply$37/RDD$$anonfun$subtract$3$$anonfun$apply$37(org.apache.spark.rdd.RDD$$anonfun$subtract$3)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$subtract$3$$anonfun$apply$38/RDD$$anonfun$subtract$3$$anonfun$apply$38(org.apache.spark.rdd.RDD$$anonfun$subtract$3)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$subtract$3$$anonfun$apply$39/RDD$$anonfun$subtract$3$$anonfun$apply$39(org.apache.spark.rdd.RDD$$anonfun$subtract$3)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/keys()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$subtract$3/apply()|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$subtract$3$$anon$2/RDD$$anonfun$subtract$3$$anon$2(org.apache.spark.rdd.RDD$$anonfun$subtract$3)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$subtract$3$$anonfun$apply$36/RDD$$anonfun$subtract$3$$anonfun$apply$36(org.apache.spark.rdd.RDD$$anonfun$subtract$3)|",
      "|java+method:///org/apache/spark/rdd/RDD/partitioner()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/subtractByKey(org.apache.spark.rdd.RDD,org.apache.spark.Partitioner,scala.reflect.ClassTag)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$$anonfun$fn$8$1/apply(scala.collection.Iterator)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/asScalaIterator(java.util.Iterator)|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterator(scala.collection.Iterator)|",
      "|java+method:///org/apache/spark/api/java/function/DoubleFlatMapFunction/call(java.lang.Object)|",
      "|java+method:///java/lang/Iterable/iterator()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaIteratorConverter(java.util.Iterator)|",
      "|java+method:///org/apache/spark/api/java/function/DoubleFlatMapFunction/call(java.lang.Object)|",
      "|java+method:///java/lang/Iterable/iterator()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master/removeApplication(org.apache.spark.deploy.master.ApplicationInfo,scala.Enumeration$Value)|",
    "called": "|java+method:///scala/collection/mutable/ArrayBuffer/$minus$eq(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/master/Master/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/id()|",
      "|java+method:///org/apache/spark/deploy/master/Master/RETAINED_APPLICATIONS()|",
      "|java+method:///org/apache/spark/deploy/master/Master/waitingApps()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/executors()|",
      "|java+method:///scala/collection/mutable/HashSet/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/master/Master/rebuildSparkUI(org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$removeApplication$1/Master$$anonfun$removeApplication$1(org.apache.spark.deploy.master.Master,org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/markFinished(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/driver()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$minus$eq(java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/size()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/trimStart(int)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationState$/FINISHED()|",
      "|java+method:///org/apache/spark/deploy/master/Master/idToApp()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$removeApplication$4/Master$$anonfun$removeApplication$4(org.apache.spark.deploy.master.Master,org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+method:///scala/math/package$/max(int,int)|",
      "|java+method:///org/apache/spark/deploy/master/Master/apps()|",
      "|java+method:///org/apache/spark/deploy/master/Master/addressToApp()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/address()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/take(int)|",
      "|java+method:///org/apache/spark/deploy/master/Master/endpointToApp()|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$ApplicationRemoved/DeployMessages$ApplicationRemoved(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/workers()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$removeApplication$3/Master$$anonfun$removeApplication$3(org.apache.spark.deploy.master.Master)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$removeApplication$2/Master$$anonfun$removeApplication$2(org.apache.spark.deploy.master.Master)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$completedApps()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$persistenceEngine()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$schedule()|",
      "|java+method:///org/apache/spark/deploy/master/PersistenceEngine/removeApplication(org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+method:///scala/collection/mutable/HashMap/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ResizableArray/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/master/Master/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/id()|",
      "|java+method:///org/apache/spark/deploy/master/Master/RETAINED_APPLICATIONS()|",
      "|java+method:///org/apache/spark/deploy/master/Master/waitingApps()|",
      "|java+method:///scala/collection/mutable/HashSet/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$removeApplication$1/Master$$anonfun$removeApplication$1(org.apache.spark.deploy.master.Master,org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/markFinished(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/driver()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$minus$eq(java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/size()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/trimStart(int)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationState$/FINISHED()|",
      "|java+method:///org/apache/spark/deploy/master/Master/idToApp()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$removeApplication$4/Master$$anonfun$removeApplication$4(org.apache.spark.deploy.master.Master,org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+method:///scala/math/package$/max(int,int)|",
      "|java+method:///org/apache/spark/deploy/master/Master/apps()|",
      "|java+method:///org/apache/spark/deploy/master/Master/addressToApp()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/address()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/take(int)|",
      "|java+method:///org/apache/spark/deploy/master/Master/endpointToApp()|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$ApplicationRemoved/DeployMessages$ApplicationRemoved(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/workers()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$removeApplication$3/Master$$anonfun$removeApplication$3(org.apache.spark.deploy.master.Master)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$removeApplication$2/Master$$anonfun$removeApplication$2(org.apache.spark.deploy.master.Master)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$completedApps()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$persistenceEngine()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$schedule()|",
      "|java+method:///org/apache/spark/deploy/master/PersistenceEngine/removeApplication(org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+method:///scala/collection/mutable/HashMap/$minus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/executors()|",
      "|java+method:///org/apache/spark/deploy/master/Master/asyncRebuildSparkUI(org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+method:///scala/collection/mutable/ResizableArray/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$44/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$44$$anonfun$apply$16/StagePage$$anonfun$44$$anonfun$apply$16(org.apache.spark.ui.jobs.StagePage$$anonfun$44)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$44$$anonfun$apply$3/StagePage$$anonfun$44$$anonfun$apply$3(org.apache.spark.ui.jobs.StagePage$$anonfun$44)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/inputMetrics()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$/getGettingResultTime(org.apache.spark.scheduler.TaskInfo,long)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskInfo()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/transferCredentials(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.security.UserGroupInformation)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+method:///scala/collection/JavaConversions$/collectionAsScalaIterable(java.util.Collection)|",
      "|java+method:///org/apache/hadoop/security/UserGroupInformation/getTokens()|",
      "|java+constructor:///org/apache/spark/deploy/SparkHadoopUtil$$anonfun$transferCredentials$1/SparkHadoopUtil$$anonfun$transferCredentials$1(org.apache.spark.deploy.SparkHadoopUtil,org.apache.hadoop.security.UserGroupInformation)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/hadoop/security/UserGroupInformation/getTokens()|",
      "|java+constructor:///org/apache/spark/deploy/SparkHadoopUtil$$anonfun$transferCredentials$1/SparkHadoopUtil$$anonfun$transferCredentials$1(org.apache.spark.deploy.SparkHadoopUtil,org.apache.hadoop.security.UserGroupInformation)|",
      "|java+method:///scala/collection/JavaConverters$/collectionAsScalaIterableConverter(java.util.Collection)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$$anonfun$fn$4$1/apply(scala.collection.Iterator)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/java/function/FlatMapFunction/call(java.lang.Object)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaIterator(java.util.Iterator)|",
      "|java+method:///java/lang/Iterable/iterator()|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterator(scala.collection.Iterator)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
      "|java+method:///org/apache/spark/api/java/function/FlatMapFunction/call(java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///java/lang/Iterable/iterator()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaIteratorConverter(java.util.Iterator)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$saveAsHadoopFile$4/apply$mcV$sp()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/hadoop/mapred/JobConf/setOutputKeyClass(java.lang.Class)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/hadoop/mapred/JobConf/getOutputCommitter()|",
      "|java+method:///org/apache/hadoop/mapred/JobConf/setOutputCommitter(java.lang.Class)|",
      "|java+method:///org/apache/hadoop/mapred/FileOutputFormat/setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/SparkHadoopWriter$/createPathFromString(java.lang.String,org.apache.hadoop.mapred.JobConf)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$saveAsHadoopFile$4$$anonfun$apply$mcV$sp$1/PairRDDFunctions$$anonfun$saveAsHadoopFile$4$$anonfun$apply$mcV$sp$1(org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4,org.apache.hadoop.mapred.JobConf)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/saveAsHadoopDataset(org.apache.hadoop.mapred.JobConf)|",
      "|java+method:///org/apache/hadoop/mapred/JobConf/set(java.lang.String,java.lang.String)|",
      "|java+method:///java/lang/Class/getName()|",
      "|java+method:///org/apache/hadoop/mapred/JobConf/setOutputValueClass(java.lang.Class)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/hadoop/mapred/JobConf/setOutputKeyClass(java.lang.Class)|",
      "|java+method:///org/apache/hadoop/mapred/FileOutputFormat/setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$saveAsHadoopFile$4$$anonfun$apply$mcV$sp$2/PairRDDFunctions$$anonfun$saveAsHadoopFile$4$$anonfun$apply$mcV$sp$2(org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4,java.lang.String)|",
      "|java+method:///org/apache/hadoop/mapred/JobConf/setOutputValueClass(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkHadoopWriter$/createPathFromString(java.lang.String,org.apache.hadoop.mapred.JobConf)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///java/lang/String/contains(java.lang.CharSequence)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$saveAsHadoopFile$4$$anonfun$apply$mcV$sp$1/PairRDDFunctions$$anonfun$saveAsHadoopFile$4$$anonfun$apply$mcV$sp$1(org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4,org.apache.hadoop.mapred.JobConf)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/logWarning(scala.Function0)|",
      "|java+method:///org/apache/hadoop/mapred/JobConf/setOutputFormat(java.lang.Class)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/rdd/RDD/conf()|",
      "|java+method:///org/apache/hadoop/mapred/JobConf/getOutputCommitter()|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/hadoop/mapred/JobConf/setOutputCommitter(java.lang.Class)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/hadoop/mapred/JobConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/saveAsHadoopDataset(org.apache.hadoop.mapred.JobConf)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$96/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$96/apply(long)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$96/apply(org.apache.spark.executor.ShuffleReadMetrics)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$class/pipe(org.apache.spark.api.java.JavaRDDLike,java.util.List)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe(scala.collection.Seq,scala.collection.Map,scala.Function1,scala.Function2,boolean)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe$default$3()|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe$default$4()|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe$default$5()|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe$default$2()|",
      "|java+method:///org/apache/spark/api/java/JavaRDD$/fromRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe(scala.collection.Seq,scala.collection.Map,scala.Function1,scala.Function2,boolean)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe$default$3()|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe$default$4()|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe$default$5()|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe$default$2()|",
      "|java+method:///org/apache/spark/api/java/JavaRDD$/fromRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$6/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$6/apply(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$6/apply(org.apache.spark.scheduler.Stage)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$submitStage(org.apache.spark.scheduler.Stage)|",
    "called": "|java+method:///scala/collection/immutable/List/foreach(scala.Function1)|",
    "v1Body": [
      "|java+method:///scala/collection/immutable/List/isEmpty()|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///scala/collection/mutable/HashSet/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$activeJobForStage(org.apache.spark.scheduler.Stage)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/scheduler/Stage/id()|",
      "|java+method:///scala/collection/immutable/List/sortBy(scala.Function1,scala.math.Ordering)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(org.apache.spark.scheduler.Stage,int)|",
      "|java+method:///scala/collection/immutable/List/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/runningStages()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/failedStages()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/abortStage(org.apache.spark.scheduler.Stage,java.lang.String,scala.Option)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$2/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$2(org.apache.spark.scheduler.DAGScheduler,scala.collection.immutable.List)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$3/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$3(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Stage)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$1/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$1(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Stage)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$13/DAGScheduler$$anonfun$13(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$getMissingParentStages(org.apache.spark.scheduler.Stage)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/waitingStages()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/activeJobForStage(org.apache.spark.scheduler.Stage)|",
      "|java+method:///scala/collection/immutable/List/isEmpty()|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///scala/collection/mutable/HashSet/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logInfo(scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/scheduler/Stage/id()|",
      "|java+method:///scala/collection/immutable/List/sortBy(scala.Function1,scala.math.Ordering)|",
      "|java+method:///scala/collection/immutable/List/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/runningStages()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/failedStages()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/abortStage(org.apache.spark.scheduler.Stage,java.lang.String,scala.Option)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$2/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$2(org.apache.spark.scheduler.DAGScheduler,scala.collection.immutable.List)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$3/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$3(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Stage)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$1/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$1(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Stage)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/submitMissingTasks(org.apache.spark.scheduler.Stage,int)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$13/DAGScheduler$$anonfun$13(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$getMissingParentStages(org.apache.spark.scheduler.Stage)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/waitingStages()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$9/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$9/apply(org.apache.spark.scheduler.ActiveJob)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$9/apply(org.apache.spark.scheduler.ActiveJob)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaPairRDD/lookup(java.lang.Object)|",
    "called": "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/kClassTag()|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/rdd()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/lookup(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/vClassTag()|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions$default$4(org.apache.spark.rdd.RDD)|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/kClassTag()|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/rdd()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/lookup(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/vClassTag()|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions$default$4(org.apache.spark.rdd.RDD)|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$/taskInfoFromJson(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$68/JsonProtocol$$anonfun$68()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/finishTime_$eq(long)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class,scala.reflect.Manifest,scala.collection.Seq)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$8/JsonProtocol$$anonfun$8()|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extractOpt(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/failed_$eq(boolean)|",
      "|java+method:///scala/reflect/ManifestFactory$/Boolean()|",
      "|java+method:///org/apache/spark/scheduler/TaskLocality$/withName(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$taskInfoFromJson$1/JsonProtocol$$anonfun$taskInfoFromJson$1(org.apache.spark.scheduler.TaskInfo)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskInfo/TaskInfo(long,int,int,long,java.lang.String,java.lang.String,scala.Enumeration$Value,boolean)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$9/JsonProtocol$$anonfun$9()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/gettingResultTime_$eq(long)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$71/JsonProtocol$$anonfun$71()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/finishTime_$eq(long)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class,scala.reflect.Manifest,scala.collection.Seq)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$8/JsonProtocol$$anonfun$8()|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extractOpt(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/failed_$eq(boolean)|",
      "|java+method:///scala/reflect/ManifestFactory$/Boolean()|",
      "|java+method:///org/apache/spark/scheduler/TaskLocality$/withName(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$taskInfoFromJson$1/JsonProtocol$$anonfun$taskInfoFromJson$1(org.apache.spark.scheduler.TaskInfo)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskInfo/TaskInfo(long,int,int,long,java.lang.String,java.lang.String,scala.Enumeration$Value,boolean)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$9/JsonProtocol$$anonfun$9()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/gettingResultTime_$eq(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$8/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$8/apply(org.apache.mesos.Protos$Offer)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$8/apply(org.apache.mesos.Protos$Offer)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$removeRdd(int)|",
    "called": "|java+method:///scala/concurrent/Future$/sequence(scala.collection.TraversableOnce,scala.collection.generic.CanBuildFrom,scala.concurrent.ExecutionContext)|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeRdd$1/BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeRdd$1(org.apache.spark.storage.BlockManagerMasterEndpoint)|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerMessages$RemoveRdd/BlockManagerMessages$RemoveRdd(int)|",
      "|java+method:///scala/collection/mutable/Map/keys()|",
      "|java+method:///scala/collection/TraversableOnce/toSeq()|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerMasterEndpoint$$anonfun$2/BlockManagerMasterEndpoint$$anonfun$2(org.apache.spark.storage.BlockManagerMasterEndpoint,int)|",
      "|java+method:///scala/collection/Iterable/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerMasterEndpoint$$anonfun$1/BlockManagerMasterEndpoint$$anonfun$1(org.apache.spark.storage.BlockManagerMasterEndpoint)|",
      "|java+method:///scala/concurrent/Future$/sequence(scala.collection.TraversableOnce,scala.collection.generic.CanBuildFrom,scala.concurrent.ExecutionContext)|",
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$askExecutionContext()|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeRdd$2/BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeRdd$2(org.apache.spark.storage.BlockManagerMasterEndpoint,org.apache.spark.storage.BlockManagerMessages$RemoveRdd)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$blockManagerInfo()|",
      "|java+method:///scala/collection/TraversableLike/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$blockLocations()|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+method:///scala/collection/Iterable/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeRdd$1/BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeRdd$1(org.apache.spark.storage.BlockManagerMasterEndpoint)|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerMessages$RemoveRdd/BlockManagerMessages$RemoveRdd(int)|",
      "|java+method:///scala/collection/MapLike/keys()|",
      "|java+method:///scala/collection/TraversableOnce/toSeq()|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerMasterEndpoint$$anonfun$2/BlockManagerMasterEndpoint$$anonfun$2(org.apache.spark.storage.BlockManagerMasterEndpoint,int)|",
      "|java+method:///scala/collection/Iterable/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerMasterEndpoint$$anonfun$1/BlockManagerMasterEndpoint$$anonfun$1(org.apache.spark.storage.BlockManagerMasterEndpoint)|",
      "|java+method:///scala/concurrent/Future$/sequence(scala.collection.TraversableOnce,scala.collection.generic.CanBuildFrom,scala.concurrent.ExecutionContext)|",
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$askExecutionContext()|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeRdd$2/BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$removeRdd$2(org.apache.spark.storage.BlockManagerMasterEndpoint,org.apache.spark.storage.BlockManagerMessages$RemoveRdd)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$blockManagerInfo()|",
      "|java+method:///scala/collection/TraversableLike/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$blockLocations()|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+method:///scala/collection/Iterable/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/MesosExternalShuffleBlockHandler(org.apache.spark.network.util.TransportConf)|",
    "called": "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/network/shuffle/ExternalShuffleBlockHandler/ExternalShuffleBlockHandler(org.apache.spark.network.util.TransportConf)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/network/shuffle/ExternalShuffleBlockHandler/ExternalShuffleBlockHandler(org.apache.spark.network.util.TransportConf,java.io.File)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$reduceByKeyLocally$1/apply()|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$reduceByKeyLocally$1$$anonfun$4/PairRDDFunctions$$anonfun$reduceByKeyLocally$1$$anonfun$4(org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1,scala.Function2)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$reduceByKeyLocally$1$$anonfun$3/PairRDDFunctions$$anonfun$reduceByKeyLocally$1$$anonfun$3(org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1,scala.Function2)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$reduceByKeyLocally$1/apply()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/keyClass()|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///java/lang/Class/isArray()|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions$default$2()|",
      "|java+method:///org/apache/spark/rdd/RDD/sparkContext()|",
      "|java+method:///org/apache/spark/rdd/RDD/reduce(scala.Function2)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions$default$2()|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$reduceByKeyLocally$1$$anonfun$4/PairRDDFunctions$$anonfun$reduceByKeyLocally$1$$anonfun$4(org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1,scala.Function2)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$reduceByKeyLocally$1$$anonfun$3/PairRDDFunctions$$anonfun$reduceByKeyLocally$1$$anonfun$3(org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1,scala.Function2)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$reduceByKeyLocally$1/apply()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/keyClass()|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///java/lang/Class/isArray()|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/rdd/RDD/sparkContext()|",
      "|java+method:///org/apache/spark/rdd/RDD/reduce(scala.Function2)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/TimeStampedHashMap/iterator()|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/asScalaIterator(java.util.Iterator)|",
      "|java+method:///java/util/Set/iterator()|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/TimeStampedHashMap$$anonfun$iterator$1/TimeStampedHashMap$$anonfun$iterator$1(org.apache.spark.util.TimeStampedHashMap)|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/getEntrySet()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaIteratorConverter(java.util.Iterator)|",
      "|java+method:///java/util/Set/iterator()|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/TimeStampedHashMap$$anonfun$iterator$1/TimeStampedHashMap$$anonfun$iterator$1(org.apache.spark.util.TimeStampedHashMap)|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/getEntrySet()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/hashCode()|",
    "called": "|java+method:///scala/runtime/Statics/finalizeHash(int,int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/id()|",
      "|java+method:///scala/runtime/Statics/finalizeHash(int,int)|",
      "|java+method:///scala/runtime/Statics/mix(int,int)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/cached()|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/name()|",
      "|java+method:///scala/runtime/Statics/anyHash(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/id()|",
      "|java+method:///scala/runtime/Statics/finalizeHash(int,int)|",
      "|java+method:///scala/runtime/Statics/mix(int,int)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/cached()|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/callsite()|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/name()|",
      "|java+method:///scala/runtime/Statics/anyHash(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$26/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$26(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1,java.lang.String)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$taskSetManagerForAttempt$1/apply(scala.collection.mutable.HashMap)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$taskSetManagerForAttempt$1$$anonfun$apply$12/TaskSchedulerImpl$$anonfun$taskSetManagerForAttempt$1$$anonfun$apply$12(org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$taskSetManagerForAttempt$1)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$taskSetManagerForAttempt$1$$anonfun$apply$13/TaskSchedulerImpl$$anonfun$taskSetManagerForAttempt$1$$anonfun$apply$13(org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$taskSetManagerForAttempt$1)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$38/StagePage$$anonfun$38(org.apache.spark.ui.jobs.StagePage)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$118/TaskDataSource$$anonfun$118(org.apache.spark.ui.jobs.TaskDataSource)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/client/TestClient$/main(java.lang.String%5B%5D)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/ApplicationDescription/ApplicationDescription(java.lang.String,scala.Option,int,org.apache.spark.deploy.Command,java.lang.String,scala.Option,scala.Option,scala.Option)|",
      "|java+constructor:///org/apache/spark/deploy/client/TestClient$TestListener/TestClient$TestListener()|",
      "|java+constructor:///org/apache/spark/deploy/Command/Command(java.lang.String,scala.collection.Seq,scala.collection.Map,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+constructor:///org/apache/spark/SparkConf/SparkConf()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient/AppClient(org.apache.spark.rpc.RpcEnv,java.lang.String%5B%5D,org.apache.spark.deploy.ApplicationDescription,org.apache.spark.deploy.client.AppClientListener,org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/start()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription$/$lessinit$greater$default$6()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription$/$lessinit$greater$default$7()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription$/$lessinit$greater$default$8()|",
      "|java+method:///scala/collection/immutable/StringOps/stripSuffix(java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/awaitTermination()|",
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/util/Utils$/localHostName()|",
      "|java+method:///java/lang/Class/getCanonicalName()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/client/TestClient$TestListener/TestClient$TestListener()|",
      "|java+constructor:///org/apache/spark/deploy/Command/Command(java.lang.String,scala.collection.Seq,scala.collection.Map,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///java/lang/Class/getCanonicalName()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient/AppClient(org.apache.spark.rpc.RpcEnv,java.lang.String%5B%5D,org.apache.spark.deploy.ApplicationDescription,org.apache.spark.deploy.client.AppClientListener,org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/start()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription$/$lessinit$greater$default$6()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription$/$lessinit$greater$default$7()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription$/$lessinit$greater$default$8()|",
      "|java+method:///scala/collection/immutable/StringOps/stripSuffix(java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/awaitTermination()|",
      "|java+constructor:///org/apache/spark/deploy/ApplicationDescription/ApplicationDescription(java.lang.String,scala.Option,int,org.apache.spark.deploy.Command,java.lang.String,scala.Option,scala.Option,scala.Option,java.lang.String)|",
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/util/Utils$/localHostName()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create$default$6()|",
      "|java+constructor:///org/apache/spark/SparkConf/SparkConf()|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription$/$lessinit$greater$default$9()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/getTimeFromNowToRenewal(org.apache.spark.SparkConf,double,org.apache.hadoop.security.Credentials)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///scala/concurrent/duration/package$DurationInt/package$DurationInt(int)|",
      "|java+method:///scala/concurrent/duration/package$/DurationInt(int)|",
      "|java+method:///scala/collection/JavaConversions$/collectionAsScalaIterable(java.util.Collection)|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///scala/collection/Iterable/filter(scala.Function1)|",
      "|java+method:///org/apache/hadoop/security/Credentials/getAllTokens()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/collection/TraversableOnce/foldLeft(java.lang.Object,scala.Function2)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/deploy/SparkHadoopUtil$$anonfun$getTimeFromNowToRenewal$3/SparkHadoopUtil$$anonfun$getTimeFromNowToRenewal$3(org.apache.spark.deploy.SparkHadoopUtil,double,long,long)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///scala/concurrent/duration/FiniteDuration/toMillis()|",
      "|java+method:///org/apache/spark/SparkConf/getLong(java.lang.String,long)|",
      "|java+method:///scala/concurrent/duration/package$DurationInt/hours()|",
      "|java+constructor:///org/apache/spark/deploy/SparkHadoopUtil$$anonfun$getTimeFromNowToRenewal$1/SparkHadoopUtil$$anonfun$getTimeFromNowToRenewal$1(org.apache.spark.deploy.SparkHadoopUtil)|",
      "|java+constructor:///org/apache/spark/deploy/SparkHadoopUtil$$anonfun$getTimeFromNowToRenewal$2/SparkHadoopUtil$$anonfun$getTimeFromNowToRenewal$2(org.apache.spark.deploy.SparkHadoopUtil)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///scala/concurrent/duration/package$DurationInt/package$DurationInt(int)|",
      "|java+method:///scala/concurrent/duration/package$/DurationInt(int)|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///scala/collection/TraversableLike/filter(scala.Function1)|",
      "|java+method:///org/apache/hadoop/security/Credentials/getAllTokens()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/collection/TraversableOnce/foldLeft(java.lang.Object,scala.Function2)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/deploy/SparkHadoopUtil$$anonfun$getTimeFromNowToRenewal$3/SparkHadoopUtil$$anonfun$getTimeFromNowToRenewal$3(org.apache.spark.deploy.SparkHadoopUtil,double,long,long)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///scala/concurrent/duration/FiniteDuration/toMillis()|",
      "|java+method:///org/apache/spark/SparkConf/getLong(java.lang.String,long)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/concurrent/duration/package$DurationInt/hours()|",
      "|java+method:///scala/collection/JavaConverters$/collectionAsScalaIterableConverter(java.util.Collection)|",
      "|java+constructor:///org/apache/spark/deploy/SparkHadoopUtil$$anonfun$getTimeFromNowToRenewal$1/SparkHadoopUtil$$anonfun$getTimeFromNowToRenewal$1(org.apache.spark.deploy.SparkHadoopUtil)|",
      "|java+constructor:///org/apache/spark/deploy/SparkHadoopUtil$$anonfun$getTimeFromNowToRenewal$2/SparkHadoopUtil$$anonfun$getTimeFromNowToRenewal$2(org.apache.spark.deploy.SparkHadoopUtil)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/ZookeeperMesosClusterPersistenceEngine$$anonfun$fetchAll$1/apply(java.lang.String)|",
    "called": "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/ZookeeperMesosClusterPersistenceEngine/fetch(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/ZookeeperMesosClusterPersistenceEngine/fetch(java.lang.String)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$start$1/apply$mcV$sp()|",
    "called": "|java+constructor:///scala/Some/Some(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/org$apache$spark$deploy$worker$ExecutorRunner$$killProcess(scala.Option)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/org$apache$spark$deploy$worker$ExecutorRunner$$killProcess(scala.Option)|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/FAILED()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/RUNNING()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/state()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/state_$eq(scala.Enumeration$Value)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/init(org.apache.spark.network.BlockDataManager)|",
    "called": "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/network/sasl/SaslServerBootstrap/SaslServerBootstrap(org.apache.spark.network.util.TransportConf,org.apache.spark.network.sasl.SecretKeyHolder)|",
      "|java+method:///scala/Option/toList()|",
      "|java+constructor:///org/apache/spark/network/netty/NettyBlockTransferService$$anonfun$init$1/NettyBlockTransferService$$anonfun$init$1(org.apache.spark.network.netty.NettyBlockTransferService)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/authEnabled()|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/createServer(scala.collection.immutable.List)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/serializer()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/network/sasl/SaslClientBootstrap/SaslClientBootstrap(org.apache.spark.network.util.TransportConf,java.lang.String,org.apache.spark.network.sasl.SecretKeyHolder,boolean)|",
      "|java+constructor:///org/apache/spark/network/TransportContext/TransportContext(org.apache.spark.network.util.TransportConf,org.apache.spark.network.server.RpcHandler)|",
      "|java+method:///org/apache/spark/SecurityManager/isSaslEncryptionEnabled()|",
      "|java+constructor:///org/apache/spark/network/netty/NettyBlockRpcServer/NettyBlockRpcServer(org.apache.spark.serializer.Serializer,org.apache.spark.network.BlockDataManager)|",
      "|java+method:///org/apache/spark/network/TransportContext/createClientFactory(java.util.List)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/transportConf()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkConf/getAppId()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/network/sasl/SaslServerBootstrap/SaslServerBootstrap(org.apache.spark.network.util.TransportConf,org.apache.spark.network.sasl.SecretKeyHolder)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
      "|java+method:///scala/Option/toList()|",
      "|java+constructor:///org/apache/spark/network/netty/NettyBlockTransferService$$anonfun$init$1/NettyBlockTransferService$$anonfun$init$1(org.apache.spark.network.netty.NettyBlockTransferService)|",
      "|java+method:///scala/collection/Iterable/toSeq()|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/authEnabled()|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/createServer(scala.collection.immutable.List)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/serializer()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkConf/getAppId()|",
      "|java+constructor:///org/apache/spark/network/sasl/SaslClientBootstrap/SaslClientBootstrap(org.apache.spark.network.util.TransportConf,java.lang.String,org.apache.spark.network.sasl.SecretKeyHolder,boolean)|",
      "|java+constructor:///org/apache/spark/network/TransportContext/TransportContext(org.apache.spark.network.util.TransportConf,org.apache.spark.network.server.RpcHandler)|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SecurityManager/isSaslEncryptionEnabled()|",
      "|java+constructor:///org/apache/spark/network/netty/NettyBlockRpcServer/NettyBlockRpcServer(java.lang.String,org.apache.spark.serializer.Serializer,org.apache.spark.network.BlockDataManager)|",
      "|java+method:///org/apache/spark/network/TransportContext/createClientFactory(java.util.List)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/transportConf()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$8/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$8(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/shuffle/IndexShuffleBlockResolver/blockManager$lzycompute()|",
    "called": "|java+method:///scala/Option/getOrElse(scala.Function0)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|"
    ],
    "v2Body": [
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/shuffle/IndexShuffleBlockResolver$$anonfun$blockManager$1/IndexShuffleBlockResolver$$anonfun$blockManager$1(org.apache.spark.shuffle.IndexShuffleBlockResolver)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$tryToPut$3/apply(org.apache.spark.storage.BlockStatus)|",
    "called": "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/Buffer/$plus$eq(java.lang.Object)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$34/StagePage$$anonfun$34(org.apache.spark.ui.jobs.StagePage)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$10/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$10/apply(org.apache.spark.scheduler.ActiveJob)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$10/apply(org.apache.spark.scheduler.Stage)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$58/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$58/apply(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$58/apply(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage/render(javax.servlet.http.HttpServletRequest)|",
    "called": "|java+method:///scala/xml/Unparsed$/apply(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///scala/Predef$/$scope()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$33/StagePage$$anonfun$33(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$44/StagePage$$anonfun$44(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$55/StagePage$$anonfun$55(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/formatDuration(long)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/shuffleReadRecords()|",
      "|java+method:///javax/servlet/http/HttpServletRequest/getParameter(java.lang.String)|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/stageIdToData()|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/TASK_DESERIALIZATION_TIME()|",
      "|java+method:///org/apache/spark/ui/jobs/ExecutorTable/toNodeSeq()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$1/StagePage$$anonfun$1(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/ToolTips$/GETTING_RESULT_TIME()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$34/StagePage$$anonfun$34(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$3/StagePage$$anonfun$3(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/makeTimeline(scala.collection.Seq,long)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$2/StagePage$$anonfun$2(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/hasShuffleRead()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$45/StagePage$$anonfun$45(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/hasBytesSpilled()|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/SHUFFLE_READ_REMOTE_SIZE()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable(scala.collection.Seq,scala.Function1,scala.collection.Iterable,boolean,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/hasOutput()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$35/StagePage$$anonfun$35(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$46/StagePage$$anonfun$46(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/ToolTips$/SHUFFLE_READ()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$57/StagePage$$anonfun$57(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///scala/collection/immutable/StringOps/toBoolean()|",
      "|java+method:///scala/collection/Seq/count(scala.Function1)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/progressListener()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/shuffleWriteRecords()|",
      "|java+method:///org/apache/spark/ui/ToolTips$/TASK_DESERIALIZATION_TIME()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/inputRecords()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$36/StagePage$$anonfun$36(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$47/StagePage$$anonfun$47(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$render$1/StagePage$$anonfun$render$1(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/showDagVizForStage(int,scala.Option)|",
      "|java+method:///scala/collection/Seq/sortBy(scala.Function1,scala.math.Ordering)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/RESULT_SERIALIZATION_TIME()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$4()|",
      "|java+method:///scala/Predef$/Set()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$render$2/StagePage$$anonfun$render$2(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$37/StagePage$$anonfun$37(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$48/StagePage$$anonfun$48(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/liftedTree1$1(int,java.lang.String,boolean,int,int,int,org.apache.spark.ui.jobs.UIData$StageUIData,scala.collection.Seq,boolean,long)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/getFormattedTimeQuantiles$1(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/hasShuffleWrite()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$5()|",
      "|java+method:///scala/collection/Seq/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/SCHEDULER_DELAY()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$6()|",
      "|java+method:///org/apache/spark/ui/jobs/TaskPagedTable/dataSource()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/outputRecords()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$38/StagePage$$anonfun$38(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$49/StagePage$$anonfun$49(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/diskBytesSpilled()|",
      "|java+method:///scala/xml/Elem/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$27/StagePage$$anonfun$27(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$7()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/HashMap/isEmpty()|",
      "|java+method:///scala/xml/NodeSeq$/canBuildFrom()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/toSeq()|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/ui/ToolTips$/RESULT_SERIALIZATION_TIME()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/headerSparkPage(java.lang.String,scala.Function0,org.apache.spark.ui.SparkUITab,scala.Option,scala.Option,boolean)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$28/StagePage$$anonfun$28(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$39/StagePage$$anonfun$39(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$8()|",
      "|java+constructor:///org/apache/spark/ui/jobs/ExecutorTable/ExecutorTable(int,int,org.apache.spark.ui.jobs.StagesTab)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/SHUFFLE_READ_BLOCKED_TIME()|",
      "|java+method:///scala/collection/Seq/$plus$colon(java.lang.Object,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/ui/ToolTips$/SCHEDULER_DELAY()|",
      "|java+method:///scala/collection/Iterable/filter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$render$4/StagePage$$anonfun$render$4(org.apache.spark.ui.jobs.StagePage,scala.xml.Elem)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/inputBytes()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/executorRunTime()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/shuffleWriteBytes()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$29/StagePage$$anonfun$29(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/memoryBytesSpilled()|",
      "|java+method:///scala/xml/Unparsed$/apply(java.lang.String)|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$42/StagePage$$anonfun$42(org.apache.spark.ui.jobs.StagePage,long)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$render$3/StagePage$$anonfun$render$3(org.apache.spark.ui.jobs.StagePage,scala.xml.Elem)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/Iterable/size()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/headerSparkPage$default$4()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/expandDagVizOnLoad(boolean)|",
      "|java+method:///scala/collection/immutable/Set$/empty()|",
      "|java+method:///org/apache/spark/ui/ToolTips$/SHUFFLE_READ_BLOCKED_TIME()|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/headerSparkPage$default$5()|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/PEAK_EXECUTION_MEMORY()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/headerSparkPage$default$6()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/outputBytes()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$40/StagePage$$anonfun$40(org.apache.spark.ui.jobs.StagePage,long)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/getFormattedSizeQuantiles$1(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$50/StagePage$$anonfun$50(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/accumulables()|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/ui/ToolTips$/PEAK_EXECUTION_MEMORY()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$51/StagePage$$anonfun$51(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$56/StagePage$$anonfun$56(org.apache.spark.ui.jobs.StagePage,scala.collection.immutable.Set)|",
      "|java+constructor:///scala/Tuple2$mcII$sp/Tuple2$mcII$sp(int,int)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationGraphListener/getOperationGraphForStage(int)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/getFormattedSizeQuantilesWithRecords$1(scala.collection.Seq,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$30/StagePage$$anonfun$30(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$41/StagePage$$anonfun$41(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$52/StagePage$$anonfun$52(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/displayPeakExecutionMemory()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$31/StagePage$$anonfun$31(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$render$5/StagePage$$anonfun$render$5(org.apache.spark.ui.jobs.StagePage,scala.xml.NodeSeq)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$53/StagePage$$anonfun$53(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/taskData()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/hasInput()|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///org/apache/spark/util/Utils$/bytesToString(long)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$32/StagePage$$anonfun$32(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$43/StagePage$$anonfun$43(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource/slicedTaskIds()|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/GETTING_RESULT_TIME()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$54/StagePage$$anonfun$54(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/ui/ToolTips$/GC_TIME()|",
      "|java+method:///scala/collection/immutable/StringOps/nonEmpty()|",
      "|java+method:///org/apache/spark/ui/ToolTips$/SHUFFLE_READ_REMOTE_SIZE()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/shuffleReadTotalBytes()|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/operationGraphListener()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///scala/Predef$/$scope()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$33/StagePage$$anonfun$33(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$55/StagePage$$anonfun$55(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/formatDuration(long)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/shuffleReadRecords()|",
      "|java+method:///javax/servlet/http/HttpServletRequest/getParameter(java.lang.String)|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/stageIdToData()|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/TASK_DESERIALIZATION_TIME()|",
      "|java+method:///org/apache/spark/ui/jobs/ExecutorTable/toNodeSeq()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$1/StagePage$$anonfun$1(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/ToolTips$/GETTING_RESULT_TIME()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$34/StagePage$$anonfun$34(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$3/StagePage$$anonfun$3(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$56/StagePage$$anonfun$56(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/makeTimeline(scala.collection.Seq,long)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$2/StagePage$$anonfun$2(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/hasShuffleRead()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$45/StagePage$$anonfun$45(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/hasBytesSpilled()|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/SHUFFLE_READ_REMOTE_SIZE()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable(scala.collection.Seq,scala.Function1,scala.collection.Iterable,boolean,scala.Option,scala.collection.Seq,boolean,boolean)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/hasOutput()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$35/StagePage$$anonfun$35(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/ToolTips$/SHUFFLE_READ()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$57/StagePage$$anonfun$57(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///scala/collection/immutable/StringOps/toBoolean()|",
      "|java+method:///scala/collection/Seq/count(scala.Function1)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/progressListener()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/shuffleWriteRecords()|",
      "|java+method:///org/apache/spark/ui/ToolTips$/TASK_DESERIALIZATION_TIME()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/inputRecords()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$60/StagePage$$anonfun$60(org.apache.spark.ui.jobs.StagePage,scala.collection.immutable.Set)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$36/StagePage$$anonfun$36(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$47/StagePage$$anonfun$47(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$58/StagePage$$anonfun$58(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$render$1/StagePage$$anonfun$render$1(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/showDagVizForStage(int,scala.Option)|",
      "|java+method:///scala/collection/Seq/sortBy(scala.Function1,scala.math.Ordering)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$46/StagePage$$anonfun$46(org.apache.spark.ui.jobs.StagePage,long)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/RESULT_SERIALIZATION_TIME()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$4()|",
      "|java+method:///scala/Predef$/Set()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$render$2/StagePage$$anonfun$render$2(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$37/StagePage$$anonfun$37(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$48/StagePage$$anonfun$48(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$59/StagePage$$anonfun$59(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/getLocalitySummaryString(org.apache.spark.ui.jobs.UIData$StageUIData)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/liftedTree1$1(int,java.lang.String,boolean,int,int,int,org.apache.spark.ui.jobs.UIData$StageUIData,scala.collection.Seq,boolean,long)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/getFormattedTimeQuantiles$1(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/hasShuffleWrite()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$5()|",
      "|java+method:///scala/collection/Seq/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/SCHEDULER_DELAY()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$6()|",
      "|java+method:///org/apache/spark/ui/jobs/TaskPagedTable/dataSource()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/outputRecords()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/xml/Elem/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$38/StagePage$$anonfun$38(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$49/StagePage$$anonfun$49(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/diskBytesSpilled()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$7()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/HashMap/isEmpty()|",
      "|java+method:///scala/xml/NodeSeq$/canBuildFrom()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/toSeq()|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/ui/ToolTips$/RESULT_SERIALIZATION_TIME()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/headerSparkPage(java.lang.String,scala.Function0,org.apache.spark.ui.SparkUITab,scala.Option,scala.Option,boolean)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$44/StagePage$$anonfun$44(org.apache.spark.ui.jobs.StagePage,long)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$39/StagePage$$anonfun$39(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/listingTable$default$8()|",
      "|java+constructor:///org/apache/spark/ui/jobs/ExecutorTable/ExecutorTable(int,int,org.apache.spark.ui.jobs.StagesTab)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/SHUFFLE_READ_BLOCKED_TIME()|",
      "|java+method:///scala/collection/Seq/$plus$colon(java.lang.Object,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/ui/ToolTips$/SCHEDULER_DELAY()|",
      "|java+method:///scala/collection/Iterable/filter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$render$4/StagePage$$anonfun$render$4(org.apache.spark.ui.jobs.StagePage,scala.xml.Elem)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/inputBytes()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/executorRunTime()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/shuffleWriteBytes()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/memoryBytesSpilled()|",
      "|java+method:///scala/xml/Unparsed$/apply(java.lang.String)|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$render$3/StagePage$$anonfun$render$3(org.apache.spark.ui.jobs.StagePage,scala.xml.Elem)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/Iterable/size()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/headerSparkPage$default$4()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/expandDagVizOnLoad(boolean)|",
      "|java+method:///scala/collection/immutable/Set$/empty()|",
      "|java+method:///org/apache/spark/ui/ToolTips$/SHUFFLE_READ_BLOCKED_TIME()|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/headerSparkPage$default$5()|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/PEAK_EXECUTION_MEMORY()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/headerSparkPage$default$6()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/outputBytes()|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/getFormattedSizeQuantiles$1(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$50/StagePage$$anonfun$50(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$61/StagePage$$anonfun$61(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/accumulables()|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/ui/ToolTips$/PEAK_EXECUTION_MEMORY()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$40/StagePage$$anonfun$40(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$51/StagePage$$anonfun$51(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///scala/Tuple2$mcII$sp/Tuple2$mcII$sp(int,int)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationGraphListener/getOperationGraphForStage(int)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/getFormattedSizeQuantilesWithRecords$1(scala.collection.Seq,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$41/StagePage$$anonfun$41(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$52/StagePage$$anonfun$52(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/displayPeakExecutionMemory()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$31/StagePage$$anonfun$31(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$42/StagePage$$anonfun$42(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$render$5/StagePage$$anonfun$render$5(org.apache.spark.ui.jobs.StagePage,scala.xml.NodeSeq)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$53/StagePage$$anonfun$53(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/taskData()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/hasInput()|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///org/apache/spark/util/Utils$/bytesToString(long)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$32/StagePage$$anonfun$32(org.apache.spark.ui.jobs.StagePage)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$43/StagePage$$anonfun$43(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource/slicedTaskIds()|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/GETTING_RESULT_TIME()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$54/StagePage$$anonfun$54(org.apache.spark.ui.jobs.StagePage)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/ui/ToolTips$/GC_TIME()|",
      "|java+method:///scala/collection/immutable/StringOps/nonEmpty()|",
      "|java+method:///org/apache/spark/ui/ToolTips$/SHUFFLE_READ_REMOTE_SIZE()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/shuffleReadTotalBytes()|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage/operationGraphListener()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$86/TaskDataSource$$anonfun$86(org.apache.spark.ui.jobs.TaskDataSource)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$class/take(org.apache.spark.api.java.JavaRDDLike,int)|",
    "called": "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+constructor:///java/util/ArrayList/ArrayList(java.util.Collection)|",
      "|java+method:///org/apache/spark/rdd/RDD/take(int)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/RDD/take(int)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/collection/ExternalSorter/insertAll(scala.collection.Iterator)|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/Iterator/next()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/map()|",
      "|java+method:///org/apache/spark/util/collection/PartitionedAppendOnlyMap/changeValue(java.lang.Object,scala.Function2)|",
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+method:///scala/Product2/_2()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/maybeSpillCollection(boolean)|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalSorter$$anonfun$5/ExternalSorter$$anonfun$5(org.apache.spark.util.collection.ExternalSorter,scala.Function2,scala.Function1,scala.runtime.ObjectRef)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/addElementsRead()|",
      "|java+method:///org/apache/spark/Aggregator/createCombiner()|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/buffer()|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/getPartition(java.lang.Object)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/util/collection/WritablePartitionedPairCollection/insert(int,java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Product2/_1()|",
      "|java+method:///org/apache/spark/Aggregator/mergeValue()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Iterator/next()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/collection/PartitionedPairBuffer/insert(int,java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/map()|",
      "|java+method:///org/apache/spark/util/collection/PartitionedAppendOnlyMap/changeValue(java.lang.Object,scala.Function2)|",
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+method:///scala/Product2/_2()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/maybeSpillCollection(boolean)|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalSorter$$anonfun$5/ExternalSorter$$anonfun$5(org.apache.spark.util.collection.ExternalSorter,scala.Function2,scala.Function1,scala.runtime.ObjectRef)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/addElementsRead()|",
      "|java+method:///org/apache/spark/Aggregator/createCombiner()|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/buffer()|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/getPartition(java.lang.Object)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///scala/Product2/_1()|",
      "|java+method:///org/apache/spark/Aggregator/mergeValue()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/metrics/MetricsConfig/initialize()|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/spark/metrics/MetricsConfig/propertyCategories()|",
      "|java+method:///org/apache/spark/metrics/MetricsConfig/properties()|",
      "|java+method:///org/apache/spark/metrics/MetricsConfig/subProperties(java.util.Properties,scala.util.matching.Regex)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/metrics/MetricsConfig/org$apache$spark$metrics$MetricsConfig$$DEFAULT_PREFIX()|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$initialize$1/MetricsConfig$$anonfun$initialize$1(org.apache.spark.metrics.MetricsConfig,java.lang.String)|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$initialize$2/MetricsConfig$$anonfun$initialize$2(org.apache.spark.metrics.MetricsConfig)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$initialize$3/MetricsConfig$$anonfun$initialize$3(org.apache.spark.metrics.MetricsConfig)|",
      "|java+method:///org/apache/spark/metrics/MetricsConfig/INSTANCE_REGEX()|",
      "|java+method:///org/apache/spark/metrics/MetricsConfig/setDefaultProperties(java.util.Properties)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/metrics/MetricsConfig/loadPropertiesFromFile(scala.Option)|",
      "|java+method:///org/apache/spark/SparkConf/getAll()|",
      "|java+method:///scala/collection/generic/FilterMonadic/withFilter(scala.Function1)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/metrics/MetricsConfig/propertyCategories_$eq(scala.collection.mutable.HashMap)|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$initialize$4/MetricsConfig$$anonfun$initialize$4(org.apache.spark.metrics.MetricsConfig,java.util.Properties)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///scala/collection/JavaConverters$/propertiesAsScalaMapConverter(java.util.Properties)|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$initialize$4/MetricsConfig$$anonfun$initialize$4(org.apache.spark.metrics.MetricsConfig,scala.collection.mutable.Map)|",
      "|java+method:///scala/collection/mutable/HashMap/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/spark/metrics/MetricsConfig/propertyCategories()|",
      "|java+method:///org/apache/spark/metrics/MetricsConfig/subProperties(java.util.Properties,scala.util.matching.Regex)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/metrics/MetricsConfig/org$apache$spark$metrics$MetricsConfig$$DEFAULT_PREFIX()|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$initialize$1/MetricsConfig$$anonfun$initialize$1(org.apache.spark.metrics.MetricsConfig,java.lang.String)|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$initialize$2/MetricsConfig$$anonfun$initialize$2(org.apache.spark.metrics.MetricsConfig)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$initialize$3/MetricsConfig$$anonfun$initialize$3(org.apache.spark.metrics.MetricsConfig)|",
      "|java+method:///org/apache/spark/metrics/MetricsConfig/INSTANCE_REGEX()|",
      "|java+method:///org/apache/spark/metrics/MetricsConfig/setDefaultProperties(java.util.Properties)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/metrics/MetricsConfig/loadPropertiesFromFile(scala.Option)|",
      "|java+method:///org/apache/spark/metrics/MetricsConfig/propertyCategories_$eq(scala.collection.mutable.HashMap)|",
      "|java+method:///org/apache/spark/metrics/MetricsConfig/properties()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///org/apache/spark/SparkConf/getAll()|",
      "|java+method:///scala/collection/generic/FilterMonadic/withFilter(scala.Function1)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/storage/MemoryStore$$anonfun$1/MemoryStore$$anonfun$1(org.apache.spark.storage.MemoryStore)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0$mcJ$sp/AbstractFunction0$mcJ$sp()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/start()|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/rpcEnv()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/sc()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$$anonfun$start$1/CoarseGrainedSchedulerBackend$$anonfun$start$1(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/CoarseGrainedSchedulerBackend$DriverEndpoint(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend,org.apache.spark.rpc.RpcEnv,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$$anonfun$start$2/CoarseGrainedSchedulerBackend$$anonfun$start$2(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend,scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/driverEndpoint_$eq(org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///scala/collection/mutable/ArrayOps/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$/ENDPOINT_NAME()|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/SparkConf/getAll()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/rpcEnv()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/sc()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$$anonfun$start$1/CoarseGrainedSchedulerBackend$$anonfun$start$1(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$$anonfun$start$2/CoarseGrainedSchedulerBackend$$anonfun$start$2(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend,scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/driverEndpoint_$eq(org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///scala/collection/mutable/ArrayOps/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/createDriverEndpoint(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$/ENDPOINT_NAME()|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/SparkConf/getAll()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/history/HistoryPage/org$apache$spark$deploy$history$HistoryPage$$attemptRow(boolean,org.apache.spark.deploy.history.ApplicationHistoryInfo,org.apache.spark.deploy.history.ApplicationAttemptInfo,boolean)|",
    "called": "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/Option/isDefined()|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+constructor:///scala/xml/EntityRef/EntityRef(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationAttemptInfo/lastUpdated()|",
      "|java+method:///scala/collection/immutable/List/size()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/formatDuration(long)|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationAttemptInfo/startTime()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationHistoryInfo/attempts()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationHistoryInfo/name()|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationHistoryInfo/id()|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationAttemptInfo/endTime()|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationAttemptInfo/sparkUser()|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationAttemptInfo/attemptId()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/formatDate(long)|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServer$/getAttemptURI(java.lang.String,scala.Option)|"
    ],
    "v2Body": [
      "|java+method:///scala/Option/isDefined()|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+constructor:///scala/xml/EntityRef/EntityRef(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationAttemptInfo/lastUpdated()|",
      "|java+method:///scala/collection/immutable/List/size()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/prependBaseUri(java.lang.String,java.lang.String)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/formatDuration(long)|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationAttemptInfo/startTime()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationHistoryInfo/attempts()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/prependBaseUri$default$2()|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationHistoryInfo/name()|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationHistoryInfo/id()|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationAttemptInfo/endTime()|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationAttemptInfo/sparkUser()|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationAttemptInfo/attemptId()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/formatDate(long)|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServer$/getAttemptURI(java.lang.String,scala.Option)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$1/MesosSchedulerBackend$$anonfun$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDDCheckpointData/checkpoint()|",
    "called": "|java+constructor:///scala/Some/Some(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/rdd/CheckpointState$/Initialized()|",
      "|java+method:///org/apache/spark/rdd/RDDCheckpointData/cpRDD_$eq(scala.Option)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/RDDCheckpointData/cpState()|",
      "|java+method:///org/apache/spark/rdd/CheckpointState$/Checkpointed()|",
      "|java+method:///org/apache/spark/rdd/RDD/markCheckpointed()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/CheckpointState$/CheckpointingInProgress()|",
      "|java+method:///org/apache/spark/rdd/RDDCheckpointData/cpState_$eq(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/rdd/RDDCheckpointData/doCheckpoint()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/CheckpointState$/Initialized()|",
      "|java+method:///org/apache/spark/rdd/RDDCheckpointData/cpRDD_$eq(scala.Option)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/RDDCheckpointData/cpState()|",
      "|java+method:///org/apache/spark/rdd/CheckpointState$/Checkpointed()|",
      "|java+method:///org/apache/spark/rdd/RDD/markCheckpointed()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/CheckpointState$/CheckpointingInProgress()|",
      "|java+method:///org/apache/spark/rdd/RDDCheckpointData/cpState_$eq(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/rdd/RDDCheckpointData/rdd()|",
      "|java+method:///org/apache/spark/rdd/RDDCheckpointData/doCheckpoint()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/shuffle/sort/SortShuffleManager$$anonfun$unregisterShuffle$1/apply$mcVI$sp(int)|",
    "called": "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleManager/shuffleBlockResolver()|",
      "|java+method:///org/apache/spark/shuffle/IndexShuffleBlockResolver/removeDataByMap(int,int)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/shuffle/sort/SortShuffleManager$$anonfun$unregisterShuffle$1$$anonfun$apply$mcVI$sp$1/SortShuffleManager$$anonfun$unregisterShuffle$1$$anonfun$apply$mcVI$sp$1(org.apache.spark.shuffle.sort.SortShuffleManager$$anonfun$unregisterShuffle$1)|",
      "|java+method:///scala/collection/immutable/Range/foreach$mVc$sp(scala.Function1)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/SparkContext$$anonfun$27/SparkContext$$anonfun$27(org.apache.spark.SparkContext)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$1/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$1(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0$mcZ$sp/AbstractFunction0$mcZ$sp()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0$mcZ$sp/AbstractFunction0$mcZ$sp()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$62/JsonProtocol$$anonfun$62()|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$3/apply()|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1/org$apache$spark$deploy$worker$Worker$$anonfun$$$outer()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$3/apply()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/workDir()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/address()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$3/apply()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/toSparkURL()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$2/apply()|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$unrollMemoryThreshold()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$2/apply()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Utils$/bytesToString(long)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$2/apply()|",
      "|java+method:///org/apache/spark/util/Utils$/bytesToString(long)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$maxMemory()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleExecutorLost(java.lang.String,boolean,scala.Option)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/failedEpoch()|",
      "|java+method:///scala/collection/mutable/HashMap/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleExecutorLost$3/DAGScheduler$$anonfun$handleExecutorLost$3(org.apache.spark.scheduler.DAGScheduler,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleExecutorLost$2/DAGScheduler$$anonfun$handleExecutorLost$2(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMaster/removeExecutor(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManager/externalShuffleServiceEnabled()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/clearCacheLocs()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/submitWaitingStages()|",
      "|java+method:///scala/collection/mutable/HashMap/isEmpty()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$4/DAGScheduler$$anonfun$4(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/shuffleToMapStage()|",
      "|java+method:///org/apache/spark/MapOutputTrackerMaster/incrementEpoch()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleExecutorLost$4/DAGScheduler$$anonfun$handleExecutorLost$4(org.apache.spark.scheduler.DAGScheduler,java.lang.String,long)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleExecutorLost$1/DAGScheduler$$anonfun$handleExecutorLost$1(org.apache.spark.scheduler.DAGScheduler,java.lang.String,long)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/failedEpoch()|",
      "|java+method:///scala/collection/mutable/HashMap/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleExecutorLost$3/DAGScheduler$$anonfun$handleExecutorLost$3(org.apache.spark.scheduler.DAGScheduler,java.lang.String)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$2/DAGScheduler$$anonfun$2(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleExecutorLost$2/DAGScheduler$$anonfun$handleExecutorLost$2(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMaster/removeExecutor(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManager/externalShuffleServiceEnabled()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/clearCacheLocs()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/submitWaitingStages()|",
      "|java+method:///scala/collection/mutable/HashMap/isEmpty()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logDebug(scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/shuffleToMapStage()|",
      "|java+method:///org/apache/spark/MapOutputTrackerMaster/incrementEpoch()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleExecutorLost$4/DAGScheduler$$anonfun$handleExecutorLost$4(org.apache.spark.scheduler.DAGScheduler,java.lang.String,long)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleExecutorLost$1/DAGScheduler$$anonfun$handleExecutorLost$1(org.apache.spark.scheduler.DAGScheduler,java.lang.String,long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$79/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$79/apply(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$79/apply(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/MesosSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.SparkContext,java.lang.String)|",
    "called": "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$class/$init$(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///org/apache/spark/SparkConf/getDouble(java.lang.String,double)|",
      "|java+method:///org/apache/spark/scheduler/SchedulerBackend$class/$init$(org.apache.spark.scheduler.SchedulerBackend)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/SparkContext/listenerBus()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/parseConstraintString(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/getRejectOfferDurationForUnmetConstraints(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$class/$init$(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///org/apache/spark/SparkConf/getDouble(java.lang.String,double)|",
      "|java+method:///org/apache/spark/scheduler/SchedulerBackend$class/$init$(org.apache.spark.scheduler.SchedulerBackend)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/SparkContext/listenerBus()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/parseConstraintString(java.lang.String)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/org$apache$spark$network$netty$NettyBlockTransferService$$startService$1(int,scala.collection.immutable.List)|",
    "called": "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/network/server/TransportServer/getPort()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/network/TransportContext/createServer(int,java.util.List)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///org/apache/spark/network/server/TransportServer/getPort()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/network/TransportContext/createServer(int,java.util.List)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/metrics/MetricsConfig$$anonfun$initialize$4$$anonfun$apply$2/apply(scala.Tuple2)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///java/util/Properties/getProperty(java.lang.String)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///java/util/Properties/get(java.lang.Object)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$13/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$13/apply(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$13/apply(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$class/collect(org.apache.spark.api.java.JavaRDDLike)|",
    "called": "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/rdd/RDD/collect()|",
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+constructor:///java/util/ArrayList/ArrayList(java.util.Collection)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/RDD/collect()|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/PythonRunner$MonitorThread/run()|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///java/lang/Thread/sleep(long)|",
      "|java+method:///org/apache/spark/TaskContext/isInterrupted()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///org/apache/spark/api/python/PythonRunner/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/api/python/PythonRunner$MonitorThread/org$apache$spark$api$python$PythonRunner$MonitorThread$$$outer()|",
      "|java+method:///org/apache/spark/TaskContext/isCompleted()|",
      "|java+method:///scala/collection/mutable/Map/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///org/apache/spark/api/python/PythonRunner/logWarning(scala.Function0)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$MonitorThread$$anonfun$run$4/PythonRunner$MonitorThread$$anonfun$run$4(org.apache.spark.api.python.PythonRunner$MonitorThread)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$MonitorThread$$anonfun$run$5/PythonRunner$MonitorThread$$anonfun$run$5(org.apache.spark.api.python.PythonRunner$MonitorThread)|",
      "|java+method:///org/apache/spark/SparkEnv/destroyPythonWorker(java.lang.String,scala.collection.immutable.Map,java.net.Socket)|",
      "|java+method:///scala/Predef$/conforms()|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Thread/sleep(long)|",
      "|java+method:///org/apache/spark/TaskContext/isInterrupted()|",
      "|java+method:///org/apache/spark/api/python/PythonRunner/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///org/apache/spark/TaskContext/isCompleted()|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$MonitorThread$$anonfun$run$4/PythonRunner$MonitorThread$$anonfun$run$4(org.apache.spark.api.python.PythonRunner$MonitorThread)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$MonitorThread$$anonfun$run$5/PythonRunner$MonitorThread$$anonfun$run$5(org.apache.spark.api.python.PythonRunner$MonitorThread)|",
      "|java+method:///org/apache/spark/SparkEnv/destroyPythonWorker(java.lang.String,scala.collection.immutable.Map,java.net.Socket)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///org/apache/spark/api/python/PythonRunner$MonitorThread/org$apache$spark$api$python$PythonRunner$MonitorThread$$$outer()|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///org/apache/spark/api/python/PythonRunner/logWarning(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/next()|",
    "called": "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$FailureFetchResult/blockId()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/executor/ShuffleReadMetrics/incFetchWaitTime(long)|",
      "|java+method:///java/util/concurrent/LinkedBlockingQueue/take()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$SuccessFetchResult/blockId()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$FailureFetchResult/address()|",
      "|java+constructor:///org/apache/spark/storage/BufferReleasingInputStream/BufferReleasingInputStream(java.io.InputStream,org.apache.spark.storage.ShuffleBlockFetcherIterator)|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$SuccessFetchResult/buf()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/next()|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///scala/collection/mutable/Queue/dequeue()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/sendRequest(org.apache.spark.storage.ShuffleBlockFetcherIterator$FetchRequest)|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$FailureFetchResult/e()|",
      "|java+method:///scala/collection/mutable/Queue/nonEmpty()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$FetchResult/blockId()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$SuccessFetchResult/address()|",
      "|java+method:///org/apache/spark/network/buffer/ManagedBuffer/createInputStream()|",
      "|java+method:///scala/collection/mutable/Queue/front()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$FetchRequest/size()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$SuccessFetchResult/size()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/throwFetchFailedException(org.apache.spark.storage.BlockId,org.apache.spark.storage.BlockManagerId,java.lang.Throwable)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$FailureFetchResult/blockId()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/executor/ShuffleReadMetrics/incFetchWaitTime(long)|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$SuccessFetchResult/blockId()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$FetchResult/blockId()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$FailureFetchResult/address()|",
      "|java+constructor:///org/apache/spark/storage/BufferReleasingInputStream/BufferReleasingInputStream(java.io.InputStream,org.apache.spark.storage.ShuffleBlockFetcherIterator)|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$SuccessFetchResult/buf()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/next()|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$FailureFetchResult/e()|",
      "|java+method:///java/util/concurrent/LinkedBlockingQueue/take()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/fetchUpToMaxBytes()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$SuccessFetchResult/address()|",
      "|java+method:///org/apache/spark/network/buffer/ManagedBuffer/createInputStream()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$SuccessFetchResult/size()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/throwFetchFailedException(org.apache.spark.storage.BlockId,org.apache.spark.storage.BlockManagerId,java.lang.Throwable)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/TestInputValueConverter/convert(java.lang.Object)|",
    "called": "|java+method:///scala/collection/JavaConverters$/asScalaSetConverter(java.util.Set)|",
    "v1Body": [
      "|java+method:///scala/collection/SetLike/toSeq()|",
      "|java+method:///scala/collection/mutable/Set/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/hadoop/io/MapWritable/keySet()|",
      "|java+constructor:///org/apache/spark/api/python/TestInputValueConverter$$anonfun$convert$1/TestInputValueConverter$$anonfun$convert$1(org.apache.spark.api.python.TestInputValueConverter)|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/Set$/canBuildFrom()|",
      "|java+method:///org/apache/spark/api/python/TestInputValueConverter/convert(java.lang.Object)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaSet(java.util.Set)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/SetLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///org/apache/hadoop/io/MapWritable/keySet()|",
      "|java+constructor:///org/apache/spark/api/python/TestInputValueConverter$$anonfun$convert$1/TestInputValueConverter$$anonfun$convert$1(org.apache.spark.api.python.TestInputValueConverter)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///scala/collection/SetLike/toSeq()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaSetConverter(java.util.Set)|",
      "|java+method:///scala/collection/mutable/Set$/canBuildFrom()|",
      "|java+method:///org/apache/spark/api/python/TestInputValueConverter/convert(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$zip$1/apply()|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$zip$1/apply()|",
      "|java+method:///org/apache/spark/rdd/RDD/zipPartitions(org.apache.spark.rdd.RDD,boolean,scala.Function2,scala.reflect.ClassTag,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$zip$1$$anonfun$apply$27/RDD$$anonfun$zip$1$$anonfun$apply$27(org.apache.spark.rdd.RDD$$anonfun$zip$1)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$zip$1/apply()|",
      "|java+method:///org/apache/spark/rdd/RDD/zipPartitions(org.apache.spark.rdd.RDD,boolean,scala.Function2,scala.reflect.ClassTag,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$zip$1$$anonfun$apply$31/RDD$$anonfun$zip$1$$anonfun$apply$31(org.apache.spark.rdd.RDD$$anonfun$zip$1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$glom$1/apply()|",
    "called": "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$glom$1/apply()|",
      "|java+method:///scala/reflect/ClassTag/runtimeClass()|",
      "|java+method:///org/apache/spark/rdd/MapPartitionsRDD$/$lessinit$greater$default$3()|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/rdd/MapPartitionsRDD/MapPartitionsRDD(org.apache.spark.rdd.RDD,scala.Function3,boolean,scala.reflect.ClassTag,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$glom$1$$anonfun$apply$15/RDD$$anonfun$glom$1$$anonfun$apply$15(org.apache.spark.rdd.RDD$$anonfun$glom$1)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$glom$1/apply()|",
      "|java+method:///scala/reflect/ClassTag/runtimeClass()|",
      "|java+constructor:///org/apache/spark/rdd/MapPartitionsRDD/MapPartitionsRDD(org.apache.spark.rdd.RDD,scala.Function3,boolean,scala.reflect.ClassTag,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$glom$1$$anonfun$apply$18/RDD$$anonfun$glom$1$$anonfun$apply$18(org.apache.spark.rdd.RDD$$anonfun$glom$1)|",
      "|java+method:///org/apache/spark/rdd/MapPartitionsRDD$/$lessinit$greater$default$3()|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$reduce$1/apply()|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$reduce$1$$anonfun$15/RDD$$anonfun$reduce$1$$anonfun$15(org.apache.spark.rdd.RDD$$anonfun$reduce$1,scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$reduce$1$$anonfun$14/RDD$$anonfun$reduce$1$$anonfun$14(org.apache.spark.rdd.RDD$$anonfun$reduce$1,scala.Function2)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/RDD/org$apache$spark$rdd$RDD$$sc()|",
      "|java+method:///org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function1,scala.Function2,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$reduce$1$$anonfun$apply$36/RDD$$anonfun$reduce$1$$anonfun$apply$36(org.apache.spark.rdd.RDD$$anonfun$reduce$1)|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$reduce$1$$anonfun$15/RDD$$anonfun$reduce$1$$anonfun$15(org.apache.spark.rdd.RDD$$anonfun$reduce$1,scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$reduce$1$$anonfun$14/RDD$$anonfun$reduce$1$$anonfun$14(org.apache.spark.rdd.RDD$$anonfun$reduce$1,scala.Function2)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$reduce$1$$anonfun$apply$40/RDD$$anonfun$reduce$1$$anonfun$apply$40(org.apache.spark.rdd.RDD$$anonfun$reduce$1)|",
      "|java+method:///org/apache/spark/rdd/RDD/org$apache$spark$rdd$RDD$$sc()|",
      "|java+method:///org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function1,scala.Function2,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$reduceByKeyLocally$1$$anonfun$4/apply(java.util.HashMap,java.util.HashMap)|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$reduceByKeyLocally$1$$anonfun$4$$anonfun$apply$16/PairRDDFunctions$$anonfun$reduceByKeyLocally$1$$anonfun$4$$anonfun$apply$16(org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1$$anonfun$4,java.util.HashMap)|",
      "|java+method:///scala/collection/mutable/Map/foreach(scala.Function1)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$reduceByKeyLocally$1$$anonfun$4$$anonfun$apply$16/PairRDDFunctions$$anonfun$reduceByKeyLocally$1$$anonfun$4$$anonfun$apply$16(org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKeyLocally$1$$anonfun$4,java.util.HashMap)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/SparkSubmit$/prepareSubmitEnvironment(org.apache.spark.deploy.SparkSubmitArguments)|",
    "called": "|java+constructor:///scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)|",
    "v1Body": [
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/keytab()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/YARN()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/deployMode_$eq(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/classIsLoadable(java.lang.String)|",
      "|java+method:///org/apache/hadoop/fs/Path/getName()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/STANDALONE()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/SPARKR_SHELL()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/driverExtraLibraryPath()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$1/SparkSubmit$$anonfun$prepareSubmitEnvironment$1()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/jars()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$6/SparkSubmit$$anonfun$prepareSubmitEnvironment$6(scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/isSqlShell(java.lang.String)|",
      "|java+method:///org/apache/commons/lang3/StringUtils/isBlank(java.lang.CharSequence)|",
      "|java+method:///scala/collection/mutable/HashMap/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/files()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/numExecutors()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/Predef$/assert(boolean,scala.Function0)|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/LOCAL()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/childArgs()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/mergeFileLists(scala.collection.Seq)|",
      "|java+method:///org/apache/hadoop/security/UserGroupInformation/loginUserFromKeytab(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/pyFiles_$eq(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/printWarning(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/archives_$eq(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/totalExecutorCores()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/printStream()|",
      "|java+constructor:///org/apache/spark/deploy/OptionAssigner/OptionAssigner(java.lang.String,int,int,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/CLIENT()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/useRest()|",
      "|java+method:///scala/collection/mutable/HashMap/withFilter(scala.Function1)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayOps/mkString(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/isTesting()|",
      "|java+method:///java/net/URI/toString()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$5/SparkSubmit$$anonfun$prepareSubmitEnvironment$5()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/mainClass()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/immutable/List/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/api/r/RUtils$/localSparkRPackagePath()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/master()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/supervise()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/executorCores()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/childArgs_$eq(scala.collection.mutable.ArrayBuffer)|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$7/SparkSubmit$$anonfun$prepareSubmitEnvironment$7()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/jars_$eq(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/resolveURI(java.lang.String)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/verbose()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/files_$eq(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/printErrorAndExit(java.lang.String)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/ivyRepoPath()|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/isPython()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$8/SparkSubmit$$anonfun$prepareSubmitEnvironment$8()|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/nonLocalPaths(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/ALL_DEPLOY_MODES()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/repositories()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$3/SparkSubmit$$anonfun$3()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$2/SparkSubmit$$anonfun$2()|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/zipRLibraries(java.io.File,java.lang.String)|",
      "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/driverMemory()|",
      "|java+constructor:///scala/runtime/IntRef/IntRef(int)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/CLUSTER()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/isR()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$11/SparkSubmit$$anonfun$prepareSubmitEnvironment$11(scala.collection.mutable.HashMap)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/deployMode()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$9/SparkSubmit$$anonfun$prepareSubmitEnvironment$9(scala.collection.mutable.HashMap)|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$10/SparkSubmit$$anonfun$prepareSubmitEnvironment$10(scala.collection.mutable.HashMap)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/name()|",
      "|java+method:///java/io/File/getAbsolutePath()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/packages()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/isStandaloneCluster()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/mainClass_$eq(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/master_$eq(java.lang.String)|",
      "|java+method:///java/lang/String/startsWith(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/driverCores()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///scala/Tuple2$mcII$sp/Tuple2$mcII$sp(int,int)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/MESOS()|",
      "|java+method:///scala/collection/mutable/HashMap/$minus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/pyFiles()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$2/SparkSubmit$$anonfun$prepareSubmitEnvironment$2(scala.collection.mutable.ArrayBuffer,scala.collection.mutable.HashMap,int,scala.runtime.IntRef)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/ALL_CLUSTER_MGRS()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/principal()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/SPARKR_PACKAGE_ARCHIVE()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/archives()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/SPARK_INTERNAL()|",
      "|java+method:///org/apache/spark/deploy/OptionAssigner$/apply$default$4()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/isUserJar(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitUtils$/resolveMavenCoordinates(java.lang.String,scala.Option,scala.Option,scala.collection.Seq,boolean)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/isShell(java.lang.String)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/sparkProperties()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/primaryResource()|",
      "|java+method:///org/apache/spark/deploy/OptionAssigner$/apply$default$5()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/packagesExclusions()|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$plus$eq(scala.collection.TraversableOnce)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/isThriftServer(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/queue()|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$3/SparkSubmit$$anonfun$prepareSubmitEnvironment$3(scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/executorMemory()|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/driverExtraJavaOptions()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/util/Utils$/nonLocalPaths$default$2()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object,java.lang.Object,scala.collection.Seq)|",
      "|java+method:///scala/Tuple2/_1$mcI$sp()|",
      "|java+method:///scala/collection/mutable/ArrayOps/nonEmpty()|",
      "|java+method:///scala/collection/immutable/StringOps/nonEmpty()|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/checkAndBuildRPackage(java.lang.String,java.io.PrintStream,boolean)|",
      "|java+method:///scala/collection/mutable/BufferLike/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitUtils$/resolveMavenCoordinates$default$5()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/driverExtraClassPath()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$4/SparkSubmit$$anonfun$prepareSubmitEnvironment$4(scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/PYSPARK_SHELL()|",
      "|java+constructor:///scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/keytab()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/YARN()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/deployMode_$eq(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/classIsLoadable(java.lang.String)|",
      "|java+method:///org/apache/hadoop/fs/Path/getName()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/STANDALONE()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/SPARKR_SHELL()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/driverExtraLibraryPath()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$1/SparkSubmit$$anonfun$prepareSubmitEnvironment$1()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/jars()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$6/SparkSubmit$$anonfun$prepareSubmitEnvironment$6(scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/isSqlShell(java.lang.String)|",
      "|java+method:///org/apache/commons/lang3/StringUtils/isBlank(java.lang.CharSequence)|",
      "|java+method:///scala/collection/mutable/HashMap/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/files()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/numExecutors()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/Predef$/assert(boolean,scala.Function0)|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/LOCAL()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/childArgs()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/mergeFileLists(scala.collection.Seq)|",
      "|java+method:///org/apache/hadoop/security/UserGroupInformation/loginUserFromKeytab(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/pyFiles_$eq(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/printWarning(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/archives_$eq(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/totalExecutorCores()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/printStream()|",
      "|java+constructor:///org/apache/spark/deploy/OptionAssigner/OptionAssigner(java.lang.String,int,int,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/CLIENT()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/useRest()|",
      "|java+method:///scala/collection/mutable/HashMap/withFilter(scala.Function1)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayOps/mkString(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/isTesting()|",
      "|java+method:///java/net/URI/toString()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$5/SparkSubmit$$anonfun$prepareSubmitEnvironment$5()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/mainClass()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/immutable/List/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/api/r/RUtils$/localSparkRPackagePath()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/master()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/supervise()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/executorCores()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/childArgs_$eq(scala.collection.mutable.ArrayBuffer)|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$7/SparkSubmit$$anonfun$prepareSubmitEnvironment$7()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/jars_$eq(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/resolveURI(java.lang.String)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/verbose()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/files_$eq(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/printErrorAndExit(java.lang.String)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/ivyRepoPath()|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/isPython()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$8/SparkSubmit$$anonfun$prepareSubmitEnvironment$8()|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/nonLocalPaths(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/ALL_DEPLOY_MODES()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/repositories()|",
      "|java+constructor:///java/io/File/File(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$3/SparkSubmit$$anonfun$3()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$2/SparkSubmit$$anonfun$2()|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/zipRLibraries(java.io.File,java.lang.String)|",
      "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/driverMemory()|",
      "|java+constructor:///scala/runtime/IntRef/IntRef(int)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/CLUSTER()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/isR()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$11/SparkSubmit$$anonfun$prepareSubmitEnvironment$11(scala.collection.mutable.HashMap)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/deployMode()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$9/SparkSubmit$$anonfun$prepareSubmitEnvironment$9(scala.collection.mutable.HashMap)|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$10/SparkSubmit$$anonfun$prepareSubmitEnvironment$10(scala.collection.mutable.HashMap)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/name()|",
      "|java+method:///org/apache/spark/api/r/RUtils$/rPackages()|",
      "|java+method:///java/io/File/getAbsolutePath()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/R_PACKAGE_ARCHIVE()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/packages()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/isStandaloneCluster()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/mainClass_$eq(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/master_$eq(java.lang.String)|",
      "|java+method:///java/lang/String/startsWith(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/driverCores()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///scala/Tuple2$mcII$sp/Tuple2$mcII$sp(int,int)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/MESOS()|",
      "|java+method:///scala/collection/mutable/HashMap/$minus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/pyFiles()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$2/SparkSubmit$$anonfun$prepareSubmitEnvironment$2(scala.collection.mutable.ArrayBuffer,scala.collection.mutable.HashMap,int,scala.runtime.IntRef)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/ALL_CLUSTER_MGRS()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/principal()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/SPARKR_PACKAGE_ARCHIVE()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/archives()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/SPARK_INTERNAL()|",
      "|java+method:///org/apache/spark/deploy/OptionAssigner$/apply$default$4()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/isUserJar(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitUtils$/resolveMavenCoordinates(java.lang.String,scala.Option,scala.Option,scala.collection.Seq,boolean)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/isShell(java.lang.String)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/sparkProperties()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/primaryResource()|",
      "|java+method:///org/apache/spark/deploy/OptionAssigner$/apply$default$5()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/packagesExclusions()|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$plus$eq(scala.collection.TraversableOnce)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/isThriftServer(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/queue()|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$3/SparkSubmit$$anonfun$prepareSubmitEnvironment$3(scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/executorMemory()|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/driverExtraJavaOptions()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/util/Utils$/nonLocalPaths$default$2()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object,java.lang.Object,scala.collection.Seq)|",
      "|java+method:///scala/Tuple2/_1$mcI$sp()|",
      "|java+method:///scala/collection/mutable/ArrayOps/nonEmpty()|",
      "|java+method:///scala/collection/immutable/StringOps/nonEmpty()|",
      "|java+method:///org/apache/spark/deploy/RPackageUtils$/checkAndBuildRPackage(java.lang.String,java.io.PrintStream,boolean)|",
      "|java+method:///scala/collection/mutable/BufferLike/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitUtils$/resolveMavenCoordinates$default$5()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/driverExtraClassPath()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$prepareSubmitEnvironment$4/SparkSubmit$$anonfun$prepareSubmitEnvironment$4(scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/PYSPARK_SHELL()|",
      "|java+constructor:///scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext$$anonfun$wholeTextFiles$1/apply()|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkContext$$anonfun$wholeTextFiles$1/apply()|",
      "|java+method:///org/apache/spark/rdd/WholeTextFileRDD/setName(java.lang.String)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/getConfiguration()|",
      "|java+constructor:///org/apache/hadoop/mapreduce/Job/Job(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileInputFormat/setInputPaths(org.apache.hadoop.mapreduce.Job,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$assertNotStopped()|",
      "|java+method:///org/apache/spark/SparkContext/hadoopConfiguration()|",
      "|java+constructor:///org/apache/spark/rdd/WholeTextFileRDD/WholeTextFileRDD(org.apache.spark.SparkContext,java.lang.Class,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration,int)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$wholeTextFiles$1$$anonfun$apply$10/SparkContext$$anonfun$wholeTextFiles$1$$anonfun$apply$10(org.apache.spark.SparkContext$$anonfun$wholeTextFiles$1)|",
      "|java+method:///org/apache/spark/SparkContext$$anonfun$wholeTextFiles$1/apply()|",
      "|java+method:///org/apache/spark/rdd/WholeTextFileRDD/setName(java.lang.String)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/Job/Job(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/getConfigurationFromJobContext(org.apache.hadoop.mapreduce.JobContext)|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/FileInputFormat/setInputPaths(org.apache.hadoop.mapreduce.Job,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$assertNotStopped()|",
      "|java+method:///org/apache/spark/SparkContext/hadoopConfiguration()|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/rdd/WholeTextFileRDD/WholeTextFileRDD(org.apache.spark.SparkContext,java.lang.Class,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration,int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/scope/RDDOperationGraph$/makeDotFile(org.apache.spark.ui.scope.RDDOperationGraph)|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.String)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/ui/scope/RDDOperationGraph$$anonfun$makeDotFile$1/RDDOperationGraph$$anonfun$makeDotFile$1(scala.collection.mutable.StringBuilder)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationGraph$/org$apache$spark$ui$scope$RDDOperationGraph$$makeDotSubgraph(org.apache.spark.ui.scope.RDDOperationCluster,java.lang.String)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationGraph/edges()|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationGraph/rootCluster()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/ui/scope/RDDOperationGraph$$anonfun$makeDotFile$2/RDDOperationGraph$$anonfun$makeDotFile$2(java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationGraph$/logDebug(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.String)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/ui/scope/RDDOperationGraph$$anonfun$makeDotFile$1/RDDOperationGraph$$anonfun$makeDotFile$1(scala.collection.mutable.StringBuilder)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationGraph/edges()|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationGraph/rootCluster()|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationGraph$/org$apache$spark$ui$scope$RDDOperationGraph$$makeDotSubgraph(scala.collection.mutable.StringBuilder,org.apache.spark.ui.scope.RDDOperationCluster,java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/ui/scope/RDDOperationGraph$$anonfun$makeDotFile$2/RDDOperationGraph$$anonfun$makeDotFile$2(java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationGraph$/logDebug(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/ExecutorExited$/unapply(org.apache.spark.scheduler.ExecutorExited)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/ExecutorExited/exitCode()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/ExecutorExited/reason()|",
      "|java+method:///org/apache/spark/scheduler/ExecutorExited/exitCode()|",
      "|java+method:///org/apache/spark/scheduler/ExecutorExited/exitCausedByApp()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rpc/RpcTimeout$/apply(org.apache.spark.SparkConf,scala.collection.Seq,java.lang.String)|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/rpc/RpcTimeout$$anonfun$apply$1/RpcTimeout$$anonfun$apply$1(scala.runtime.ObjectRef,java.lang.String)|",
      "|java+method:///scala/concurrent/duration/package$DurationLong/seconds()|",
      "|java+method:///scala/collection/Iterator/next()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/util/Utils$/timeStringAsSeconds(java.lang.String)|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///scala/concurrent/duration/package$/DurationLong(long)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/Seq/iterator()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+constructor:///scala/concurrent/duration/package$DurationLong/package$DurationLong(long)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+constructor:///org/apache/spark/rpc/RpcTimeout$$anonfun$2/RpcTimeout$$anonfun$2(scala.collection.Seq,java.lang.String)|",
      "|java+constructor:///org/apache/spark/rpc/RpcTimeout/RpcTimeout(scala.concurrent.duration.FiniteDuration,java.lang.String)|",
      "|java+method:///scala/Predef$/require(boolean)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/rpc/RpcTimeout$$anonfun$apply$1/RpcTimeout$$anonfun$apply$1(scala.runtime.ObjectRef,java.lang.String)|",
      "|java+method:///scala/concurrent/duration/package$DurationLong/seconds()|",
      "|java+method:///scala/collection/Iterator/next()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/util/Utils$/timeStringAsSeconds(java.lang.String)|",
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+constructor:///org/apache/spark/rpc/RpcTimeout$$anonfun$1/RpcTimeout$$anonfun$1(scala.collection.Seq,java.lang.String)|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///scala/concurrent/duration/package$/DurationLong(long)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/Seq/iterator()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+constructor:///scala/concurrent/duration/package$DurationLong/package$DurationLong(long)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+constructor:///org/apache/spark/rpc/RpcTimeout/RpcTimeout(scala.concurrent.duration.FiniteDuration,java.lang.String)|",
      "|java+method:///scala/Predef$/require(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$67/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$67/apply(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$67/apply(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$/applicationStartFromJson(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerApplicationStart/SparkListenerApplicationStart(java.lang.String,scala.Option,long,java.lang.String,scala.Option,scala.Option)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$58/JsonProtocol$$anonfun$58()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$57/JsonProtocol$$anonfun$57()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$56/JsonProtocol$$anonfun$56()|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerApplicationStart/SparkListenerApplicationStart(java.lang.String,scala.Option,long,java.lang.String,scala.Option,scala.Option)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$61/JsonProtocol$$anonfun$61()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$59/JsonProtocol$$anonfun$59()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$60/JsonProtocol$$anonfun$60()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$8/apply()|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1/org$apache$spark$deploy$worker$Worker$$anonfun$$$outer()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$8/apply()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$8$$anonfun$apply$2/Worker$$anonfun$receive$1$$anonfun$8$$anonfun$apply$2(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1$$anonfun$8)|",
      "|java+method:///org/apache/spark/util/Utils$/getOrCreateLocalRootDirs(org.apache.spark.SparkConf)|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/conf()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1/org$apache$spark$deploy$worker$Worker$$anonfun$$$outer()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$8$$anonfun$apply$4/Worker$$anonfun$receive$1$$anonfun$8$$anonfun$apply$4(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1$$anonfun$8)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$8/apply()|",
      "|java+method:///org/apache/spark/util/Utils$/getOrCreateLocalRootDirs(org.apache.spark.SparkConf)|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/conf()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/WritableToJavaConverter/org$apache$spark$api$python$WritableToJavaConverter$$convertWritable(org.apache.hadoop.io.Writable)|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/Any()|",
      "|java+constructor:///org/apache/spark/api/python/WritableToJavaConverter$$anonfun$org$apache$spark$api$python$WritableToJavaConverter$$convertWritable$2/WritableToJavaConverter$$anonfun$org$apache$spark$api$python$WritableToJavaConverter$$convertWritable$2(org.apache.spark.api.python.WritableToJavaConverter,java.util.HashMap)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/hadoop/io/ArrayWritable/get()|",
      "|java+constructor:///org/apache/spark/api/python/WritableToJavaConverter$$anonfun$org$apache$spark$api$python$WritableToJavaConverter$$convertWritable$1/WritableToJavaConverter$$anonfun$org$apache$spark$api$python$WritableToJavaConverter$$convertWritable$1(org.apache.spark.api.python.WritableToJavaConverter)|",
      "|java+method:///org/apache/hadoop/io/DoubleWritable/get()|",
      "|java+method:///org/apache/hadoop/io/BytesWritable/getBytes()|",
      "|java+method:///org/apache/hadoop/io/WritableUtils/clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///scala/collection/mutable/Map/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/broadcast/Broadcast/value()|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/hadoop/io/FloatWritable/get()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToFloat(float)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+constructor:///java/util/HashMap/HashMap()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/util/SerializableConfiguration/value()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/hadoop/io/Text/toString()|",
      "|java+method:///org/apache/hadoop/io/BooleanWritable/get()|",
      "|java+method:///org/apache/hadoop/io/LongWritable/get()|",
      "|java+method:///java/lang/System/arraycopy(java.lang.Object,int,java.lang.Object,int,int)|",
      "|java+method:///org/apache/hadoop/io/IntWritable/get()|",
      "|java+method:///org/apache/hadoop/io/BytesWritable/getLength()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/Any()|",
      "|java+constructor:///org/apache/spark/api/python/WritableToJavaConverter$$anonfun$org$apache$spark$api$python$WritableToJavaConverter$$convertWritable$2/WritableToJavaConverter$$anonfun$org$apache$spark$api$python$WritableToJavaConverter$$convertWritable$2(org.apache.spark.api.python.WritableToJavaConverter,java.util.HashMap)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/hadoop/io/ArrayWritable/get()|",
      "|java+constructor:///org/apache/spark/api/python/WritableToJavaConverter$$anonfun$org$apache$spark$api$python$WritableToJavaConverter$$convertWritable$1/WritableToJavaConverter$$anonfun$org$apache$spark$api$python$WritableToJavaConverter$$convertWritable$1(org.apache.spark.api.python.WritableToJavaConverter)|",
      "|java+method:///org/apache/hadoop/io/DoubleWritable/get()|",
      "|java+method:///org/apache/hadoop/io/WritableUtils/clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/broadcast/Broadcast/value()|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/hadoop/io/FloatWritable/get()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToFloat(float)|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///org/apache/hadoop/io/BytesWritable/getBytes()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+constructor:///java/util/HashMap/HashMap()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/util/SerializableConfiguration/value()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/hadoop/io/Text/toString()|",
      "|java+method:///org/apache/hadoop/io/BooleanWritable/get()|",
      "|java+method:///org/apache/hadoop/io/LongWritable/get()|",
      "|java+method:///java/lang/System/arraycopy(java.lang.Object,int,java.lang.Object,int,int)|",
      "|java+method:///org/apache/hadoop/io/IntWritable/get()|",
      "|java+method:///org/apache/hadoop/io/BytesWritable/getLength()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$56/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/immutable/Set/contains(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/taskId()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskInfo()|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$56$$anonfun$apply$24/StagePage$$anonfun$56$$anonfun$apply$24(org.apache.spark.ui.jobs.StagePage$$anonfun$56)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/shuffleWriteMetrics()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$56$$anonfun$apply$11/StagePage$$anonfun$56$$anonfun$apply$11(org.apache.spark.ui.jobs.StagePage$$anonfun$56)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/r/RRDD$/createSparkContext(java.lang.String,java.lang.String,java.lang.String,java.lang.String%5B%5D,java.util.Map,java.util.Map)|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkConf/setIfMissing(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/setMaster(java.lang.String)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkConf/setSparkHome(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkConf/SparkConf()|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/api/r/RRDD$$anonfun$createSparkContext$5/RRDD$$anonfun$createSparkContext$5(org.apache.spark.api.java.JavaSparkContext)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+constructor:///org/apache/spark/api/java/JavaSparkContext/JavaSparkContext(org.apache.spark.SparkConf)|",
      "|java+constructor:///org/apache/spark/api/r/RRDD$$anonfun$createSparkContext$1/RRDD$$anonfun$createSparkContext$1()|",
      "|java+constructor:///org/apache/spark/api/r/RRDD$$anonfun$createSparkContext$3/RRDD$$anonfun$createSparkContext$3()|",
      "|java+method:///org/apache/spark/SparkConf/setAppName(java.lang.String)|",
      "|java+method:///scala/collection/mutable/Map/withFilter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/api/r/RRDD$$anonfun$createSparkContext$2/RRDD$$anonfun$createSparkContext$2(org.apache.spark.SparkConf)|",
      "|java+constructor:///org/apache/spark/api/r/RRDD$$anonfun$createSparkContext$4/RRDD$$anonfun$createSparkContext$4(org.apache.spark.SparkConf)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkConf/setIfMissing(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/setMaster(java.lang.String)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkConf/setSparkHome(java.lang.String)|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+constructor:///org/apache/spark/SparkConf/SparkConf()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/api/r/RRDD$$anonfun$createSparkContext$5/RRDD$$anonfun$createSparkContext$5(org.apache.spark.api.java.JavaSparkContext)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkConf/setAppName(java.lang.String)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/TraversableLike/withFilter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/api/java/JavaSparkContext/JavaSparkContext(org.apache.spark.SparkConf)|",
      "|java+constructor:///org/apache/spark/api/r/RRDD$$anonfun$createSparkContext$1/RRDD$$anonfun$createSparkContext$1()|",
      "|java+constructor:///org/apache/spark/api/r/RRDD$$anonfun$createSparkContext$3/RRDD$$anonfun$createSparkContext$3()|",
      "|java+constructor:///org/apache/spark/api/r/RRDD$$anonfun$createSparkContext$2/RRDD$$anonfun$createSparkContext$2(org.apache.spark.SparkConf)|",
      "|java+constructor:///org/apache/spark/api/r/RRDD$$anonfun$createSparkContext$4/RRDD$$anonfun$createSparkContext$4(org.apache.spark.SparkConf)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkEnv$/create(org.apache.spark.SparkConf,java.lang.String,java.lang.String,int,boolean,boolean,int,org.apache.spark.scheduler.LiveListenerBus,scala.Option)|",
    "called": "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/port()|",
      "|java+method:///scala/collection/immutable/Map/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///java/io/File/getAbsolutePath()|",
      "|java+method:///org/apache/spark/shuffle/ShuffleMemoryManager$/create(org.apache.spark.SparkConf,int)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem$/createMetricsSystem(java.lang.String,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+constructor:///org/apache/spark/unsafe/memory/ExecutorMemoryManager/ExecutorMemoryManager(org.apache.spark.unsafe.memory.MemoryAllocator)|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$5/SparkEnv$$anonfun$5(org.apache.spark.rpc.RpcEnv,org.apache.spark.scheduler.OutputCommitCoordinator)|",
      "|java+method:///org/apache/spark/SparkEnv$/registerOrLookupEndpoint$1(java.lang.String,scala.Function0,org.apache.spark.SparkConf,boolean,org.apache.spark.rpc.RpcEnv)|",
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/MapOutputTracker$/ENDPOINT_NAME()|",
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/util/Utils$/getLocalDir(org.apache.spark.SparkConf)|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$create$2/SparkEnv$$anonfun$create$2(org.apache.spark.serializer.Serializer)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMaster$/DRIVER_ENDPOINT_NAME()|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/Predef$/assert(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/HttpFileServer/serverUri()|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$1/SparkEnv$$anonfun$1(java.lang.String)|",
      "|java+constructor:///org/apache/spark/broadcast/BroadcastManager/BroadcastManager(boolean,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/coordinatorRef_$eq(scala.Option)|",
      "|java+method:///java/lang/String/toLowerCase()|",
      "|java+method:///org/apache/spark/SparkEnv$/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$create$1/SparkEnv$$anonfun$create$1()|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkEnv/org$apache$spark$SparkEnv$$driverTmpDirToDelete_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/HttpFileServer/initialize()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/start()|",
      "|java+constructor:///org/apache/spark/network/nio/NioBlockTransferService/NioBlockTransferService(org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$2/SparkEnv$$anonfun$2()|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$create$3/SparkEnv$$anonfun$create$3(org.apache.spark.SparkConf,org.apache.spark.rpc.RpcEnv,org.apache.spark.MapOutputTracker)|",
      "|java+constructor:///org/apache/spark/MapOutputTrackerMaster/MapOutputTrackerMaster(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+constructor:///org/apache/spark/MapOutputTrackerWorker/MapOutputTrackerWorker(org.apache.spark.SparkConf)|",
      "|java+method:///scala/Predef$/any2ArrowAssoc(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkEnv$/instantiateClass$1(java.lang.String,org.apache.spark.SparkConf,boolean)|",
      "|java+method:///org/apache/spark/MapOutputTracker/trackerEndpoint_$eq(org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+constructor:///org/apache/spark/HttpFileServer/HttpFileServer(org.apache.spark.SparkConf,org.apache.spark.SecurityManager,int)|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager/BlockManager(java.lang.String,org.apache.spark.rpc.RpcEnv,org.apache.spark.storage.BlockManagerMaster,org.apache.spark.serializer.Serializer,org.apache.spark.SparkConf,org.apache.spark.MapOutputTracker,org.apache.spark.shuffle.ShuffleManager,org.apache.spark.network.BlockTransferService,org.apache.spark.SecurityManager,int)|",
      "|java+constructor:///org/apache/spark/CacheManager/CacheManager(org.apache.spark.storage.BlockManager)|",
      "|java+method:///org/apache/spark/SparkEnv$/instantiateClassFromConf$1(java.lang.String,java.lang.String,org.apache.spark.SparkConf,boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/SparkEnv$/executorActorSystemName()|",
      "|java+method:///org/apache/spark/SparkConf/set(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/akka/AkkaRpcEnv/actorSystem()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$4/SparkEnv$$anonfun$4(org.apache.spark.SparkConf,boolean)|",
      "|java+method:///org/apache/spark/util/Utils$/createTempDir(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkEnv/SparkEnv(java.lang.String,org.apache.spark.rpc.RpcEnv,org.apache.spark.serializer.Serializer,org.apache.spark.serializer.Serializer,org.apache.spark.CacheManager,org.apache.spark.MapOutputTracker,org.apache.spark.shuffle.ShuffleManager,org.apache.spark.broadcast.BroadcastManager,org.apache.spark.network.BlockTransferService,org.apache.spark.storage.BlockManager,org.apache.spark.SecurityManager,org.apache.spark.HttpFileServer,java.lang.String,org.apache.spark.metrics.MetricsSystem,org.apache.spark.shuffle.ShuffleMemoryManager,org.apache.spark.unsafe.memory.ExecutorMemoryManager,org.apache.spark.scheduler.OutputCommitCoordinator,org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/address()|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerMaster/BlockManagerMaster(org.apache.spark.rpc.RpcEndpointRef,org.apache.spark.SparkConf,boolean)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkEnv$/driverActorSystemName()|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$3/SparkEnv$$anonfun$3(org.apache.spark.SparkConf,boolean,org.apache.spark.scheduler.LiveListenerBus,org.apache.spark.rpc.RpcEnv)|",
      "|java+method:///org/apache/spark/SparkEnv$/logWarning(scala.Function0)|",
      "|java+constructor:///org/apache/spark/network/netty/NettyBlockTransferService/NettyBlockTransferService(org.apache.spark.SparkConf,org.apache.spark.SecurityManager,int)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$1/SparkEnv$$anonfun$1(java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/port()|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$create$3/SparkEnv$$anonfun$create$3(org.apache.spark.SparkConf,org.apache.spark.rpc.RpcEnv,org.apache.spark.MapOutputTracker)|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/SparkEnv/SparkEnv(java.lang.String,org.apache.spark.rpc.RpcEnv,akka.actor.ActorSystem,org.apache.spark.serializer.Serializer,org.apache.spark.serializer.Serializer,org.apache.spark.CacheManager,org.apache.spark.MapOutputTracker,org.apache.spark.shuffle.ShuffleManager,org.apache.spark.broadcast.BroadcastManager,org.apache.spark.network.BlockTransferService,org.apache.spark.storage.BlockManager,org.apache.spark.SecurityManager,java.lang.String,org.apache.spark.metrics.MetricsSystem,org.apache.spark.memory.MemoryManager,org.apache.spark.scheduler.OutputCommitCoordinator,org.apache.spark.SparkConf)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Utils$/createTempDir(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/network/netty/NettyBlockTransferService/NettyBlockTransferService(org.apache.spark.SparkConf,org.apache.spark.SecurityManager,int)|",
      "|java+method:///org/apache/spark/SparkEnv$/registerOrLookupEndpoint$1(java.lang.String,scala.Function0,org.apache.spark.SparkConf,boolean,org.apache.spark.rpc.RpcEnv)|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$2/SparkEnv$$anonfun$2(org.apache.spark.SparkConf,boolean,org.apache.spark.scheduler.LiveListenerBus,org.apache.spark.rpc.RpcEnv)|",
      "|java+method:///org/apache/spark/util/AkkaUtils$/createActorSystem(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/MapOutputTracker$/ENDPOINT_NAME()|",
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/util/Utils$/getLocalDir(org.apache.spark.SparkConf)|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$create$2/SparkEnv$$anonfun$create$2(org.apache.spark.serializer.Serializer)|",
      "|java+method:///org/apache/spark/memory/UnifiedMemoryManager$/apply(org.apache.spark.SparkConf,int)|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMaster$/DRIVER_ENDPOINT_NAME()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///scala/Predef$/assert(boolean,scala.Function0)|",
      "|java+constructor:///org/apache/spark/broadcast/BroadcastManager/BroadcastManager(boolean,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/coordinatorRef_$eq(scala.Option)|",
      "|java+method:///java/lang/String/toLowerCase()|",
      "|java+method:///org/apache/spark/SparkEnv$/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$create$1/SparkEnv$$anonfun$create$1()|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkEnv/org$apache$spark$SparkEnv$$driverTmpDirToDelete_$eq(scala.Option)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager/BlockManager(java.lang.String,org.apache.spark.rpc.RpcEnv,org.apache.spark.storage.BlockManagerMaster,org.apache.spark.serializer.Serializer,org.apache.spark.SparkConf,org.apache.spark.memory.MemoryManager,org.apache.spark.MapOutputTracker,org.apache.spark.shuffle.ShuffleManager,org.apache.spark.network.BlockTransferService,org.apache.spark.SecurityManager,int)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/start()|",
      "|java+constructor:///org/apache/spark/memory/StaticMemoryManager/StaticMemoryManager(org.apache.spark.SparkConf,int)|",
      "|java+method:///scala/collection/immutable/Map/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+constructor:///org/apache/spark/MapOutputTrackerMaster/MapOutputTrackerMaster(org.apache.spark.SparkConf)|",
      "|java+constructor:///org/apache/spark/MapOutputTrackerWorker/MapOutputTrackerWorker(org.apache.spark.SparkConf)|",
      "|java+method:///scala/Predef$/any2ArrowAssoc(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkEnv$/instantiateClass$1(java.lang.String,org.apache.spark.SparkConf,boolean)|",
      "|java+method:///org/apache/spark/MapOutputTracker/trackerEndpoint_$eq(org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+constructor:///org/apache/spark/CacheManager/CacheManager(org.apache.spark.storage.BlockManager)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,boolean)|",
      "|java+method:///org/apache/spark/SparkEnv$/instantiateClassFromConf$1(java.lang.String,java.lang.String,org.apache.spark.SparkConf,boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/SparkEnv$/executorActorSystemName()|",
      "|java+method:///org/apache/spark/SparkConf/set(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/akka/AkkaRpcEnv/actorSystem()|",
      "|java+method:///java/io/File/getAbsolutePath()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$3/SparkEnv$$anonfun$3(org.apache.spark.SparkConf,boolean)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem$/createMetricsSystem(java.lang.String,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/address()|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerMaster/BlockManagerMaster(org.apache.spark.rpc.RpcEndpointRef,org.apache.spark.SparkConf,boolean)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkEnv$/driverActorSystemName()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$4/SparkEnv$$anonfun$4(org.apache.spark.rpc.RpcEnv,org.apache.spark.scheduler.OutputCommitCoordinator)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/SecurityManager$$anonfun$7/SecurityManager$$anonfun$7(org.apache.spark.SecurityManager)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$cogroup$2/apply()|",
    "called": "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
    "v1Body": [
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$cogroup$2/apply()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/mapValues(scala.Function1)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD/CoGroupedRDD(scala.collection.Seq,org.apache.spark.Partitioner)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/keyClass()|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///java/lang/Class/isArray()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$cogroup$2$$anonfun$apply$47/PairRDDFunctions$$anonfun$cogroup$2$$anonfun$apply$47(org.apache.spark.rdd.PairRDDFunctions$$anonfun$cogroup$2)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$cogroup$2/apply()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/mapValues(scala.Function1)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD/CoGroupedRDD(scala.collection.Seq,org.apache.spark.Partitioner,scala.reflect.ClassTag)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/keyClass()|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///java/lang/Class/isArray()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$cogroup$2$$anonfun$apply$47/PairRDDFunctions$$anonfun$cogroup$2$$anonfun$apply$47(org.apache.spark.rdd.PairRDDFunctions$$anonfun$cogroup$2)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/EventLoggingListener/start()|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+method:///java/io/OutputStream/close()|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener$/IN_PROGRESS()|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/hadoopDataStream()|",
      "|java+constructor:///java/net/URI/URI(java.lang.String)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/getDefaultUri(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/getFileStatus(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/logWarning(scala.Function0)|",
      "|java+method:///org/apache/hadoop/fs/FileStatus/isDir()|",
      "|java+constructor:///org/apache/spark/scheduler/EventLoggingListener$$anonfun$4/EventLoggingListener$$anonfun$4(org.apache.spark.scheduler.EventLoggingListener,java.io.OutputStream)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/hadoopDataStream_$eq(scala.Option)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/delete(org.apache.hadoop.fs.Path,boolean)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/compressionCodec()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/create(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/logPath()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///java/io/FileOutputStream/FileOutputStream(java.lang.String)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)|",
      "|java+method:///java/net/URI/getPath()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.net.URI)|",
      "|java+method:///java/net/URI/getScheme()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener$/org$apache$spark$scheduler$EventLoggingListener$$LOG_FILE_PERMISSIONS()|",
      "|java+constructor:///org/apache/spark/scheduler/EventLoggingListener$$anonfun$5/EventLoggingListener$$anonfun$5(org.apache.spark.scheduler.EventLoggingListener,java.io.OutputStream)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/outputBufferSize()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///java/io/BufferedOutputStream/BufferedOutputStream(java.io.OutputStream,int)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener$/initEventLog(java.io.OutputStream)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/EventLoggingListener$$anonfun$start$1/EventLoggingListener$$anonfun$start$1(org.apache.spark.scheduler.EventLoggingListener,org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/shouldOverwrite()|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/writer_$eq(scala.Option)|",
      "|java+constructor:///java/io/PrintWriter/PrintWriter(java.io.OutputStream)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/scheduler/EventLoggingListener$$anonfun$start$2/EventLoggingListener$$anonfun$start$2(org.apache.spark.scheduler.EventLoggingListener)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/fileSystem()|"
    ],
    "v2Body": [
      "|java+method:///java/io/OutputStream/close()|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener$/IN_PROGRESS()|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/hadoopDataStream()|",
      "|java+constructor:///java/net/URI/URI(java.lang.String)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/getDefaultUri(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/getFileStatus(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/logWarning(scala.Function0)|",
      "|java+method:///org/apache/hadoop/fs/FileStatus/isDir()|",
      "|java+constructor:///org/apache/spark/scheduler/EventLoggingListener$$anonfun$4/EventLoggingListener$$anonfun$4(org.apache.spark.scheduler.EventLoggingListener,java.io.OutputStream)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/hadoopDataStream_$eq(scala.Option)|",
      "|java+constructor:///org/apache/spark/scheduler/EventLoggingListener$$anonfun$start$2/EventLoggingListener$$anonfun$start$2(org.apache.spark.scheduler.EventLoggingListener,org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/delete(org.apache.hadoop.fs.Path,boolean)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/compressionCodec()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/create(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/logPath()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///java/io/FileOutputStream/FileOutputStream(java.lang.String)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)|",
      "|java+method:///java/net/URI/getPath()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.net.URI)|",
      "|java+method:///java/net/URI/getScheme()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener$/org$apache$spark$scheduler$EventLoggingListener$$LOG_FILE_PERMISSIONS()|",
      "|java+constructor:///org/apache/spark/scheduler/EventLoggingListener$$anonfun$5/EventLoggingListener$$anonfun$5(org.apache.spark.scheduler.EventLoggingListener,java.io.OutputStream)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/outputBufferSize()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///java/io/BufferedOutputStream/BufferedOutputStream(java.io.OutputStream,int)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener$/initEventLog(java.io.OutputStream)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/EventLoggingListener$$anonfun$start$1/EventLoggingListener$$anonfun$start$1(org.apache.spark.scheduler.EventLoggingListener,org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/shouldOverwrite()|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/writer_$eq(scala.Option)|",
      "|java+constructor:///java/io/PrintWriter/PrintWriter(java.io.OutputStream)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/EventLoggingListener$$anonfun$start$3/EventLoggingListener$$anonfun$start$3(org.apache.spark.scheduler.EventLoggingListener)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/fileSystem()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1/applyOrElse(java.lang.Object,scala.Function1)|",
    "called": "|java+method:///scala/collection/mutable/ArrayBuffer/$minus$eq(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$state()|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/port()|",
      "|java+method:///org/apache/spark/deploy/master/Master/apps()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$formatExecutorIds(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$createDriver(org.apache.spark.deploy.DriverDescription)|",
      "|java+method:///scala/collection/mutable/HashSet/find(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/master/DriverInfo/exception()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$completedDrivers()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$11/Master$$anonfun$receiveAndReply$1$$anonfun$11(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$DriverStatusResponse/DeployMessages$DriverStatusResponse(boolean,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/master/DriverInfo/state()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$drivers()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$27/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$27(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$KillExecutors/executorIds()|",
      "|java+method:///org/apache/spark/deploy/master/DriverInfo/id()|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/reply(java.lang.Object)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Set/find(scala.Function1)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$10/Master$$anonfun$receiveAndReply$1$$anonfun$10(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$KillDriverResponse/DeployMessages$KillDriverResponse(org.apache.spark.rpc.RpcEndpointRef,java.lang.String,boolean,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$28/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$28(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$MasterStateResponse/DeployMessages$MasterStateResponse(java.lang.String,int,scala.Option,org.apache.spark.deploy.master.WorkerInfo%5B%5D,org.apache.spark.deploy.master.ApplicationInfo%5B%5D,org.apache.spark.deploy.master.ApplicationInfo%5B%5D,org.apache.spark.deploy.master.DriverInfo%5B%5D,org.apache.spark.deploy.master.DriverInfo%5B%5D,scala.Enumeration$Value)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$minus$eq(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/master/MasterMessages$BoundPortsResponse/MasterMessages$BoundPortsResponse(int,int,scala.Option)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI/boundPort()|",
      "|java+method:///org/apache/spark/deploy/master/Master/logWarning(scala.Function0)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$26/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$26(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$DriverStateChanged/DeployMessages$DriverStateChanged(java.lang.String,scala.Enumeration$Value,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/master/DriverInfo/worker()|",
      "|java+method:///org/apache/spark/deploy/master/Master/self()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$25/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$25(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$24/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$24(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RequestKillDriver/driverId()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$23/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$23(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1,java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RequestExecutors/appId()|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$SubmitDriverResponse/DeployMessages$SubmitDriverResponse(org.apache.spark.rpc.RpcEndpointRef,boolean,scala.Option,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/RecoveryState$/ALIVE()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$22/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$22(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1,org.apache.spark.deploy.DriverDescription)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/host()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/HashSet/toArray(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/deploy/master/Master/workers()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$handleRequestExecutors(java.lang.String,int)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$completedApps()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Utils$/BACKUP_STANDALONE_MASTER_PREFIX()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/toArray(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RequestDriverStatus/driverId()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$persistenceEngine()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$schedule()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$KillExecutors/appId()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$handleKillExecutors(java.lang.String,scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/Exception/Exception(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/PersistenceEngine/addDriver(org.apache.spark.deploy.master.DriverInfo)|",
      "|java+method:///scala/collection/mutable/HashSet/add(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RequestExecutors/requestedTotal()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RequestSubmitDriver/driverDescription()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$restServerBoundPort()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$waitingDrivers()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/deploy/master/DriverState$/KILLED()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$webUi()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterWorker/id()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$completedDrivers()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$28/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$28(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$11/Master$$anonfun$receiveAndReply$1$$anonfun$11(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$DriverStatusResponse/DeployMessages$DriverStatusResponse(boolean,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/master/DriverInfo/state()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$drivers()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$KillExecutors/executorIds()|",
      "|java+method:///org/apache/spark/deploy/master/DriverInfo/id()|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/reply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$27/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$27(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1,java.lang.String)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Set/find(scala.Function1)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$10/Master$$anonfun$receiveAndReply$1$$anonfun$10(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$KillDriverResponse/DeployMessages$KillDriverResponse(org.apache.spark.rpc.RpcEndpointRef,java.lang.String,boolean,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$MasterStateResponse/DeployMessages$MasterStateResponse(java.lang.String,int,scala.Option,org.apache.spark.deploy.master.WorkerInfo%5B%5D,org.apache.spark.deploy.master.ApplicationInfo%5B%5D,org.apache.spark.deploy.master.ApplicationInfo%5B%5D,org.apache.spark.deploy.master.DriverInfo%5B%5D,org.apache.spark.deploy.master.DriverInfo%5B%5D,scala.Enumeration$Value)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$22/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$22(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1,java.lang.String,int,int,int)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$minus$eq(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/master/MasterMessages$BoundPortsResponse/MasterMessages$BoundPortsResponse(int,int,scala.Option)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI/boundPort()|",
      "|java+method:///org/apache/spark/deploy/master/Master/logWarning(scala.Function0)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$26/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$26(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$DriverStateChanged/DeployMessages$DriverStateChanged(java.lang.String,scala.Enumeration$Value,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/master/DriverInfo/worker()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$29/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$29(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1)|",
      "|java+method:///org/apache/spark/deploy/master/WorkerInfo/endpoint()|",
      "|java+method:///org/apache/spark/deploy/master/Master/self()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$25/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$25(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$23/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$23(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1,org.apache.spark.rpc.RpcAddress)|",
      "|java+constructor:///org/apache/spark/deploy/master/WorkerInfo/WorkerInfo(java.lang.String,java.lang.String,int,int,int,org.apache.spark.rpc.RpcEndpointRef,int,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$state()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterWorker/webUiPort()|",
      "|java+method:///org/apache/spark/deploy/master/Master/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/port()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$masterWebUiUrl()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RequestKillDriver/driverId()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterWorker/publicAddress()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RequestExecutors/appId()|",
      "|java+method:///org/apache/spark/deploy/master/Master/apps()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/address()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$formatExecutorIds(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$registerWorker(org.apache.spark.deploy.master.WorkerInfo)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$RegisterWorkerFailed/DeployMessages$RegisterWorkerFailed(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$RegisteredWorker/DeployMessages$RegisteredWorker(org.apache.spark.rpc.RpcEndpointRef,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$createDriver(org.apache.spark.deploy.DriverDescription)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$SubmitDriverResponse/DeployMessages$SubmitDriverResponse(org.apache.spark.rpc.RpcEndpointRef,boolean,scala.Option,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/RecoveryState$/ALIVE()|",
      "|java+method:///org/apache/spark/deploy/master/RecoveryState$/STANDBY()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/host()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/deploy/master/PersistenceEngine/addWorker(org.apache.spark.deploy.master.WorkerInfo)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/HashSet/toArray(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$handleRequestExecutors(java.lang.String,int)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$30/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$30(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1)|",
      "|java+method:///org/apache/spark/deploy/master/Master/workers()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterWorker/cores()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$completedApps()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$24/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$24(org.apache.spark.deploy.master.Master$$anonfun$receiveAndReply$1,org.apache.spark.deploy.DriverDescription)|",
      "|java+method:///scala/collection/mutable/HashSet/find(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$idToWorker()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Utils$/BACKUP_STANDALONE_MASTER_PREFIX()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterWorker/worker()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/toArray(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RequestDriverStatus/driverId()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$persistenceEngine()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$schedule()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$KillExecutors/appId()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterWorker/memory()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$handleKillExecutors(java.lang.String,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterWorker/port()|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/Exception/Exception(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/PersistenceEngine/addDriver(org.apache.spark.deploy.master.DriverInfo)|",
      "|java+method:///scala/collection/mutable/HashSet/add(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RequestExecutors/requestedTotal()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RequestSubmitDriver/driverDescription()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$restServerBoundPort()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$waitingDrivers()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/deploy/master/DriverState$/KILLED()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$webUi()|",
      "|java+method:///org/apache/spark/deploy/master/DriverInfo/exception()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterWorker/host()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskResultGetter$$anon$3$$anonfun$run$2/apply$mcV$sp()|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/handleFailedTask(org.apache.spark.scheduler.TaskSetManager,long,scala.Enumeration$Value,org.apache.spark.TaskEndReason)|",
      "|java+method:///java/lang/ThreadLocal/get()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskResultGetter$$anon$3$$anonfun$run$2$$anonfun$apply$mcV$sp$2/TaskResultGetter$$anon$3$$anonfun$run$2$$anonfun$apply$mcV$sp$2(org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2,java.lang.ClassLoader)|",
      "|java+method:///org/apache/spark/util/Utils$/getContextOrSparkClassLoader()|",
      "|java+method:///org/apache/spark/scheduler/TaskResultGetter$$anon$3/org$apache$spark$scheduler$TaskResultGetter$$anon$$$outer()|",
      "|java+method:///org/apache/spark/util/Utils$/getSparkClassLoader()|",
      "|java+method:///org/apache/spark/scheduler/TaskResultGetter/serializer()|",
      "|java+method:///org/apache/spark/scheduler/TaskResultGetter/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/deserialize(java.nio.ByteBuffer,java.lang.ClassLoader,scala.reflect.ClassTag)|",
      "|java+method:///java/nio/ByteBuffer/limit()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/scheduler/TaskResultGetter$$anon$3/org$apache$spark$scheduler$TaskResultGetter$$anon$$$outer()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/handleFailedTask(org.apache.spark.scheduler.TaskSetManager,long,scala.Enumeration$Value,org.apache.spark.TaskEndReason)|",
      "|java+method:///java/lang/ThreadLocal/get()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskResultGetter$$anon$3$$anonfun$run$2$$anonfun$apply$mcV$sp$2/TaskResultGetter$$anon$3$$anonfun$run$2$$anonfun$apply$mcV$sp$2(org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2,java.lang.ClassLoader)|",
      "|java+method:///org/apache/spark/util/Utils$/getContextOrSparkClassLoader()|",
      "|java+method:///org/apache/spark/scheduler/TaskResultGetter/serializer()|",
      "|java+method:///org/apache/spark/scheduler/TaskResultGetter/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/deserialize(java.nio.ByteBuffer,java.lang.ClassLoader,scala.reflect.ClassTag)|",
      "|java+method:///java/nio/ByteBuffer/limit()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/SubtractedRDD/setSerializer(org.apache.spark.serializer.Serializer)|",
    "called": "|java+method:///scala/Option$/apply(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/rdd/SubtractedRDD/org$apache$spark$rdd$SubtractedRDD$$serializer_$eq(scala.Option)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/SubtractedRDD/serializer_$eq(scala.Option)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$50/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/shuffleReadMetrics()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$50$$anonfun$apply$22/StagePage$$anonfun$50$$anonfun$apply$22(org.apache.spark.ui.jobs.StagePage$$anonfun$50)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Option/get()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$50$$anonfun$apply$9/StagePage$$anonfun$50$$anonfun$apply$9(org.apache.spark.ui.jobs.StagePage$$anonfun$50)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$50$$anonfun$apply$18/StagePage$$anonfun$50$$anonfun$apply$18(org.apache.spark.ui.jobs.StagePage$$anonfun$50)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/outputMetrics()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Option/get()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$50$$anonfun$apply$5/StagePage$$anonfun$50$$anonfun$apply$5(org.apache.spark.ui.jobs.StagePage$$anonfun$50)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$executorLost$2/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$executorLost$2/apply(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$executorLost$2/apply(scala.Tuple2)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/rdd/ReliableRDDCheckpointData$$anonfun$doCheckpoint$2/ReliableRDDCheckpointData$$anonfun$doCheckpoint$2(org.apache.spark.rdd.ReliableRDDCheckpointData,org.apache.spark.rdd.ReliableCheckpointRDD)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$11/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$11/apply()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$11/apply()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/TaskPagedTable/headers()|",
    "called": "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/TASK_DESERIALIZATION_TIME()|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/SHUFFLE_READ_REMOTE_SIZE()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/RESULT_SERIALIZATION_TIME()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/SHUFFLE_READ_BLOCKED_TIME()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/PEAK_EXECUTION_MEMORY()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/SCHEDULER_DELAY()|",
      "|java+method:///scala/collection/SeqLike/contains(java.lang.Object)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskPagedTable$$anonfun$119/TaskPagedTable$$anonfun$119(org.apache.spark.ui.jobs.TaskPagedTable)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskPagedTable$$anonfun$headers$1/TaskPagedTable$$anonfun$headers$1(org.apache.spark.ui.jobs.TaskPagedTable)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskPagedTable/displayPeakExecutionMemory()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/GETTING_RESULT_TIME()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/TASK_DESERIALIZATION_TIME()|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/SHUFFLE_READ_REMOTE_SIZE()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/RESULT_SERIALIZATION_TIME()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/SHUFFLE_READ_BLOCKED_TIME()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/PEAK_EXECUTION_MEMORY()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/SCHEDULER_DELAY()|",
      "|java+method:///scala/collection/SeqLike/contains(java.lang.Object)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskPagedTable$$anonfun$headers$1/TaskPagedTable$$anonfun$headers$1(org.apache.spark.ui.jobs.TaskPagedTable)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskPagedTable/displayPeakExecutionMemory()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDetailsClassNames$/GETTING_RESULT_TIME()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskPagedTable$$anonfun$123/TaskPagedTable$$anonfun$123(org.apache.spark.ui.jobs.TaskPagedTable)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/SizeEstimator$/SizeEstimator$()|",
    "called": "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
    "v1Body": [
      "|java+constructor:///java/util/concurrent/ConcurrentHashMap/ConcurrentHashMap()|",
      "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/initialize()|",
      "|java+method:///scala/Predef$/wrapIntArray(int%5B%5D)|"
    ],
    "v2Body": [
      "|java+constructor:///org/spark-project/guava/collect/MapMaker/MapMaker()|",
      "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/spark-project/guava/collect/MapMaker/weakKeys()|",
      "|java+method:///org/spark-project/guava/collect/MapMaker/makeMap()|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/initialize()|",
      "|java+method:///scala/Predef$/wrapIntArray(int%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/rdd/ReliableCheckpointRDD$$anonfun$2/ReliableCheckpointRDD$$anonfun$2(org.apache.spark.rdd.ReliableCheckpointRDD)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/checkForLogs()|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/Iterator/foreach(scala.Function1)|",
      "|java+method:///scala/collection/Seq/filter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$checkForLogs$3/FsHistoryProvider$$anonfun$checkForLogs$3(org.apache.spark.deploy.history.FsHistoryProvider)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$checkForLogs$2/FsHistoryProvider$$anonfun$checkForLogs$2(org.apache.spark.deploy.history.FsHistoryProvider)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$checkForLogs$1/FsHistoryProvider$$anonfun$checkForLogs$1(org.apache.spark.deploy.history.FsHistoryProvider)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$7/FsHistoryProvider$$anonfun$7(org.apache.spark.deploy.history.FsHistoryProvider,scala.runtime.LongRef)|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+method:///scala/collection/Seq/grouped(int)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$lastModifiedTime()|",
      "|java+constructor:///scala/runtime/LongRef/LongRef(long)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/collection/TraversableLike/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/collection/SeqLike/sortWith(scala.Function2)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/listStatus(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$lastModifiedTime_$eq(long)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$logDir()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$fs()|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$5/FsHistoryProvider$$anonfun$5(org.apache.spark.deploy.history.FsHistoryProvider)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$6/FsHistoryProvider$$anonfun$6(org.apache.spark.deploy.history.FsHistoryProvider)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$8/FsHistoryProvider$$anonfun$8(org.apache.spark.deploy.history.FsHistoryProvider)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$9/FsHistoryProvider$$anonfun$9(org.apache.spark.deploy.history.FsHistoryProvider)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$lastScanTime_$eq(long)|",
      "|java+method:///scala/collection/Iterator/foreach(scala.Function1)|",
      "|java+method:///scala/collection/Seq/filter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$checkForLogs$3/FsHistoryProvider$$anonfun$checkForLogs$3(org.apache.spark.deploy.history.FsHistoryProvider)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$checkForLogs$2/FsHistoryProvider$$anonfun$checkForLogs$2(org.apache.spark.deploy.history.FsHistoryProvider)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$checkForLogs$1/FsHistoryProvider$$anonfun$checkForLogs$1(org.apache.spark.deploy.history.FsHistoryProvider)|",
      "|java+method:///scala/collection/Seq/grouped(int)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///scala/collection/TraversableLike/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/collection/SeqLike/sortWith(scala.Function2)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/listStatus(org.apache.hadoop.fs.Path)|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/getNewLastScanTime()|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$logDir()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$fs()|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$5/FsHistoryProvider$$anonfun$5(org.apache.spark.deploy.history.FsHistoryProvider)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$6/FsHistoryProvider$$anonfun$6(org.apache.spark.deploy.history.FsHistoryProvider)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$7/FsHistoryProvider$$anonfun$7(org.apache.spark.deploy.history.FsHistoryProvider)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$8/FsHistoryProvider$$anonfun$8(org.apache.spark.deploy.history.FsHistoryProvider)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$9/FsHistoryProvider$$anonfun$9(org.apache.spark.deploy.history.FsHistoryProvider)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$6/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/name()|",
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$6/apply()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/id()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/master/ExecutorDesc/fullId()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$6/apply()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/BlockManager/doPut(org.apache.spark.storage.BlockId,org.apache.spark.storage.BlockValues,org.apache.spark.storage.StorageLevel,boolean,scala.Option)|",
    "called": "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$7/BlockManager$$anonfun$doPut$7(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId,long)|",
      "|java+method:///org/apache/spark/storage/BlockStore/putBytes(org.apache.spark.storage.BlockId,java.nio.ByteBuffer,org.apache.spark.storage.StorageLevel)|",
      "|java+method:///scala/concurrent/duration/Duration$/Inf()|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///java/nio/ByteBuffer/rewind()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/util/Left/a()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$3/BlockManager$$anonfun$doPut$3(org.apache.spark.storage.BlockManager)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$6/BlockManager$$anonfun$doPut$6(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/PutResult/droppedBlocks()|",
      "|java+method:///org/apache/spark/storage/BlockManager/reportBlockStatus$default$4()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$3/BlockManager$$anonfun$3(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId,org.apache.spark.storage.StorageLevel,java.nio.ByteBuffer)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$2/BlockManager$$anonfun$doPut$2(org.apache.spark.storage.BlockManager)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManager/org$apache$spark$storage$BlockManager$$blockInfo()|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$5/BlockManager$$anonfun$doPut$5(org.apache.spark.storage.BlockManager,scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///org/apache/spark/storage/ByteBufferValues/buffer()|",
      "|java+method:///org/apache/spark/storage/StorageLevel$/NONE()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockInfo/markReady(long)|",
      "|java+method:///org/apache/spark/storage/BlockManager/org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(org.apache.spark.storage.BlockId,org.apache.spark.storage.BlockInfo)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/isValid()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$1/BlockManager$$anonfun$doPut$1(org.apache.spark.storage.BlockManager)|",
      "|java+method:///org/apache/spark/storage/PutResult/data()|",
      "|java+method:///scala/util/Right/b()|",
      "|java+method:///org/apache/spark/storage/BlockManager$/dispose(java.nio.ByteBuffer)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$9/BlockManager$$anonfun$9(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$9/BlockManager$$anonfun$doPut$9(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId,long)|",
      "|java+method:///org/apache/spark/storage/BlockInfo/markFailure()|",
      "|java+method:///org/apache/spark/storage/BlockManager/futureExecutionContext()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$8/BlockManager$$anonfun$doPut$8(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId,long)|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/storage/BlockManager/logTrace(scala.Function0)|",
      "|java+method:///java/nio/ByteBuffer/duplicate()|",
      "|java+constructor:///org/apache/spark/storage/BlockInfo/BlockInfo(org.apache.spark.storage.StorageLevel,boolean)|",
      "|java+method:///org/apache/spark/storage/BlockManager/memoryStore()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/storage/BlockManager/org$apache$spark$storage$BlockManager$$replicate(org.apache.spark.storage.BlockId,java.nio.ByteBuffer,org.apache.spark.storage.StorageLevel)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/storage/IteratorValues/iterator()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataSerialize$default$3()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataSerialize(org.apache.spark.storage.BlockId,scala.collection.Iterator,org.apache.spark.serializer.Serializer)|",
      "|java+method:///org/apache/spark/storage/ArrayValues/buffer()|",
      "|java+method:///org/apache/spark/storage/BlockManager/externalBlockStore()|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/putIfAbsent(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Tuple2/_1$mcZ$sp()|",
      "|java+method:///org/apache/spark/storage/BlockManager/diskStore()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$10/BlockManager$$anonfun$10(org.apache.spark.storage.BlockManager,org.apache.spark.storage.StorageLevel)|",
      "|java+method:///scala/concurrent/Await$/ready(scala.concurrent.Awaitable,scala.concurrent.duration.Duration)|",
      "|java+method:///org/apache/spark/storage/BlockStore/putIterator(org.apache.spark.storage.BlockId,scala.collection.Iterator,org.apache.spark.storage.StorageLevel,boolean)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$4/BlockManager$$anonfun$doPut$4(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId,long)|",
      "|java+method:///org/apache/spark/storage/BlockStatus/storageLevel()|",
      "|java+method:///org/apache/spark/storage/BlockManager/logWarning(scala.Function0)|",
      "|java+method:///scala/concurrent/Future$/apply(scala.Function0,scala.concurrent.ExecutionContext)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/useDisk()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$10/BlockManager$$anonfun$doPut$10(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId,long)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/useOffHeap()|",
      "|java+method:///org/apache/spark/storage/BlockManager/logDebug(scala.Function0)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///org/apache/spark/storage/StorageLevel/useMemory()|",
      "|java+method:///org/apache/spark/storage/BlockInfo/waitForReady()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/storage/PutResult/size()|",
      "|java+method:///org/apache/spark/storage/BlockManager/reportBlockStatus(org.apache.spark.storage.BlockId,org.apache.spark.storage.BlockInfo,org.apache.spark.storage.BlockStatus,long)|",
      "|java+constructor:///org/apache/spark/storage/BlockException/BlockException(org.apache.spark.storage.BlockId,java.lang.String)|",
      "|java+method:///org/apache/spark/storage/BlockStore/putArray(org.apache.spark.storage.BlockId,java.lang.Object%5B%5D,org.apache.spark.storage.StorageLevel,boolean)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/replication()|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/remove(java.lang.Object)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$7/BlockManager$$anonfun$doPut$7(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId,long)|",
      "|java+method:///org/apache/spark/storage/BlockStore/putBytes(org.apache.spark.storage.BlockId,java.nio.ByteBuffer,org.apache.spark.storage.StorageLevel)|",
      "|java+method:///scala/concurrent/duration/Duration$/Inf()|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/util/Left/a()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$3/BlockManager$$anonfun$doPut$3(org.apache.spark.storage.BlockManager)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$6/BlockManager$$anonfun$doPut$6(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/PutResult/droppedBlocks()|",
      "|java+method:///org/apache/spark/storage/BlockManager/reportBlockStatus$default$4()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$3/BlockManager$$anonfun$3(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId,org.apache.spark.storage.StorageLevel,java.nio.ByteBuffer)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$2/BlockManager$$anonfun$doPut$2(org.apache.spark.storage.BlockManager)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManager/org$apache$spark$storage$BlockManager$$blockInfo()|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$5/BlockManager$$anonfun$doPut$5(org.apache.spark.storage.BlockManager,scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///org/apache/spark/storage/ByteBufferValues/buffer()|",
      "|java+method:///org/apache/spark/storage/StorageLevel$/NONE()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockInfo/markReady(long)|",
      "|java+method:///org/apache/spark/storage/BlockManager/org$apache$spark$storage$BlockManager$$getCurrentBlockStatus(org.apache.spark.storage.BlockId,org.apache.spark.storage.BlockInfo)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/isValid()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$1/BlockManager$$anonfun$doPut$1(org.apache.spark.storage.BlockManager)|",
      "|java+method:///org/apache/spark/storage/PutResult/data()|",
      "|java+method:///scala/util/Right/b()|",
      "|java+method:///org/apache/spark/storage/BlockManager$/dispose(java.nio.ByteBuffer)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$9/BlockManager$$anonfun$doPut$9(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId,long)|",
      "|java+method:///org/apache/spark/storage/BlockInfo/markFailure()|",
      "|java+method:///org/apache/spark/storage/BlockManager/futureExecutionContext()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataSerialize(org.apache.spark.storage.BlockId,scala.collection.Iterator)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$8/BlockManager$$anonfun$doPut$8(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId,long)|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/storage/BlockManager/logTrace(scala.Function0)|",
      "|java+method:///java/nio/ByteBuffer/duplicate()|",
      "|java+constructor:///org/apache/spark/storage/BlockInfo/BlockInfo(org.apache.spark.storage.StorageLevel,boolean)|",
      "|java+method:///org/apache/spark/storage/BlockManager/memoryStore()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/storage/BlockManager/org$apache$spark$storage$BlockManager$$replicate(org.apache.spark.storage.BlockId,java.nio.ByteBuffer,org.apache.spark.storage.StorageLevel)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/storage/IteratorValues/iterator()|",
      "|java+method:///org/apache/spark/storage/ArrayValues/buffer()|",
      "|java+method:///org/apache/spark/storage/BlockManager/externalBlockStore()|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/putIfAbsent(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Tuple2/_1$mcZ$sp()|",
      "|java+method:///org/apache/spark/storage/BlockManager/diskStore()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$11/BlockManager$$anonfun$11(org.apache.spark.storage.BlockManager,org.apache.spark.storage.StorageLevel)|",
      "|java+method:///scala/concurrent/Await$/ready(scala.concurrent.Awaitable,scala.concurrent.duration.Duration)|",
      "|java+method:///org/apache/spark/storage/BlockStore/putIterator(org.apache.spark.storage.BlockId,scala.collection.Iterator,org.apache.spark.storage.StorageLevel,boolean)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$4/BlockManager$$anonfun$doPut$4(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId,long)|",
      "|java+method:///org/apache/spark/storage/BlockStatus/storageLevel()|",
      "|java+method:///org/apache/spark/storage/BlockManager/logWarning(scala.Function0)|",
      "|java+method:///scala/concurrent/Future$/apply(scala.Function0,scala.concurrent.ExecutionContext)|",
      "|java+method:///java/nio/ByteBuffer/rewind()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$10/BlockManager$$anonfun$10(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/useDisk()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doPut$10/BlockManager$$anonfun$doPut$10(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId,long)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/useOffHeap()|",
      "|java+method:///org/apache/spark/storage/BlockManager/logDebug(scala.Function0)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///org/apache/spark/storage/StorageLevel/useMemory()|",
      "|java+method:///org/apache/spark/storage/BlockInfo/waitForReady()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/storage/PutResult/size()|",
      "|java+method:///org/apache/spark/storage/BlockManager/reportBlockStatus(org.apache.spark.storage.BlockId,org.apache.spark.storage.BlockInfo,org.apache.spark.storage.BlockStatus,long)|",
      "|java+constructor:///org/apache/spark/storage/BlockException/BlockException(org.apache.spark.storage.BlockId,java.lang.String)|",
      "|java+method:///org/apache/spark/storage/BlockStore/putArray(org.apache.spark.storage.BlockId,java.lang.Object%5B%5D,org.apache.spark.storage.StorageLevel,boolean)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/replication()|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/remove(java.lang.Object)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master/rebuildSparkUI(org.apache.spark.deploy.master.ApplicationInfo)|",
    "called": "|java+method:///scala/concurrent/Await$/result(scala.concurrent.Awaitable,scala.concurrent.duration.Duration)|",
    "v1Body": [
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/SparkConf/SparkConf()|",
      "|java+method:///scala/runtime/NonLocalReturnControl/value()|",
      "|java+method:///java/io/InputStream/close()|",
      "|java+method:///org/apache/spark/deploy/master/Master/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/eventLogDir()|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener$/getLogPath$default$4()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$rebuildSparkUI$1/Master$$anonfun$rebuildSparkUI$1(org.apache.spark.deploy.master.Master,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/ReplayListenerBus/ReplayListenerBus()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$appIdToUI()|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener$/IN_PROGRESS()|",
      "|java+method:///org/apache/spark/ui/SparkUI/basePath()|",
      "|java+method:///org/apache/spark/util/Utils$/getHadoopFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/deploy/master/Master/hadoopConf()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/eventLogCodec()|",
      "|java+method:///org/apache/spark/deploy/master/Master/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServer$/UI_PATH_PREFIX()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/appUiUrl_$eq(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$16/Master$$anonfun$16(org.apache.spark.deploy.master.Master,org.apache.spark.deploy.master.ApplicationInfo,java.lang.String,java.lang.Object)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)|",
      "|java+method:///java/net/URLEncoder/encode(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/name()|",
      "|java+method:///org/apache/spark/scheduler/ReplayListenerBus/replay(java.io.InputStream,java.lang.String,boolean)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/desc()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/startTime()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/ui/SparkUI$/createHistoryUI(org.apache.spark.SparkConf,org.apache.spark.scheduler.SparkListenerBus,org.apache.spark.SecurityManager,java.lang.String,java.lang.String,long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener$/getLogPath(java.net.URI,java.lang.String,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/id()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener$/openEventLog(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)|",
      "|java+method:///org/apache/spark/util/Utils$/exceptionString(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI/attachSparkUI(org.apache.spark.ui.SparkUI)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$rebuildSparkUI$3/Master$$anonfun$rebuildSparkUI$3(org.apache.spark.deploy.master.Master,scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$rebuildSparkUI$2/Master$$anonfun$rebuildSparkUI$2(org.apache.spark.deploy.master.Master,scala.runtime.ObjectRef)|",
      "|java+method:///scala/runtime/NonLocalReturnControl/key()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/deploy/master/Master/conf()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$webUi()|",
      "|java+method:///java/lang/String/endsWith(java.lang.String)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///scala/concurrent/duration/Duration$/Inf()|",
      "|java+method:///scala/concurrent/Await$/result(scala.concurrent.Awaitable,scala.concurrent.duration.Duration)|",
      "|java+method:///org/apache/spark/deploy/master/Master/asyncRebuildSparkUI(org.apache.spark.deploy.master.ApplicationInfo)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/ZippedPartitionsPartition$$anonfun$writeObject$1/apply$mcV$sp()|",
    "called": "|java+method:///scala/collection/Seq$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///java/io/ObjectOutputStream/defaultWriteObject()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/rdd/ZippedPartitionsPartition$$anonfun$writeObject$1$$anonfun$apply$mcV$sp$1/ZippedPartitionsPartition$$anonfun$writeObject$1$$anonfun$apply$mcV$sp$1(org.apache.spark.rdd.ZippedPartitionsPartition$$anonfun$writeObject$1)|",
      "|java+method:///org/apache/spark/rdd/ZippedPartitionsPartition/partitionValues_$eq(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/ZippedPartitionsPartition/org$apache$spark$rdd$ZippedPartitionsPartition$$rdds()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///java/io/ObjectOutputStream/defaultWriteObject()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/rdd/ZippedPartitionsPartition$$anonfun$writeObject$1$$anonfun$apply$mcV$sp$1/ZippedPartitionsPartition$$anonfun$writeObject$1$$anonfun$apply$mcV$sp$1(org.apache.spark.rdd.ZippedPartitionsPartition$$anonfun$writeObject$1)|",
      "|java+method:///org/apache/spark/rdd/ZippedPartitionsPartition/partitionValues_$eq(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/ListenerBus$class/findListenersByClass(org.apache.spark.util.ListenerBus,scala.reflect.ClassTag)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///scala/collection/mutable/Buffer/filter(scala.Function1)|",
      "|java+method:///scala/reflect/ClassTag/runtimeClass()|",
      "|java+method:///scala/Predef$/implicitly(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ListenerBus$$anonfun$findListenersByClass$2/ListenerBus$$anonfun$findListenersByClass$2(org.apache.spark.util.ListenerBus)|",
      "|java+method:///org/apache/spark/util/ListenerBus/listeners()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/util/ListenerBus$$anonfun$findListenersByClass$1/ListenerBus$$anonfun$findListenersByClass$1(org.apache.spark.util.ListenerBus,java.lang.Class)|",
      "|java+method:///scala/collection/SeqLike/toSeq()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/Predef$/implicitly(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/ListenerBus/listeners()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/util/ListenerBus$$anonfun$findListenersByClass$1/ListenerBus$$anonfun$findListenersByClass$1(org.apache.spark.util.ListenerBus,java.lang.Class)|",
      "|java+method:///scala/collection/SeqLike/toSeq()|",
      "|java+method:///scala/collection/TraversableLike/filter(scala.Function1)|",
      "|java+method:///scala/reflect/ClassTag/runtimeClass()|",
      "|java+constructor:///org/apache/spark/util/ListenerBus$$anonfun$findListenersByClass$2/ListenerBus$$anonfun$findListenersByClass$2(org.apache.spark.util.ListenerBus)|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/JettyUtils$$anonfun$2/JettyUtils$$anonfun$2()|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/rdd/HadoopRDD/HadoopRDD(org.apache.spark.SparkContext,org.apache.spark.broadcast.Broadcast,scala.Option,java.lang.Class,java.lang.Class,java.lang.Class,int)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+constructor:///org/apache/spark/rdd/RDD/RDD(org.apache.spark.SparkContext,scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+constructor:///java/util/Date/Date()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/id()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+constructor:///org/apache/spark/rdd/RDD/RDD(org.apache.spark.SparkContext,scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+constructor:///java/util/Date/Date()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/id()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/sparkContext()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$25/apply()|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$25/apply()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$25/apply()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$org$apache$spark$scheduler$TaskSchedulerImpl$$resourceOfferSingleTaskSet$1$$anonfun$apply$mcVI$sp$1/apply(org.apache.spark.scheduler.TaskDescription)|",
    "called": "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/CPUS_PER_TASK()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/executorsByHost()|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$org$apache$spark$scheduler$TaskSchedulerImpl$$resourceOfferSingleTaskSet$1/org$apache$spark$scheduler$TaskSchedulerImpl$$anonfun$$$outer()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/taskIdToExecutorId()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/taskIdToTaskSetManager()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/apply(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/taskId()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/CPUS_PER_TASK()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/executorsByHost()|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$org$apache$spark$scheduler$TaskSchedulerImpl$$resourceOfferSingleTaskSet$1/org$apache$spark$scheduler$TaskSchedulerImpl$$anonfun$$$outer()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/org$apache$spark$scheduler$TaskSchedulerImpl$$executorIdToTaskCount()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/taskIdToExecutorId()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/taskIdToTaskSetManager()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/apply(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/taskId()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/createCommand(org.apache.mesos.Protos$Offer,int,int)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID/getValue()|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable/newBuilder()|",
      "|java+method:///org/apache/spark/SparkContext/executorEnvs()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$URI$Builder/setValue(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/appId()|",
      "|java+constructor:///java/io/File/File(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/build()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$1/CoarseMesosSchedulerBackend$$anonfun$1(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$2/CoarseMesosSchedulerBackend$$anonfun$2(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$3/CoarseMesosSchedulerBackend$$anonfun$3(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$4/CoarseMesosSchedulerBackend$$anonfun$4(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$5/CoarseMesosSchedulerBackend$$anonfun$5(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+method:///org/apache/mesos/Protos$Environment/newBuilder()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/conf()|",
      "|java+method:///org/apache/mesos/Protos$Environment$Builder/addVariables(org.apache.mesos.Protos$Environment$Variable)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable$Builder/setName(java.lang.String)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/addUris(org.apache.mesos.Protos$CommandInfo$URI$Builder)|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable$Builder/build()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/setEnvironment(org.apache.mesos.Protos$Environment$Builder)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$createCommand$3/CoarseMesosSchedulerBackend$$anonfun$createCommand$3(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,org.apache.mesos.Protos$CommandInfo$Builder)|",
      "|java+method:///scala/collection/mutable/ArrayOps/last()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/setValue(java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$URI/newBuilder()|",
      "|java+method:///java/io/File/getCanonicalPath()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$createCommand$1/CoarseMesosSchedulerBackend$$anonfun$createCommand$1(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,org.apache.mesos.Protos$Environment$Builder)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$createCommand$2/CoarseMesosSchedulerBackend$$anonfun$createCommand$2(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,org.apache.mesos.Protos$Environment$Builder)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/driverURL()|",
      "|java+method:///scala/collection/immutable/StringOps/split(char)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/sparkExecutorId(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getSlaveId()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo/newBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable$Builder/setValue(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/head()|",
      "|java+method:///org/apache/mesos/Protos$Offer/getHostname()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID/getValue()|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable/newBuilder()|",
      "|java+method:///org/apache/spark/SparkContext/executorEnvs()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$URI$Builder/setValue(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/appId()|",
      "|java+constructor:///java/io/File/File(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/build()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$6/CoarseMesosSchedulerBackend$$anonfun$6(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$2/CoarseMesosSchedulerBackend$$anonfun$2(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$3/CoarseMesosSchedulerBackend$$anonfun$3(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$4/CoarseMesosSchedulerBackend$$anonfun$4(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$5/CoarseMesosSchedulerBackend$$anonfun$5(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+method:///org/apache/mesos/Protos$Environment/newBuilder()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/conf()|",
      "|java+method:///org/apache/mesos/Protos$Environment$Builder/addVariables(org.apache.mesos.Protos$Environment$Variable)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable$Builder/setName(java.lang.String)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/addUris(org.apache.mesos.Protos$CommandInfo$URI$Builder)|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable$Builder/build()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/setEnvironment(org.apache.mesos.Protos$Environment$Builder)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$createCommand$3/CoarseMesosSchedulerBackend$$anonfun$createCommand$3(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,org.apache.mesos.Protos$CommandInfo$Builder)|",
      "|java+method:///scala/collection/mutable/ArrayOps/last()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/setValue(java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$URI/newBuilder()|",
      "|java+method:///java/io/File/getCanonicalPath()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$createCommand$1/CoarseMesosSchedulerBackend$$anonfun$createCommand$1(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,org.apache.mesos.Protos$Environment$Builder)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$createCommand$2/CoarseMesosSchedulerBackend$$anonfun$createCommand$2(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,org.apache.mesos.Protos$Environment$Builder)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/driverURL()|",
      "|java+method:///scala/collection/immutable/StringOps/split(char)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/sparkExecutorId(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getSlaveId()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo/newBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable$Builder/setValue(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/head()|",
      "|java+method:///org/apache/mesos/Protos$Offer/getHostname()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$class/pipe(org.apache.spark.api.java.JavaRDDLike,java.util.List,java.util.Map)|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe(scala.collection.Seq,scala.collection.Map,scala.Function1,scala.Function2,boolean)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///org/apache/spark/api/java/JavaRDD$/fromRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe$default$3()|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe$default$4()|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe$default$5()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///org/apache/spark/api/java/JavaRDD$/fromRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe$default$3()|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe$default$4()|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe$default$5()|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///org/apache/spark/rdd/RDD/pipe(scala.collection.Seq,scala.collection.Map,scala.Function1,scala.Function2,boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/r/RBackend$/main(java.lang.String%5B%5D)|",
    "called": "|java+method:///scala/Option/getOrElse(scala.Function0)|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/r/RBackend/close()|",
      "|java+method:///java/io/DataOutputStream/writeInt(int)|",
      "|java+method:///org/apache/spark/api/r/RBackend$$anon$2/start()|",
      "|java+method:///java/io/PrintStream/println(java.lang.String)|",
      "|java+method:///org/apache/spark/api/r/RBackend$/logError(scala.Function0,java.lang.Throwable)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/api/r/RBackend/init()|",
      "|java+method:///java/net/InetAddress/getByName(java.lang.String)|",
      "|java+method:///java/net/ServerSocket/getLocalPort()|",
      "|java+method:///java/io/DataOutputStream/close()|",
      "|java+method:///java/lang/System/exit(int)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+constructor:///java/net/ServerSocket/ServerSocket(int,int,java.net.InetAddress)|",
      "|java+method:///java/io/File/renameTo(java.io.File)|",
      "|java+method:///org/apache/spark/api/r/RBackend/run()|",
      "|java+constructor:///org/apache/spark/api/r/RBackend/RBackend()|",
      "|java+constructor:///org/apache/spark/api/r/RBackend$$anonfun$main$1/RBackend$$anonfun$main$1()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/io/DataOutputStream/DataOutputStream(java.io.OutputStream)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/api/r/RBackend$$anon$2/RBackend$$anon$2(org.apache.spark.api.r.RBackend,java.net.ServerSocket)|",
      "|java+constructor:///java/io/FileOutputStream/FileOutputStream(java.io.File)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/api/r/RBackend/close()|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeString(java.io.DataOutputStream,java.lang.String)|",
      "|java+method:///java/io/DataOutputStream/writeInt(int)|",
      "|java+method:///org/apache/spark/api/r/RBackend$$anon$2/start()|",
      "|java+method:///java/io/PrintStream/println(java.lang.String)|",
      "|java+method:///org/apache/spark/api/r/RBackend$/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/api/r/RBackend/init()|",
      "|java+method:///java/net/InetAddress/getByName(java.lang.String)|",
      "|java+method:///java/net/ServerSocket/getLocalPort()|",
      "|java+method:///java/io/DataOutputStream/close()|",
      "|java+method:///java/lang/System/exit(int)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+constructor:///java/net/ServerSocket/ServerSocket(int,int,java.net.InetAddress)|",
      "|java+method:///java/io/File/renameTo(java.io.File)|",
      "|java+method:///org/apache/spark/api/r/RBackend/run()|",
      "|java+method:///org/apache/spark/api/r/RUtils$/rPackages()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///org/apache/spark/api/r/RBackend/RBackend()|",
      "|java+constructor:///org/apache/spark/api/r/RBackend$$anonfun$main$2/RBackend$$anonfun$main$2()|",
      "|java+constructor:///org/apache/spark/api/r/RBackend$$anonfun$main$1/RBackend$$anonfun$main$1()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/io/DataOutputStream/DataOutputStream(java.io.OutputStream)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/api/r/RBackend$$anon$2/RBackend$$anon$2(org.apache.spark.api.r.RBackend,java.net.ServerSocket)|",
      "|java+constructor:///java/io/FileOutputStream/FileOutputStream(java.io.File)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$13/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$13/apply()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$13/apply()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkConf$$anonfun$validateSettings$7/apply(java.lang.String)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$7$$anonfun$apply$10/SparkConf$$anonfun$validateSettings$7$$anonfun$apply$10(org.apache.spark.SparkConf$$anonfun$validateSettings$7,java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/SparkConf/logWarning(scala.Function0)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkConf/set(java.lang.String,java.lang.String)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/SparkConf/logWarning(scala.Function0)|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$7$$anonfun$apply$8/SparkConf$$anonfun$validateSettings$7$$anonfun$apply$8(org.apache.spark.SparkConf$$anonfun$validateSettings$7,java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$7$$anonfun$apply$7/SparkConf$$anonfun$validateSettings$7$$anonfun$apply$7(org.apache.spark.SparkConf$$anonfun$validateSettings$7,java.lang.String)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/LocalRDDCheckpointData/doCheckpoint()|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/Int()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function2,scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD/sparkContext()|",
      "|java+method:///org/apache/spark/rdd/RDD/getStorageLevel()|",
      "|java+constructor:///org/apache/spark/rdd/LocalRDDCheckpointData$$anonfun$3/LocalRDDCheckpointData$$anonfun$3(org.apache.spark.rdd.LocalRDDCheckpointData)|",
      "|java+constructor:///org/apache/spark/rdd/LocalRDDCheckpointData$$anonfun$2/LocalRDDCheckpointData$$anonfun$2(org.apache.spark.rdd.LocalRDDCheckpointData)|",
      "|java+method:///scala/collection/mutable/ArrayOps/nonEmpty()|",
      "|java+method:///scala/Predef$/intArrayOps(int%5B%5D)|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/rdd/LocalCheckpointRDD/LocalCheckpointRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag)|",
      "|java+method:///scala/Predef$/wrapIntArray(int%5B%5D)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/useDisk()|",
      "|java+constructor:///org/apache/spark/rdd/LocalRDDCheckpointData$$anonfun$doCheckpoint$1/LocalRDDCheckpointData$$anonfun$doCheckpoint$1(org.apache.spark.rdd.LocalRDDCheckpointData,org.apache.spark.storage.StorageLevel)|",
      "|java+constructor:///org/apache/spark/rdd/LocalRDDCheckpointData$$anonfun$1/LocalRDDCheckpointData$$anonfun$1(org.apache.spark.rdd.LocalRDDCheckpointData)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/ClassTag$/Long()|",
      "|java+method:///scala/collection/mutable/ArrayOps/filter(scala.Function1)|",
      "|java+method:///scala/Predef$/assume(boolean,scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/Int()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function2,scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD/sparkContext()|",
      "|java+method:///org/apache/spark/rdd/LocalRDDCheckpointData/org$apache$spark$rdd$LocalRDDCheckpointData$$rdd()|",
      "|java+method:///org/apache/spark/rdd/RDD/getStorageLevel()|",
      "|java+constructor:///org/apache/spark/rdd/LocalRDDCheckpointData$$anonfun$3/LocalRDDCheckpointData$$anonfun$3(org.apache.spark.rdd.LocalRDDCheckpointData)|",
      "|java+constructor:///org/apache/spark/rdd/LocalRDDCheckpointData$$anonfun$2/LocalRDDCheckpointData$$anonfun$2(org.apache.spark.rdd.LocalRDDCheckpointData)|",
      "|java+method:///scala/collection/mutable/ArrayOps/nonEmpty()|",
      "|java+method:///scala/Predef$/intArrayOps(int%5B%5D)|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/rdd/LocalCheckpointRDD/LocalCheckpointRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag)|",
      "|java+method:///scala/Predef$/wrapIntArray(int%5B%5D)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/useDisk()|",
      "|java+constructor:///org/apache/spark/rdd/LocalRDDCheckpointData$$anonfun$doCheckpoint$1/LocalRDDCheckpointData$$anonfun$doCheckpoint$1(org.apache.spark.rdd.LocalRDDCheckpointData,org.apache.spark.storage.StorageLevel)|",
      "|java+constructor:///org/apache/spark/rdd/LocalRDDCheckpointData$$anonfun$1/LocalRDDCheckpointData$$anonfun$1(org.apache.spark.rdd.LocalRDDCheckpointData)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/ClassTag$/Long()|",
      "|java+method:///scala/collection/mutable/ArrayOps/filter(scala.Function1)|",
      "|java+method:///scala/Predef$/assume(boolean,scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$31/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$31/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$31/apply(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$34/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$34/apply(org.apache.spark.scheduler.AccumulableInfo)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$34/apply(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/insertAll(scala.collection.Iterator)|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/maybeSpill(java.lang.Object,long)|",
      "|java+method:///org/apache/spark/util/collection/SizeTrackingAppendOnlyMap/changeValue(java.lang.Object,scala.Function2)|",
      "|java+method:///scala/collection/Iterator/next()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///scala/Product2/_1()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$currentMap_$eq(org.apache.spark.util.collection.SizeTrackingAppendOnlyMap)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/addElementsRead()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$currentMap()|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$$anonfun$2/ExternalAppendOnlyMap$$anonfun$2(org.apache.spark.util.collection.ExternalAppendOnlyMap,scala.runtime.ObjectRef)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/_peakMemoryUsedBytes()|",
      "|java+method:///org/apache/spark/util/collection/SizeTrackingAppendOnlyMap/estimateSize()|",
      "|java+constructor:///org/apache/spark/util/collection/SizeTrackingAppendOnlyMap/SizeTrackingAppendOnlyMap()|",
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/_peakMemoryUsedBytes_$eq(long)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/maybeSpill(java.lang.Object,long)|",
      "|java+constructor:///java/lang/IllegalStateException/IllegalStateException(java.lang.String)|",
      "|java+method:///org/apache/spark/util/collection/SizeTrackingAppendOnlyMap/changeValue(java.lang.Object,scala.Function2)|",
      "|java+method:///scala/collection/Iterator/next()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///scala/Product2/_1()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$currentMap_$eq(org.apache.spark.util.collection.SizeTrackingAppendOnlyMap)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/addElementsRead()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$currentMap()|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$$anonfun$3/ExternalAppendOnlyMap$$anonfun$3(org.apache.spark.util.collection.ExternalAppendOnlyMap,scala.runtime.ObjectRef)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/_peakMemoryUsedBytes()|",
      "|java+method:///org/apache/spark/util/collection/SizeTrackingAppendOnlyMap/estimateSize()|",
      "|java+constructor:///org/apache/spark/util/collection/SizeTrackingAppendOnlyMap/SizeTrackingAppendOnlyMap()|",
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/_peakMemoryUsedBytes_$eq(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/FailedStageTable/stageRow(org.apache.spark.scheduler.StageInfo)|",
    "called": "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/Predef$/$scope()|",
      "|java+constructor:///org/apache/spark/ui/jobs/FailedStageTable$$anonfun$8/FailedStageTable$$anonfun$8(org.apache.spark.ui.jobs.FailedStageTable)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/commons/lang3/StringEscapeUtils/escapeHtml4(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/failureReason()|",
      "|java+method:///scala/xml/Elem/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/xml/NodeSeq$/canBuildFrom()|",
      "|java+method:///java/lang/String/substring(int,int)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+method:///java/lang/String/indexOf(int)|",
      "|java+method:///org/apache/spark/ui/jobs/StageTableBase/stageRow(org.apache.spark.scheduler.StageInfo)|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/commons/lang3/StringEscapeUtils/escapeHtml4(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/failureReason()|",
      "|java+method:///scala/xml/Elem/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/xml/NodeSeq$/canBuildFrom()|",
      "|java+method:///java/lang/String/substring(int,int)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+method:///java/lang/String/indexOf(int)|",
      "|java+method:///scala/Predef$/$scope()|",
      "|java+constructor:///org/apache/spark/ui/jobs/FailedStageTable$$anonfun$9/FailedStageTable$$anonfun$9(org.apache.spark.ui.jobs.FailedStageTable)|",
      "|java+method:///org/apache/spark/ui/jobs/StageTableBase/stageRow(org.apache.spark.scheduler.StageInfo)|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$2/apply$mcV$sp()|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1/org$apache$spark$deploy$worker$Worker$$anonfun$$$outer()|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/workDir()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///java/io/File/listFiles()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$2$$anonfun$apply$mcV$sp$6/Worker$$anonfun$receive$1$$anonfun$2$$anonfun$apply$mcV$sp$6(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1$$anonfun$2)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$2$$anonfun$apply$mcV$sp$7/Worker$$anonfun$receive$1$$anonfun$2$$anonfun$apply$mcV$sp$7(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1$$anonfun$2)|",
      "|java+constructor:///java/io/IOException/IOException(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/filter(scala.Function1)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1/org$apache$spark$deploy$worker$Worker$$anonfun$$$outer()|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/workDir()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///java/io/File/listFiles()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$2$$anonfun$apply$mcV$sp$7/Worker$$anonfun$receive$1$$anonfun$2$$anonfun$apply$mcV$sp$7(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1$$anonfun$2)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$2$$anonfun$apply$mcV$sp$8/Worker$$anonfun$receive$1$$anonfun$2$$anonfun$apply$mcV$sp$8(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1$$anonfun$2)|",
      "|java+constructor:///java/io/IOException/IOException(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/filter(scala.Function1)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/registered(org.apache.mesos.SchedulerDriver,org.apache.mesos.Protos$FrameworkID,org.apache.mesos.Protos$MasterInfo)|",
    "called": "|java+method:///scala/collection/mutable/Iterable$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/stateLock()|",
      "|java+method:///scala/collection/mutable/HashMap/isEmpty()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$1/MesosClusterScheduler$$anonfun$1(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+method:///scala/collection/mutable/Iterable$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$registered$1/MesosClusterScheduler$$anonfun$registered$1(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,org.apache.mesos.Protos$FrameworkID)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$frameworkId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/markRegistered()|",
      "|java+method:///org/apache/mesos/Protos$FrameworkID/getValue()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/schedulerState()|",
      "|java+method:///scala/collection/JavaConversions$/asJavaCollection(scala.collection.Iterable)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$frameworkId_$eq(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$pendingRecover()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterPersistenceEngine/persist(java.lang.String,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/collect(scala.PartialFunction,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/masterInfo_$eq(scala.Option)|",
      "|java+method:///org/apache/mesos/SchedulerDriver/reconcileTasks(java.util.Collection)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/schedulerState()|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/stateLock()|",
      "|java+method:///scala/collection/mutable/HashMap/isEmpty()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$1/MesosClusterScheduler$$anonfun$1(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+method:///scala/collection/mutable/Iterable$/canBuildFrom()|",
      "|java+method:///scala/collection/mutable/Iterable/toSeq()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$registered$1/MesosClusterScheduler$$anonfun$registered$1(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,org.apache.mesos.Protos$FrameworkID)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$frameworkId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/markRegistered()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///org/apache/mesos/Protos$FrameworkID/getValue()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$frameworkId_$eq(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$pendingRecover()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterPersistenceEngine/persist(java.lang.String,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/collect(scala.PartialFunction,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/masterInfo_$eq(scala.Option)|",
      "|java+method:///org/apache/mesos/SchedulerDriver/reconcileTasks(java.util.Collection)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4$$anonfun$apply$5/apply(org.apache.spark.deploy.history.FsApplicationAttemptInfo)|",
    "called": "|java+method:///scala/Option/getOrElse(scala.Function0)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/SparkUI/getSecurityManager()|",
      "|java+method:///org/apache/spark/deploy/history/FsApplicationAttemptInfo/sparkUser()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1/org$apache$spark$deploy$history$FsHistoryProvider$$anonfun$$$outer()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/SecurityManager/setAdminAcls(java.lang.String)|",
      "|java+method:///org/apache/spark/SecurityManager/setViewAcls(java.lang.String,java.lang.String)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/ApplicationEventListener/adminAcls()|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4$$anonfun$apply$5$$anonfun$apply$6/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4$$anonfun$apply$5$$anonfun$apply$6(org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4$$anonfun$apply$5)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4$$anonfun$apply$5$$anonfun$apply$7/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4$$anonfun$apply$5$$anonfun$apply$7(org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4$$anonfun$apply$5)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/deploy/history/FsApplicationAttemptInfo/name()|",
      "|java+method:///org/apache/spark/SecurityManager/setAcls(boolean)|",
      "|java+method:///org/apache/spark/ui/SparkUI/setAppName(java.lang.String)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/ApplicationEventListener/viewAcls()|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4/org$apache$spark$deploy$history$FsHistoryProvider$$anonfun$$anonfun$$$outer()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/SparkUI/getSecurityManager()|",
      "|java+method:///org/apache/spark/deploy/history/FsApplicationAttemptInfo/sparkUser()|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1/org$apache$spark$deploy$history$FsHistoryProvider$$anonfun$$$outer()|",
      "|java+method:///org/apache/spark/SecurityManager/setAdminAcls(java.lang.String)|",
      "|java+method:///org/apache/spark/SecurityManager/setViewAcls(java.lang.String,java.lang.String)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/ApplicationEventListener/adminAcls()|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4$$anonfun$apply$5$$anonfun$apply$6/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4$$anonfun$apply$5$$anonfun$apply$6(org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4$$anonfun$apply$5)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4$$anonfun$apply$5$$anonfun$apply$7/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4$$anonfun$apply$5$$anonfun$apply$7(org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4$$anonfun$apply$5)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/SecurityManager/setAcls(boolean)|",
      "|java+method:///org/apache/spark/scheduler/ApplicationEventListener/viewAcls()|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4/org$apache$spark$deploy$history$FsHistoryProvider$$anonfun$$anonfun$$$outer()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$cleanLogs$2/apply(org.apache.spark.deploy.history.FsApplicationAttemptInfo)|",
    "called": "|java+method:///scala/collection/mutable/ListBuffer/$plus$eq(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/collection/mutable/ListBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$logDir()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/delete(org.apache.hadoop.fs.Path,boolean)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$fs()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/history/FsApplicationAttemptInfo/logPath()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$cleanLogs$2$$anonfun$apply$15/FsHistoryProvider$$anonfun$cleanLogs$2$$anonfun$apply$15(org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$cleanLogs$2,org.apache.spark.deploy.history.FsApplicationAttemptInfo)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$cleanLogs$2$$anonfun$apply$16/FsHistoryProvider$$anonfun$cleanLogs$2$$anonfun$apply$16(org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$cleanLogs$2,org.apache.spark.deploy.history.FsApplicationAttemptInfo)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$logDir()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/delete(org.apache.hadoop.fs.Path,boolean)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$fs()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/history/FsApplicationAttemptInfo/logPath()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$cleanLogs$2$$anonfun$apply$16/FsHistoryProvider$$anonfun$cleanLogs$2$$anonfun$apply$16(org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$cleanLogs$2,org.apache.spark.deploy.history.FsApplicationAttemptInfo)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$cleanLogs$2$$anonfun$apply$17/FsHistoryProvider$$anonfun$cleanLogs$2$$anonfun$apply$17(org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$cleanLogs$2,org.apache.spark.deploy.history.FsApplicationAttemptInfo)|",
      "|java+method:///scala/collection/mutable/ListBuffer/$plus$eq(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$cleanLogs$2$$anonfun$apply$15/FsHistoryProvider$$anonfun$cleanLogs$2$$anonfun$apply$15(org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$cleanLogs$2,org.apache.hadoop.fs.Path)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$19/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$19/apply(org.apache.spark.scheduler.TaskInfo)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$19/apply(org.apache.spark.scheduler.TaskInfo)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$77/apply(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaSparkContext/union(org.apache.spark.api.java.JavaPairRDD,java.util.List)|",
    "called": "|java+method:///scala/collection/Seq$/canBuildFrom()|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/api/java/JavaPairRDD/JavaPairRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/classTag()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/api/java/JavaSparkContext$$anonfun$2/JavaSparkContext$$anonfun$2(org.apache.spark.api.java.JavaSparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/union(scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/kClassTag()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/vClassTag()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext/sc()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/api/java/JavaPairRDD/JavaPairRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/classTag()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/api/java/JavaSparkContext$$anonfun$2/JavaSparkContext$$anonfun$2(org.apache.spark.api.java.JavaSparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/union(scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/kClassTag()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/vClassTag()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext/sc()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaSparkContext/parallelize(java.util.List,int)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/java/JavaSparkContext$/fakeClassTag()|",
      "|java+method:///org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext/sc()|",
      "|java+method:///org/apache/spark/api/java/JavaRDD$/fromRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/api/java/JavaSparkContext$/fakeClassTag()|",
      "|java+method:///org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext/sc()|",
      "|java+method:///org/apache/spark/api/java/JavaRDD$/fromRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/ZooKeeperPersistenceEngine/read(java.lang.String,scala.reflect.ClassTag)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/Buffer/filter(scala.Function1)|",
      "|java+method:///org/apache/curator/framework/CuratorFramework/getChildren()|",
      "|java+constructor:///org/apache/spark/deploy/master/ZooKeeperPersistenceEngine$$anonfun$1/ZooKeeperPersistenceEngine$$anonfun$1(org.apache.spark.deploy.master.ZooKeeperPersistenceEngine,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/ZooKeeperPersistenceEngine/WORKING_DIR()|",
      "|java+constructor:///org/apache/spark/deploy/master/ZooKeeperPersistenceEngine$$anonfun$read$2/ZooKeeperPersistenceEngine$$anonfun$read$2(org.apache.spark.deploy.master.ZooKeeperPersistenceEngine)|",
      "|java+method:///org/apache/curator/framework/api/GetChildrenBuilder/forPath(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/ZooKeeperPersistenceEngine/zk()|",
      "|java+constructor:///org/apache/spark/deploy/master/ZooKeeperPersistenceEngine$$anonfun$read$1/ZooKeeperPersistenceEngine$$anonfun$read$1(org.apache.spark.deploy.master.ZooKeeperPersistenceEngine,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/Buffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/collection/generic/GenericTraversableTemplate/flatten(scala.Function1)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///org/apache/curator/framework/CuratorFramework/getChildren()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+constructor:///org/apache/spark/deploy/master/ZooKeeperPersistenceEngine$$anonfun$read$3/ZooKeeperPersistenceEngine$$anonfun$read$3(org.apache.spark.deploy.master.ZooKeeperPersistenceEngine)|",
      "|java+method:///org/apache/curator/framework/api/GetChildrenBuilder/forPath(java.lang.String)|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+constructor:///org/apache/spark/deploy/master/ZooKeeperPersistenceEngine$$anonfun$read$1/ZooKeeperPersistenceEngine$$anonfun$read$1(org.apache.spark.deploy.master.ZooKeeperPersistenceEngine,java.lang.String)|",
      "|java+method:///scala/collection/generic/GenericTraversableTemplate/flatten(scala.Function1)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/deploy/master/ZooKeeperPersistenceEngine/zk()|",
      "|java+constructor:///org/apache/spark/deploy/master/ZooKeeperPersistenceEngine$$anonfun$read$2/ZooKeeperPersistenceEngine$$anonfun$read$2(org.apache.spark.deploy.master.ZooKeeperPersistenceEngine,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/deploy/master/ZooKeeperPersistenceEngine/WORKING_DIR()|",
      "|java+method:///scala/collection/TraversableLike/filter(scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext$$anonfun$makeRDD$2/apply()|",
    "called": "|java+method:///scala/collection/Seq$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/rdd/ParallelCollectionRDD/ParallelCollectionRDD(org.apache.spark.SparkContext,scala.collection.Seq,int,scala.collection.Map,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$assertNotStopped()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///org/apache/spark/SparkContext$$anonfun$makeRDD$2/apply()|",
      "|java+method:///scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$makeRDD$2$$anonfun$apply$8/SparkContext$$anonfun$makeRDD$2$$anonfun$apply$8(org.apache.spark.SparkContext$$anonfun$makeRDD$2)|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$makeRDD$2$$anonfun$30/SparkContext$$anonfun$makeRDD$2$$anonfun$30(org.apache.spark.SparkContext$$anonfun$makeRDD$2)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/rdd/ParallelCollectionRDD/ParallelCollectionRDD(org.apache.spark.SparkContext,scala.collection.Seq,int,scala.collection.Map,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$assertNotStopped()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$makeRDD$2$$anonfun$31/SparkContext$$anonfun$makeRDD$2$$anonfun$31(org.apache.spark.SparkContext$$anonfun$makeRDD$2)|",
      "|java+method:///org/apache/spark/SparkContext$$anonfun$makeRDD$2/apply()|",
      "|java+method:///scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$makeRDD$2$$anonfun$apply$8/SparkContext$$anonfun$makeRDD$2$$anonfun$apply$8(org.apache.spark.SparkContext$$anonfun$makeRDD$2)|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/collection/Seq/size()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$56/apply(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/PythonRunner$/main(java.lang.String%5B%5D)|",
    "called": "|java+method:///scala/collection/Seq$/canBuildFrom()|",
    "v1Body": [
      "|java+constructor:///java/lang/ProcessBuilder/ProcessBuilder(java.util.List)|",
      "|java+method:///scala/sys/package$/env()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///java/lang/String/valueOf(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/python/PythonUtils$/mergePythonPaths(scala.collection.Seq)|",
      "|java+method:///py4j/GatewayServer/shutdown()|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPath(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/api/python/PythonUtils$/sparkPythonPath()|",
      "|java+constructor:///org/apache/spark/deploy/PythonRunner$$anonfun$main$1/PythonRunner$$anonfun$main$1()|",
      "|java+constructor:///org/apache/spark/util/RedirectThread/RedirectThread(java.io.InputStream,java.io.OutputStream,java.lang.String,boolean)|",
      "|java+method:///java/lang/Thread/join()|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPaths(java.lang.String,boolean)|",
      "|java+method:///java/lang/ProcessBuilder/environment()|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/lang/ProcessBuilder/redirectErrorStream(boolean)|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPaths$default$2()|",
      "|java+method:///java/lang/Thread/start()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/slice(int,int)|",
      "|java+constructor:///java/lang/Thread/Thread(java.lang.Runnable)|",
      "|java+method:///scala/collection/immutable/Map/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+method:///java/lang/Thread/setDaemon(boolean)|",
      "|java+constructor:///org/apache/spark/SparkUserAppException/SparkUserAppException(int)|",
      "|java+method:///java/lang/Thread/setName(java.lang.String)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/util/RedirectThread$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/util/RedirectThread/start()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPath$default$2()|",
      "|java+constructor:///py4j/GatewayServer/GatewayServer(java.lang.Object,int)|",
      "|java+constructor:///org/apache/spark/deploy/PythonRunner$$anon$1/PythonRunner$$anon$1(py4j.GatewayServer)|",
      "|java+method:///py4j/GatewayServer/getListeningPort()|",
      "|java+constructor:///org/apache/spark/deploy/PythonRunner$$anonfun$1/PythonRunner$$anonfun$1()|",
      "|java+method:///java/lang/Process/waitFor()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$plus$eq(scala.collection.TraversableOnce)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+constructor:///java/lang/ProcessBuilder/ProcessBuilder(java.util.List)|",
      "|java+method:///scala/sys/package$/env()|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///java/lang/String/valueOf(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/python/PythonUtils$/mergePythonPaths(scala.collection.Seq)|",
      "|java+method:///py4j/GatewayServer/shutdown()|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPath(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/api/python/PythonUtils$/sparkPythonPath()|",
      "|java+constructor:///org/apache/spark/deploy/PythonRunner$$anonfun$main$1/PythonRunner$$anonfun$main$1()|",
      "|java+constructor:///org/apache/spark/util/RedirectThread/RedirectThread(java.io.InputStream,java.io.OutputStream,java.lang.String,boolean)|",
      "|java+method:///java/lang/Thread/join()|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPaths(java.lang.String,boolean)|",
      "|java+method:///java/lang/ProcessBuilder/environment()|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/lang/ProcessBuilder/redirectErrorStream(boolean)|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPaths$default$2()|",
      "|java+method:///java/lang/Thread/start()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/slice(int,int)|",
      "|java+constructor:///java/lang/Thread/Thread(java.lang.Runnable)|",
      "|java+method:///scala/collection/immutable/Map/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+method:///java/lang/Thread/setDaemon(boolean)|",
      "|java+constructor:///org/apache/spark/SparkUserAppException/SparkUserAppException(int)|",
      "|java+method:///java/lang/Thread/setName(java.lang.String)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/util/RedirectThread$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/util/RedirectThread/start()|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPath$default$2()|",
      "|java+constructor:///py4j/GatewayServer/GatewayServer(java.lang.Object,int)|",
      "|java+constructor:///org/apache/spark/deploy/PythonRunner$$anon$1/PythonRunner$$anon$1(py4j.GatewayServer)|",
      "|java+method:///py4j/GatewayServer/getListeningPort()|",
      "|java+constructor:///org/apache/spark/deploy/PythonRunner$$anonfun$1/PythonRunner$$anonfun$1()|",
      "|java+method:///java/lang/Process/waitFor()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$plus$eq(scala.collection.TraversableOnce)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkConf$$anonfun$validateSettings$4/apply(java.lang.String)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkConf/getDouble(java.lang.String,double)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkConf/getDouble(java.lang.String,double)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$class/toLocalIterator(org.apache.spark.api.java.JavaRDDLike)|",
    "called": "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///org/apache/spark/rdd/RDD/toLocalIterator()|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterator(scala.collection.Iterator)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///org/apache/spark/rdd/RDD/toLocalIterator()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/TaskDataSource/org$apache$spark$ui$jobs$TaskDataSource$$taskRow(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/TaskInfo/executorId()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/index()|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$108/TaskDataSource$$anonfun$108(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/formatDuration(long)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$117/TaskDataSource$$anonfun$117(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$106/TaskDataSource$$anonfun$106(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/host()|",
      "|java+method:///scala/collection/mutable/ListBuffer/find(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/taskLocality()|",
      "|java+method:///scala/collection/mutable/ListBuffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$116/TaskDataSource$$anonfun$116(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$105/TaskDataSource$$anonfun$105(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/accumulables()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ListBuffer/mkString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$115/TaskDataSource$$anonfun$115(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$104/TaskDataSource$$anonfun$104(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/errorMessage()|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$114/TaskDataSource$$anonfun$114(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$103/TaskDataSource$$anonfun$103(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$102/TaskDataSource$$anonfun$102(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///scala/Option/flatMap(scala.Function1)|",
      "|java+method:///scala/Tuple3/_1()|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$113/TaskDataSource$$anonfun$113(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage$/getGettingResultTime(org.apache.spark.scheduler.TaskInfo,long)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple3/_2()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/Tuple3/_3()|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$112/TaskDataSource$$anonfun$112(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$101/TaskDataSource$$anonfun$101(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskTableRowShuffleWriteData/TaskTableRowShuffleWriteData(long,java.lang.String,long,java.lang.String)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskTableRowBytesSpilledData/TaskTableRowBytesSpilledData(long,java.lang.String,long,java.lang.String)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/status()|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskTableRowInputData/TaskTableRowInputData(long,java.lang.String)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$111/TaskDataSource$$anonfun$111(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$100/TaskDataSource$$anonfun$100(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///scala/collection/mutable/ListBuffer/partition(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/speculative()|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$110/TaskDataSource$$anonfun$110(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskTableRowData/TaskTableRowData(int,long,int,boolean,java.lang.String,java.lang.String,java.lang.String,long,long,java.lang.String,long,long,long,long,long,long,scala.Option,scala.Option,scala.Option,scala.Option,scala.Option,scala.Option,java.lang.String)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskTableRowShuffleReadData/TaskTableRowShuffleReadData(long,java.lang.String,long,java.lang.String,long,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/timeRunning(long)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/attemptNumber()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskTableRowOutputData/TaskTableRowOutputData(long,java.lang.String)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/taskId()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/launchTime()|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$71/TaskDataSource$$anonfun$71(org.apache.spark.ui.jobs.TaskDataSource,org.apache.spark.scheduler.TaskInfo)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$17/TaskDataSource$$anonfun$17(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$org$apache$spark$ui$jobs$TaskDataSource$$taskRow$1/TaskDataSource$$anonfun$org$apache$spark$ui$jobs$TaskDataSource$$taskRow$1(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$18/TaskDataSource$$anonfun$18(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$19/TaskDataSource$$anonfun$19(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$109/TaskDataSource$$anonfun$109(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$90/TaskDataSource$$anonfun$90(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$15/TaskDataSource$$anonfun$15(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$16/TaskDataSource$$anonfun$16(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$77/TaskDataSource$$anonfun$77(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$11/TaskDataSource$$anonfun$11(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$88/TaskDataSource$$anonfun$88(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$22/TaskDataSource$$anonfun$22(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$99/TaskDataSource$$anonfun$99(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$82/TaskDataSource$$anonfun$82(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$93/TaskDataSource$$anonfun$93(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$89/TaskDataSource$$anonfun$89(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$23/TaskDataSource$$anonfun$23(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$78/TaskDataSource$$anonfun$78(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$12/TaskDataSource$$anonfun$12(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$72/TaskDataSource$$anonfun$72(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$83/TaskDataSource$$anonfun$83(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$94/TaskDataSource$$anonfun$94(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$68/TaskDataSource$$anonfun$68(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$79/TaskDataSource$$anonfun$79(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$13/TaskDataSource$$anonfun$13(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$24/TaskDataSource$$anonfun$24(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$73/TaskDataSource$$anonfun$73(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$84/TaskDataSource$$anonfun$84(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$95/TaskDataSource$$anonfun$95(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$25/TaskDataSource$$anonfun$25(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$69/TaskDataSource$$anonfun$69(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$14/TaskDataSource$$anonfun$14(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$96/TaskDataSource$$anonfun$96(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$74/TaskDataSource$$anonfun$74(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$85/TaskDataSource$$anonfun$85(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskInfo()|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$75/TaskDataSource$$anonfun$75(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$86/TaskDataSource$$anonfun$86(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$20/TaskDataSource$$anonfun$20(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$97/TaskDataSource$$anonfun$97(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$80/TaskDataSource$$anonfun$80(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$91/TaskDataSource$$anonfun$91(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$98/TaskDataSource$$anonfun$98(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$76/TaskDataSource$$anonfun$76(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$10/TaskDataSource$$anonfun$10(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$87/TaskDataSource$$anonfun$87(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$21/TaskDataSource$$anonfun$21(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$70/TaskDataSource$$anonfun$70(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$81/TaskDataSource$$anonfun$81(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$92/TaskDataSource$$anonfun$92(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///scala/collection/mutable/ListBuffer$/canBuildFrom()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$118/TaskDataSource$$anonfun$118(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$107/TaskDataSource$$anonfun$107(org.apache.spark.ui.jobs.TaskDataSource)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/TaskInfo/executorId()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/index()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/formatDuration(long)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$117/TaskDataSource$$anonfun$117(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$106/TaskDataSource$$anonfun$106(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/host()|",
      "|java+method:///scala/collection/mutable/ListBuffer/find(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/taskLocality()|",
      "|java+method:///scala/collection/mutable/ListBuffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$116/TaskDataSource$$anonfun$116(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$105/TaskDataSource$$anonfun$105(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/accumulables()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ListBuffer/mkString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$115/TaskDataSource$$anonfun$115(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$104/TaskDataSource$$anonfun$104(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/errorMessage()|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$114/TaskDataSource$$anonfun$114(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$103/TaskDataSource$$anonfun$103(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$102/TaskDataSource$$anonfun$102(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///scala/Option/flatMap(scala.Function1)|",
      "|java+method:///scala/Tuple3/_1()|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$113/TaskDataSource$$anonfun$113(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage$/getGettingResultTime(org.apache.spark.scheduler.TaskInfo,long)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple3/_2()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/Tuple3/_3()|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$112/TaskDataSource$$anonfun$112(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$101/TaskDataSource$$anonfun$101(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskTableRowShuffleWriteData/TaskTableRowShuffleWriteData(long,java.lang.String,long,java.lang.String)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskTableRowBytesSpilledData/TaskTableRowBytesSpilledData(long,java.lang.String,long,java.lang.String)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/status()|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskTableRowInputData/TaskTableRowInputData(long,java.lang.String)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$122/TaskDataSource$$anonfun$122(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$111/TaskDataSource$$anonfun$111(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$100/TaskDataSource$$anonfun$100(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///scala/collection/mutable/ListBuffer/partition(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/speculative()|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$121/TaskDataSource$$anonfun$121(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$110/TaskDataSource$$anonfun$110(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskTableRowData/TaskTableRowData(int,long,int,boolean,java.lang.String,java.lang.String,java.lang.String,long,long,java.lang.String,long,long,long,long,long,long,scala.Option,scala.Option,scala.Option,scala.Option,scala.Option,scala.Option,java.lang.String)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskTableRowShuffleReadData/TaskTableRowShuffleReadData(long,java.lang.String,long,java.lang.String,long,java.lang.String)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$120/TaskDataSource$$anonfun$120(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/timeRunning(long)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/attemptNumber()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskTableRowOutputData/TaskTableRowOutputData(long,java.lang.String)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/taskId()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/launchTime()|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$75/TaskDataSource$$anonfun$75(org.apache.spark.ui.jobs.TaskDataSource,org.apache.spark.scheduler.TaskInfo)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$17/TaskDataSource$$anonfun$17(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$org$apache$spark$ui$jobs$TaskDataSource$$taskRow$1/TaskDataSource$$anonfun$org$apache$spark$ui$jobs$TaskDataSource$$taskRow$1(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$18/TaskDataSource$$anonfun$18(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$19/TaskDataSource$$anonfun$19(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$109/TaskDataSource$$anonfun$109(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$90/TaskDataSource$$anonfun$90(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$15/TaskDataSource$$anonfun$15(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$16/TaskDataSource$$anonfun$16(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$77/TaskDataSource$$anonfun$77(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$11/TaskDataSource$$anonfun$11(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$88/TaskDataSource$$anonfun$88(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$22/TaskDataSource$$anonfun$22(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$99/TaskDataSource$$anonfun$99(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$82/TaskDataSource$$anonfun$82(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$93/TaskDataSource$$anonfun$93(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$89/TaskDataSource$$anonfun$89(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$23/TaskDataSource$$anonfun$23(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$78/TaskDataSource$$anonfun$78(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$12/TaskDataSource$$anonfun$12(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$72/TaskDataSource$$anonfun$72(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$83/TaskDataSource$$anonfun$83(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$94/TaskDataSource$$anonfun$94(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$79/TaskDataSource$$anonfun$79(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$13/TaskDataSource$$anonfun$13(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$24/TaskDataSource$$anonfun$24(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$73/TaskDataSource$$anonfun$73(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$84/TaskDataSource$$anonfun$84(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$95/TaskDataSource$$anonfun$95(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$25/TaskDataSource$$anonfun$25(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$14/TaskDataSource$$anonfun$14(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$96/TaskDataSource$$anonfun$96(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$74/TaskDataSource$$anonfun$74(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$85/TaskDataSource$$anonfun$85(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskInfo()|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$86/TaskDataSource$$anonfun$86(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$20/TaskDataSource$$anonfun$20(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$97/TaskDataSource$$anonfun$97(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$80/TaskDataSource$$anonfun$80(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$91/TaskDataSource$$anonfun$91(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$98/TaskDataSource$$anonfun$98(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$76/TaskDataSource$$anonfun$76(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$10/TaskDataSource$$anonfun$10(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$87/TaskDataSource$$anonfun$87(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$21/TaskDataSource$$anonfun$21(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$81/TaskDataSource$$anonfun$81(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$92/TaskDataSource$$anonfun$92(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$119/TaskDataSource$$anonfun$119(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$108/TaskDataSource$$anonfun$108(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///scala/collection/mutable/ListBuffer$/canBuildFrom()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$118/TaskDataSource$$anonfun$118(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$107/TaskDataSource$$anonfun$107(org.apache.spark.ui.jobs.TaskDataSource)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/shuffle/IndexShuffleBlockResolver/removeDataByMap(int,int)|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+method:///java/io/File/exists()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///java/io/File/delete()|",
      "|java+method:///org/apache/spark/shuffle/IndexShuffleBlockResolver/getIndexFile(int,int)|",
      "|java+method:///org/apache/spark/shuffle/IndexShuffleBlockResolver/getDataFile(int,int)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/shuffle/IndexShuffleBlockResolver/logWarning(scala.Function0)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///java/io/File/delete()|",
      "|java+constructor:///org/apache/spark/shuffle/IndexShuffleBlockResolver$$anonfun$removeDataByMap$1/IndexShuffleBlockResolver$$anonfun$removeDataByMap$1(org.apache.spark.shuffle.IndexShuffleBlockResolver,scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/shuffle/IndexShuffleBlockResolver$$anonfun$removeDataByMap$2/IndexShuffleBlockResolver$$anonfun$removeDataByMap$2(org.apache.spark.shuffle.IndexShuffleBlockResolver,scala.runtime.ObjectRef)|",
      "|java+method:///org/apache/spark/shuffle/IndexShuffleBlockResolver/getIndexFile(int,int)|",
      "|java+method:///org/apache/spark/shuffle/IndexShuffleBlockResolver/getDataFile(int,int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/ShutdownHookManager$$anonfun$1/apply$mcV$sp()|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/ShutdownHookManager$/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/util/ShutdownHookManager$/org$apache$spark$util$ShutdownHookManager$$shutdownDeletePaths()|",
      "|java+method:///scala/collection/mutable/HashSet/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3/ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3(org.apache.spark.util.ShutdownHookManager$$anonfun$1)|",
      "|java+constructor:///org/apache/spark/util/ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$2/ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$2(org.apache.spark.util.ShutdownHookManager$$anonfun$1)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/util/ShutdownHookManager$/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/mutable/HashSet/toArray(scala.reflect.ClassTag)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/util/ShutdownHookManager$/org$apache$spark$util$ShutdownHookManager$$shutdownDeletePaths()|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3/ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3(org.apache.spark.util.ShutdownHookManager$$anonfun$1)|",
      "|java+constructor:///org/apache/spark/util/ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$2/ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$2(org.apache.spark.util.ShutdownHookManager$$anonfun$1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/Utils$$anonfun$11/Utils$$anonfun$11()|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$2/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$2/apply(long)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$2/apply(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$7/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$7/apply(long)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$7/apply(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$cancelTasks$2/apply(scala.collection.mutable.HashMap)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/foreach(scala.Function1)|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/HashMap/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$2/TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$2(org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/HashMap/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3/TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3(org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkConf$$anonfun$6/apply(java.lang.String)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/immutable/StringOps/toDouble()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/StringOps/toLong()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$63/JsonProtocol$$anonfun$63()|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$2/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$2/apply(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$2/apply(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/history/HistoryServerArguments/parse(scala.collection.immutable.List)|",
    "called": "|java+method:///scala/collection/immutable/List/length()|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServerArguments$$anonfun$parse$1/HistoryServerArguments$$anonfun$parse$1(org.apache.spark.deploy.history.HistoryServerArguments)|",
      "|java+method:///scala/collection/immutable/$colon$colon/tl$1()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/$colon$colon/hd$1()|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServerArguments/propertiesFile_$eq(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServerArguments/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServerArguments/printUsageAndExit(int)|",
      "|java+method:///org/apache/spark/SparkConf/set(java.lang.String,java.lang.String)|",
      "|java+method:///java/lang/System/setProperty(java.lang.String,java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/immutable/$colon$colon/tl$1()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/$colon$colon/hd$1()|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServerArguments/setLogDirectory(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServerArguments/propertiesFile_$eq(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServerArguments/printUsageAndExit(int)|",
      "|java+method:///scala/collection/immutable/List/head()|",
      "|java+method:///scala/collection/immutable/List/length()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$42/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$/getSchedulerDelay(org.apache.spark.scheduler.TaskInfo,org.apache.spark.executor.TaskMetrics,long)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskInfo()|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/executor/TaskMetrics/jvmGCTime()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend/doKillExecutors(scala.collection.Seq)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend/yarnSchedulerEndpoint()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$KillExecutors/CoarseGrainedClusterMessages$KillExecutors(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///scala/reflect/ClassTag$/Boolean()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend/yarnSchedulerEndpointRef()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$KillExecutors/CoarseGrainedClusterMessages$KillExecutors(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///scala/reflect/ClassTag$/Boolean()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/r/RBackendHandler/handleMethodCall(boolean,java.lang.String,java.lang.String,int,java.io.DataInputStream,java.io.DataOutputStream)|",
    "called": "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/r/SerDe$/writeString(java.io.DataOutputStream,java.lang.String)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/api/r/RBackendHandler$$anonfun$handleMethodCall$3/RBackendHandler$$anonfun$handleMethodCall$3(org.apache.spark.api.r.RBackendHandler,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeObject(java.io.DataOutputStream,java.lang.Object)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/api/r/JVMObjectTracker$/get(java.lang.String)|",
      "|java+method:///org/apache/spark/api/r/RBackendHandler/readArgs(int,java.io.DataInputStream)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeInt(java.io.DataOutputStream,int)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Utils$/exceptionString(java.lang.Throwable)|",
      "|java+method:///java/lang/reflect/Method/invoke(java.lang.Object,java.lang.Object%5B%5D)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/head()|",
      "|java+method:///java/lang/Class/getConstructors()|",
      "|java+method:///scala/Some/x()|",
      "|java+constructor:///org/apache/spark/api/r/RBackendHandler$$anonfun$handleMethodCall$2/RBackendHandler$$anonfun$handleMethodCall$2(org.apache.spark.api.r.RBackendHandler,java.lang.String)|",
      "|java+method:///java/lang/Class/getMethods()|",
      "|java+method:///org/apache/spark/api/r/RBackendHandler/logWarning(scala.Function0)|",
      "|java+method:///java/lang/reflect/Constructor/newInstance(java.lang.Object%5B%5D)|",
      "|java+method:///java/lang/Exception/getCause()|",
      "|java+constructor:///org/apache/spark/api/r/RBackendHandler$$anonfun$handleMethodCall$1/RBackendHandler$$anonfun$handleMethodCall$1(org.apache.spark.api.r.RBackendHandler,java.lang.String,java.lang.Class)|",
      "|java+method:///org/apache/spark/util/Utils$/classForName(java.lang.String)|",
      "|java+constructor:///org/apache/spark/api/r/RBackendHandler$$anonfun$2/RBackendHandler$$anonfun$2(org.apache.spark.api.r.RBackendHandler,int,java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/api/r/RBackendHandler$$anonfun$3/RBackendHandler$$anonfun$3(org.apache.spark.api.r.RBackendHandler,int,java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/api/r/RBackendHandler/logError(scala.Function0)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///java/lang/Exception/Exception(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/isEmpty()|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/ArrayOps/filter(scala.Function1)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/api/r/RBackendHandler$$anonfun$1/RBackendHandler$$anonfun$1(org.apache.spark.api.r.RBackendHandler,java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/api/r/SerDe$/writeString(java.io.DataOutputStream,java.lang.String)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeObject(java.io.DataOutputStream,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/api/r/RBackendHandler$$anonfun$handleMethodCall$5/RBackendHandler$$anonfun$handleMethodCall$5(org.apache.spark.api.r.RBackendHandler,java.lang.String,java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/api/r/RBackendHandler$$anonfun$handleMethodCall$3/RBackendHandler$$anonfun$handleMethodCall$3(org.apache.spark.api.r.RBackendHandler,java.lang.Class)|",
      "|java+method:///org/apache/spark/api/r/JVMObjectTracker$/get(java.lang.String)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/api/r/RBackendHandler/findMatchedSignature(java.lang.Class%5B%5D%5B%5D,java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeInt(java.io.DataOutputStream,int)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Utils$/exceptionString(java.lang.Throwable)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///java/lang/reflect/Method/invoke(java.lang.Object,java.lang.Object%5B%5D)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///java/lang/Class/getConstructors()|",
      "|java+method:///scala/Some/x()|",
      "|java+constructor:///org/apache/spark/api/r/RBackendHandler$$anonfun$handleMethodCall$2/RBackendHandler$$anonfun$handleMethodCall$2(org.apache.spark.api.r.RBackendHandler,java.lang.String)|",
      "|java+method:///java/lang/Class/getMethods()|",
      "|java+method:///org/apache/spark/api/r/RBackendHandler/logWarning(scala.Function0)|",
      "|java+method:///java/lang/reflect/Constructor/newInstance(java.lang.Object%5B%5D)|",
      "|java+method:///java/lang/Exception/getCause()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/api/r/RBackendHandler$$anonfun$3/RBackendHandler$$anonfun$3(org.apache.spark.api.r.RBackendHandler)|",
      "|java+constructor:///org/apache/spark/api/r/RBackendHandler$$anonfun$2/RBackendHandler$$anonfun$2(org.apache.spark.api.r.RBackendHandler)|",
      "|java+constructor:///org/apache/spark/api/r/RBackendHandler$$anonfun$handleMethodCall$1/RBackendHandler$$anonfun$handleMethodCall$1(org.apache.spark.api.r.RBackendHandler,java.lang.String,java.lang.Class)|",
      "|java+method:///org/apache/spark/util/Utils$/classForName(java.lang.String)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///org/apache/spark/api/r/RBackendHandler/readArgs(int,java.io.DataInputStream)|",
      "|java+constructor:///org/apache/spark/api/r/RBackendHandler$$anonfun$handleMethodCall$4/RBackendHandler$$anonfun$handleMethodCall$4(org.apache.spark.api.r.RBackendHandler,java.lang.Class)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/api/r/RBackendHandler/logError(scala.Function0)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///java/lang/Exception/Exception(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/ArrayOps/filter(scala.Function1)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/api/r/RBackendHandler$$anonfun$1/RBackendHandler$$anonfun$1(org.apache.spark.api.r.RBackendHandler,java.lang.String)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$13/apply()|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$13/apply()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$13/apply()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/org$apache$spark$deploy$worker$ExecutorRunner$$killProcess(scala.Option)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/execId()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/stdoutAppender()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/process()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/state()|",
      "|java+method:///org/apache/spark/util/logging/FileAppender/stop()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/DeployMessages$ExecutorStateChanged(java.lang.String,int,scala.Enumeration$Value,scala.Option,scala.Option)|",
      "|java+method:///java/lang/Process/waitFor()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/appId()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/stderrAppender()|",
      "|java+method:///java/lang/Process/destroy()|",
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$killProcess$1/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$killProcess$1(org.apache.spark.deploy.worker.ExecutorRunner)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/worker()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/execId()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/stdoutAppender()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$killProcess$2/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$killProcess$2(org.apache.spark.deploy.worker.ExecutorRunner,java.lang.IllegalStateException)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/process()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/state()|",
      "|java+method:///org/apache/spark/util/logging/FileAppender/stop()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/DeployMessages$ExecutorStateChanged(java.lang.String,int,scala.Enumeration$Value,scala.Option,scala.Option)|",
      "|java+method:///java/lang/Process/waitFor()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/appId()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/stderrAppender()|",
      "|java+method:///java/lang/Process/destroy()|",
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$killProcess$1/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$killProcess$1(org.apache.spark.deploy.worker.ExecutorRunner)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/worker()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/statusUpdate(org.apache.mesos.SchedulerDriver,org.apache.mesos.Protos$TaskStatus)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$1/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$1(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$3/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$3(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$5/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$5(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,int)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$4/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$4(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,java.lang.String,int,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$slaveIdToHost()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/shuffleServiceEnabled()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///org/apache/spark/network/shuffle/mesos/MesosExternalShuffleClient/registerDriverWithShuffleService(java.lang.String,int)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID/getValue()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/failuresBySlaveId()|",
      "|java+method:///scala/collection/mutable/Map/$minus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/conf()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/taskIdToSlaveId()|",
      "|java+method:///org/apache/spark/TaskState$/isFailed(scala.Enumeration$Value)|",
      "|java+method:///scala/Enumeration$Value/equals(java.lang.Object)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getTaskId()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/TaskState$/fromMesos(org.apache.mesos.Protos$TaskState)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/slaveIdsWithExecutors()|",
      "|java+method:///org/apache/spark/TaskState$/RUNNING()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/MAX_SLAVE_FAILURES()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/stateLock()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$6/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$6(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,java.lang.String)|",
      "|java+method:///scala/collection/mutable/Map/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logInfo(scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/mesos/Protos$TaskID/getValue()|",
      "|java+method:///org/apache/spark/TaskState$/isFinished(scala.Enumeration$Value)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/mesos/SchedulerDriver/reviveOffers()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/executorTerminated(org.apache.mesos.SchedulerDriver,java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$2/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$2(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,int,org.apache.mesos.Protos$TaskState)|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getState()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/mesosExternalShuffleClient()|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getSlaveId()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/assume(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/coresByTaskId()|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/remove(java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/conf()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$1/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$1(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$3/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$3(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$5/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$5(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$4/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$4(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,java.lang.String,int,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$slaveIdToHost()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/shuffleServiceEnabled()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///org/apache/spark/network/shuffle/mesos/MesosExternalShuffleClient/registerDriverWithShuffleService(java.lang.String,int)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID/getValue()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/failuresBySlaveId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/taskIdToSlaveId()|",
      "|java+method:///org/apache/spark/TaskState$/isFailed(scala.Enumeration$Value)|",
      "|java+method:///scala/Enumeration$Value/equals(java.lang.Object)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getTaskId()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/TaskState$/fromMesos(org.apache.mesos.Protos$TaskState)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/slaveIdsWithExecutors()|",
      "|java+method:///org/apache/spark/TaskState$/RUNNING()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/MAX_SLAVE_FAILURES()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/stateLock()|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/get(java.lang.Object)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$6/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$6(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/mesos/Protos$TaskID/getValue()|",
      "|java+method:///org/apache/spark/TaskState$/isFinished(scala.Enumeration$Value)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/remove(java.lang.Object)|",
      "|java+method:///org/apache/mesos/SchedulerDriver/reviveOffers()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/executorTerminated(org.apache.mesos.SchedulerDriver,java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$2/CoarseMesosSchedulerBackend$$anonfun$statusUpdate$2(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,int,org.apache.mesos.Protos$TaskState)|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getState()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/mesosExternalShuffleClient()|",
      "|java+method:///org/apache/mesos/Protos$TaskStatus/getSlaveId()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/assume(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/coresByTaskId()|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/remove(java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$117/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$117/apply(long)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$117/apply(org.apache.spark.executor.TaskMetrics)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$onStart$3/apply(scala.util.Try)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/util/Failure/exception()|",
      "|java+method:///org/apache/spark/util/Utils$/tryLogNonFatalError(scala.Function0)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$onStart$3$$anonfun$apply$1/CoarseGrainedExecutorBackend$$anonfun$onStart$3$$anonfun$apply$1(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$onStart$3,org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages$RegisteredExecutor$)|",
      "|java+method:///scala/util/Success/value()|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$onStart$3$$anonfun$apply$2/CoarseGrainedExecutorBackend$$anonfun$onStart$3$$anonfun$apply$2(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$onStart$3)|",
      "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///java/lang/System/exit(int)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/Failure/exception()|",
      "|java+method:///org/apache/spark/util/Utils$/tryLogNonFatalError(scala.Function0)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$onStart$3$$anonfun$apply$1/CoarseGrainedExecutorBackend$$anonfun$onStart$3$$anonfun$apply$1(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$onStart$3,org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages$RegisterExecutorResponse)|",
      "|java+method:///scala/util/Success/value()|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$onStart$3$$anonfun$apply$2/CoarseGrainedExecutorBackend$$anonfun$onStart$3$$anonfun$apply$2(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$onStart$3)|",
      "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///java/lang/System/exit(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/ExecutorTable/executorTable()|",
    "called": "|java+constructor:///scala/runtime/BooleanRef/BooleanRef(boolean)|",
    "v1Body": [
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/stageIdToData()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/ui/ToolTips$/SHUFFLE_READ()|",
      "|java+method:///org/apache/spark/ui/ToolTips$/SHUFFLE_WRITE()|",
      "|java+method:///org/apache/spark/ui/jobs/ExecutorTable/listener()|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+method:///org/apache/spark/ui/ToolTips$/INPUT()|",
      "|java+constructor:///scala/runtime/BooleanRef/BooleanRef(boolean)|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///org/apache/spark/ui/jobs/ExecutorTable/createExecutorTable()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)|",
      "|java+method:///org/apache/spark/ui/ToolTips$/OUTPUT()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/TABLE_CLASS_STRIPED_SORTABLE()|",
      "|java+constructor:///org/apache/spark/ui/jobs/ExecutorTable$$anonfun$executorTable$1/ExecutorTable$$anonfun$executorTable$1(org.apache.spark.ui.jobs.ExecutorTable,scala.runtime.BooleanRef,scala.runtime.BooleanRef,scala.runtime.BooleanRef,scala.runtime.BooleanRef,scala.runtime.BooleanRef)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+constructor:///scala/Tuple2$mcII$sp/Tuple2$mcII$sp(int,int)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/stageIdToData()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/ui/ToolTips$/SHUFFLE_READ()|",
      "|java+method:///org/apache/spark/ui/ToolTips$/SHUFFLE_WRITE()|",
      "|java+method:///org/apache/spark/ui/jobs/ExecutorTable/listener()|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+method:///scala/xml/Unparsed$/apply(java.lang.String)|",
      "|java+method:///org/apache/spark/ui/ToolTips$/INPUT()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///org/apache/spark/ui/jobs/ExecutorTable/createExecutorTable()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)|",
      "|java+method:///org/apache/spark/ui/ToolTips$/OUTPUT()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/TABLE_CLASS_STRIPED_SORTABLE()|",
      "|java+constructor:///org/apache/spark/ui/jobs/ExecutorTable$$anonfun$executorTable$1/ExecutorTable$$anonfun$executorTable$1(org.apache.spark.ui.jobs.ExecutorTable,scala.runtime.BooleanRef,scala.runtime.BooleanRef,scala.runtime.BooleanRef,scala.runtime.BooleanRef,scala.runtime.BooleanRef)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+constructor:///scala/Tuple2$mcII$sp/Tuple2$mcII$sp(int,int)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+constructor:///scala/runtime/BooleanRef/BooleanRef(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$groupByKey$1/apply()|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$8/PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$8(org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKey(scala.Function1,scala.Function2,scala.Function2,org.apache.spark.Partitioner,boolean,org.apache.spark.serializer.Serializer)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$groupByKey$1/apply()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKey$default$6()|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10/PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10(org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$9/PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$9(org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKeyWithClassTag$default$6()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKeyWithClassTag(scala.Function1,scala.Function2,scala.Function2,org.apache.spark.Partitioner,boolean,org.apache.spark.serializer.Serializer,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$groupByKey$1/apply()|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10/PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10(org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$9/PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$9(org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$8/PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$8(org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$8/apply()|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$8/apply()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$8/apply()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore/currentUnrollMemory()|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/sum(scala.math.Numeric)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/accountingLock()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/unrollMemoryMap()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/pendingUnrollMemoryMap()|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/sum(scala.math.Numeric)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/unrollMemoryMap()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/pendingUnrollMemoryMap()|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/org$apache$spark$deploy$master$ApplicationInfo$$init()|",
    "called": "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/coresGranted_$eq(int)|",
      "|java+constructor:///org/apache/spark/deploy/master/ApplicationSource/ApplicationSource(org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/appSource_$eq(org.apache.spark.deploy.master.ApplicationSource)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/nextExecutorId_$eq(int)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationState$/WAITING()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/executors_$eq(scala.collection.mutable.HashMap)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/endTime_$eq(long)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/state_$eq(scala.Enumeration$Value)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/removedExecutors_$eq(scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/executorLimit_$eq(int)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/coresGranted_$eq(int)|",
      "|java+constructor:///org/apache/spark/deploy/master/ApplicationSource/ApplicationSource(org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/appSource_$eq(org.apache.spark.deploy.master.ApplicationSource)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/nextExecutorId_$eq(int)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationState$/WAITING()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/executors_$eq(scala.collection.mutable.HashMap)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/endTime_$eq(long)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/appUIUrlAtHistoryServer_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/state_$eq(scala.Enumeration$Value)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/removedExecutors_$eq(scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/executorLimit_$eq(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$MesosSchedulerBackend$$getTasksSummary(java.util.ArrayList)|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/Buffer/foreach(scala.Function1)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$org$apache$spark$scheduler$cluster$mesos$MesosSchedulerBackend$$getTasksSummary$1/MesosSchedulerBackend$$anonfun$org$apache$spark$scheduler$cluster$mesos$MesosSchedulerBackend$$getTasksSummary$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend,scala.collection.mutable.StringBuilder)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$org$apache$spark$scheduler$cluster$mesos$MesosSchedulerBackend$$getTasksSummary$1/MesosSchedulerBackend$$anonfun$org$apache$spark$scheduler$cluster$mesos$MesosSchedulerBackend$$getTasksSummary$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend,scala.collection.mutable.StringBuilder)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$1$$anonfun$apply$mcVI$sp$1/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$1$$anonfun$apply$mcVI$sp$1(org.apache.spark.shuffle.FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$1,int)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1$mcVI$sp/AbstractFunction1$mcVI$sp()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1$mcZI$sp/AbstractFunction1$mcZI$sp()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1$mcVI$sp/AbstractFunction1$mcVI$sp()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$9/apply()|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/name()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$9/apply()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$9/apply()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$74/apply(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
    "v1Body": [
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/ShuffleMapStage$$anonfun$removeOutputsOnExecutor$2/apply()|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/isAvailable()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/numPartitions()|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage$$anonfun$removeOutputsOnExecutor$2/apply()|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/numAvailableOutputs()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/isAvailable()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/numPartitions()|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage$$anonfun$removeOutputsOnExecutor$2/apply()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal(org.apache.spark.rdd.RDD,int,scala.collection.mutable.HashSet)|",
    "called": "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/shuffleLocalityEnabled()|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2(org.apache.spark.scheduler.DAGScheduler,int,scala.collection.mutable.HashSet,java.lang.Object)|",
      "|java+method:///scala/collection/IndexedSeq/apply(int)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/getCacheLocs(org.apache.spark.rdd.RDD)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/immutable/List/nonEmpty()|",
      "|java+method:///org/apache/spark/rdd/RDD/preferredLocations(org.apache.spark.Partition)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$1/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$1(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///scala/runtime/NonLocalReturnControl/key()|",
      "|java+method:///scala/collection/mutable/HashSet/add(java.lang.Object)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/RDD/dependencies()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///scala/runtime/NonLocalReturnControl/value()|",
      "|java+method:///scala/collection/Seq/toList()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$3/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$3(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.rdd.RDD,int,java.lang.Object)|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2(org.apache.spark.scheduler.DAGScheduler,int,scala.collection.mutable.HashSet,java.lang.Object)|",
      "|java+method:///scala/collection/IndexedSeq/apply(int)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/getCacheLocs(org.apache.spark.rdd.RDD)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///scala/collection/immutable/List/nonEmpty()|",
      "|java+method:///org/apache/spark/rdd/RDD/preferredLocations(org.apache.spark.Partition)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$1/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$1(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///scala/runtime/NonLocalReturnControl/key()|",
      "|java+method:///scala/collection/mutable/HashSet/add(java.lang.Object)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/RDD/dependencies()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///scala/runtime/NonLocalReturnControl/value()|",
      "|java+method:///scala/collection/Seq/toList()|",
      "|java+method:///scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$58/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$58/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$58/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/start()|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/start()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/createSchedulerDriver$default$6()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/createSchedulerDriver$default$7()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/createSchedulerDriver$default$8()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/createSchedulerDriver$default$9()|",
      "|java+method:///org/apache/spark/SparkContext/sparkUser()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/createSchedulerDriver(java.lang.String,org.apache.mesos.Scheduler,java.lang.String,java.lang.String,org.apache.spark.SparkConf,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/startScheduler(org.apache.mesos.SchedulerDriver)|",
      "|java+method:///org/apache/spark/SparkContext/appName()|",
      "|java+method:///org/apache/spark/SparkContext/conf()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/start()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/createSchedulerDriver$default$7()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/createSchedulerDriver$default$8()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/createSchedulerDriver$default$9()|",
      "|java+method:///org/apache/spark/SparkContext/sparkUser()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/createSchedulerDriver(java.lang.String,org.apache.mesos.Scheduler,java.lang.String,java.lang.String,org.apache.spark.SparkConf,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/startScheduler(org.apache.mesos.SchedulerDriver)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$1/CoarseMesosSchedulerBackend$$anonfun$1(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
      "|java+method:///org/apache/spark/SparkContext/ui()|",
      "|java+method:///org/apache/spark/SparkContext/appName()|",
      "|java+method:///org/apache/spark/SparkContext/conf()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/SizeEstimator$/visitSingleObject(java.lang.Object,org.apache.spark.util.SizeEstimator$SearchState)|",
    "called": "|java+method:///scala/collection/immutable/List/foreach(scala.Function1)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/util/SizeEstimator$$anonfun$visitSingleObject$1/SizeEstimator$$anonfun$visitSingleObject$1(java.lang.Object,org.apache.spark.util.SizeEstimator$SearchState)|",
      "|java+method:///java/lang/Class/isArray()|",
      "|java+method:///org/apache/spark/util/SizeEstimator$SearchState/size_$eq(long)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$SearchState/size()|",
      "|java+method:///scala/collection/immutable/List/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/getClassInfo(java.lang.Class)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/visitArray(java.lang.Object,java.lang.Class,org.apache.spark.util.SizeEstimator$SearchState)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+method:///org/apache/spark/util/SizeEstimator$ClassInfo/shellSize()|",
      "|java+method:///org/apache/spark/util/SizeEstimator$ClassInfo/pointerFields()|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/alignSize(long)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/util/SizeEstimator$$anonfun$visitSingleObject$1/SizeEstimator$$anonfun$visitSingleObject$1(java.lang.Object,org.apache.spark.util.SizeEstimator$SearchState)|",
      "|java+method:///org/apache/spark/util/KnownSizeEstimation/estimatedSize()|",
      "|java+method:///java/lang/Class/isArray()|",
      "|java+method:///org/apache/spark/util/SizeEstimator$SearchState/size_$eq(long)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$SearchState/size()|",
      "|java+method:///scala/collection/immutable/List/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/getClassInfo(java.lang.Class)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/visitArray(java.lang.Object,java.lang.Class,org.apache.spark.util.SizeEstimator$SearchState)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+method:///org/apache/spark/util/SizeEstimator$ClassInfo/shellSize()|",
      "|java+method:///org/apache/spark/util/SizeEstimator$ClassInfo/pointerFields()|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/alignSize(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/SparkShutdownHookManager/remove(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
    "v1Body": [
      "|java+method:///java/util/PriorityQueue/remove(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/SparkShutdownHookManager/org$apache$spark$util$SparkShutdownHookManager$$hooks()|"
    ],
    "v2Body": [
      "|java+method:///java/util/PriorityQueue/remove(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/SparkShutdownHookManager/hooks()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/BlockManager$$anonfun$10/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/BlockManager$$anonfun$10/apply()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/BlockManager$$anonfun$10/apply()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/deploy/client/AppClient/AppClient(org.apache.spark.rpc.RpcEnv,java.lang.String%5B%5D,org.apache.spark.deploy.ApplicationDescription,org.apache.spark.deploy.client.AppClientListener,org.apache.spark.SparkConf)|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$$anonfun$1/AppClient$$anonfun$1(org.apache.spark.deploy.client.AppClient)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$$anonfun$1/AppClient$$anonfun$1(org.apache.spark.deploy.client.AppClient)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///java/util/concurrent/atomic/AtomicReference/AtomicReference()|",
      "|java+constructor:///java/util/concurrent/atomic/AtomicBoolean/AtomicBoolean(boolean)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$57/apply(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/iterator()|",
    "called": "|java+method:///scala/collection/mutable/ArrayBuffer/isEmpty()|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$spilledMaps()|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$ExternalIterator/ExternalAppendOnlyMap$ExternalIterator(org.apache.spark.util.collection.ExternalAppendOnlyMap)|",
      "|java+method:///org/apache/spark/util/collection/SizeTrackingAppendOnlyMap/iterator()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/isEmpty()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$currentMap()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$spilledMaps()|",
      "|java+constructor:///java/lang/IllegalStateException/IllegalStateException(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$ExternalIterator/ExternalAppendOnlyMap$ExternalIterator(org.apache.spark.util.collection.ExternalAppendOnlyMap)|",
      "|java+method:///org/apache/spark/util/collection/SizeTrackingAppendOnlyMap/iterator()|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$$anonfun$iterator$1/ExternalAppendOnlyMap$$anonfun$iterator$1(org.apache.spark.util.collection.ExternalAppendOnlyMap)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/isEmpty()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$currentMap()|",
      "|java+method:///org/apache/spark/util/CompletionIterator$/apply(scala.collection.Iterator,scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ExecutorLostFailure$/ExecutorLostFailure$()|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction3/AbstractFunction3()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction3/AbstractFunction3()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$81/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$81/apply(org.apache.spark.executor.InputMetrics)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$81/apply(org.apache.spark.scheduler.AccumulableInfo)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/r/RRDD$$anonfun$createSparkContext$4/apply(scala.Tuple2)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/SparkConf/setExecutorEnv(java.lang.String,java.lang.String)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/SparkConf/setExecutorEnv(java.lang.String,java.lang.String)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$timeOutDeadWorkers()|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$org$apache$spark$deploy$master$Master$$timeOutDeadWorkers$1/Master$$anonfun$org$apache$spark$deploy$master$Master$$timeOutDeadWorkers$1(org.apache.spark.deploy.master.Master,long)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$17/Master$$anonfun$17(org.apache.spark.deploy.master.Master,long)|",
      "|java+method:///org/apache/spark/deploy/master/Master/workers()|",
      "|java+method:///scala/collection/mutable/HashSet/filter(scala.Function1)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$org$apache$spark$deploy$master$Master$$timeOutDeadWorkers$1/Master$$anonfun$org$apache$spark$deploy$master$Master$$timeOutDeadWorkers$1(org.apache.spark.deploy.master.Master,long)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$19/Master$$anonfun$19(org.apache.spark.deploy.master.Master,long)|",
      "|java+method:///org/apache/spark/deploy/master/Master/workers()|",
      "|java+method:///scala/collection/mutable/HashSet/filter(scala.Function1)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/WebUI/attachPage(org.apache.spark.ui.WebUIPage)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/JettyUtils$/createServletHandler(java.lang.String,org.apache.spark.ui.JettyUtils$ServletParams,org.apache.spark.SecurityManager,java.lang.String,scala.Function1)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/BufferLike/append(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/ui/WebUI$$anonfun$attachPage$1/WebUI$$anonfun$attachPage$1(org.apache.spark.ui.WebUI)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElseUpdate(java.lang.Object,scala.Function0)|",
      "|java+method:///org/apache/spark/ui/WebUI/securityManager()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/StringOps/stripSuffix(java.lang.String)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/ui/WebUI/pageToHandlers()|",
      "|java+method:///org/apache/spark/ui/WebUI/attachHandler(org.spark-project.jetty.servlet.ServletContextHandler)|",
      "|java+constructor:///org/apache/spark/ui/WebUI$$anonfun$2/WebUI$$anonfun$2(org.apache.spark.ui.WebUI,org.apache.spark.ui.WebUIPage)|",
      "|java+constructor:///org/apache/spark/ui/WebUI$$anonfun$3/WebUI$$anonfun$3(org.apache.spark.ui.WebUI,org.apache.spark.ui.WebUIPage)|",
      "|java+method:///org/apache/spark/ui/WebUIPage/prefix()|",
      "|java+method:///org/apache/spark/ui/JettyUtils$/htmlResponderToServlet(scala.Function1)|",
      "|java+method:///org/apache/spark/ui/JettyUtils$/jsonResponderToServlet(scala.Function1)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/BufferLike/append(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/ui/WebUI$$anonfun$attachPage$1/WebUI$$anonfun$attachPage$1(org.apache.spark.ui.WebUI)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElseUpdate(java.lang.Object,scala.Function0)|",
      "|java+method:///org/apache/spark/ui/WebUI/securityManager()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/StringOps/stripSuffix(java.lang.String)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///org/apache/spark/ui/WebUI/pageToHandlers()|",
      "|java+method:///org/apache/spark/ui/WebUI/attachHandler(org.spark-project.jetty.servlet.ServletContextHandler)|",
      "|java+constructor:///org/apache/spark/ui/WebUI$$anonfun$2/WebUI$$anonfun$2(org.apache.spark.ui.WebUI,org.apache.spark.ui.WebUIPage)|",
      "|java+constructor:///org/apache/spark/ui/WebUI$$anonfun$3/WebUI$$anonfun$3(org.apache.spark.ui.WebUI,org.apache.spark.ui.WebUIPage)|",
      "|java+method:///org/apache/spark/ui/WebUIPage/prefix()|",
      "|java+method:///org/apache/spark/ui/JettyUtils$/htmlResponderToServlet(scala.Function1)|",
      "|java+method:///org/apache/spark/ui/JettyUtils$/jsonResponderToServlet(scala.Function1)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/ui/JettyUtils$/createServletHandler(java.lang.String,org.apache.spark.ui.JettyUtils$ServletParams,org.apache.spark.SecurityManager,org.apache.spark.SparkConf,java.lang.String,scala.Function1)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$37/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$37/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$37/apply(org.apache.spark.scheduler.AccumulableInfo)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receive$1/applyOrElse(java.lang.Object,scala.Function1)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$MasterChangeAcknowledged/DeployMessages$MasterChangeAcknowledged(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorAdded/id()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$$outer()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/client/AppClientListener/executorRemoved(java.lang.String,java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/org$apache$spark$deploy$client$AppClient$$appId()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorUpdated/exitStatus()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisteredApplication/appId()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/org$apache$spark$deploy$client$AppClient$$appId_$eq(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$3/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$3(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$3/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$3(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$2/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$2(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$receive$1,scala.Enumeration$Value,java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$2/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$2(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$receive$1)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorAdded/workerId()|",
      "|java+method:///org/apache/spark/deploy/client/AppClientListener/executorAdded(java.lang.String,java.lang.String,java.lang.String,int,int)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorUpdated/message()|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$4/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$4(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$receive$1,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///org/apache/spark/deploy/client/AppClientListener/connected(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$sendToMaster(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterChanged/master()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/stop()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorUpdated/id()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisteredApplication/master()|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/RUNNING()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorAdded/memory()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$master_$eq(scala.Option)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorUpdated/state()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ApplicationRemoved/message()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/org$apache$spark$deploy$client$AppClient$$registered_$eq(boolean)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorAdded/hostPort()|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$1/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$1(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$receive$1,java.lang.String,java.lang.String,java.lang.String,int)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/markDead(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/isFinished(scala.Enumeration$Value)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/DeployMessages$ExecutorStateChanged(java.lang.String,int,scala.Enumeration$Value,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorAdded/cores()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/logInfo(scala.Function0)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$alreadyDisconnected_$eq(boolean)|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$MasterChangeAcknowledged/DeployMessages$MasterChangeAcknowledged(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorAdded/id()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterChanged/master()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$$outer()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/org$apache$spark$deploy$client$AppClient$$appId()|",
      "|java+method:///scala/Predef$/any2stringadd(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorUpdated/exitStatus()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisteredApplication/appId()|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$3/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$3(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$3/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$3(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$2/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$2(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$receive$1,scala.Enumeration$Value,java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$2/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$2(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$receive$1)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorAdded/workerId()|",
      "|java+method:///org/apache/spark/deploy/client/AppClientListener/executorAdded(java.lang.String,java.lang.String,java.lang.String,int,int)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorUpdated/message()|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$4/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$4(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$receive$1,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///org/apache/spark/deploy/client/AppClientListener/connected(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/stop()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorUpdated/id()|",
      "|java+method:///scala/runtime/StringAdd$/$plus$extension(java.lang.Object,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisteredApplication/master()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$master_$eq(scala.Option)|",
      "|java+method:///java/util/concurrent/atomic/AtomicReference/set(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorAdded/memory()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorUpdated/state()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ApplicationRemoved/message()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/client/AppClientListener/executorRemoved(java.lang.String,java.lang.String,scala.Option)|",
      "|java+method:///java/util/concurrent/atomic/AtomicReference/get()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorAdded/hostPort()|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$1/AppClient$ClientEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$1(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$receive$1,java.lang.String,java.lang.String,java.lang.String,int)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/org$apache$spark$deploy$client$AppClient$$registered()|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/markDead(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/isFinished(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorAdded/cores()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/logInfo(scala.Function0)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///java/util/concurrent/atomic/AtomicBoolean/set(boolean)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$alreadyDisconnected_$eq(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$76/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$76/apply(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$76/apply(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/PythonRunner$WriterThread$$anonfun$run$3/apply()|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///org/apache/spark/TaskContext/isInterrupted()|",
      "|java+method:///java/io/DataOutputStream/write(byte%5B%5D)|",
      "|java+method:///org/apache/spark/api/python/PythonRunner$WriterThread/org$apache$spark$api$python$PythonRunner$WriterThread$$_exception_$eq(java.lang.Exception)|",
      "|java+method:///org/apache/spark/TaskContext$/setTaskContext(org.apache.spark.TaskContext)|",
      "|java+method:///org/apache/spark/TaskContext/isCompleted()|",
      "|java+method:///scala/collection/mutable/Set/foreach(scala.Function1)|",
      "|java+method:///scala/collection/Set/size()|",
      "|java+method:///org/apache/spark/api/python/PythonRunner$WriterThread/org$apache$spark$api$python$PythonRunner$WriterThread$$$outer()|",
      "|java+method:///scala/collection/TraversableOnce/toSet()|",
      "|java+constructor:///java/io/DataOutputStream/DataOutputStream(java.io.OutputStream)|",
      "|java+method:///scala/collection/mutable/Buffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///org/apache/spark/api/python/PythonRDD$/getWorkerBroadcasts(java.net.Socket)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$2/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$2(org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3)|",
      "|java+method:///java/io/DataOutputStream/writeInt(int)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$3/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$3(org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3)|",
      "|java+method:///scala/collection/immutable/Set/diff(scala.collection.GenSet)|",
      "|java+method:///org/apache/spark/api/python/PythonRDD$/writeUTF(java.lang.String,java.io.DataOutputStream)|",
      "|java+method:///org/apache/spark/api/python/SpecialLengths$/END_OF_STREAM()|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$7/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$7(org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$2/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$2(org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3)|",
      "|java+method:///scala/collection/mutable/Buffer/foreach(scala.Function1)|",
      "|java+method:///java/net/Socket/isClosed()|",
      "|java+method:///scala/collection/mutable/Set/diff(scala.collection.GenSet)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$1/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$1(org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3,java.io.DataOutputStream,scala.collection.mutable.Set)|",
      "|java+method:///java/io/DataOutputStream/flush()|",
      "|java+method:///java/net/Socket/getOutputStream()|",
      "|java+constructor:///java/io/BufferedOutputStream/BufferedOutputStream(java.io.OutputStream,int)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$5/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$5(org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3,java.io.DataOutputStream)|",
      "|java+method:///org/apache/spark/api/python/PythonRDD$/writeIteratorToStream(scala.collection.Iterator,java.io.DataOutputStream)|",
      "|java+method:///org/apache/spark/util/Utils$/tryLog(scala.Function0)|",
      "|java+method:///org/apache/spark/api/python/SpecialLengths$/END_OF_DATA_SECTION()|",
      "|java+method:///scala/collection/mutable/Set/size()|",
      "|java+method:///scala/collection/mutable/Buffer/length()|",
      "|java+method:///org/apache/spark/api/python/PythonRunner/logDebug(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/SparkFiles$/getRootDirectory()|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$6/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$6(org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3,java.io.DataOutputStream,scala.collection.mutable.Set)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///org/apache/spark/TaskContext/isInterrupted()|",
      "|java+method:///java/io/DataOutputStream/write(byte%5B%5D)|",
      "|java+method:///org/apache/spark/api/python/PythonRunner$WriterThread/org$apache$spark$api$python$PythonRunner$WriterThread$$_exception_$eq(java.lang.Exception)|",
      "|java+method:///org/apache/spark/TaskContext$/setTaskContext(org.apache.spark.TaskContext)|",
      "|java+method:///scala/collection/mutable/Set/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$1/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$1(org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3,java.io.DataOutputStream,scala.collection.mutable.Set)|",
      "|java+method:///scala/collection/TraversableOnce/toSet()|",
      "|java+constructor:///java/io/DataOutputStream/DataOutputStream(java.io.OutputStream)|",
      "|java+method:///org/apache/spark/api/python/PythonRDD$/getWorkerBroadcasts(java.net.Socket)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$2/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$2(org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3)|",
      "|java+method:///java/io/DataOutputStream/writeInt(int)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$3/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$3(org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3)|",
      "|java+method:///scala/collection/immutable/Set/diff(scala.collection.GenSet)|",
      "|java+method:///org/apache/spark/api/python/PythonRDD$/writeUTF(java.lang.String,java.io.DataOutputStream)|",
      "|java+method:///org/apache/spark/api/python/SpecialLengths$/END_OF_STREAM()|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$7/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$7(org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$2/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$2(org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3)|",
      "|java+method:///java/net/Socket/isClosed()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/Set/diff(scala.collection.GenSet)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///java/util/List/size()|",
      "|java+method:///org/apache/spark/TaskContext/isCompleted()|",
      "|java+method:///scala/collection/Set/size()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///java/io/DataOutputStream/flush()|",
      "|java+method:///java/net/Socket/getOutputStream()|",
      "|java+constructor:///java/io/BufferedOutputStream/BufferedOutputStream(java.io.OutputStream,int)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/api/python/PythonRunner$WriterThread/org$apache$spark$api$python$PythonRunner$WriterThread$$$outer()|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$5/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$5(org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3,java.io.DataOutputStream)|",
      "|java+method:///org/apache/spark/api/python/PythonRDD$/writeIteratorToStream(scala.collection.Iterator,java.io.DataOutputStream)|",
      "|java+method:///org/apache/spark/util/Utils$/tryLog(scala.Function0)|",
      "|java+method:///org/apache/spark/api/python/SpecialLengths$/END_OF_DATA_SECTION()|",
      "|java+method:///scala/collection/mutable/Set/size()|",
      "|java+method:///org/apache/spark/api/python/PythonRunner/logDebug(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/SparkFiles$/getRootDirectory()|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$6/PythonRunner$WriterThread$$anonfun$run$3$$anonfun$apply$6(org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3,java.io.DataOutputStream,scala.collection.mutable.Set)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$executorLost$1/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$executorLost$1/apply()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$executorLost$1/apply()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/BlockManager/doGetLocal(org.apache.spark.storage.BlockId,boolean)|",
    "called": "|java+method:///scala/Option/orNull(scala.Predef$$less$colon$less)|",
    "v1Body": [
      "|java+method:///scala/Some/x()|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$7/BlockManager$$anonfun$7(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockInfo)|",
      "|java+method:///org/apache/spark/storage/BlockManager/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/storage/ExternalBlockStore/getValues(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/executor/DataReadMethod$/Disk()|",
      "|java+method:///org/apache/spark/storage/BlockInfo/waitForReady()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/storage/ExternalBlockStore/contains(org.apache.spark.storage.BlockId)|",
      "|java+method:///java/nio/ByteBuffer/position()|",
      "|java+method:///org/apache/spark/storage/BlockManager/org$apache$spark$storage$BlockManager$$blockInfo()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$9/BlockManager$$anonfun$doGetLocal$9(org.apache.spark.storage.BlockManager,java.nio.ByteBuffer)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/putIterator(org.apache.spark.storage.BlockId,scala.collection.Iterator,org.apache.spark.storage.StorageLevel,boolean,boolean)|",
      "|java+method:///org/apache/spark/storage/PutResult/data()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataDeserialize(org.apache.spark.storage.BlockId,java.nio.ByteBuffer,org.apache.spark.serializer.Serializer)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/putBytes(org.apache.spark.storage.BlockId,long,scala.Function0)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/deserialized()|",
      "|java+method:///org/apache/spark/storage/BlockManager/memoryStore()|",
      "|java+method:///org/apache/spark/storage/DiskStore/getBytes(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataDeserialize$default$3()|",
      "|java+method:///org/apache/spark/storage/BlockManager/externalBlockStore()|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/get(java.lang.Object)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$6/BlockManager$$anonfun$6(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockInfo)|",
      "|java+method:///org/apache/spark/storage/BlockInfo/size()|",
      "|java+method:///org/apache/spark/storage/BlockManager/diskStore()|",
      "|java+method:///org/apache/spark/storage/BlockInfo/level()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/getBytes(org.apache.spark.storage.BlockId)|",
      "|java+method:///scala/Option/orNull(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///java/nio/ByteBuffer/rewind()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$1/BlockManager$$anonfun$doGetLocal$1(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$2/BlockManager$$anonfun$doGetLocal$2(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$4/BlockManager$$anonfun$doGetLocal$4(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$5/BlockManager$$anonfun$doGetLocal$5(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$6/BlockManager$$anonfun$doGetLocal$6(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$7/BlockManager$$anonfun$doGetLocal$7(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$8/BlockManager$$anonfun$doGetLocal$8(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/ExternalBlockStore/getBytes(org.apache.spark.storage.BlockId)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/useDisk()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/storage/BlockResult/BlockResult(scala.collection.Iterator,scala.Enumeration$Value,long)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/useOffHeap()|",
      "|java+method:///java/nio/ByteBuffer/limit()|",
      "|java+method:///scala/util/Left/a()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/getValues(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/BlockManager/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$10/BlockManager$$anonfun$doGetLocal$10(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/useMemory()|",
      "|java+constructor:///org/apache/spark/storage/BlockException/BlockException(org.apache.spark.storage.BlockId,java.lang.String)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$3/BlockManager$$anonfun$doGetLocal$3(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId,org.apache.spark.storage.StorageLevel)|",
      "|java+method:///scala/Predef$/conforms()|"
    ],
    "v2Body": [
      "|java+method:///scala/Some/x()|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///org/apache/spark/storage/DiskStore/getBytes(org.apache.spark.storage.BlockId)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$7/BlockManager$$anonfun$7(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockInfo)|",
      "|java+method:///org/apache/spark/storage/BlockManager/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/storage/ExternalBlockStore/getValues(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/executor/DataReadMethod$/Disk()|",
      "|java+method:///org/apache/spark/storage/BlockInfo/waitForReady()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/storage/ExternalBlockStore/contains(org.apache.spark.storage.BlockId)|",
      "|java+method:///java/nio/ByteBuffer/position()|",
      "|java+method:///org/apache/spark/storage/BlockManager/org$apache$spark$storage$BlockManager$$blockInfo()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$9/BlockManager$$anonfun$doGetLocal$9(org.apache.spark.storage.BlockManager,java.nio.ByteBuffer)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/putIterator(org.apache.spark.storage.BlockId,scala.collection.Iterator,org.apache.spark.storage.StorageLevel,boolean,boolean)|",
      "|java+method:///org/apache/spark/storage/PutResult/data()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/putBytes(org.apache.spark.storage.BlockId,long,scala.Function0)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/deserialized()|",
      "|java+method:///org/apache/spark/storage/BlockManager/memoryStore()|",
      "|java+method:///org/apache/spark/storage/BlockManager/externalBlockStore()|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/get(java.lang.Object)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$6/BlockManager$$anonfun$6(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockInfo)|",
      "|java+method:///org/apache/spark/storage/BlockInfo/size()|",
      "|java+method:///org/apache/spark/storage/BlockManager/diskStore()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataDeserialize(org.apache.spark.storage.BlockId,java.nio.ByteBuffer)|",
      "|java+method:///org/apache/spark/storage/BlockInfo/level()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/getBytes(org.apache.spark.storage.BlockId)|",
      "|java+method:///scala/Option/orNull(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///java/nio/ByteBuffer/rewind()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$1/BlockManager$$anonfun$doGetLocal$1(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$2/BlockManager$$anonfun$doGetLocal$2(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$4/BlockManager$$anonfun$doGetLocal$4(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$5/BlockManager$$anonfun$doGetLocal$5(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$6/BlockManager$$anonfun$doGetLocal$6(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$7/BlockManager$$anonfun$doGetLocal$7(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$8/BlockManager$$anonfun$doGetLocal$8(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/ExternalBlockStore/getBytes(org.apache.spark.storage.BlockId)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/useDisk()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/storage/BlockResult/BlockResult(scala.collection.Iterator,scala.Enumeration$Value,long)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/useOffHeap()|",
      "|java+method:///java/nio/ByteBuffer/limit()|",
      "|java+method:///scala/util/Left/a()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/getValues(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/BlockManager/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$10/BlockManager$$anonfun$doGetLocal$10(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/useMemory()|",
      "|java+constructor:///org/apache/spark/storage/BlockException/BlockException(org.apache.spark.storage.BlockId,java.lang.String)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetLocal$3/BlockManager$$anonfun$doGetLocal$3(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId,org.apache.spark.storage.StorageLevel)|",
      "|java+method:///scala/Predef$/conforms()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$take$1/apply()|",
    "called": "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/size()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$take$1$$anonfun$apply$45/RDD$$anonfun$take$1$$anonfun$apply$45(org.apache.spark.rdd.RDD$$anonfun$take$1,scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///scala/reflect/ClassTag/runtimeClass()|",
      "|java+method:///scala/reflect/ClassTag/newArray(int)|",
      "|java+method:///org/apache/spark/rdd/RDD/org$apache$spark$rdd$RDD$$sc()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$take$1$$anonfun$28/RDD$$anonfun$take$1$$anonfun$28(org.apache.spark.rdd.RDD$$anonfun$take$1,int)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/toArray(scala.reflect.ClassTag)|",
      "|java+method:///scala/math/package$/min(int,int)|",
      "|java+method:///java/lang/Math/min(int,int)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+method:///java/lang/Math/max(int,int)|",
      "|java+method:///org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function1,scala.collection.Seq,scala.reflect.ClassTag)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/ArrayBuffer/size()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///scala/reflect/ClassTag/runtimeClass()|",
      "|java+method:///scala/reflect/ClassTag/newArray(int)|",
      "|java+method:///org/apache/spark/rdd/RDD/org$apache$spark$rdd$RDD$$sc()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$take$1$$anonfun$28/RDD$$anonfun$take$1$$anonfun$28(org.apache.spark.rdd.RDD$$anonfun$take$1,int)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/toArray(scala.reflect.ClassTag)|",
      "|java+method:///scala/math/package$/min(int,int)|",
      "|java+method:///java/lang/Math/min(int,int)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$take$1$$anonfun$apply$49/RDD$$anonfun$take$1$$anonfun$apply$49(org.apache.spark.rdd.RDD$$anonfun$take$1,scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+method:///java/lang/Math/max(int,int)|",
      "|java+method:///org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function1,scala.collection.Seq,scala.reflect.ClassTag)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/deploy/worker/Worker/Worker(org.apache.spark.rpc.RpcEnv,int,int,int,org.apache.spark.rpc.RpcAddress%5B%5D,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
    "called": "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/ArrayOps/size()|",
      "|java+constructor:///java/util/concurrent/ThreadPoolExecutor/ThreadPoolExecutor(int,int,long,java.util.concurrent.TimeUnit,java.util.concurrent.BlockingQueue,java.util.concurrent.ThreadFactory)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/testing()|",
      "|java+method:///org/apache/spark/SparkConf/getLong(java.lang.String,long)|",
      "|java+method:///scala/sys/SystemProperties/apply(java.lang.Object)|",
      "|java+method:///scala/concurrent/ExecutionContext$/fromExecutorService(java.util.concurrent.ExecutorService)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$5/Worker$$anonfun$5(org.apache.spark.deploy.worker.Worker)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///scala/sys/package$/env()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$port()|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/uriOf(java.lang.String,org.apache.spark.rpc.RpcAddress,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///scala/sys/package$/props()|",
      "|java+method:///scala/Predef$/assert(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/SparkConf/getenv(java.lang.String)|",
      "|java+method:///scala/math/package$/round(double)|",
      "|java+method:///scala/util/Random/nextDouble()|",
      "|java+constructor:///org/apache/spark/deploy/worker/WorkerSource/WorkerSource(org.apache.spark.deploy.worker.Worker)|",
      "|java+method:///org/apache/spark/util/Utils$/checkHost(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$host()|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/generateWorkerId()|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/port()|",
      "|java+constructor:///org/apache/spark/deploy/ExternalShuffleService/ExternalShuffleService(org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+method:///org/apache/spark/deploy/worker/ui/WorkerWebUI$/DEFAULT_RETAINED_EXECUTORS()|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/namedThreadFactory(java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/LinkedHashMap/LinkedHashMap()|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonSingleThreadScheduledExecutor(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/FUZZ_MULTIPLIER_INTERVAL_LOWER_BOUND()|",
      "|java+method:///java/util/UUID/randomUUID()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/REGISTRATION_RETRY_FUZZ_MULTIPLIER()|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/host()|",
      "|java+method:///java/util/UUID/getMostSignificantBits()|",
      "|java+constructor:///scala/util/Random/Random(long)|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonSingleThreadExecutor(java.lang.String)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpoint$class/$init$(org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/ui/WorkerWebUI$/DEFAULT_RETAINED_DRIVERS()|",
      "|java+method:///scala/sys/SystemProperties/contains(java.lang.String)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+constructor:///java/util/concurrent/SynchronousQueue/SynchronousQueue()|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem$/createMetricsSystem(java.lang.String,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$4/Worker$$anonfun$4(org.apache.spark.deploy.worker.Worker)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/address()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$INITIAL_REGISTRATION_RETRIES()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/ArrayOps/size()|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonCachedThreadPool$default$3()|",
      "|java+constructor:///scala/collection/mutable/LinkedHashMap/LinkedHashMap()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/testing()|",
      "|java+method:///org/apache/spark/SparkConf/getLong(java.lang.String,long)|",
      "|java+method:///scala/sys/SystemProperties/apply(java.lang.Object)|",
      "|java+method:///scala/concurrent/ExecutionContext$/fromExecutorService(java.util.concurrent.ExecutorService)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$5/Worker$$anonfun$5(org.apache.spark.deploy.worker.Worker)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///scala/sys/package$/env()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$port()|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/uriOf(java.lang.String,org.apache.spark.rpc.RpcAddress,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///scala/sys/package$/props()|",
      "|java+method:///scala/Predef$/assert(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/SparkConf/getenv(java.lang.String)|",
      "|java+method:///scala/math/package$/round(double)|",
      "|java+method:///scala/util/Random/nextDouble()|",
      "|java+constructor:///org/apache/spark/deploy/worker/WorkerSource/WorkerSource(org.apache.spark.deploy.worker.Worker)|",
      "|java+method:///org/apache/spark/util/Utils$/checkHost(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$host()|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/generateWorkerId()|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/port()|",
      "|java+constructor:///org/apache/spark/deploy/ExternalShuffleService/ExternalShuffleService(org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+method:///org/apache/spark/deploy/worker/ui/WorkerWebUI$/DEFAULT_RETAINED_EXECUTORS()|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonSingleThreadScheduledExecutor(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/FUZZ_MULTIPLIER_INTERVAL_LOWER_BOUND()|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonCachedThreadPool(java.lang.String,int,int)|",
      "|java+method:///java/util/UUID/randomUUID()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/REGISTRATION_RETRY_FUZZ_MULTIPLIER()|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/host()|",
      "|java+method:///java/util/UUID/getMostSignificantBits()|",
      "|java+constructor:///scala/util/Random/Random(long)|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonSingleThreadExecutor(java.lang.String)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpoint$class/$init$(org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/ui/WorkerWebUI$/DEFAULT_RETAINED_DRIVERS()|",
      "|java+method:///scala/sys/SystemProperties/contains(java.lang.String)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem$/createMetricsSystem(java.lang.String,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$4/Worker$$anonfun$4(org.apache.spark.deploy.worker.Worker)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/address()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$INITIAL_REGISTRATION_RETRIES()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/executor/Executor/Executor(java.lang.String,java.lang.String,org.apache.spark.SparkEnv,scala.collection.Seq,boolean)|",
    "called": "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
    "v1Body": [
      "|java+method:///org/apache/spark/executor/Executor/createClassLoader()|",
      "|java+method:///org/apache/spark/util/Utils$/setCustomHostname(java.lang.String)|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonSingleThreadScheduledExecutor(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkEnv/metricsSystem()|",
      "|java+method:///org/apache/spark/util/Utils$/parseHostPort(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/HeartbeatReceiver$/ENDPOINT_NAME()|",
      "|java+method:///org/apache/spark/SparkEnv/serializer()|",
      "|java+method:///org/apache/spark/executor/Executor/startDriverHeartbeater()|",
      "|java+constructor:///org/apache/spark/executor/ExecutorEndpoint/ExecutorEndpoint(org.apache.spark.rpc.RpcEnv,java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/getMaxResultSize(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$replClassLoader()|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$urlClassLoader()|",
      "|java+method:///org/apache/spark/util/Utils$/checkHost(java.lang.String,java.lang.String)|",
      "|java+constructor:///java/util/concurrent/ConcurrentHashMap/ConcurrentHashMap()|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///org/apache/spark/executor/ExecutorEndpoint$/EXECUTOR_ENDPOINT_NAME()|",
      "|java+method:///org/apache/spark/executor/Executor/threadPool()|",
      "|java+method:///org/apache/spark/util/AkkaUtils$/maxFrameSizeBytes(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$conf()|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/registerSource(org.apache.spark.metrics.source.Source)|",
      "|java+method:///org/apache/spark/serializer/Serializer/setDefaultClassLoader(java.lang.ClassLoader)|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|",
      "|java+method:///org/apache/spark/executor/Executor/executorSource()|",
      "|java+method:///org/apache/spark/executor/Executor/logInfo(scala.Function0)|",
      "|java+method:///java/lang/Thread/setDefaultUncaughtExceptionHandler(java.lang.Thread$UncaughtExceptionHandler)|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonCachedThreadPool(java.lang.String)|",
      "|java+constructor:///org/apache/spark/executor/Executor$$anonfun$2/Executor$$anonfun$2(org.apache.spark.executor.Executor)|",
      "|java+method:///org/apache/spark/SparkEnv/conf()|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+method:///org/apache/spark/storage/BlockManager/initialize(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getAppId()|",
      "|java+method:///java/nio/ByteBuffer/wrap(byte%5B%5D)|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///org/apache/spark/util/RpcUtils$/makeDriverRef(java.lang.String,org.apache.spark.SparkConf,org.apache.spark.rpc.RpcEnv)|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|",
      "|java+constructor:///org/apache/spark/executor/ExecutorSource/ExecutorSource(java.util.concurrent.ThreadPoolExecutor,java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///org/apache/spark/executor/Executor/addReplClassLoaderIfNeeded(java.lang.ClassLoader)|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/executor/Executor/createClassLoader()|",
      "|java+method:///org/apache/spark/util/Utils$/setCustomHostname(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/getMaxResultSize(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonSingleThreadScheduledExecutor(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkEnv/metricsSystem()|",
      "|java+method:///org/apache/spark/util/Utils$/parseHostPort(java.lang.String)|",
      "|java+method:///org/apache/spark/util/RpcUtils$/makeDriverRef(java.lang.String,org.apache.spark.SparkConf,org.apache.spark.rpc.RpcEnv)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/HeartbeatReceiver$/ENDPOINT_NAME()|",
      "|java+method:///org/apache/spark/SparkEnv/serializer()|",
      "|java+method:///org/apache/spark/executor/Executor/startDriverHeartbeater()|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$replClassLoader()|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$urlClassLoader()|",
      "|java+method:///org/apache/spark/util/Utils$/checkHost(java.lang.String,java.lang.String)|",
      "|java+constructor:///java/util/concurrent/ConcurrentHashMap/ConcurrentHashMap()|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///org/apache/spark/executor/Executor/threadPool()|",
      "|java+method:///org/apache/spark/util/AkkaUtils$/maxFrameSizeBytes(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$conf()|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/registerSource(org.apache.spark.metrics.source.Source)|",
      "|java+method:///org/apache/spark/serializer/Serializer/setDefaultClassLoader(java.lang.ClassLoader)|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|",
      "|java+method:///org/apache/spark/executor/Executor/executorSource()|",
      "|java+method:///org/apache/spark/executor/Executor/logInfo(scala.Function0)|",
      "|java+method:///java/lang/Thread/setDefaultUncaughtExceptionHandler(java.lang.Thread$UncaughtExceptionHandler)|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonCachedThreadPool(java.lang.String)|",
      "|java+constructor:///org/apache/spark/executor/Executor$$anonfun$2/Executor$$anonfun$2(org.apache.spark.executor.Executor)|",
      "|java+method:///org/apache/spark/SparkEnv/conf()|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+method:///org/apache/spark/storage/BlockManager/initialize(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getAppId()|",
      "|java+method:///java/nio/ByteBuffer/wrap(byte%5B%5D)|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|",
      "|java+constructor:///org/apache/spark/executor/ExecutorSource/ExecutorSource(java.util.concurrent.ThreadPoolExecutor,java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///org/apache/spark/executor/Executor/addReplClassLoaderIfNeeded(java.lang.ClassLoader)|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/AkkaUtils$/org$apache$spark$util$AkkaUtils$$doCreateActorSystem(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/Seq/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///akka/actor/ActorSystem$/apply(java.lang.String,com.typesafe.config.Config)|",
      "|java+method:///org/apache/log4j/Logger/getLogger(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/util/AkkaUtils$$anonfun$org$apache$spark$util$AkkaUtils$$doCreateActorSystem$2/AkkaUtils$$anonfun$org$apache$spark$util$AkkaUtils$$doCreateActorSystem$2(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/util/AkkaUtils$/logDebug(scala.Function0)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/SecurityManager/akkaSSLOptions()|",
      "|java+constructor:///java/lang/Exception/Exception(java.lang.String)|",
      "|java+method:///akka/actor/Address/port()|",
      "|java+method:///com/typesafe/config/ConfigFactory/parseString(java.lang.String)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///com/typesafe/config/Config/withFallback(com.typesafe.config.ConfigMergeable)|",
      "|java+method:///org/apache/spark/util/AkkaUtils$/maxFrameSizeBytes(org.apache.spark.SparkConf)|",
      "|java+constructor:///org/apache/spark/util/AkkaUtils$$anonfun$2/AkkaUtils$$anonfun$2()|",
      "|java+method:///org/apache/spark/SSLOptions/createAkkaConfig()|",
      "|java+method:///com/typesafe/config/ConfigFactory/parseMap(java.util.Map)|",
      "|java+method:///akka/actor/ActorRefProvider/getDefaultAddress()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/SecurityManager/isAuthenticationEnabled()|",
      "|java+constructor:///org/apache/spark/util/AkkaUtils$$anonfun$org$apache$spark$util$AkkaUtils$$doCreateActorSystem$1/AkkaUtils$$anonfun$org$apache$spark$util$AkkaUtils$$doCreateActorSystem$1()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsJavaMap(scala.collection.Map)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///akka/actor/ExtendedActorSystem/provider()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkConf/getTimeAsSeconds(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getAkkaConf()|",
      "|java+method:///org/apache/spark/SecurityManager/getSecretKey()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/Seq/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///akka/actor/ActorSystem$/apply(java.lang.String,com.typesafe.config.Config)|",
      "|java+method:///org/apache/log4j/Logger/getLogger(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/util/AkkaUtils$$anonfun$org$apache$spark$util$AkkaUtils$$doCreateActorSystem$2/AkkaUtils$$anonfun$org$apache$spark$util$AkkaUtils$$doCreateActorSystem$2(java.lang.String)|",
      "|java+method:///scala/collection/JavaConverters$/mapAsJavaMapConverter(scala.collection.Map)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/util/AkkaUtils$/logDebug(scala.Function0)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/SecurityManager/akkaSSLOptions()|",
      "|java+constructor:///java/lang/Exception/Exception(java.lang.String)|",
      "|java+method:///akka/actor/Address/port()|",
      "|java+method:///com/typesafe/config/ConfigFactory/parseString(java.lang.String)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///com/typesafe/config/Config/withFallback(com.typesafe.config.ConfigMergeable)|",
      "|java+method:///org/apache/spark/util/AkkaUtils$/maxFrameSizeBytes(org.apache.spark.SparkConf)|",
      "|java+constructor:///org/apache/spark/util/AkkaUtils$$anonfun$2/AkkaUtils$$anonfun$2()|",
      "|java+method:///org/apache/spark/SSLOptions/createAkkaConfig()|",
      "|java+method:///com/typesafe/config/ConfigFactory/parseMap(java.util.Map)|",
      "|java+method:///akka/actor/ActorRefProvider/getDefaultAddress()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/SecurityManager/isAuthenticationEnabled()|",
      "|java+constructor:///org/apache/spark/util/AkkaUtils$$anonfun$org$apache$spark$util$AkkaUtils$$doCreateActorSystem$1/AkkaUtils$$anonfun$org$apache$spark$util$AkkaUtils$$doCreateActorSystem$1()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///akka/actor/ExtendedActorSystem/provider()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkConf/getTimeAsSeconds(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getAkkaConf()|",
      "|java+method:///org/apache/spark/SecurityManager/getSecretKey()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$resourceOffer$2/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$resourceOffer$2/apply()|",
      "|java+method:///java/nio/ByteBuffer/limit()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/Task/partitionId()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$resourceOffer$2/apply()|",
      "|java+method:///java/nio/ByteBuffer/limit()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/scope/RDDOperationNode$/RDDOperationNode$()|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction4/AbstractFunction4()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction3/AbstractFunction3()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction4/AbstractFunction4()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$/blockManagerRemovedFromJson(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/blockManagerIdFromJson(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$55/JsonProtocol$$anonfun$55()|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerBlockManagerRemoved/SparkListenerBlockManagerRemoved(long,org.apache.spark.storage.BlockManagerId)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$6/JsonProtocol$$anonfun$6()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/blockManagerIdFromJson(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$58/JsonProtocol$$anonfun$58()|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerBlockManagerRemoved/SparkListenerBlockManagerRemoved(long,org.apache.spark.storage.BlockManagerId)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$6/JsonProtocol$$anonfun$6()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/NewHadoopRDD/getPartitions()|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/ArrayOps/size()|",
      "|java+method:///org/apache/hadoop/conf/Configurable/setConf(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/rdd/NewHadoopRDD/newJobContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.JobID)|",
      "|java+method:///java/lang/Class/newInstance()|",
      "|java+method:///java/util/List/toArray()|",
      "|java+method:///scala/collection/immutable/Range/foreach$mVc$sp(scala.Function1)|",
      "|java+method:///org/apache/spark/rdd/NewHadoopRDD/jobId()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+constructor:///org/apache/spark/rdd/NewHadoopRDD$$anonfun$getPartitions$1/NewHadoopRDD$$anonfun$getPartitions$1(org.apache.spark.rdd.NewHadoopRDD,java.lang.Object%5B%5D,org.apache.spark.Partition%5B%5D)|",
      "|java+method:///org/apache/hadoop/mapreduce/InputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/ArrayOps/size()|",
      "|java+method:///org/apache/spark/rdd/NewHadoopRDD/_conf()|",
      "|java+method:///org/apache/hadoop/conf/Configurable/setConf(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/rdd/NewHadoopRDD/newJobContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.JobID)|",
      "|java+method:///java/lang/Class/newInstance()|",
      "|java+method:///java/util/List/toArray()|",
      "|java+method:///scala/collection/immutable/Range/foreach$mVc$sp(scala.Function1)|",
      "|java+method:///org/apache/spark/rdd/NewHadoopRDD/jobId()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+constructor:///org/apache/spark/rdd/NewHadoopRDD$$anonfun$getPartitions$1/NewHadoopRDD$$anonfun$getPartitions$1(org.apache.spark.rdd.NewHadoopRDD,java.lang.Object%5B%5D,org.apache.spark.Partition%5B%5D)|",
      "|java+method:///org/apache/hadoop/mapreduce/InputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$executorLost$5/apply(scala.Tuple2)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/executorId()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/executorId()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/running()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$/blockStatusFromJson(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/JsonAST$JValue/toSome()|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/storage/BlockStatus/BlockStatus(org.apache.spark.storage.StorageLevel,long,long,long)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/storageLevelFromJson(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$83/JsonProtocol$$anonfun$83(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/JsonAST$JValue/toSome()|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/storage/BlockStatus/BlockStatus(org.apache.spark.storage.StorageLevel,long,long,long)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/storageLevelFromJson(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$90/JsonProtocol$$anonfun$90(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/isExecutorAlive(java.lang.String)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/activeExecutorIds()|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/org$apache$spark$scheduler$TaskSchedulerImpl$$executorIdToTaskCount()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/createMesosTask(org.apache.spark.scheduler.TaskDescription,java.util.List,java.lang.String)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/createExecutorInfo(java.util.List,java.lang.String)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/addAllResources(java.lang.Iterable)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setExecutor(org.apache.mesos.Protos$ExecutorInfo)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID$Builder/build()|",
      "|java+method:///org/apache/mesos/Protos$TaskID/newBuilder()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/serializedTask()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/name()|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/attemptNumber()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/CPUS_PER_TASK()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo/newBuilder()|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/build()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID/newBuilder()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/partitionResources(java.util.List,java.lang.String,double)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setSlaveId(org.apache.mesos.Protos$SlaveID)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/setValue(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/slaveIdToExecutorInfo()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/taskId()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosTaskLaunchData/MesosTaskLaunchData(java.nio.ByteBuffer,int)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setName(java.lang.String)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/build()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setTaskId(org.apache.mesos.Protos$TaskID)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID$Builder/setValue(java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setData(org.apache.mesos.protobuf.ByteString)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosTaskLaunchData/toByteString()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/createExecutorInfo(java.util.List,java.lang.String)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/addAllResources(java.lang.Iterable)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setExecutor(org.apache.mesos.Protos$ExecutorInfo)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID$Builder/build()|",
      "|java+method:///org/apache/mesos/Protos$TaskID/newBuilder()|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/serializedTask()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/name()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/CPUS_PER_TASK()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo/newBuilder()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/build()|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///org/apache/mesos/Protos$SlaveID/newBuilder()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/partitionResources(java.util.List,java.lang.String,double)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setSlaveId(org.apache.mesos.Protos$SlaveID)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/setValue(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/slaveIdToExecutorInfo()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/taskId()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosTaskLaunchData/MesosTaskLaunchData(java.nio.ByteBuffer,int)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setName(java.lang.String)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/build()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setTaskId(org.apache.mesos.Protos$TaskID)|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/attemptNumber()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID$Builder/setValue(java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setData(org.apache.mesos.protobuf.ByteString)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosTaskLaunchData/toByteString()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$8/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$8/apply(org.apache.spark.scheduler.Stage)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$8/apply(org.apache.spark.scheduler.ActiveJob)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$resourceOffers$1/apply(org.apache.spark.scheduler.WorkerOffer)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/executorAdded(java.lang.String,java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$resourceOffers$1$$anonfun$apply$4/TaskSchedulerImpl$$anonfun$resourceOffers$1$$anonfun$apply$4(org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$1,org.apache.spark.scheduler.WorkerOffer)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/getRackForHost(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/WorkerOffer/host()|",
      "|java+method:///org/apache/spark/scheduler/WorkerOffer/executorId()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/executorIdToHost()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/executorsByHost()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/activeExecutorIds()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/executorsByHost()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/executorAdded(java.lang.String,java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElseUpdate(java.lang.Object,scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$resourceOffers$1$$anonfun$apply$5/TaskSchedulerImpl$$anonfun$resourceOffers$1$$anonfun$apply$5(org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$1,org.apache.spark.scheduler.WorkerOffer)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/getRackForHost(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/WorkerOffer/host()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$resourceOffers$1$$anonfun$apply$2/TaskSchedulerImpl$$anonfun$resourceOffers$1$$anonfun$apply$2(org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$1)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/org$apache$spark$scheduler$TaskSchedulerImpl$$executorIdToTaskCount()|",
      "|java+method:///org/apache/spark/scheduler/WorkerOffer/executorId()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/executorIdToHost()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/client/AppClient/requestTotalExecutors(int)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$$anonfun$requestTotalExecutors$1/AppClient$$anonfun$requestTotalExecutors$1(org.apache.spark.deploy.client.AppClient)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$RequestExecutors/DeployMessages$RequestExecutors(java.lang.String,int)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/endpoint()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/logWarning(scala.Function0)|",
      "|java+method:///scala/reflect/ClassTag$/Boolean()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/org$apache$spark$deploy$client$AppClient$$appId()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$$anonfun$requestTotalExecutors$1/AppClient$$anonfun$requestTotalExecutors$1(org.apache.spark.deploy.client.AppClient)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$RequestExecutors/DeployMessages$RequestExecutors(java.lang.String,int)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///java/util/concurrent/atomic/AtomicReference/get()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/endpoint()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/logWarning(scala.Function0)|",
      "|java+method:///scala/reflect/ClassTag$/Boolean()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/org$apache$spark$deploy$client$AppClient$$appId()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/metrics/MetricsConfig$$anonfun$initialize$4/apply(scala.Tuple2)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/propertiesAsScalaMap(java.util.Properties)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Map/withFilter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$initialize$4$$anonfun$apply$1/MetricsConfig$$anonfun$initialize$4$$anonfun$apply$1(org.apache.spark.metrics.MetricsConfig$$anonfun$initialize$4)|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$initialize$4$$anonfun$apply$2/MetricsConfig$$anonfun$initialize$4$$anonfun$apply$2(org.apache.spark.metrics.MetricsConfig$$anonfun$initialize$4,java.util.Properties)|",
      "|java+method:///scala/collection/generic/FilterMonadic/withFilter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$initialize$4$$anonfun$apply$3/MetricsConfig$$anonfun$initialize$4$$anonfun$apply$3(org.apache.spark.metrics.MetricsConfig$$anonfun$initialize$4,java.util.Properties)|"
    ],
    "v2Body": [
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Map/withFilter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$initialize$4$$anonfun$apply$1/MetricsConfig$$anonfun$initialize$4$$anonfun$apply$1(org.apache.spark.metrics.MetricsConfig$$anonfun$initialize$4)|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$initialize$4$$anonfun$apply$2/MetricsConfig$$anonfun$initialize$4$$anonfun$apply$2(org.apache.spark.metrics.MetricsConfig$$anonfun$initialize$4,java.util.Properties)|",
      "|java+method:///scala/collection/generic/FilterMonadic/withFilter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$initialize$4$$anonfun$apply$3/MetricsConfig$$anonfun$initialize$4$$anonfun$apply$3(org.apache.spark.metrics.MetricsConfig$$anonfun$initialize$4,java.util.Properties)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/FileSystemPersistenceEngine/unpersist(java.lang.String)|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/master/FileSystemPersistenceEngine/dir()|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///java/io/File/delete()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/master/FileSystemPersistenceEngine/dir()|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///org/apache/spark/deploy/master/FileSystemPersistenceEngine$$anonfun$unpersist$1/FileSystemPersistenceEngine$$anonfun$unpersist$1(org.apache.spark.deploy.master.FileSystemPersistenceEngine,java.io.File)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/FileSystemPersistenceEngine/logWarning(scala.Function0)|",
      "|java+method:///java/io/File/delete()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$115/TaskDataSource$$anonfun$115(org.apache.spark.ui.jobs.TaskDataSource)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/Utils$$anonfun$nonLocalPaths$1/apply(java.lang.String)|",
    "called": "|java+method:///scala/Option/getOrElse(scala.Function0)|",
    "v1Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/util/matching/Regex/unapplySeq(java.lang.Object)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$nonLocalPaths$1$$anonfun$10/Utils$$anonfun$nonLocalPaths$1$$anonfun$10(org.apache.spark.util.Utils$$anonfun$nonLocalPaths$1)|",
      "|java+method:///java/net/URI/getScheme()|",
      "|java+method:///scala/collection/LinearSeqOptimized/lengthCompare(int)|",
      "|java+method:///org/apache/spark/util/Utils$/windowsDrive()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/util/Utils$/resolveURI(java.lang.String)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/util/matching/Regex/unapplySeq(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$nonLocalPaths$1$$anonfun$9/Utils$$anonfun$nonLocalPaths$1$$anonfun$9(org.apache.spark.util.Utils$$anonfun$nonLocalPaths$1)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///java/net/URI/getScheme()|",
      "|java+method:///scala/collection/LinearSeqOptimized/lengthCompare(int)|",
      "|java+method:///org/apache/spark/util/Utils$/windowsDrive()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/util/Utils$/resolveURI(java.lang.String)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/PythonHadoopUtil$/mapToConf(java.util.Map)|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///scala/collection/mutable/Map/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/api/python/PythonHadoopUtil$$anonfun$mapToConf$1/PythonHadoopUtil$$anonfun$mapToConf$1(org.apache.hadoop.conf.Configuration)|",
      "|java+constructor:///org/apache/hadoop/conf/Configuration/Configuration()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+constructor:///org/apache/spark/api/python/PythonHadoopUtil$$anonfun$mapToConf$1/PythonHadoopUtil$$anonfun$mapToConf$1(org.apache.hadoop.conf.Configuration)|",
      "|java+constructor:///org/apache/hadoop/conf/Configuration/Configuration()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskLocation$/apply(java.lang.String)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/HostTaskLocation/HostTaskLocation(java.lang.String)|",
      "|java+method:///java/lang/String/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskLocation$/inMemoryLocationTag()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripPrefix(java.lang.String)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/scheduler/ExecutorCacheTaskLocation/ExecutorCacheTaskLocation(java.lang.String,java.lang.String)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/TaskLocation$/executorLocationTag()|",
      "|java+constructor:///org/apache/spark/scheduler/HostTaskLocation/HostTaskLocation(java.lang.String)|",
      "|java+method:///java/lang/String/equals(java.lang.Object)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///org/apache/spark/scheduler/HDFSCacheTaskLocation/HDFSCacheTaskLocation(java.lang.String)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripPrefix(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/scheduler/TaskLocation$/inMemoryLocationTag()|",
      "|java+method:///java/lang/String/startsWith(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$49/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/shuffleReadMetrics()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$49$$anonfun$apply$21/StagePage$$anonfun$49$$anonfun$apply$21(org.apache.spark.ui.jobs.StagePage$$anonfun$49)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$49$$anonfun$apply$8/StagePage$$anonfun$49$$anonfun$apply$8(org.apache.spark.ui.jobs.StagePage$$anonfun$49)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$49$$anonfun$apply$17/StagePage$$anonfun$49$$anonfun$apply$17(org.apache.spark.ui.jobs.StagePage$$anonfun$49)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/outputMetrics()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$49$$anonfun$apply$4/StagePage$$anonfun$49$$anonfun$apply$4(org.apache.spark.ui.jobs.StagePage$$anonfun$49)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/r/RRDD$/createRProcess(int,java.lang.String)|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/lang/ProcessBuilder/redirectErrorStream(boolean)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/api/r/RUtils$/sparkRPackagePath(boolean)|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/r/RRDD$/startStdoutThread(java.lang.Process)|",
      "|java+constructor:///java/lang/ProcessBuilder/ProcessBuilder(java.util.List)|",
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/SparkEnv/conf()|",
      "|java+method:///java/lang/ProcessBuilder/environment()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/util/Arrays/asList(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/api/r/RUtils$/sparkRPackagePath(boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/r/RRDD$/startStdoutThread(java.lang.Process)|",
      "|java+constructor:///java/lang/ProcessBuilder/ProcessBuilder(java.util.List)|",
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/Seq/apply(int)|",
      "|java+method:///java/lang/ProcessBuilder/redirectErrorStream(boolean)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/SparkEnv/conf()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///java/lang/ProcessBuilder/environment()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$/blockManagerAddedFromJson(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/blockManagerIdFromJson(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$54/JsonProtocol$$anonfun$54()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$5/JsonProtocol$$anonfun$5()|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerBlockManagerAdded/SparkListenerBlockManagerAdded(long,org.apache.spark.storage.BlockManagerId,long)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/blockManagerIdFromJson(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$57/JsonProtocol$$anonfun$57()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$5/JsonProtocol$$anonfun$5()|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerBlockManagerAdded/SparkListenerBlockManagerAdded(long,org.apache.spark.storage.BlockManagerId,long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$taskMetricsFromJson$5/apply(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$taskMetricsFromJson$5$$anonfun$apply$15/JsonProtocol$$anonfun$taskMetricsFromJson$5$$anonfun$apply$15(org.apache.spark.util.JsonProtocol$$anonfun$taskMetricsFromJson$5)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class,scala.reflect.Manifest,scala.collection.Seq)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class,scala.reflect.Manifest,scala.collection.Seq)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$taskMetricsFromJson$5$$anonfun$apply$16/JsonProtocol$$anonfun$taskMetricsFromJson$5$$anonfun$apply$16(org.apache.spark.util.JsonProtocol$$anonfun$taskMetricsFromJson$5)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SSLOptions/createAkkaConfig()|",
    "called": "|java+method:///scala/collection/JavaConverters$/setAsJavaSetConverter(scala.collection.Set)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SSLOptions/keyPassword()|",
      "|java+method:///org/apache/spark/SSLOptions/protocol()|",
      "|java+method:///org/apache/spark/SSLOptions/enabled()|",
      "|java+method:///org/apache/spark/SSLOptions/keyStorePassword()|",
      "|java+method:///com/typesafe/config/ConfigFactory/empty()|",
      "|java+method:///org/apache/spark/SSLOptions/keyStore()|",
      "|java+method:///com/typesafe/config/Config/withValue(java.lang.String,com.typesafe.config.ConfigValue)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/SSLOptions/supportedAlgorithms()|",
      "|java+method:///scala/collection/immutable/Set/toSeq()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/SSLOptions$$anonfun$createAkkaConfig$7/SSLOptions$$anonfun$createAkkaConfig$7(org.apache.spark.SSLOptions)|",
      "|java+constructor:///org/apache/spark/SSLOptions$$anonfun$createAkkaConfig$2/SSLOptions$$anonfun$createAkkaConfig$2(org.apache.spark.SSLOptions)|",
      "|java+constructor:///org/apache/spark/SSLOptions$$anonfun$createAkkaConfig$6/SSLOptions$$anonfun$createAkkaConfig$6(org.apache.spark.SSLOptions)|",
      "|java+constructor:///org/apache/spark/SSLOptions$$anonfun$createAkkaConfig$5/SSLOptions$$anonfun$createAkkaConfig$5(org.apache.spark.SSLOptions)|",
      "|java+method:///org/apache/spark/SSLOptions/trustStorePassword()|",
      "|java+constructor:///org/apache/spark/SSLOptions$$anonfun$createAkkaConfig$4/SSLOptions$$anonfun$createAkkaConfig$4(org.apache.spark.SSLOptions)|",
      "|java+constructor:///org/apache/spark/SSLOptions$$anonfun$createAkkaConfig$8/SSLOptions$$anonfun$createAkkaConfig$8(org.apache.spark.SSLOptions)|",
      "|java+constructor:///org/apache/spark/SSLOptions$$anonfun$createAkkaConfig$3/SSLOptions$$anonfun$createAkkaConfig$3(org.apache.spark.SSLOptions)|",
      "|java+constructor:///org/apache/spark/SSLOptions$$anonfun$createAkkaConfig$1/SSLOptions$$anonfun$createAkkaConfig$1(org.apache.spark.SSLOptions)|",
      "|java+method:///com/typesafe/config/ConfigValueFactory/fromIterable(java.lang.Iterable)|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SSLOptions/trustStore()|",
      "|java+method:///com/typesafe/config/ConfigValueFactory/fromAnyRef(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SSLOptions/keyPassword()|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///org/apache/spark/SSLOptions/protocol()|",
      "|java+method:///org/apache/spark/SSLOptions/enabled()|",
      "|java+method:///com/typesafe/config/ConfigFactory/empty()|",
      "|java+method:///org/apache/spark/SSLOptions/keyStore()|",
      "|java+method:///com/typesafe/config/Config/withValue(java.lang.String,com.typesafe.config.ConfigValue)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/SSLOptions/trustStore()|",
      "|java+method:///org/apache/spark/SSLOptions/supportedAlgorithms()|",
      "|java+method:///org/apache/spark/SSLOptions/keyStorePassword()|",
      "|java+method:///scala/collection/JavaConverters$/setAsJavaSetConverter(scala.collection.Set)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/SSLOptions$$anonfun$createAkkaConfig$7/SSLOptions$$anonfun$createAkkaConfig$7(org.apache.spark.SSLOptions)|",
      "|java+constructor:///org/apache/spark/SSLOptions$$anonfun$createAkkaConfig$2/SSLOptions$$anonfun$createAkkaConfig$2(org.apache.spark.SSLOptions)|",
      "|java+constructor:///org/apache/spark/SSLOptions$$anonfun$createAkkaConfig$6/SSLOptions$$anonfun$createAkkaConfig$6(org.apache.spark.SSLOptions)|",
      "|java+constructor:///org/apache/spark/SSLOptions$$anonfun$createAkkaConfig$5/SSLOptions$$anonfun$createAkkaConfig$5(org.apache.spark.SSLOptions)|",
      "|java+method:///org/apache/spark/SSLOptions/trustStorePassword()|",
      "|java+constructor:///org/apache/spark/SSLOptions$$anonfun$createAkkaConfig$4/SSLOptions$$anonfun$createAkkaConfig$4(org.apache.spark.SSLOptions)|",
      "|java+constructor:///org/apache/spark/SSLOptions$$anonfun$createAkkaConfig$8/SSLOptions$$anonfun$createAkkaConfig$8(org.apache.spark.SSLOptions)|",
      "|java+constructor:///org/apache/spark/SSLOptions$$anonfun$createAkkaConfig$3/SSLOptions$$anonfun$createAkkaConfig$3(org.apache.spark.SSLOptions)|",
      "|java+constructor:///org/apache/spark/SSLOptions$$anonfun$createAkkaConfig$1/SSLOptions$$anonfun$createAkkaConfig$1(org.apache.spark.SSLOptions)|",
      "|java+method:///com/typesafe/config/ConfigValueFactory/fromIterable(java.lang.Iterable)|",
      "|java+method:///com/typesafe/config/ConfigValueFactory/fromAnyRef(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/resourceOffers(org.apache.mesos.SchedulerDriver,java.util.List)|",
    "called": "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/HashMap/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/Buffer/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/logTrace(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$resourceOffers$6/MesosClusterScheduler$$anonfun$resourceOffers$6(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,org.apache.mesos.SchedulerDriver)|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$resourceOffers$4/MesosClusterScheduler$$anonfun$resourceOffers$4(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,org.apache.mesos.SchedulerDriver)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$resourceOffers$2/MesosClusterScheduler$$anonfun$resourceOffers$2(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$queuedDrivers()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$pendingRetryDrivers()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/filter(scala.Function1)|",
      "|java+method:///scala/collection/TraversableOnce/toList()|",
      "|java+method:///scala/collection/mutable/Buffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$13/MesosClusterScheduler$$anonfun$13(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,java.util.Date)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/stateLock()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$resourceOffers$5/MesosClusterScheduler$$anonfun$resourceOffers$5(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,scala.collection.mutable.HashMap)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$resourceOffers$1/MesosClusterScheduler$$anonfun$resourceOffers$1(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,scala.collection.immutable.List)|",
      "|java+constructor:///java/util/Date/Date()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$12/MesosClusterScheduler$$anonfun$12(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$resourceOffers$3/MesosClusterScheduler$$anonfun$resourceOffers$3(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/copyBuffer(scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/scheduleTasks(scala.collection.Seq,scala.Function1,scala.collection.immutable.List,scala.collection.mutable.HashMap)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///scala/collection/mutable/HashMap/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$14/MesosClusterScheduler$$anonfun$14(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,java.util.Date)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/stateLock()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/logTrace(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$resourceOffers$6/MesosClusterScheduler$$anonfun$resourceOffers$6(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,org.apache.mesos.SchedulerDriver)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$resourceOffers$4/MesosClusterScheduler$$anonfun$resourceOffers$4(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,org.apache.mesos.SchedulerDriver)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$13/MesosClusterScheduler$$anonfun$13(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$resourceOffers$2/MesosClusterScheduler$$anonfun$resourceOffers$2(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$pendingRetryDrivers()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/filter(scala.Function1)|",
      "|java+method:///scala/collection/TraversableOnce/toList()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$resourceOffers$5/MesosClusterScheduler$$anonfun$resourceOffers$5(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,scala.collection.mutable.HashMap)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$resourceOffers$1/MesosClusterScheduler$$anonfun$resourceOffers$1(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,scala.collection.immutable.List)|",
      "|java+method:///scala/collection/TraversableLike/filter(scala.Function1)|",
      "|java+constructor:///java/util/Date/Date()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$queuedDrivers()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$resourceOffers$3/MesosClusterScheduler$$anonfun$resourceOffers$3(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/copyBuffer(scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/scheduleTasks(scala.collection.Seq,scala.Function1,scala.collection.immutable.List,scala.collection.mutable.HashMap)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler/updateJobIdStageIdMapsList$1(scala.collection.immutable.List,int)|",
    "called": "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///scala/collection/immutable/List/tail()|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElseUpdate(java.lang.Object,scala.Function0)|",
      "|java+method:///scala/collection/immutable/List/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$updateJobIdStageIdMapsList$1$1/DAGScheduler$$anonfun$updateJobIdStageIdMapsList$1$1(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/getParentStages(org.apache.spark.rdd.RDD,int)|",
      "|java+method:///org/apache/spark/scheduler/Stage/rdd()|",
      "|java+method:///org/apache/spark/scheduler/Stage/id()|",
      "|java+method:///org/apache/spark/scheduler/Stage/jobIds()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$8/DAGScheduler$$anonfun$8(org.apache.spark.scheduler.DAGScheduler,int)|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/jobIdToStageIds()|",
      "|java+method:///scala/collection/immutable/List/filter(scala.Function1)|",
      "|java+method:///scala/collection/immutable/List/head()|",
      "|java+method:///scala/collection/immutable/List/nonEmpty()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///scala/collection/immutable/List/tail()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$6/DAGScheduler$$anonfun$6(org.apache.spark.scheduler.DAGScheduler,int)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElseUpdate(java.lang.Object,scala.Function0)|",
      "|java+method:///scala/collection/immutable/List/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$updateJobIdStageIdMapsList$1$1/DAGScheduler$$anonfun$updateJobIdStageIdMapsList$1$1(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/getParentStages(org.apache.spark.rdd.RDD,int)|",
      "|java+method:///org/apache/spark/scheduler/Stage/rdd()|",
      "|java+method:///org/apache/spark/scheduler/Stage/id()|",
      "|java+method:///org/apache/spark/scheduler/Stage/jobIds()|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/jobIdToStageIds()|",
      "|java+method:///scala/collection/immutable/List/filter(scala.Function1)|",
      "|java+method:///scala/collection/immutable/List/head()|",
      "|java+method:///scala/collection/immutable/List/nonEmpty()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$visit$2$1/apply(org.apache.spark.Dependency)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/shuffleToMapStage()|",
      "|java+method:///org/apache/spark/Dependency/rdd()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/ShuffleDependency/rdd()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/ShuffleDependency/shuffleId()|",
      "|java+method:///scala/collection/mutable/Stack/push(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/shuffleToMapStage()|",
      "|java+method:///org/apache/spark/Dependency/rdd()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/ShuffleDependency/shuffleId()|",
      "|java+method:///scala/collection/mutable/Stack/push(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$5/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/name()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$5/apply()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$5/apply()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$4/apply()|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$4/apply()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/name()|",
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$4/apply()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/id()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$treeAggregate$1/apply()|",
    "called": "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions$default$2()|",
      "|java+constructor:///org/apache/spark/HashPartitioner/HashPartitioner(int)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitionsWithIndex(scala.Function2,boolean,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/util/Utils$/clone(java.lang.Object,org.apache.spark.serializer.SerializerInstance,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkEnv/closureSerializer()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeAggregate$1$$anonfun$24/RDD$$anonfun$treeAggregate$1$$anonfun$24(org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1,scala.Function1)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeAggregate$1$$anonfun$25/RDD$$anonfun$treeAggregate$1$$anonfun$25(org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1,int)|",
      "|java+method:///scala/math/package$/pow(double,double)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+method:///scala/math/package$/ceil(double)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/reduceByKey(org.apache.spark.Partitioner,scala.Function2)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/values()|",
      "|java+method:///scala/reflect/ClassTag$/Int()|",
      "|java+method:///scala/math/package$/max(int,int)|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///org/apache/spark/rdd/RDD/context()|",
      "|java+method:///org/apache/spark/serializer/Serializer/newInstance()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeAggregate$1$$anonfun$23/RDD$$anonfun$treeAggregate$1$$anonfun$23(org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1,scala.Function2,scala.Function2)|",
      "|java+method:///org/apache/spark/rdd/RDD/reduce(scala.Function2)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitionsWithIndex$default$2()|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeAggregate$1$$anonfun$apply$39/RDD$$anonfun$treeAggregate$1$$anonfun$apply$39(org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions$default$2()|",
      "|java+constructor:///org/apache/spark/HashPartitioner/HashPartitioner(int)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitionsWithIndex(scala.Function2,boolean,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/util/Utils$/clone(java.lang.Object,org.apache.spark.serializer.SerializerInstance,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkEnv/closureSerializer()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeAggregate$1$$anonfun$24/RDD$$anonfun$treeAggregate$1$$anonfun$24(org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1,scala.Function1)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeAggregate$1$$anonfun$25/RDD$$anonfun$treeAggregate$1$$anonfun$25(org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1,int)|",
      "|java+method:///scala/math/package$/pow(double,double)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+method:///scala/math/package$/ceil(double)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/reduceByKey(org.apache.spark.Partitioner,scala.Function2)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/values()|",
      "|java+method:///scala/reflect/ClassTag$/Int()|",
      "|java+method:///scala/math/package$/max(int,int)|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///org/apache/spark/rdd/RDD/context()|",
      "|java+method:///org/apache/spark/serializer/Serializer/newInstance()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeAggregate$1$$anonfun$23/RDD$$anonfun$treeAggregate$1$$anonfun$23(org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1,scala.Function2,scala.Function2)|",
      "|java+method:///org/apache/spark/rdd/RDD/reduce(scala.Function2)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitionsWithIndex$default$2()|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeAggregate$1$$anonfun$apply$43/RDD$$anonfun$treeAggregate$1$$anonfun$apply$43(org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/ZookeeperMesosClusterPersistenceEngine/fetchAll()|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/ZookeeperMesosClusterPersistenceEngine/WORKING_DIR()|",
      "|java+method:///org/apache/curator/framework/CuratorFramework/getChildren()|",
      "|java+method:///org/apache/curator/framework/api/GetChildrenBuilder/forPath(java.lang.String)|",
      "|java+method:///scala/collection/mutable/Buffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/ZookeeperMesosClusterPersistenceEngine$$anonfun$fetchAll$1/ZookeeperMesosClusterPersistenceEngine$$anonfun$fetchAll$1(org.apache.spark.scheduler.cluster.mesos.ZookeeperMesosClusterPersistenceEngine)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/ZookeeperMesosClusterPersistenceEngine$$anonfun$fetchAll$2/ZookeeperMesosClusterPersistenceEngine$$anonfun$fetchAll$2(org.apache.spark.scheduler.cluster.mesos.ZookeeperMesosClusterPersistenceEngine)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/collection/generic/GenericTraversableTemplate/flatten(scala.Function1)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/ZookeeperMesosClusterPersistenceEngine/WORKING_DIR()|",
      "|java+method:///org/apache/curator/framework/CuratorFramework/getChildren()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/ZookeeperMesosClusterPersistenceEngine$$anonfun$fetchAll$1/ZookeeperMesosClusterPersistenceEngine$$anonfun$fetchAll$1(org.apache.spark.scheduler.cluster.mesos.ZookeeperMesosClusterPersistenceEngine)|",
      "|java+method:///scala/collection/TraversableLike/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/curator/framework/api/GetChildrenBuilder/forPath(java.lang.String)|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$54/apply(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/collection/Seq$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class,scala.reflect.Manifest,scala.collection.Seq)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$54$$anonfun$apply$13/JsonProtocol$$anonfun$54$$anonfun$apply$13(org.apache.spark.util.JsonProtocol$$anonfun$54)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisteredExecutor$/CoarseGrainedClusterMessages$RegisteredExecutor$()|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+method:///scala/Product$class/$init$(scala.Product)|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$1/CoarseMesosSchedulerBackend$$anonfun$1(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$intersection$2/apply()|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/reflect/ClassTag$/Null()|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$intersection$2/apply()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/cogroup(org.apache.spark.rdd.RDD,org.apache.spark.Partitioner)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/keys()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$intersection$2$$anonfun$apply$12/RDD$$anonfun$intersection$2$$anonfun$apply$12(org.apache.spark.rdd.RDD$$anonfun$intersection$2)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$intersection$2$$anonfun$apply$13/RDD$$anonfun$intersection$2$$anonfun$apply$13(org.apache.spark.rdd.RDD$$anonfun$intersection$2)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$intersection$2$$anonfun$apply$14/RDD$$anonfun$intersection$2$$anonfun$apply$14(org.apache.spark.rdd.RDD$$anonfun$intersection$2)|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+method:///org/apache/spark/rdd/RDD/filter(scala.Function1)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$intersection$2$$anonfun$apply$16/RDD$$anonfun$intersection$2$$anonfun$apply$16(org.apache.spark.rdd.RDD$$anonfun$intersection$2)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$intersection$2$$anonfun$apply$17/RDD$$anonfun$intersection$2$$anonfun$apply$17(org.apache.spark.rdd.RDD$$anonfun$intersection$2)|",
      "|java+method:///scala/reflect/ClassTag$/Null()|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$intersection$2/apply()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/cogroup(org.apache.spark.rdd.RDD,org.apache.spark.Partitioner)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/keys()|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$intersection$2$$anonfun$apply$15/RDD$$anonfun$intersection$2$$anonfun$apply$15(org.apache.spark.rdd.RDD$$anonfun$intersection$2)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+method:///org/apache/spark/rdd/RDD/filter(scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$9/apply(java.lang.String)|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+method:///org/apache/mesos/Protos$ContainerInfo$Builder/build()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackendUtil$/addDockerInfo$default$4()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/getContainerBuilder()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$9$$anonfun$9/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$9$$anonfun$9(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$9)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$9$$anonfun$10/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$9$$anonfun$10(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$9)|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackendUtil$/addDockerInfo(org.apache.mesos.Protos$ContainerInfo$Builder,java.lang.String,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/schedulerProperties()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setContainer(org.apache.mesos.Protos$ContainerInfo)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$9$$anonfun$10/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$9$$anonfun$10(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$9)|",
      "|java+method:///org/apache/mesos/Protos$ContainerInfo$Builder/build()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$9$$anonfun$11/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$9$$anonfun$11(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$9)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackendUtil$/addDockerInfo$default$4()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/getContainerBuilder()|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackendUtil$/addDockerInfo(org.apache.mesos.Protos$ContainerInfo$Builder,java.lang.String,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/schedulerProperties()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setContainer(org.apache.mesos.Protos$ContainerInfo)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$77/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$77/apply(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$77/apply(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/ClosureCleaner$/org$apache$spark$util$ClosureCleaner$$clean(java.lang.Object,boolean,boolean,scala.collection.mutable.Map)|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/ClosureCleaner$/org$apache$spark$util$ClosureCleaner$$isClosure(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$5/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$5(java.lang.reflect.Method%5B%5D)|",
      "|java+method:///java/lang/reflect/Field/setAccessible(boolean)|",
      "|java+method:///java/lang/Class/getDeclaredMethods()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$1/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$1(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$2/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$2(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/org$apache$spark$util$ClosureCleaner$$getOuterClassesAndObjects(java.lang.Object)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14(scala.collection.mutable.Map)|",
      "|java+method:///scala/collection/immutable/List/withFilter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$16/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$16(scala.collection.mutable.Map)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/logDebug(scala.Function0)|",
      "|java+method:///scala/collection/immutable/List/reverse()|",
      "|java+method:///scala/collection/SetLike/contains(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/List/tail()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$4/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$4()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///java/lang/Class/getDeclaredField(java.lang.String)|",
      "|java+method:///scala/collection/immutable/List/foreach(scala.Function1)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/immutable/List/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/Map/contains(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$12/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$12()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$17/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$17()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$13/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$13()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$20/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$20()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$6/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$6()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$21/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$21()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$10/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$10()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$9/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$9(scala.collection.immutable.List)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/ensureSerializable(java.lang.Object)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$23/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$23(scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$8/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$8()|",
      "|java+method:///scala/collection/mutable/Map/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$19/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$19(scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$18/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$18(scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$7/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$7(scala.collection.immutable.List)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$22/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$22(boolean,scala.collection.mutable.Map,scala.runtime.ObjectRef)|",
      "|java+method:///scala/collection/mutable/Map/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$11/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$11(scala.collection.immutable.List)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$3/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$3(java.lang.reflect.Field%5B%5D)|",
      "|java+method:///scala/collection/immutable/List/head()|",
      "|java+method:///java/lang/reflect/Field/set(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/immutable/List/size()|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getClassReader(java.lang.Class)|",
      "|java+method:///com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/ClassReader/accept(com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassVisitor,int)|",
      "|java+method:///scala/collection/immutable/List/$colon$colon(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ReturnStatementFinder/ReturnStatementFinder()|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/logWarning(scala.Function0)|",
      "|java+method:///scala/collection/mutable/Map/isEmpty()|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$24/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$24(java.lang.Object)|",
      "|java+method:///java/lang/Class/getDeclaredFields()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$15/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$15(boolean,scala.collection.mutable.Map)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getInnerClosureClasses(java.lang.Object)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/ClosureCleaner$/org$apache$spark$util$ClosureCleaner$$isClosure(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$5/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$5(java.lang.reflect.Method%5B%5D)|",
      "|java+method:///java/lang/reflect/Field/setAccessible(boolean)|",
      "|java+method:///java/lang/Class/getDeclaredMethods()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$1/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$1(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$2/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$2(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getClassReader(java.lang.Class)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/org$apache$spark$util$ClosureCleaner$$getOuterClassesAndObjects(java.lang.Object)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14(scala.collection.mutable.Map)|",
      "|java+method:///scala/collection/immutable/List/withFilter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$16/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$16(scala.collection.mutable.Map)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/logDebug(scala.Function0)|",
      "|java+method:///scala/collection/immutable/List/reverse()|",
      "|java+method:///scala/collection/SetLike/contains(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/List/tail()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$4/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$4()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///java/lang/Class/getDeclaredField(java.lang.String)|",
      "|java+method:///scala/collection/immutable/List/foreach(scala.Function1)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/immutable/List/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/Map/contains(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$12/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$12()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$17/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$17()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$13/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$13()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$20/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$20()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$6/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$6()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$21/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$21()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$10/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$10()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$9/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$9(scala.collection.immutable.List)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/ensureSerializable(java.lang.Object)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$23/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$23(scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$8/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$8()|",
      "|java+method:///scala/collection/mutable/Map/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$19/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$19(scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$18/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$18(scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$7/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$7(scala.collection.immutable.List)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$22/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$22(boolean,scala.collection.mutable.Map,scala.runtime.ObjectRef)|",
      "|java+method:///scala/collection/mutable/Map/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$11/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$11(scala.collection.immutable.List)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$3/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$3(java.lang.reflect.Field%5B%5D)|",
      "|java+method:///scala/collection/immutable/List/head()|",
      "|java+method:///java/lang/reflect/Field/set(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/immutable/List/size()|",
      "|java+method:///scala/collection/immutable/List/$colon$colon(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/ReturnStatementFinder/ReturnStatementFinder()|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/logWarning(scala.Function0)|",
      "|java+method:///scala/collection/mutable/Map/isEmpty()|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$24/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$24(java.lang.Object)|",
      "|java+method:///java/lang/Class/getDeclaredFields()|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$15/ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$15(boolean,scala.collection.mutable.Map)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getInnerClosureClasses(java.lang.Object)|",
      "|java+method:///org/apache/xbean/asm5/ClassReader/accept(org.apache.xbean.asm5.ClassVisitor,int)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/TestUtils$$anonfun$7/TestUtils$$anonfun$7()|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource/TaskDataSource(scala.collection.Seq,boolean,boolean,boolean,boolean,boolean,boolean,long,int,java.lang.String,boolean)|",
    "called": "|java+method:///scala/collection/Seq$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource/ordering(java.lang.String,boolean)|",
      "|java+constructor:///org/apache/spark/ui/PagedDataSource/PagedDataSource(int)|",
      "|java+method:///scala/collection/SeqLike/sorted(scala.math.Ordering)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$67/TaskDataSource$$anonfun$67(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource/ordering(java.lang.String,boolean)|",
      "|java+constructor:///org/apache/spark/ui/PagedDataSource/PagedDataSource(int)|",
      "|java+method:///scala/collection/SeqLike/sorted(scala.math.Ordering)|",
      "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$71/TaskDataSource$$anonfun$71(org.apache.spark.ui.jobs.TaskDataSource)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/getFileSystemThreadStatistics()|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/collection/mutable/Buffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/deploy/SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1/SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1(org.apache.spark.deploy.SparkHadoopUtil)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/getAllStatistics()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/deploy/SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1/SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1(org.apache.spark.deploy.SparkHadoopUtil)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/getAllStatistics()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext/getExecutorThreadDump(java.lang.String)|",
    "called": "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///org/apache/spark/storage/BlockManager/master()|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|",
      "|java+method:///org/apache/spark/SparkContext$/DRIVER_IDENTIFIER()|",
      "|java+method:///org/apache/spark/util/Utils$/getThreadDump()|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMaster/getRpcHostPortForExecutor(java.lang.String)|",
      "|java+method:///org/apache/spark/executor/ExecutorEndpoint$/EXECUTOR_ENDPOINT_NAME()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/SparkEnv$/executorActorSystemName()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$getExecutorThreadDump$1/SparkContext$$anonfun$getExecutorThreadDump$1(org.apache.spark.SparkContext,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/rpc/RpcAddress/RpcAddress(java.lang.String,int)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpointRef(java.lang.String,org.apache.spark.rpc.RpcAddress,java.lang.String)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///org/apache/spark/storage/BlockManager/master()|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|",
      "|java+method:///org/apache/spark/SparkContext$/DRIVER_IDENTIFIER()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Utils$/getThreadDump()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMaster/getExecutorEndpointRef(java.lang.String)|",
      "|java+method:///scala/Option/get()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$getExecutorThreadDump$1/SparkContext$$anonfun$getExecutorThreadDump$1(org.apache.spark.SparkContext,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/env()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master$/startRpcEnvAndEndpoint(java.lang.String,int,int,org.apache.spark.SparkConf)|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/deploy/master/MasterMessages$BoundPortsResponse/restPort()|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master/Master(org.apache.spark.rpc.RpcEnv,org.apache.spark.rpc.RpcAddress,int,org.apache.spark.SecurityManager,org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/master/MasterMessages$BoundPortsResponse/webUIPort()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/master/Master$/ENDPOINT_NAME()|",
      "|java+constructor:///scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///org/apache/spark/deploy/master/Master$/SYSTEM_NAME()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/address()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/deploy/master/MasterMessages$BoundPortsResponse/restPort()|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create$default$6()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master/Master(org.apache.spark.rpc.RpcEnv,org.apache.spark.rpc.RpcAddress,int,org.apache.spark.SecurityManager,org.apache.spark.SparkConf)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/master/Master$/ENDPOINT_NAME()|",
      "|java+constructor:///scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,boolean)|",
      "|java+method:///org/apache/spark/deploy/master/MasterMessages$BoundPortsResponse/webUIPort()|",
      "|java+method:///org/apache/spark/deploy/master/Master$/SYSTEM_NAME()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/address()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$5/apply()|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/address()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/toSparkURL()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$5/apply()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$5/apply()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/input/PortableDataStream/open()|",
    "called": "|java+method:///scala/Predef$/Integer2int(java.lang.Integer)|",
    "v1Body": [
      "|java+method:///org/apache/spark/input/PortableDataStream/isOpen()|",
      "|java+method:///org/apache/spark/input/PortableDataStream/fileIn()|",
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/CombineFileSplit/getPath(int)|",
      "|java+method:///org/apache/spark/input/PortableDataStream/isOpen_$eq(boolean)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/open(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/input/PortableDataStream/fileIn_$eq(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/input/PortableDataStream/split()|",
      "|java+method:///scala/Predef$/Integer2int(java.lang.Integer)|",
      "|java+method:///org/apache/spark/input/PortableDataStream/conf()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/CombineFileSplit/getPath(int)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/open(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/input/PortableDataStream/split()|",
      "|java+method:///scala/Predef$/Integer2int(java.lang.Integer)|",
      "|java+method:///org/apache/spark/input/PortableDataStream/conf()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor()|",
    "called": "|java+method:///scala/collection/immutable/StringOps/$times(int)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$1/ExecutorRunner$$anonfun$1(org.apache.spark.deploy.worker.ExecutorRunner)|",
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$1/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$1(org.apache.spark.deploy.worker.ExecutorRunner,java.util.List)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/publicAddress()|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+method:///org/apache/spark/util/logging/FileAppender$/apply(java.io.InputStream,java.io.File,org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/state()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/state_$eq(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/stderrAppender_$eq(org.apache.spark.util.logging.FileAppender)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/executorDir()|",
      "|java+method:///java/lang/Process/waitFor()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///java/lang/ProcessBuilder/directory(java.io.File)|",
      "|java+method:///java/lang/Process/getErrorStream()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/org$apache$spark$deploy$worker$ExecutorRunner$$killProcess(scala.Option)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/execId()|",
      "|java+method:///org/apache/spark/deploy/worker/CommandUtils$/buildProcessBuilder$default$6()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/stdoutAppender_$eq(org.apache.spark.util.logging.FileAppender)|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/EXITED()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/command()|",
      "|java+method:///scala/collection/mutable/Buffer/mkString(java.lang.String,java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$2/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$2(org.apache.spark.deploy.worker.ExecutorRunner)|",
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/FAILED()|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/KILLED()|",
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$3/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$3(org.apache.spark.deploy.worker.ExecutorRunner)|",
      "|java+method:///org/apache/spark/deploy/worker/CommandUtils$/buildProcessBuilder$default$7()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/spark-project/guava/io/Files/write(java.lang.CharSequence,java.io.File,java.nio.charset.Charset)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/logInfo(scala.Function0)|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/appLocalDirs()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/webUiPort()|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/process_$eq(java.lang.Process)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/worker/CommandUtils$/buildProcessBuilder(org.apache.spark.deploy.Command,org.apache.spark.SecurityManager,int,java.lang.String,scala.Function1,scala.collection.Seq,scala.collection.Map)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/process()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/worker()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/appDesc()|",
      "|java+method:///java/io/File/getAbsolutePath()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///java/lang/ProcessBuilder/command()|",
      "|java+method:///scala/collection/immutable/StringOps/$times(int)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/memory()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///java/lang/Exception/toString()|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/DeployMessages$ExecutorStateChanged(java.lang.String,int,scala.Enumeration$Value,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/appId()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/sparkHome()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///java/lang/ProcessBuilder/environment()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$1/ExecutorRunner$$anonfun$1(org.apache.spark.deploy.worker.ExecutorRunner)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/publicAddress()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/state_$eq(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/stderrAppender_$eq(org.apache.spark.util.logging.FileAppender)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/executorDir()|",
      "|java+method:///java/lang/Process/waitFor()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///java/lang/ProcessBuilder/directory(java.io.File)|",
      "|java+method:///java/lang/Process/getErrorStream()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/org$apache$spark$deploy$worker$ExecutorRunner$$killProcess(scala.Option)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/execId()|",
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$1/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$1(org.apache.spark.deploy.worker.ExecutorRunner,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/CommandUtils$/buildProcessBuilder$default$6()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/stdoutAppender_$eq(org.apache.spark.util.logging.FileAppender)|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/EXITED()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/command()|",
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$2/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$2(org.apache.spark.deploy.worker.ExecutorRunner)|",
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/FAILED()|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/KILLED()|",
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$3/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$3(org.apache.spark.deploy.worker.ExecutorRunner)|",
      "|java+method:///org/apache/spark/deploy/worker/CommandUtils$/buildProcessBuilder$default$7()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/spark-project/guava/io/Files/write(java.lang.CharSequence,java.io.File,java.nio.charset.Charset)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/logInfo(scala.Function0)|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/appLocalDirs()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/webUiPort()|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/process_$eq(java.lang.Process)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/worker/CommandUtils$/buildProcessBuilder(org.apache.spark.deploy.Command,org.apache.spark.SecurityManager,int,java.lang.String,scala.Function1,scala.collection.Seq,scala.collection.Map)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+method:///org/apache/spark/util/logging/FileAppender$/apply(java.io.InputStream,java.io.File,org.apache.spark.SparkConf)|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/process()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/state()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/worker()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/appDesc()|",
      "|java+method:///java/io/File/getAbsolutePath()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///java/lang/ProcessBuilder/command()|",
      "|java+method:///scala/collection/immutable/StringOps/$times(int)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/memory()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///java/lang/Exception/toString()|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/DeployMessages$ExecutorStateChanged(java.lang.String,int,scala.Enumeration$Value,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/appId()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/sparkHome()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///java/lang/ProcessBuilder/environment()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$10/MesosSubmitRequestServlet$$anonfun$10(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$1/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/Buffer/mkString(java.lang.String,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$1/apply()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner$$anonfun$org$apache$spark$deploy$worker$ExecutorRunner$$fetchAndRunExecutor$1/apply()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/executorLost(java.lang.String,org.apache.spark.scheduler.ExecutorLossReason)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/SchedulerBackend/reviveOffers()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/activeExecutorIds()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$executorLost$2/TaskSchedulerImpl$$anonfun$executorLost$2(org.apache.spark.scheduler.TaskSchedulerImpl,java.lang.String,org.apache.spark.scheduler.ExecutorLossReason)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/executorLost(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$executorLost$1/TaskSchedulerImpl$$anonfun$executorLost$1(org.apache.spark.scheduler.TaskSchedulerImpl,java.lang.String,org.apache.spark.scheduler.ExecutorLossReason,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/backend()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/dagScheduler()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/executorIdToHost()|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/removeExecutor(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/logError(scala.Function0)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/executorLost(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/logExecutorLoss(java.lang.String,java.lang.String,org.apache.spark.scheduler.ExecutorLossReason)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/backend()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/removeExecutor(java.lang.String,org.apache.spark.scheduler.ExecutorLossReason)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/scheduler/SchedulerBackend/reviveOffers()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/dagScheduler()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/executorIdToHost()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$executorLost$1/TaskSchedulerImpl$$anonfun$executorLost$1(org.apache.spark.scheduler.TaskSchedulerImpl,java.lang.String,org.apache.spark.scheduler.ExecutorLossReason)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/org$apache$spark$scheduler$TaskSchedulerImpl$$executorIdToTaskCount()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$47/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$47/apply(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$47/apply(boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$80/JsonProtocol$$anonfun$80()|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$51/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/shuffleWriteMetrics()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$51$$anonfun$apply$10/StagePage$$anonfun$51$$anonfun$apply$10(org.apache.spark.ui.jobs.StagePage$$anonfun$51)|",
      "|java+method:///scala/Option/get()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$51$$anonfun$apply$23/StagePage$$anonfun$51$$anonfun$apply$23(org.apache.spark.ui.jobs.StagePage$$anonfun$51)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$51$$anonfun$apply$19/StagePage$$anonfun$51$$anonfun$apply$19(org.apache.spark.ui.jobs.StagePage$$anonfun$51)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/shuffleReadMetrics()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$51$$anonfun$apply$6/StagePage$$anonfun$51$$anonfun$apply$6(org.apache.spark.ui.jobs.StagePage$$anonfun$51)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext/getCallSite()|",
    "called": "|java+method:///scala/Option/getOrElse(scala.Function0)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$getCallSite$2/SparkContext$$anonfun$getCallSite$2(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$getCallSite$1/SparkContext$$anonfun$getCallSite$1(org.apache.spark.SparkContext)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkContext/getLocalProperty(java.lang.String)|",
      "|java+method:///org/apache/spark/util/CallSite$/SHORT_FORM()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/Utils$/getCallSite$default$1()|",
      "|java+method:///org/apache/spark/SparkContext/getLocalProperty(java.lang.String)|",
      "|java+method:///org/apache/spark/util/CallSite$/SHORT_FORM()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$getCallSite$2/SparkContext$$anonfun$getCallSite$2(org.apache.spark.SparkContext,org.apache.spark.util.CallSite)|",
      "|java+method:///org/apache/spark/util/CallSite$/LONG_FORM()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$getCallSite$1/SparkContext$$anonfun$getCallSite$1(org.apache.spark.SparkContext,org.apache.spark.util.CallSite)|",
      "|java+method:///org/apache/spark/util/Utils$/getCallSite(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/CallSite/CallSite(java.lang.String,java.lang.String)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/RangePartitioner$$anonfun$9/apply(int,scala.collection.Iterator)|",
    "called": "|java+method:///scala/Tuple2/_2$mcJ$sp()|",
    "v1Body": [
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/util/random/SamplingUtils$/reservoirSampleAndCount(scala.collection.Iterator,int,long,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Iterator$/apply(scala.collection.Seq)|",
      "|java+method:///scala/util/hashing/package$/byteswap32(int)|",
      "|java+method:///scala/package$/Iterator()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/util/random/SamplingUtils$/reservoirSampleAndCount(scala.collection.Iterator,int,long,scala.reflect.ClassTag)|",
      "|java+method:///scala/util/hashing/package$/byteswap32(int)|",
      "|java+method:///scala/package$/Iterator()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Tuple2/_2$mcJ$sp()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/collection/Iterator$/apply(scala.collection.Seq)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/r/SerDe$$anonfun$writeObject$2/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/r/SerDe$/writeJObj(java.io.DataOutputStream,java.lang.Object)|",
      "|java+method:///org/apache/spark/api/r/SerDe$$anonfun$writeObject$2/apply(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToFloat(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|",
      "|java+method:///org/apache/spark/api/r/SerDe$$anonfun$writeObject$2/apply(float)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$7/apply(org.apache.mesos.Protos$Offer)|",
    "called": "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/mesos/Protos$Offer/getId()|",
      "|java+method:///org/apache/mesos/SchedulerDriver/declineOffer(org.apache.mesos.Protos$OfferID)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/mesos/Protos$SlaveID/getValue()|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getSlaveId()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext$$anonfun$setupAndStartListenerBus$1/apply(java.lang.String)|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+method:///java/lang/Class/getConstructors()|",
      "|java+method:///java/lang/reflect/Constructor/newInstance(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$setupAndStartListenerBus$1$$anonfun$apply$14/SparkContext$$anonfun$setupAndStartListenerBus$1$$anonfun$apply$14(org.apache.spark.SparkContext$$anonfun$setupAndStartListenerBus$1,java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/Option/get()|",
      "|java+constructor:///scala/runtime/VolatileByteRef/VolatileByteRef(byte)|",
      "|java+method:///org/apache/spark/util/Utils$/classForName(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$setupAndStartListenerBus$1$$anonfun$40/SparkContext$$anonfun$setupAndStartListenerBus$1$$anonfun$40(org.apache.spark.SparkContext$$anonfun$setupAndStartListenerBus$1)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/addListener(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/SparkContext/listenerBus()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/SparkContext$$anonfun$setupAndStartListenerBus$1/zeroArgumentConstructor$1(java.lang.reflect.Constructor%5B%5D,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/mutable/ArrayOps/find(scala.Function1)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Class/getConstructors()|",
      "|java+method:///java/lang/reflect/Constructor/newInstance(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$setupAndStartListenerBus$1$$anonfun$apply$15/SparkContext$$anonfun$setupAndStartListenerBus$1$$anonfun$apply$15(org.apache.spark.SparkContext$$anonfun$setupAndStartListenerBus$1,java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/Option/get()|",
      "|java+constructor:///scala/runtime/VolatileByteRef/VolatileByteRef(byte)|",
      "|java+method:///org/apache/spark/util/Utils$/classForName(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$setupAndStartListenerBus$1$$anonfun$41/SparkContext$$anonfun$setupAndStartListenerBus$1$$anonfun$41(org.apache.spark.SparkContext$$anonfun$setupAndStartListenerBus$1)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/addListener(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/SparkContext/listenerBus()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/SparkContext$$anonfun$setupAndStartListenerBus$1/zeroArgumentConstructor$1(java.lang.reflect.Constructor%5B%5D,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/mutable/ArrayOps/find(scala.Function1)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$32/StagePage$$anonfun$32(org.apache.spark.ui.jobs.StagePage)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/r/SerDe$/readList(java.io.DataInputStream)|",
    "called": "|java+method:///scala/collection/immutable/IndexedSeq$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/Object()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readBytesArr(java.io.DataInputStream)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readIntArr(java.io.DataInputStream)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readObjectType(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readBooleanArr(java.io.DataInputStream)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToCharacter(char)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readDoubleArr(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readStringArr(java.io.DataInputStream)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/api/r/SerDe$$anonfun$readList$1/SerDe$$anonfun$readList$1()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/Object()|",
      "|java+method:///scala/collection/immutable/Range/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readInt(java.io.DataInputStream)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+constructor:///org/apache/spark/api/r/SerDe$$anonfun$readList$1/SerDe$$anonfun$readList$1(java.io.DataInputStream)|",
      "|java+method:///scala/collection/immutable/IndexedSeq$/canBuildFrom()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1/apply(org.apache.spark.scheduler.ActiveJob)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/listener()|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerJobEnd/SparkListenerJobEnd(int,long,org.apache.spark.scheduler.JobResult)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/runningStages()|",
      "|java+constructor:///org/apache/spark/scheduler/JobFailed/JobFailed(java.lang.Exception)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1$$anonfun$apply$12/DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1$$anonfun$apply$12(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1,java.lang.String)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/jobId()|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|",
      "|java+method:///scala/collection/mutable/HashSet/toArray(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/scheduler/JobListener/jobFailed(java.lang.Exception)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/listener()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerJobEnd/SparkListenerJobEnd(int,long,org.apache.spark.scheduler.JobResult)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/runningStages()|",
      "|java+constructor:///org/apache/spark/scheduler/JobFailed/JobFailed(java.lang.Exception)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|",
      "|java+method:///scala/collection/mutable/HashSet/toArray(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/scheduler/JobListener/jobFailed(java.lang.Exception)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1$$anonfun$apply$11/DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1$$anonfun$apply$11(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1,java.lang.String)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/jobId()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$66/JsonProtocol$$anonfun$66()|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/handleAskPermissionToCommit(int,int,int)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Map/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/logDebug(scala.Function0)|",
      "|java+method:///scala/collection/mutable/Map/get(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/OutputCommitCoordinator$$anonfun$handleAskPermissionToCommit$1/OutputCommitCoordinator$$anonfun$handleAskPermissionToCommit$1(org.apache.spark.scheduler.OutputCommitCoordinator,int,int,int,int)|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/authorizedCommittersByStage()|",
      "|java+constructor:///org/apache/spark/scheduler/OutputCommitCoordinator$$anonfun$handleAskPermissionToCommit$3/OutputCommitCoordinator$$anonfun$handleAskPermissionToCommit$3(org.apache.spark.scheduler.OutputCommitCoordinator,int,int,int)|",
      "|java+constructor:///org/apache/spark/scheduler/OutputCommitCoordinator$$anonfun$handleAskPermissionToCommit$2/OutputCommitCoordinator$$anonfun$handleAskPermissionToCommit$2(org.apache.spark.scheduler.OutputCommitCoordinator,int,int,int)|"
    ],
    "v2Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/NO_AUTHORIZED_COMMITTER()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/logDebug(scala.Function0)|",
      "|java+method:///scala/collection/mutable/Map/get(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/OutputCommitCoordinator$$anonfun$handleAskPermissionToCommit$2/OutputCommitCoordinator$$anonfun$handleAskPermissionToCommit$2(org.apache.spark.scheduler.OutputCommitCoordinator,int,int,int,int)|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/authorizedCommittersByStage()|",
      "|java+constructor:///org/apache/spark/scheduler/OutputCommitCoordinator$$anonfun$handleAskPermissionToCommit$3/OutputCommitCoordinator$$anonfun$handleAskPermissionToCommit$3(org.apache.spark.scheduler.OutputCommitCoordinator,int,int,int)|",
      "|java+constructor:///org/apache/spark/scheduler/OutputCommitCoordinator$$anonfun$handleAskPermissionToCommit$1/OutputCommitCoordinator$$anonfun$handleAskPermissionToCommit$1(org.apache.spark.scheduler.OutputCommitCoordinator,int,int,int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/partial/GroupedCountEvaluator$$anonfun$currentResult$2/apply(scala.Tuple2)|",
    "called": "|java+method:///scala/Tuple2/_2$mcJ$sp()|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+constructor:///org/apache/spark/partial/BoundedDouble/BoundedDouble(double,double,double,double)|",
      "|java+method:///scala/collection/mutable/Map/update(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/Tuple2/_2$mcJ$sp()|",
      "|java+method:///scala/math/package$/sqrt(double)|"
    ],
    "v2Body": [
      "|java+method:///scala/Tuple2/_1()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/partial/BoundedDouble/BoundedDouble(double,double,double,double)|",
      "|java+method:///java/util/HashMap/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Tuple2/_2$mcJ$sp()|",
      "|java+method:///scala/math/package$/sqrt(double)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$1/apply$mcVI$sp(int)|",
    "called": "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$ShuffleState/numBuckets()|",
      "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$1$$anonfun$apply$mcVI$sp$1/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$1$$anonfun$apply$mcVI$sp$1(org.apache.spark.shuffle.FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$1,int)|",
      "|java+method:///scala/collection/immutable/Range/foreach(scala.Function1)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$ShuffleState/numReducers()|",
      "|java+method:///scala/collection/immutable/Range/foreach$mVc$sp(scala.Function1)|",
      "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$1$$anonfun$apply$mcVI$sp$1/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$1$$anonfun$apply$mcVI$sp$1(org.apache.spark.shuffle.FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$1,int)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/spill(org.apache.spark.util.collection.SizeTracker)|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$ser()|",
      "|java+method:///org/apache/spark/storage/DiskBlockObjectWriter/revertPartialWritesAndClose()|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/ExternalAppendOnlyMap$DiskMapIterator(org.apache.spark.util.collection.ExternalAppendOnlyMap,java.io.File,org.apache.spark.storage.BlockId,scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+method:///org/apache/spark/util/collection/SizeTrackingAppendOnlyMap/destructiveSortedIterator(java.util.Comparator)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$keyComparator()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$currentMap()|",
      "|java+constructor:///org/apache/spark/executor/ShuffleWriteMetrics/ShuffleWriteMetrics()|",
      "|java+method:///org/apache/spark/storage/BlockManager/getDiskWriter(org.apache.spark.storage.BlockId,java.io.File,org.apache.spark.serializer.SerializerInstance,int,org.apache.spark.executor.ShuffleWriteMetrics)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/DiskBlockManager/createTempLocalBlock()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$spilledMaps()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/append(scala.collection.Seq)|",
      "|java+method:///scala/collection/Iterator/next()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/diskBlockManager()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$serializerBatchSize()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/fileBufferSize()|",
      "|java+method:///org/apache/spark/storage/DiskBlockObjectWriter/write(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/curWriteMetrics()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/flush$1(scala.runtime.ObjectRef,scala.runtime.IntRef,scala.collection.mutable.ArrayBuffer)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+constructor:///scala/runtime/IntRef/IntRef(int)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/curWriteMetrics_$eq(org.apache.spark.executor.ShuffleWriteMetrics)|",
      "|java+method:///java/io/File/delete()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$ser()|",
      "|java+method:///org/apache/spark/storage/DiskBlockObjectWriter/revertPartialWritesAndClose()|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/ExternalAppendOnlyMap$DiskMapIterator(org.apache.spark.util.collection.ExternalAppendOnlyMap,java.io.File,org.apache.spark.storage.BlockId,scala.collection.mutable.ArrayBuffer)|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$$anonfun$spill$1/ExternalAppendOnlyMap$$anonfun$spill$1(org.apache.spark.util.collection.ExternalAppendOnlyMap,java.io.File)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/util/collection/SizeTrackingAppendOnlyMap/destructiveSortedIterator(java.util.Comparator)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$keyComparator()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$currentMap()|",
      "|java+constructor:///org/apache/spark/executor/ShuffleWriteMetrics/ShuffleWriteMetrics()|",
      "|java+method:///org/apache/spark/storage/BlockManager/getDiskWriter(org.apache.spark.storage.BlockId,java.io.File,org.apache.spark.serializer.SerializerInstance,int,org.apache.spark.executor.ShuffleWriteMetrics)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/DiskBlockManager/createTempLocalBlock()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$spilledMaps()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/append(scala.collection.Seq)|",
      "|java+method:///scala/collection/Iterator/next()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/diskBlockManager()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$serializerBatchSize()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/fileBufferSize()|",
      "|java+method:///org/apache/spark/storage/DiskBlockObjectWriter/write(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/curWriteMetrics()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/flush$1(scala.runtime.ObjectRef,scala.runtime.IntRef,scala.collection.mutable.ArrayBuffer)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+constructor:///scala/runtime/IntRef/IntRef(int)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/curWriteMetrics_$eq(org.apache.spark.executor.ShuffleWriteMetrics)|",
      "|java+method:///java/io/File/delete()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext/getAllPools()|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/collectionAsScalaIterable(java.util.Collection)|",
      "|java+method:///scala/collection/Iterable/toSeq()|",
      "|java+method:///org/apache/spark/scheduler/TaskScheduler/rootPool()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$assertNotStopped()|",
      "|java+method:///org/apache/spark/scheduler/Pool/schedulableQueue()|",
      "|java+method:///org/apache/spark/SparkContext/taskScheduler()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///org/apache/spark/scheduler/TaskScheduler/rootPool()|",
      "|java+method:///scala/collection/TraversableOnce/toSeq()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$assertNotStopped()|",
      "|java+method:///org/apache/spark/scheduler/Pool/schedulableQueue()|",
      "|java+method:///org/apache/spark/SparkContext/taskScheduler()|",
      "|java+method:///scala/collection/JavaConverters$/collectionAsScalaIterableConverter(java.util.Collection)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/WholeTextFileRDD/getPartitions()|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///org/apache/spark/input/WholeTextFileInputFormat/setConf(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///java/lang/Class/newInstance()|",
      "|java+method:///java/util/List/toArray()|",
      "|java+method:///org/apache/spark/rdd/WholeTextFileRDD/newJobContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.JobID)|",
      "|java+constructor:///org/apache/spark/rdd/WholeTextFileRDD$$anonfun$getPartitions$2/WholeTextFileRDD$$anonfun$getPartitions$2(org.apache.spark.rdd.WholeTextFileRDD,java.lang.Object%5B%5D,org.apache.spark.Partition%5B%5D)|",
      "|java+method:///scala/collection/immutable/Range/foreach$mVc$sp(scala.Function1)|",
      "|java+method:///org/apache/spark/input/WholeTextFileInputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)|",
      "|java+method:///org/apache/spark/rdd/WholeTextFileRDD/jobId()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/rdd/WholeTextFileRDD/getConf()|",
      "|java+method:///scala/collection/mutable/ArrayOps/size()|",
      "|java+method:///org/apache/spark/input/WholeTextFileInputFormat/setMinPartitions(org.apache.hadoop.mapreduce.JobContext,int)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/input/WholeTextFileInputFormat/setConf(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///java/lang/Class/newInstance()|",
      "|java+method:///java/util/List/toArray()|",
      "|java+method:///org/apache/spark/rdd/WholeTextFileRDD/newJobContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.JobID)|",
      "|java+constructor:///org/apache/spark/rdd/WholeTextFileRDD$$anonfun$getPartitions$1/WholeTextFileRDD$$anonfun$getPartitions$1(org.apache.spark.rdd.WholeTextFileRDD,java.lang.Object%5B%5D,org.apache.spark.Partition%5B%5D)|",
      "|java+method:///scala/collection/immutable/Range/foreach$mVc$sp(scala.Function1)|",
      "|java+method:///org/apache/spark/input/WholeTextFileInputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)|",
      "|java+method:///org/apache/spark/rdd/WholeTextFileRDD/jobId()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/rdd/WholeTextFileRDD/getConf()|",
      "|java+method:///scala/collection/mutable/ArrayOps/size()|",
      "|java+method:///org/apache/spark/input/WholeTextFileInputFormat/setMinPartitions(org.apache.hadoop.mapreduce.JobContext,int)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$/taskEndReasonFromJson(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/Option/orNull(scala.Predef$$less$colon$less)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/ExecutorLostFailure/ExecutorLostFailure(java.lang.String)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/TaskCommitDenied/TaskCommitDenied(int,int,int)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/blockManagerIdFromJson(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$taskEndReasonFromJson$2/JsonProtocol$$anonfun$taskEndReasonFromJson$2()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ExceptionFailure/ExceptionFailure(java.lang.String,java.lang.String,java.lang.StackTraceElement%5B%5D,java.lang.String,scala.Option,scala.Option)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+constructor:///org/apache/spark/FetchFailed/FetchFailed(org.apache.spark.storage.BlockManagerId,int,int,int,java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$72/JsonProtocol$$anonfun$72()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$76/JsonProtocol$$anonfun$76()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$71/JsonProtocol$$anonfun$71()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$75/JsonProtocol$$anonfun$75()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$13/JsonProtocol$$anonfun$13()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$74/JsonProtocol$$anonfun$74()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$12/JsonProtocol$$anonfun$12()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$73/JsonProtocol$$anonfun$73()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$11/JsonProtocol$$anonfun$11()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$70/JsonProtocol$$anonfun$70()|",
      "|java+method:///org/apache/spark/util/Utils$/getFormattedClassName(java.lang.Object)|",
      "|java+method:///scala/Option/orNull(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$taskEndReasonFromJson$1/JsonProtocol$$anonfun$taskEndReasonFromJson$1()|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/stackTraceFromJson(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$taskEndReasonFromJson$3/JsonProtocol$$anonfun$taskEndReasonFromJson$3()|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/TaskCommitDenied/TaskCommitDenied(int,int,int)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/ExecutorLostFailure/ExecutorLostFailure(java.lang.String,boolean,scala.Option)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/blockManagerIdFromJson(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$taskEndReasonFromJson$2/JsonProtocol$$anonfun$taskEndReasonFromJson$2()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ExceptionFailure/ExceptionFailure(java.lang.String,java.lang.String,java.lang.StackTraceElement%5B%5D,java.lang.String,scala.Option,scala.Option)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+constructor:///org/apache/spark/FetchFailed/FetchFailed(org.apache.spark.storage.BlockManagerId,int,int,int,java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$76/JsonProtocol$$anonfun$76()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$75/JsonProtocol$$anonfun$75()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$74/JsonProtocol$$anonfun$74()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$80/JsonProtocol$$anonfun$80()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$73/JsonProtocol$$anonfun$73()|",
      "|java+method:///org/apache/spark/util/Utils$/getFormattedClassName(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$81/JsonProtocol$$anonfun$81()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$13/JsonProtocol$$anonfun$13()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$79/JsonProtocol$$anonfun$79()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$12/JsonProtocol$$anonfun$12()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$78/JsonProtocol$$anonfun$78()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$11/JsonProtocol$$anonfun$11()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$77/JsonProtocol$$anonfun$77()|",
      "|java+method:///scala/Option/orNull(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$taskEndReasonFromJson$1/JsonProtocol$$anonfun$taskEndReasonFromJson$1()|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/stackTraceFromJson(org.json4s.JsonAST$JValue)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$1/apply()|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$1/apply()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/DiskBlockObjectWriter/file()|",
      "|java+method:///org/apache/spark/storage/DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$1/apply()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$cogroup$1/apply()|",
    "called": "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
    "v1Body": [
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/mapValues(scala.Function1)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD/CoGroupedRDD(scala.collection.Seq,org.apache.spark.Partitioner)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$cogroup$1$$anonfun$apply$46/PairRDDFunctions$$anonfun$cogroup$1$$anonfun$apply$46(org.apache.spark.rdd.PairRDDFunctions$$anonfun$cogroup$1)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/keyClass()|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$cogroup$1/apply()|",
      "|java+method:///java/lang/Class/isArray()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/mapValues(scala.Function1)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD/CoGroupedRDD(scala.collection.Seq,org.apache.spark.Partitioner,scala.reflect.ClassTag)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$cogroup$1$$anonfun$apply$46/PairRDDFunctions$$anonfun$cogroup$1$$anonfun$apply$46(org.apache.spark.rdd.PairRDDFunctions$$anonfun$cogroup$1)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/keyClass()|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$cogroup$1/apply()|",
      "|java+method:///java/lang/Class/isArray()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/local/LocalBackend/start()|",
    "called": "|java+method:///scala/collection/immutable/Map$/empty()|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/local/LocalEndpoint/localExecutorId()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/ExecutorInfo/ExecutorInfo(java.lang.String,int,scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///org/apache/spark/scheduler/local/LocalBackend/totalCores()|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/local/LocalBackend/localEndpoint_$eq(org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///scala/collection/immutable/Map$/empty()|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///org/apache/spark/scheduler/local/LocalBackend/listenerBus()|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerExecutorAdded/SparkListenerExecutorAdded(long,java.lang.String,org.apache.spark.scheduler.cluster.ExecutorInfo)|",
      "|java+method:///org/apache/spark/scheduler/local/LocalBackend/userClassPath()|",
      "|java+constructor:///org/apache/spark/scheduler/local/LocalEndpoint/LocalEndpoint(org.apache.spark.rpc.RpcEnv,scala.collection.Seq,org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.scheduler.local.LocalBackend,int)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///org/apache/spark/scheduler/local/LocalEndpoint/localExecutorHostname()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/local/LocalEndpoint/localExecutorId()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/ExecutorInfo/ExecutorInfo(java.lang.String,int,scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/scheduler/local/LocalBackend/launcherBackend()|",
      "|java+method:///org/apache/spark/launcher/LauncherBackend/setState(org.apache.spark.launcher.SparkAppHandle$State)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///org/apache/spark/scheduler/local/LocalBackend/totalCores()|",
      "|java+method:///org/apache/spark/launcher/LauncherBackend/setAppId(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/local/LocalBackend/appId()|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/local/LocalBackend/localEndpoint_$eq(org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///scala/collection/immutable/Map$/empty()|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///org/apache/spark/scheduler/local/LocalBackend/listenerBus()|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerExecutorAdded/SparkListenerExecutorAdded(long,java.lang.String,org.apache.spark.scheduler.cluster.ExecutorInfo)|",
      "|java+method:///org/apache/spark/scheduler/local/LocalBackend/userClassPath()|",
      "|java+constructor:///org/apache/spark/scheduler/local/LocalEndpoint/LocalEndpoint(org.apache.spark.rpc.RpcEnv,scala.collection.Seq,org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.scheduler.local.LocalBackend,int)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///org/apache/spark/scheduler/local/LocalEndpoint/localExecutorHostname()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/JettyUtils$$anonfun$1/apply(java.lang.String)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///java/lang/String/trim()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/PythonUtils$/toScalaMap(java.util.Map)|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///scala/collection/mutable/Map/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/Predef$/conforms()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/Predef$/conforms()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$9/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$9(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$56/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$56/apply(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$56/apply(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$7/StageTableBase$$anonfun$7(org.apache.spark.ui.jobs.StageTableBase)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$9/apply(org.apache.mesos.Protos$Offer)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/mesos/Protos$SlaveID/getValue()|",
      "|java+method:///scala/Predef$/any2ArrowAssoc(java.lang.Object)|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getSlaveId()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/CPUS_PER_TASK()|",
      "|java+method:///org/apache/mesos/Protos$Offer/getAttributesList()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$9$$anonfun$apply$2/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$9$$anonfun$apply$2(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$9,org.apache.mesos.Protos$Offer,double,double,scala.collection.immutable.Map,java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID/getValue()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1/org$apache$spark$scheduler$cluster$mesos$MesosSchedulerBackend$$anonfun$$$outer()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/slaveIdToExecutorInfo()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/toAttributeMap(java.util.List)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/mesosExecutorCores()|",
      "|java+method:///org/apache/mesos/Protos$Offer/getSlaveId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/getResource(java.util.List,java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getResourcesList()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/calculateTotalMemory(org.apache.spark.SparkContext)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext/addFile(java.lang.String,boolean)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/HttpFileServer/addFile(java.io.File)|",
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/SparkContext/isLocal()|",
      "|java+method:///java/net/URI/toString()|",
      "|java+method:///org/apache/hadoop/fs/FileStatus/isDir()|",
      "|java+method:///java/io/File/getCanonicalFile()|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkEnv/securityManager()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+constructor:///java/io/FileNotFoundException/FileNotFoundException(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$addFile$1/SparkContext$$anonfun$addFile$1(org.apache.spark.SparkContext,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/postEnvironmentUpdate()|",
      "|java+method:///org/apache/spark/SparkContext/addedFiles()|",
      "|java+constructor:///java/net/URI/URI(java.lang.String)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/getFileStatus(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/util/Utils$/fetchFile(java.lang.String,java.io.File,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,org.apache.hadoop.conf.Configuration,long,boolean)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/contains(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///java/net/URI/getPath()|",
      "|java+method:///java/net/URI/getScheme()|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+method:///java/io/File/toURI()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/hadoopConfiguration()|",
      "|java+method:///org/apache/spark/SparkEnv/httpFileServer()|",
      "|java+method:///org/apache/spark/SparkContext/logInfo(scala.Function0)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/SparkFiles$/getRootDirectory()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/SparkContext/isLocal()|",
      "|java+method:///java/net/URI/toString()|",
      "|java+method:///org/apache/hadoop/fs/FileStatus/isDir()|",
      "|java+method:///java/io/File/getCanonicalFile()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/fileServer()|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkEnv/securityManager()|",
      "|java+method:///org/apache/spark/SparkContext/hadoopConfiguration()|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+constructor:///java/io/FileNotFoundException/FileNotFoundException(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$addFile$1/SparkContext$$anonfun$addFile$1(org.apache.spark.SparkContext,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/postEnvironmentUpdate()|",
      "|java+method:///org/apache/spark/SparkContext/addedFiles()|",
      "|java+constructor:///java/net/URI/URI(java.lang.String)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/getFileStatus(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/util/Utils$/fetchFile(java.lang.String,java.io.File,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,org.apache.hadoop.conf.Configuration,long,boolean)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvFileServer/addFile(java.io.File)|",
      "|java+method:///scala/collection/mutable/ArrayOps/contains(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///java/net/URI/getPath()|",
      "|java+method:///java/net/URI/getScheme()|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+method:///java/io/File/toURI()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///org/apache/spark/SparkContext/logInfo(scala.Function0)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/SparkFiles$/getRootDirectory()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/TachyonBlockManager/getFile(java.lang.String)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/Utils$/nonNegativeHash(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/tachyonDirs()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///tachyon/client/TachyonFS/getFile(tachyon.TachyonURI)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/subDirs()|",
      "|java+method:///tachyon/client/TachyonFS/createFile(tachyon.TachyonURI)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///tachyon/client/TachyonFS/mkdir(tachyon.TachyonURI)|",
      "|java+constructor:///tachyon/TachyonURI/TachyonURI(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/client()|",
      "|java+method:///tachyon/client/TachyonFS/exist(tachyon.TachyonURI)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/org$apache$spark$storage$TachyonBlockManager$$subDirsPerTachyonDir()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/Utils$/nonNegativeHash(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/tachyonDirs()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///tachyon/client/TachyonFS/getFile(tachyon.TachyonURI)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/subDirs()|",
      "|java+method:///tachyon/client/TachyonFS/createFile(tachyon.TachyonURI)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///tachyon/client/TachyonFS/mkdir(tachyon.TachyonURI)|",
      "|java+constructor:///tachyon/TachyonURI/TachyonURI(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/client()|",
      "|java+method:///tachyon/client/TachyonFS/exist(tachyon.TachyonURI)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/org$apache$spark$storage$TachyonBlockManager$$subDirsPerTachyonDir()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$error$1/apply(scala.collection.mutable.HashMap)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/values()|",
    "v1Body": [
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$error$1$$anonfun$apply$9/TaskSchedulerImpl$$anonfun$error$1$$anonfun$apply$9(org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$error$1)|",
      "|java+method:///scala/collection/mutable/HashMap/values()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$error$1$$anonfun$apply$10/TaskSchedulerImpl$$anonfun$error$1$$anonfun$apply$10(org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$error$1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/BlockManager$$anonfun$doGetRemote$2/apply(org.apache.spark.storage.BlockManagerId)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/network/buffer/ManagedBuffer/nioByteBuffer()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataDeserialize(org.apache.spark.storage.BlockId,java.nio.ByteBuffer,org.apache.spark.serializer.Serializer)|",
      "|java+method:///org/apache/spark/network/BlockTransferService/fetchBlockSync(java.lang.String,int,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/storage/BlockId/toString()|",
      "|java+method:///org/apache/spark/storage/BlockManagerId/port()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetRemote$2$$anonfun$apply$2/BlockManager$$anonfun$doGetRemote$2$$anonfun$apply$2(org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2,org.apache.spark.storage.BlockManagerId)|",
      "|java+method:///org/apache/spark/storage/BlockManagerId/executorId()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetRemote$2$$anonfun$apply$3/BlockManager$$anonfun$doGetRemote$2$$anonfun$apply$3(org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2)|",
      "|java+method:///org/apache/spark/executor/DataReadMethod$/Network()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataDeserialize$default$3()|",
      "|java+constructor:///scala/runtime/NonLocalReturnControl/NonLocalReturnControl(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/storage/BlockResult/BlockResult(scala.collection.Iterator,scala.Enumeration$Value,long)|",
      "|java+method:///java/nio/ByteBuffer/limit()|",
      "|java+method:///org/apache/spark/storage/BlockManager/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/storage/BlockManagerId/host()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/network/buffer/ManagedBuffer/nioByteBuffer()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/network/BlockTransferService/fetchBlockSync(java.lang.String,int,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/executor/DataReadMethod$/Network()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataDeserialize(org.apache.spark.storage.BlockId,java.nio.ByteBuffer)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/storage/BlockManager/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/storage/BlockManagerId/executorId()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetRemote$2$$anonfun$apply$3/BlockManager$$anonfun$doGetRemote$2$$anonfun$apply$3(org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/storage/BlockId/toString()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/storage/BlockManagerId/port()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/storage/BlockFetchException/BlockFetchException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+constructor:///scala/runtime/NonLocalReturnControl/NonLocalReturnControl(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetRemote$2$$anonfun$8/BlockManager$$anonfun$doGetRemote$2$$anonfun$8(org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2,org.apache.spark.storage.BlockManagerId)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetRemote$2$$anonfun$apply$2/BlockManager$$anonfun$doGetRemote$2$$anonfun$apply$2(org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2,org.apache.spark.storage.BlockManagerId)|",
      "|java+constructor:///org/apache/spark/storage/BlockResult/BlockResult(scala.collection.Iterator,scala.Enumeration$Value,long)|",
      "|java+method:///java/nio/ByteBuffer/limit()|",
      "|java+method:///org/apache/spark/storage/BlockManager/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/storage/BlockManagerId/host()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/JettyUtils$/createRedirectHandler(java.lang.String,java.lang.String,scala.Function1,java.lang.String,scala.collection.immutable.Set)|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/ui/JettyUtils$$anon$2/JettyUtils$$anon$2(scala.Function1,scala.collection.immutable.Set,java.lang.String)|",
      "|java+method:///org/apache/spark/ui/JettyUtils$/attachPrefix(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/ui/JettyUtils$/createServletHandler(java.lang.String,javax.servlet.http.HttpServlet,java.lang.String)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/ui/JettyUtils$$anon$2/JettyUtils$$anon$2(scala.Function1,scala.collection.immutable.Set,java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/JettyUtils$/createServletHandler(java.lang.String,javax.servlet.http.HttpServlet,java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13/apply(org.apache.spark.TaskContext,scala.collection.Iterator)|",
    "called": "|java+constructor:///scala/runtime/LongRef/LongRef(long)|",
    "v1Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkHadoopWriter/open()|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6/PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6(org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13,scala.collection.Iterator,org.apache.spark.executor.OutputMetrics,scala.Option,scala.runtime.LongRef)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$56/PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$56(org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13,org.apache.spark.executor.OutputMetrics)|",
      "|java+method:///org/apache/spark/TaskContext/partitionId()|",
      "|java+method:///org/apache/spark/SparkHadoopWriter/commit()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/org$apache$spark$rdd$PairRDDFunctions$$initHadoopOutputMetrics(org.apache.spark.TaskContext)|",
      "|java+method:///org/apache/spark/SparkHadoopWriter/setup(int,int,int)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7/PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7(org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/TaskContext/stageId()|",
      "|java+constructor:///scala/runtime/LongRef/LongRef(long)|",
      "|java+method:///org/apache/spark/util/Utils$/tryWithSafeFinally(scala.Function0,scala.Function0)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$saveAsHadoopDataset$1/org$apache$spark$rdd$PairRDDFunctions$$anonfun$$$outer()|",
      "|java+method:///org/apache/spark/TaskContext/taskAttemptId()|",
      "|java+method:///org/apache/spark/util/SerializableConfiguration/value()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/OutputMetrics/setRecordsWritten(long)|"
    ],
    "v2Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkHadoopWriter/open()|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6/PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6(org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13,scala.collection.Iterator,org.apache.spark.executor.OutputMetrics,scala.Option,scala.runtime.LongRef)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$56/PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$56(org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13,org.apache.spark.executor.OutputMetrics)|",
      "|java+method:///org/apache/spark/TaskContext/taskAttemptId()|",
      "|java+method:///org/apache/spark/TaskContext/partitionId()|",
      "|java+method:///org/apache/spark/SparkHadoopWriter/commit()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/org$apache$spark$rdd$PairRDDFunctions$$initHadoopOutputMetrics(org.apache.spark.TaskContext)|",
      "|java+method:///org/apache/spark/SparkHadoopWriter/setup(int,int,int)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7/PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7(org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/TaskContext/stageId()|",
      "|java+constructor:///scala/runtime/LongRef/LongRef(long)|",
      "|java+method:///org/apache/spark/util/Utils$/tryWithSafeFinally(scala.Function0,scala.Function0)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$saveAsHadoopDataset$1/org$apache$spark$rdd$PairRDDFunctions$$anonfun$$$outer()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/OutputMetrics/setRecordsWritten(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/PartitionwiseSampledRDD/getPartitions()|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/PartitionwiseSampledRDD/firstParent(scala.reflect.ClassTag)|",
      "|java+constructor:///java/util/Random/Random(long)|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/rdd/PartitionwiseSampledRDD$$anonfun$getPartitions$1/PartitionwiseSampledRDD$$anonfun$getPartitions$1(org.apache.spark.rdd.PartitionwiseSampledRDD,java.util.Random)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/PartitionwiseSampledRDD/firstParent(scala.reflect.ClassTag)|",
      "|java+constructor:///java/util/Random/Random(long)|",
      "|java+method:///org/apache/spark/rdd/PartitionwiseSampledRDD/seed()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/rdd/PartitionwiseSampledRDD$$anonfun$getPartitions$1/PartitionwiseSampledRDD$$anonfun$getPartitions$1(org.apache.spark.rdd.PartitionwiseSampledRDD,java.util.Random)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1/applyOrElse(java.lang.Object,scala.Function1)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/keys()|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/worker/Worker/self()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/copy$default$3()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$LaunchDriver/driverId()|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$LaunchExecutor/appId()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisteredWorker/masterWebUiUrl()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$publicAddress()|",
      "|java+method:///scala/collection/mutable/HashMap/keys()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/command()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$10/Worker$$anonfun$receive$1$$anonfun$applyOrElse$10(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String,int,org.apache.spark.deploy.ApplicationDescription)|",
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner/ExecutorRunner(java.lang.String,int,org.apache.spark.deploy.ApplicationDescription,int,int,org.apache.spark.rpc.RpcEndpointRef,java.lang.String,java.lang.String,int,java.lang.String,java.io.File,java.io.File,java.lang.String,org.apache.spark.SparkConf,scala.collection.Seq,scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$registered()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/copy$default$5()|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/FAILED()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterWorkerFailed/message()|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/copy$default$6()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/start()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DriverDescription/copy(java.lang.String,int,int,boolean,org.apache.spark.deploy.Command)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/appDirectories()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/copy$default$7()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$workerUri()|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$maybeCleanupApplication(java.lang.String)|",
      "|java+constructor:///java/io/IOException/IOException(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/handleExecutorStateChanged(org.apache.spark.deploy.DeployMessages$ExecutorStateChanged)|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/drivers()|",
      "|java+method:///scala/collection/Iterable/toList()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$host()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/copy(java.lang.String,scala.Option,int,org.apache.spark.deploy.Command,java.lang.String,scala.Option,scala.Option)|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$HEARTBEAT_MILLIS()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$9/Worker$$anonfun$receive$1$$anonfun$applyOrElse$9(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String,int,org.apache.spark.deploy.ApplicationDescription)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$changeMaster(org.apache.spark.rpc.RpcEndpointRef,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$KillExecutor/appId()|",
      "|java+method:///scala/collection/Iterable/toSeq()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$registerWithMaster()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$registered_$eq(boolean)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/ui/WorkerWebUI/boundPort()|",
      "|java+method:///org/apache/spark/deploy/DriverDescription/command()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anon$5/Worker$$anonfun$receive$1$$anon$5(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterChanged/master()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$2/Worker$$anonfun$receive$1$$anonfun$2(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,scala.collection.immutable.Set)|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/LOADING()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/coresUsed()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$8/Worker$$anonfun$receive$1$$anonfun$8(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/memoryUsed_$eq(int)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$1/Worker$$anonfun$receive$1$$anonfun$applyOrElse$1(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$3/Worker$$anonfun$receive$1$$anonfun$applyOrElse$3(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$6/Worker$$anonfun$receive$1$$anonfun$6(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anon$6/Worker$$anonfun$receive$1$$anon$6(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$7/Worker$$anonfun$receive$1$$anonfun$7(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1)|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$Heartbeat/DeployMessages$Heartbeat(java.lang.String,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///scala/concurrent/Future/onFailure(scala.PartialFunction,scala.concurrent.ExecutionContext)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$5/Worker$$anonfun$receive$1$$anonfun$applyOrElse$5(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///java/lang/System/exit(int)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$LaunchExecutor/masterUrl()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$16/Worker$$anonfun$receive$1$$anonfun$applyOrElse$16(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$2/Worker$$anonfun$receive$1$$anonfun$applyOrElse$2(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/state()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$sendToMaster(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$sparkHome()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$LaunchExecutor/memory()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$KillExecutor/execId()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$15/Worker$$anonfun$receive$1$$anonfun$applyOrElse$15(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterChanged/masterWebUiUrl()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/kill()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/securityMgr()|",
      "|java+method:///org/apache/spark/deploy/DriverDescription/copy$default$1()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$14/Worker$$anonfun$receive$1$$anonfun$applyOrElse$14(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$8/Worker$$anonfun$receive$1$$anonfun$applyOrElse$8(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/DriverDescription/copy$default$2()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$6/Worker$$anonfun$receive$1$$anonfun$applyOrElse$6(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/deploy/DriverDescription/cores()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$7/Worker$$anonfun$receive$1$$anonfun$applyOrElse$7(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ApplicationFinished/id()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$KillDriver/driverId()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DriverDescription/mem()|",
      "|java+method:///org/apache/spark/deploy/DriverDescription/copy$default$3()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$LaunchDriver/driverDesc()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$LaunchExecutor/appDesc()|",
      "|java+method:///org/apache/spark/deploy/DriverDescription/copy$default$4()|",
      "|java+method:///java/util/concurrent/ScheduledExecutorService/scheduleAtFixedRate(java.lang.Runnable,long,long,java.util.concurrent.TimeUnit)|",
      "|java+method:///java/io/File/mkdirs()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$cleanupThreadExecutor()|",
      "|java+method:///scala/collection/Iterable/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$13/Worker$$anonfun$receive$1$$anonfun$applyOrElse$13(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/workDir()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$11/Worker$$anonfun$receive$1$$anonfun$applyOrElse$11(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String,int)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/memoryUsed()|",
      "|java+method:///scala/collection/TraversableOnce/toSet()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$LaunchExecutor/cores()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/worker/DriverRunner/start()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/logError(scala.Function0,java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/deploy/worker/DriverRunner/DriverRunner(org.apache.spark.SparkConf,java.lang.String,java.io.File,java.io.File,org.apache.spark.deploy.DriverDescription,org.apache.spark.rpc.RpcEndpointRef,java.lang.String,org.apache.spark.SecurityManager)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$/maybeUpdateSSLSettings(org.apache.spark.deploy.Command,org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$workerId()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$12/Worker$$anonfun$receive$1$$anonfun$applyOrElse$12(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/coresUsed_$eq(int)|",
      "|java+method:///scala/collection/mutable/HashMap/$minus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisteredWorker/master()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///java/lang/Exception/toString()|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/DeployMessages$ExecutorStateChanged(java.lang.String,int,scala.Enumeration$Value,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$CLEANUP_INTERVAL_MILLIS()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$CLEANUP_ENABLED()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ReconnectWorker/masterUrl()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/executors()|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$WorkerSchedulerStateResponse/DeployMessages$WorkerSchedulerStateResponse(java.lang.String,scala.collection.immutable.List,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$KillExecutor/masterUrl()|",
      "|java+method:///scala/concurrent/package$/future(scala.Function0,scala.concurrent.ExecutionContext)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/handleDriverStateChanged(org.apache.spark.deploy.DeployMessages$DriverStateChanged)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$forwordMessageScheduler()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$activeMasterUrl()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/copy$default$1()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$reregisterWithMaster()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$webUi()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/finishedApps()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$LaunchExecutor/execId()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/conf()|",
      "|java+method:///org/apache/spark/deploy/worker/DriverRunner/kill()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/copy$default$2()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$connected()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/coresUsed()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/copy$default$3()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$LaunchDriver/driverId()|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$LaunchExecutor/appId()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/copy(java.lang.String,scala.Option,int,org.apache.spark.deploy.Command,java.lang.String,scala.Option,scala.Option,scala.Option,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$publicAddress()|",
      "|java+method:///scala/collection/mutable/HashMap/keys()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/command()|",
      "|java+constructor:///org/apache/spark/deploy/worker/ExecutorRunner/ExecutorRunner(java.lang.String,int,org.apache.spark.deploy.ApplicationDescription,int,int,org.apache.spark.rpc.RpcEndpointRef,java.lang.String,java.lang.String,int,java.lang.String,java.io.File,java.io.File,java.lang.String,org.apache.spark.SparkConf,scala.collection.Seq,scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/copy$default$5()|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/FAILED()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/copy$default$6()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/start()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DriverDescription/copy(java.lang.String,int,int,boolean,org.apache.spark.deploy.Command)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/appDirectories()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/copy$default$7()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$workerUri()|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$maybeCleanupApplication(java.lang.String)|",
      "|java+constructor:///java/io/IOException/IOException(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/copy$default$8()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/self()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/copy$default$9()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/handleExecutorStateChanged(org.apache.spark.deploy.DeployMessages$ExecutorStateChanged)|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/drivers()|",
      "|java+method:///scala/collection/Iterable/toList()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$host()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$7/Worker$$anonfun$receive$1$$anonfun$applyOrElse$7(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String,int,org.apache.spark.deploy.ApplicationDescription)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$6/Worker$$anonfun$receive$1$$anonfun$applyOrElse$6(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String,int,org.apache.spark.deploy.ApplicationDescription)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$changeMaster(org.apache.spark.rpc.RpcEndpointRef,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$KillExecutor/appId()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/toSeq()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$registerWithMaster()|",
      "|java+method:///org/apache/spark/deploy/worker/ui/WorkerWebUI/boundPort()|",
      "|java+method:///org/apache/spark/deploy/DriverDescription/command()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterChanged/master()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$2/Worker$$anonfun$receive$1$$anonfun$2(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,scala.collection.immutable.Set)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$8/Worker$$anonfun$receive$1$$anonfun$8(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/memoryUsed_$eq(int)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$1/Worker$$anonfun$receive$1$$anonfun$applyOrElse$1(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$6/Worker$$anonfun$receive$1$$anonfun$6(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$7/Worker$$anonfun$receive$1$$anonfun$7(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1)|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/RUNNING()|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$Heartbeat/DeployMessages$Heartbeat(java.lang.String,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///scala/concurrent/Future/onFailure(scala.PartialFunction,scala.concurrent.ExecutionContext)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$3/Worker$$anonfun$receive$1$$anonfun$applyOrElse$3(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$LaunchExecutor/masterUrl()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/state()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$sendToMaster(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$sparkHome()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$8/Worker$$anonfun$receive$1$$anonfun$applyOrElse$8(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String,int)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$LaunchExecutor/memory()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$KillExecutor/execId()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterChanged/masterWebUiUrl()|",
      "|java+method:///org/apache/spark/deploy/worker/ExecutorRunner/kill()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/securityMgr()|",
      "|java+method:///org/apache/spark/deploy/DriverDescription/copy$default$1()|",
      "|java+method:///org/apache/spark/deploy/DriverDescription/copy$default$2()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$4/Worker$$anonfun$receive$1$$anonfun$applyOrElse$4(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/DriverDescription/cores()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$9/Worker$$anonfun$receive$1$$anonfun$applyOrElse$9(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$5/Worker$$anonfun$receive$1$$anonfun$applyOrElse$5(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ApplicationFinished/id()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$KillDriver/driverId()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DriverDescription/mem()|",
      "|java+method:///org/apache/spark/deploy/DriverDescription/copy$default$3()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$LaunchDriver/driverDesc()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$LaunchExecutor/appDesc()|",
      "|java+method:///org/apache/spark/deploy/DriverDescription/copy$default$4()|",
      "|java+method:///java/io/File/mkdirs()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$cleanupThreadExecutor()|",
      "|java+method:///scala/collection/Iterable/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$13/Worker$$anonfun$receive$1$$anonfun$applyOrElse$13(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/workDir()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/memoryUsed()|",
      "|java+method:///scala/collection/TraversableOnce/toSet()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$LaunchExecutor/cores()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/worker/DriverRunner/start()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/logError(scala.Function0,java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/deploy/worker/DriverRunner/DriverRunner(org.apache.spark.SparkConf,java.lang.String,java.io.File,java.io.File,org.apache.spark.deploy.DriverDescription,org.apache.spark.rpc.RpcEndpointRef,java.lang.String,org.apache.spark.SecurityManager)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$/maybeUpdateSSLSettings(org.apache.spark.deploy.Command,org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$workerId()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$12/Worker$$anonfun$receive$1$$anonfun$applyOrElse$12(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/coresUsed_$eq(int)|",
      "|java+method:///scala/collection/mutable/HashMap/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///java/lang/Exception/toString()|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/DeployMessages$ExecutorStateChanged(java.lang.String,int,scala.Enumeration$Value,scala.Option,scala.Option)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$11/Worker$$anonfun$receive$1$$anonfun$applyOrElse$11(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ReconnectWorker/masterUrl()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/executors()|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$WorkerSchedulerStateResponse/DeployMessages$WorkerSchedulerStateResponse(java.lang.String,scala.collection.immutable.List,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$KillExecutor/masterUrl()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$10/Worker$$anonfun$receive$1$$anonfun$applyOrElse$10(org.apache.spark.deploy.worker.Worker$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///scala/concurrent/package$/future(scala.Function0,scala.concurrent.ExecutionContext)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/handleDriverStateChanged(org.apache.spark.deploy.DeployMessages$DriverStateChanged)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$activeMasterUrl()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/copy$default$1()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$reregisterWithMaster()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$webUi()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/finishedApps()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$LaunchExecutor/execId()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/conf()|",
      "|java+method:///org/apache/spark/deploy/worker/DriverRunner/kill()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/copy$default$2()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$connected()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/partial/GroupedMeanEvaluator/currentResult()|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/StatCounter/sampleVariance()|",
      "|java+method:///org/apache/spark/partial/GroupedMeanEvaluator/currentResult()|",
      "|java+constructor:///org/apache/spark/partial/StudentTCacher/StudentTCacher(double)|",
      "|java+method:///org/apache/spark/partial/GroupedMeanEvaluator/outputsMerged()|",
      "|java+method:///org/apache/spark/partial/GroupedMeanEvaluator/sums()|",
      "|java+method:///scala/collection/mutable/Map/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/util/Map$Entry/getValue()|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///org/apache/spark/util/StatCounter/count()|",
      "|java+method:///org/apache/spark/partial/StudentTCacher/get(long)|",
      "|java+method:///java/util/Iterator/hasNext()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///java/util/Map$Entry/getKey()|",
      "|java+method:///java/util/HashMap/entrySet()|",
      "|java+method:///org/apache/spark/util/StatCounter/mean()|",
      "|java+method:///java/util/Iterator/next()|",
      "|java+constructor:///org/apache/spark/partial/BoundedDouble/BoundedDouble(double,double,double,double)|",
      "|java+constructor:///java/util/HashMap/HashMap(int)|",
      "|java+method:///java/util/Set/iterator()|",
      "|java+method:///java/util/HashMap/size()|",
      "|java+method:///scala/math/package$/sqrt(double)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/StatCounter/sampleVariance()|",
      "|java+method:///org/apache/spark/partial/GroupedMeanEvaluator/currentResult()|",
      "|java+method:///java/util/Map$Entry/getKey()|",
      "|java+constructor:///org/apache/spark/partial/StudentTCacher/StudentTCacher(double)|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///org/apache/spark/partial/GroupedMeanEvaluator/sums()|",
      "|java+method:///java/util/Map$Entry/getValue()|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///org/apache/spark/util/StatCounter/count()|",
      "|java+method:///org/apache/spark/partial/StudentTCacher/get(long)|",
      "|java+method:///java/util/Iterator/hasNext()|",
      "|java+method:///java/util/HashMap/entrySet()|",
      "|java+method:///org/apache/spark/util/StatCounter/mean()|",
      "|java+method:///org/apache/spark/partial/GroupedMeanEvaluator/outputsMerged()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///java/util/Iterator/next()|",
      "|java+constructor:///org/apache/spark/partial/BoundedDouble/BoundedDouble(double,double,double,double)|",
      "|java+method:///java/util/HashMap/put(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///java/util/HashMap/HashMap(int)|",
      "|java+method:///java/util/Set/iterator()|",
      "|java+method:///java/util/HashMap/size()|",
      "|java+method:///scala/math/package$/sqrt(double)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/RangePartitioner$$anonfun$6/apply(scala.Tuple3)|",
    "called": "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/ArrayOps/size()|",
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///scala/Tuple3/_1()|",
      "|java+method:///scala/Tuple3/_2()|",
      "|java+method:///scala/Tuple3/_3()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Set/$plus$eq(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/RangePartitioner$$anonfun$6$$anonfun$apply$1/RangePartitioner$$anonfun$6$$anonfun$apply$1(org.apache.spark.RangePartitioner$$anonfun$6,float)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///scala/Tuple3/_1()|",
      "|java+method:///scala/Tuple3/_2()|",
      "|java+method:///scala/Tuple3/_3()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Set/$plus$eq(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayOps/size()|",
      "|java+constructor:///org/apache/spark/RangePartitioner$$anonfun$6$$anonfun$apply$1/RangePartitioner$$anonfun$6$$anonfun$apply$1(org.apache.spark.RangePartitioner$$anonfun$6,float)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$9/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/retryCount()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/name()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/desc()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/id()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$9/apply()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$9/apply()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$aggregateByKey$1/apply()|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkEnv/serializer()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKey(scala.Function1,scala.Function2,scala.Function2,org.apache.spark.Partitioner,boolean,org.apache.spark.serializer.Serializer)|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/serialize(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$10/PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$10(org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1,scala.Function0,scala.Function2)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKey$default$6()|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$1/PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$1(org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1,byte%5B%5D,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)|",
      "|java+method:///org/apache/spark/rdd/RDD/context()|",
      "|java+method:///org/apache/spark/serializer/Serializer/newInstance()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///java/nio/ByteBuffer/get(byte%5B%5D)|",
      "|java+constructor:///scala/runtime/VolatileByteRef/VolatileByteRef(byte)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$aggregateByKey$1/apply()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKey$default$5()|",
      "|java+method:///java/nio/ByteBuffer/limit()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKeyWithClassTag$default$5()|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/serialize(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$10/PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$10(org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1,scala.Function0,scala.Function2)|",
      "|java+method:///java/nio/ByteBuffer/limit()|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$1/PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$1(org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1,byte%5B%5D,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKeyWithClassTag$default$6()|",
      "|java+method:///org/apache/spark/SparkEnv/serializer()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKeyWithClassTag(scala.Function1,scala.Function2,scala.Function2,org.apache.spark.Partitioner,boolean,org.apache.spark.serializer.Serializer,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD/context()|",
      "|java+method:///org/apache/spark/serializer/Serializer/newInstance()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///java/nio/ByteBuffer/get(byte%5B%5D)|",
      "|java+constructor:///scala/runtime/VolatileByteRef/VolatileByteRef(byte)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$aggregateByKey$1/apply()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver/org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks(int)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/JavaConversions$/collectionAsScalaIterable(java.util.Collection)|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/get(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$1/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$1(org.apache.spark.shuffle.FileShuffleBlockResolver,int,org.apache.spark.shuffle.FileShuffleBlockResolver$ShuffleState)|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver/org$apache$spark$shuffle$FileShuffleBlockResolver$$shuffleStates()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver/org$apache$spark$shuffle$FileShuffleBlockResolver$$consolidateShuffleFiles()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$ShuffleState/allFileGroups()|",
      "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$4/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$4(org.apache.spark.shuffle.FileShuffleBlockResolver,int)|",
      "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$3/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$3(org.apache.spark.shuffle.FileShuffleBlockResolver,int)|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$ShuffleState/completedMapTasks()|",
      "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$2/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$2(org.apache.spark.shuffle.FileShuffleBlockResolver)|"
    ],
    "v2Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver/logInfo(scala.Function0)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$ShuffleState/completedMapTasks()|",
      "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$1/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$1(org.apache.spark.shuffle.FileShuffleBlockResolver,int,org.apache.spark.shuffle.FileShuffleBlockResolver$ShuffleState)|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver/org$apache$spark$shuffle$FileShuffleBlockResolver$$shuffleStates()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/JavaConverters$/collectionAsScalaIterableConverter(java.util.Collection)|",
      "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$3/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$3(org.apache.spark.shuffle.FileShuffleBlockResolver,int)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$2/FileShuffleBlockResolver$$anonfun$org$apache$spark$shuffle$FileShuffleBlockResolver$$removeShuffleBlocks$2(org.apache.spark.shuffle.FileShuffleBlockResolver,int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/AsynchronousListenerBus$$anon$1$$anonfun$run$1/apply$mcV$sp()|",
    "called": "|java+method:///scala/util/DynamicVariable/withValue(java.lang.Object,scala.Function0)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/AsynchronousListenerBus/org$apache$spark$util$AsynchronousListenerBus$$eventQueue()|",
      "|java+constructor:///java/lang/IllegalStateException/IllegalStateException(java.lang.String)|",
      "|java+method:///org/apache/spark/util/AsynchronousListenerBus$$anon$1/org$apache$spark$util$AsynchronousListenerBus$$anon$$$outer()|",
      "|java+method:///java/util/concurrent/Semaphore/acquire()|",
      "|java+method:///org/apache/spark/util/AsynchronousListenerBus/org$apache$spark$util$AsynchronousListenerBus$$eventLock()|",
      "|java+method:///java/util/concurrent/atomic/AtomicBoolean/get()|",
      "|java+method:///org/apache/spark/util/AsynchronousListenerBus/org$apache$spark$util$AsynchronousListenerBus$$stopped()|",
      "|java+constructor:///scala/runtime/NonLocalReturnControl$mcV$sp/NonLocalReturnControl$mcV$sp(java.lang.Object,scala.runtime.BoxedUnit)|",
      "|java+method:///org/apache/spark/util/AsynchronousListenerBus/org$apache$spark$util$AsynchronousListenerBus$$processingEvent_$eq(boolean)|",
      "|java+method:///org/apache/spark/util/AsynchronousListenerBus/postToAll(java.lang.Object)|",
      "|java+method:///java/util/concurrent/LinkedBlockingQueue/poll()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/AsynchronousListenerBus$/withinListenerThread()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/util/DynamicVariable/withValue(java.lang.Object,scala.Function0)|",
      "|java+constructor:///org/apache/spark/util/AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1/AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1(org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/Utils$/doFetchFile(java.lang.String,java.io.File,java.lang.String,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,org.apache.hadoop.conf.Configuration)|",
    "called": "|java+method:///scala/Option/getOrElse(scala.Function0)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/Utils$/getHadoopFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///java/net/URI/URI(java.lang.String)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/SecurityManager/isAuthenticationEnabled()|",
      "|java+method:///org/apache/spark/util/Utils$/copyFile$default$5()|",
      "|java+method:///org/apache/spark/util/Utils$/setupSecureURLConnection(java.net.URLConnection,org.apache.spark.SecurityManager)|",
      "|java+method:///org/apache/spark/SparkConf/getTimeAsSeconds(java.lang.String,java.lang.String)|",
      "|java+method:///java/net/URI/isAbsolute()|",
      "|java+method:///java/net/URLConnection/connect()|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///java/net/URL/openConnection()|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///java/net/URLConnection/setConnectTimeout(int)|",
      "|java+method:///java/net/URLConnection/setAllowUserInteraction(boolean)|",
      "|java+method:///org/apache/spark/util/Utils$/copyFile(java.lang.String,java.io.File,java.io.File,boolean,boolean)|",
      "|java+constructor:///java/io/File/File(java.net.URI)|",
      "|java+method:///java/net/URI/toURL()|",
      "|java+constructor:///java/net/URL/URL(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$1/Utils$$anonfun$1()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$doFetchFile$2/Utils$$anonfun$doFetchFile$2()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$doFetchFile$1/Utils$$anonfun$doFetchFile$1()|",
      "|java+method:///java/net/URLConnection/getInputStream()|",
      "|java+method:///java/net/URLConnection/setReadTimeout(int)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.net.URI)|",
      "|java+method:///java/net/URI/getScheme()|",
      "|java+method:///org/apache/spark/util/Utils$/constructURIForAuthentication(java.net.URI,org.apache.spark.SecurityManager)|",
      "|java+method:///org/apache/spark/util/Utils$/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/util/Utils$/downloadFile(java.lang.String,java.io.InputStream,java.io.File,boolean)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Utils$/fetchHcfsFile(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.FileSystem,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,boolean,scala.Option)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/Utils$/getHadoopFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///java/net/URI/URI(java.lang.String)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/SecurityManager/isAuthenticationEnabled()|",
      "|java+method:///org/apache/spark/util/Utils$/copyFile$default$5()|",
      "|java+method:///org/apache/spark/util/Utils$/setupSecureURLConnection(java.net.URLConnection,org.apache.spark.SecurityManager)|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///org/apache/spark/SparkConf/getTimeAsSeconds(java.lang.String,java.lang.String)|",
      "|java+method:///java/net/URI/isAbsolute()|",
      "|java+constructor:///java/lang/IllegalStateException/IllegalStateException(java.lang.String)|",
      "|java+method:///java/net/URLConnection/connect()|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///java/net/URL/openConnection()|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///java/nio/channels/Channels/newInputStream(java.nio.channels.ReadableByteChannel)|",
      "|java+method:///java/net/URLConnection/setConnectTimeout(int)|",
      "|java+method:///java/net/URLConnection/setAllowUserInteraction(boolean)|",
      "|java+method:///org/apache/spark/util/Utils$/copyFile(java.lang.String,java.io.File,java.io.File,boolean,boolean)|",
      "|java+constructor:///java/io/File/File(java.net.URI)|",
      "|java+method:///java/net/URI/toURL()|",
      "|java+constructor:///java/net/URL/URL(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$1/Utils$$anonfun$1()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$doFetchFile$2/Utils$$anonfun$doFetchFile$2()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$doFetchFile$1/Utils$$anonfun$doFetchFile$1()|",
      "|java+method:///java/net/URLConnection/getInputStream()|",
      "|java+method:///java/net/URLConnection/setReadTimeout(int)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.net.URI)|",
      "|java+method:///java/net/URI/getScheme()|",
      "|java+method:///org/apache/spark/util/Utils$/constructURIForAuthentication(java.net.URI,org.apache.spark.SecurityManager)|",
      "|java+method:///org/apache/spark/util/Utils$/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/util/Utils$/downloadFile(java.lang.String,java.io.InputStream,java.io.File,boolean)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/openChannel(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/fetchHcfsFile(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.FileSystem,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration,boolean,scala.Option)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1/apply(org.apache.mesos.Protos$Offer)|",
    "called": "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/slaveIdsWithExecutors()|",
      "|java+method:///org/apache/mesos/Protos$Offer/getResourcesList()|",
      "|java+method:///scala/collection/mutable/Map/update(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///java/util/Collections/singleton(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$1/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$1(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/toAttributeMap(java.util.List)|",
      "|java+method:///org/apache/mesos/Protos$OfferID/getValue()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo/newBuilder()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/totalCoresAcquired()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/extraCoresPerSlave()|",
      "|java+method:///org/apache/mesos/Protos$Offer/getAttributesList()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/newMesosTaskId()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/build()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/totalCoresAcquired_$eq(int)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID/getValue()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/failuresBySlaveId()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/addAllResources(java.lang.Iterable)|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/taskIdToSlaveId()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setSlaveId(org.apache.mesos.Protos$SlaveID)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/size()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setCommand(org.apache.mesos.Protos$CommandInfo)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$2/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$2(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1,org.apache.mesos.Protos$TaskInfo$Builder)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/calculateTotalMemory(org.apache.spark.SparkContext)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/setValue(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$3/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$3(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.immutable.Map,double,int,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/MAX_SLAVE_FAILURES()|",
      "|java+method:///org/apache/mesos/SchedulerDriver/declineOffer(org.apache.mesos.Protos$OfferID)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$4/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$4(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.immutable.Map,double,int,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$slaveOfferConstraints()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setName(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/mesos/Protos$TaskID/newBuilder()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/maxCores()|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$slaveIdToHost()|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/partitionResources(java.util.List,java.lang.String,double)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/build()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setTaskId(org.apache.mesos.Protos$TaskID)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/createCommand(org.apache.mesos.Protos$Offer,int,int)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/math/package$/min(int,int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/executorLimit()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/mesos/SchedulerDriver/launchTasks(java.util.Collection,java.util.Collection,org.apache.mesos.Protos$Filters)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getSlaveId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/getResource(java.util.List,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/matchesAttributeRequirements(scala.collection.immutable.Map,scala.collection.immutable.Map)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/coresByTaskId()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getHostname()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/slaveIdsWithExecutors()|",
      "|java+method:///org/apache/mesos/Protos$Offer/getResourcesList()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$1/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$1(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/toAttributeMap(java.util.List)|",
      "|java+method:///org/apache/mesos/Protos$OfferID/getValue()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo/newBuilder()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/totalCoresAcquired()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/extraCoresPerSlave()|",
      "|java+method:///org/apache/mesos/Protos$Offer/getAttributesList()|",
      "|java+method:///org/apache/mesos/Protos$Filters$Builder/setRefuseSeconds(double)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/newMesosTaskId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$rejectOfferDurationForUnmetConstraints()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/build()|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/totalCoresAcquired_$eq(int)|",
      "|java+method:///org/apache/mesos/Protos$SlaveID/getValue()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/failuresBySlaveId()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/addAllResources(java.lang.Iterable)|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/taskIdToSlaveId()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setSlaveId(org.apache.mesos.Protos$SlaveID)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/size()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setCommand(org.apache.mesos.Protos$CommandInfo)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$2/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$2(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1,org.apache.mesos.Protos$TaskInfo$Builder)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/calculateTotalMemory(org.apache.spark.SparkContext)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/setValue(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$3/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$3(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.immutable.Map,double,int,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/MAX_SLAVE_FAILURES()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$5/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$5(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.immutable.Map,double,int,java.lang.String)|",
      "|java+method:///org/apache/mesos/SchedulerDriver/declineOffer(org.apache.mesos.Protos$OfferID)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$4/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$4(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.immutable.Map,double,int,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$slaveOfferConstraints()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setName(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/mesos/Protos$TaskID/newBuilder()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/maxCores()|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$slaveIdToHost()|",
      "|java+method:///org/apache/mesos/SchedulerDriver/declineOffer(org.apache.mesos.Protos$OfferID,org.apache.mesos.Protos$Filters)|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/partitionResources(java.util.List,java.lang.String,double)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/build()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setTaskId(org.apache.mesos.Protos$TaskID)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/createCommand(org.apache.mesos.Protos$Offer,int,int)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/math/package$/min(int,int)|",
      "|java+method:///java/util/Collections/singleton(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$Filters$Builder/build()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/executorLimit()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$Filters/newBuilder()|",
      "|java+method:///org/apache/mesos/SchedulerDriver/launchTasks(java.util.Collection,java.util.Collection,org.apache.mesos.Protos$Filters)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getSlaveId()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/getResource(java.util.List,java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/coresByTaskId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/matchesAttributeRequirements(scala.collection.immutable.Map,scala.collection.immutable.Map)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/put(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getHostname()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/Utils$$anonfun$getThreadDump$2/apply(java.lang.management.ThreadInfo)|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/ArrayOps/mkString(java.lang.String)|",
      "|java+method:///java/lang/management/ThreadInfo/getThreadState()|",
      "|java+method:///java/lang/management/ThreadInfo/getStackTrace()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///java/lang/management/ThreadInfo/getThreadName()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$getThreadDump$2$$anonfun$13/Utils$$anonfun$getThreadDump$2$$anonfun$13(org.apache.spark.util.Utils$$anonfun$getThreadDump$2)|",
      "|java+constructor:///org/apache/spark/util/ThreadStackTrace/ThreadStackTrace(long,java.lang.String,java.lang.Thread$State,java.lang.String)|",
      "|java+method:///java/lang/management/ThreadInfo/getThreadId()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/ArrayOps/mkString(java.lang.String)|",
      "|java+method:///java/lang/management/ThreadInfo/getThreadState()|",
      "|java+method:///java/lang/management/ThreadInfo/getStackTrace()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///java/lang/management/ThreadInfo/getThreadName()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$getThreadDump$2$$anonfun$12/Utils$$anonfun$getThreadDump$2$$anonfun$12(org.apache.spark.util.Utils$$anonfun$getThreadDump$2)|",
      "|java+constructor:///org/apache/spark/util/ThreadStackTrace/ThreadStackTrace(long,java.lang.String,java.lang.Thread$State,java.lang.String)|",
      "|java+method:///java/lang/management/ThreadInfo/getThreadId()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$takeOrdered$1/apply()|",
    "called": "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions$default$2()|",
      "|java+method:///org/apache/spark/rdd/RDD/reduce(scala.Function2)|",
      "|java+method:///scala/Array$/empty(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/sorted(scala.math.Ordering)|",
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$takeOrdered$1$$anonfun$29/RDD$$anonfun$takeOrdered$1$$anonfun$29(org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$takeOrdered$1$$anonfun$apply$46/RDD$$anonfun$takeOrdered$1$$anonfun$apply$46(org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions$default$2()|",
      "|java+method:///org/apache/spark/rdd/RDD/reduce(scala.Function2)|",
      "|java+method:///scala/Array$/empty(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/sorted(scala.math.Ordering)|",
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$takeOrdered$1$$anonfun$29/RDD$$anonfun$takeOrdered$1$$anonfun$29(org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$takeOrdered$1$$anonfun$apply$50/RDD$$anonfun$takeOrdered$1$$anonfun$apply$50(org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/BlockManager/doGetRemote(org.apache.spark.storage.BlockId,boolean)|",
    "called": "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
    "v1Body": [
      "|java+method:///scala/util/Random$/shuffle(scala.collection.TraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMaster/getLocations(org.apache.spark.storage.BlockId)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetRemote$1/BlockManager$$anonfun$doGetRemote$1(org.apache.spark.storage.BlockManager)|",
      "|java+method:///scala/runtime/NonLocalReturnControl/value()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetRemote$3/BlockManager$$anonfun$doGetRemote$3(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/BlockManager/logDebug(scala.Function0)|",
      "|java+method:///scala/runtime/NonLocalReturnControl/key()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetRemote$2/BlockManager$$anonfun$doGetRemote$2(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId,boolean,java.lang.Object)|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/storage/BlockManager/master()|"
    ],
    "v2Body": [
      "|java+method:///scala/util/Random$/shuffle(scala.collection.TraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMaster/getLocations(org.apache.spark.storage.BlockId)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/runtime/NonLocalReturnControl/value()|",
      "|java+constructor:///scala/runtime/IntRef/IntRef(int)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetRemote$3/BlockManager$$anonfun$doGetRemote$3(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/BlockManager/logDebug(scala.Function0)|",
      "|java+method:///scala/runtime/NonLocalReturnControl/key()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetRemote$2/BlockManager$$anonfun$doGetRemote$2(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId,boolean,scala.collection.Seq,scala.runtime.IntRef,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$doGetRemote$1/BlockManager$$anonfun$doGetRemote$1(org.apache.spark.storage.BlockManager)|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/storage/BlockManager/master()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$class/takeSample(org.apache.spark.api.java.JavaRDDLike,boolean,int,long)|",
    "called": "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+constructor:///java/util/ArrayList/ArrayList(java.util.Collection)|",
      "|java+method:///org/apache/spark/rdd/RDD/takeSample(boolean,int,long)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/RDD/takeSample(boolean,int,long)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$114/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$114/apply(long)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$114/apply(org.apache.spark.executor.ShuffleWriteMetrics)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/startDaemon()|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/daemon_$eq(java.lang.Process)|",
      "|java+method:///scala/Option/flatMap(scala.Function1)|",
      "|java+method:///java/io/DataInputStream/readInt()|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/daemonPort_$eq(int)|",
      "|java+method:///java/lang/String/replace(java.lang.CharSequence,java.lang.CharSequence)|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///java/lang/Process/getErrorStream()|",
      "|java+constructor:///java/lang/ProcessBuilder/ProcessBuilder(java.util.List)|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///java/util/Map/putAll(java.util.Map)|",
      "|java+constructor:///java/io/DataInputStream/DataInputStream(java.io.InputStream)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///scala/collection/JavaConversions$/mapAsJavaMap(scala.collection.Map)|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/pythonPath()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkException/setStackTrace(java.lang.StackTraceElement%5B%5D)|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/daemon()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/stopDaemon()|",
      "|java+method:///java/lang/Exception/getStackTrace()|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/redirectStreamsToStderr(java.io.InputStream,java.io.InputStream)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///java/lang/ProcessBuilder/environment()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/api/python/PythonWorkerFactory$$anonfun$3/PythonWorkerFactory$$anonfun$3(org.apache.spark.api.python.PythonWorkerFactory)|",
      "|java+constructor:///org/apache/spark/api/python/PythonWorkerFactory$$anonfun$4/PythonWorkerFactory$$anonfun$4(org.apache.spark.api.python.PythonWorkerFactory)|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/daemon_$eq(java.lang.Process)|",
      "|java+method:///scala/Option/flatMap(scala.Function1)|",
      "|java+method:///java/io/DataInputStream/readInt()|",
      "|java+method:///java/util/Arrays/asList(java.lang.Object%5B%5D)|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+method:///java/lang/String/replace(java.lang.CharSequence,java.lang.CharSequence)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///java/lang/Process/getErrorStream()|",
      "|java+constructor:///java/lang/ProcessBuilder/ProcessBuilder(java.util.List)|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///java/util/Map/putAll(java.util.Map)|",
      "|java+constructor:///java/io/DataInputStream/DataInputStream(java.io.InputStream)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/pythonPath()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/daemonPort_$eq(int)|",
      "|java+method:///scala/collection/JavaConverters$/mapAsJavaMapConverter(scala.collection.Map)|",
      "|java+method:///org/apache/spark/SparkException/setStackTrace(java.lang.StackTraceElement%5B%5D)|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/daemon()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/stopDaemon()|",
      "|java+method:///java/lang/Exception/getStackTrace()|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/redirectStreamsToStderr(java.io.InputStream,java.io.InputStream)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///java/lang/ProcessBuilder/environment()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/api/python/PythonWorkerFactory$$anonfun$3/PythonWorkerFactory$$anonfun$3(org.apache.spark.api.python.PythonWorkerFactory)|",
      "|java+constructor:///org/apache/spark/api/python/PythonWorkerFactory$$anonfun$4/PythonWorkerFactory$$anonfun$4(org.apache.spark.api.python.PythonWorkerFactory)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/AllJobsPage/render(javax.servlet.http.HttpServletRequest)|",
    "called": "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/jobs/AllJobsPage/makeTimeline(scala.collection.Seq,scala.collection.mutable.HashMap,long)|",
      "|java+constructor:///org/apache/spark/ui/jobs/AllJobsPage$$anonfun$18/AllJobsPage$$anonfun$18(org.apache.spark.ui.jobs.AllJobsPage)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/numCompletedJobs()|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/activeJobs()|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/startTime()|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+constructor:///org/apache/spark/ui/jobs/AllJobsPage$$anonfun$21/AllJobsPage$$anonfun$21(org.apache.spark.ui.jobs.AllJobsPage)|",
      "|java+method:///scala/xml/NodeSeq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/numFailedJobs()|",
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/formatDuration(long)|",
      "|java+method:///org/apache/spark/ui/jobs/JobsTab/sc()|",
      "|java+method:///org/apache/spark/ui/jobs/JobsTab/jobProgresslistener()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/completedJobs()|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/failedJobs()|",
      "|java+method:///scala/collection/Seq/sortBy(scala.Function1,scala.math.Ordering)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/AllJobsPage$$anonfun$19/AllJobsPage$$anonfun$19(org.apache.spark.ui.jobs.AllJobsPage)|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/schedulingMode()|",
      "|java+method:///scala/xml/Elem/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/xml/NodeSeq$/canBuildFrom()|",
      "|java+method:///scala/collection/Iterable/toSeq()|",
      "|java+method:///org/apache/spark/ui/jobs/AllJobsPage/jobsTable(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/headerSparkPage(java.lang.String,scala.Function0,org.apache.spark.ui.SparkUITab,scala.Option,scala.Option,boolean)|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/headerSparkPage$default$4()|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/headerSparkPage$default$6()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/ui/jobs/AllJobsPage$$anonfun$render$1/AllJobsPage$$anonfun$render$1(org.apache.spark.ui.jobs.AllJobsPage,scala.runtime.ObjectRef)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/exec/ExecutorsListener/executorIdToData()|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+constructor:///org/apache/spark/ui/jobs/AllJobsPage$$anonfun$20/AllJobsPage$$anonfun$20(org.apache.spark.ui.jobs.AllJobsPage)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/generic/TraversableForwarder/toSeq()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/AllJobsPage$$anonfun$22/AllJobsPage$$anonfun$22(org.apache.spark.ui.jobs.AllJobsPage)|",
      "|java+method:///org/apache/spark/ui/jobs/JobsTab/executorListener()|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+method:///scala/collection/SeqLike/reverse()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/ListBuffer/reverse()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/AllJobsPage/makeTimeline(scala.collection.Seq,scala.collection.mutable.HashMap,long)|",
      "|java+constructor:///org/apache/spark/ui/jobs/AllJobsPage$$anonfun$18/AllJobsPage$$anonfun$18(org.apache.spark.ui.jobs.AllJobsPage)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/numCompletedJobs()|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/activeJobs()|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/startTime()|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+constructor:///org/apache/spark/ui/jobs/AllJobsPage$$anonfun$21/AllJobsPage$$anonfun$21(org.apache.spark.ui.jobs.AllJobsPage)|",
      "|java+method:///scala/xml/NodeSeq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/numFailedJobs()|",
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/formatDuration(long)|",
      "|java+method:///org/apache/spark/ui/jobs/JobsTab/sc()|",
      "|java+method:///org/apache/spark/ui/jobs/JobsTab/jobProgresslistener()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/completedJobs()|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/failedJobs()|",
      "|java+method:///scala/collection/Seq/sortBy(scala.Function1,scala.math.Ordering)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/AllJobsPage$$anonfun$19/AllJobsPage$$anonfun$19(org.apache.spark.ui.jobs.AllJobsPage)|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/schedulingMode()|",
      "|java+method:///scala/xml/Elem/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/xml/NodeSeq$/canBuildFrom()|",
      "|java+method:///scala/collection/Iterable/toSeq()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/endTime()|",
      "|java+method:///org/apache/spark/ui/jobs/AllJobsPage/jobsTable(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/headerSparkPage(java.lang.String,scala.Function0,org.apache.spark.ui.SparkUITab,scala.Option,scala.Option,boolean)|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/headerSparkPage$default$4()|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/headerSparkPage$default$6()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/ui/jobs/AllJobsPage$$anonfun$render$1/AllJobsPage$$anonfun$render$1(org.apache.spark.ui.jobs.AllJobsPage,scala.runtime.ObjectRef)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/exec/ExecutorsListener/executorIdToData()|",
      "|java+method:///scala/collection/Seq/size()|",
      "|java+constructor:///org/apache/spark/ui/jobs/AllJobsPage$$anonfun$20/AllJobsPage$$anonfun$20(org.apache.spark.ui.jobs.AllJobsPage)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/generic/TraversableForwarder/toSeq()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/AllJobsPage$$anonfun$22/AllJobsPage$$anonfun$22(org.apache.spark.ui.jobs.AllJobsPage)|",
      "|java+method:///org/apache/spark/ui/jobs/JobsTab/executorListener()|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+method:///scala/collection/SeqLike/reverse()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/ListBuffer/reverse()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$compute$3/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$compute$3/apply(scala.Tuple2)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$compute$3/apply(scala.Tuple2)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$29/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$29/apply(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$29/apply(scala.collection.Iterable)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/status/api/v1/ApplicationsListResource$/convertApplicationInfo(org.apache.spark.deploy.master.ApplicationInfo,boolean)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/endTime()|",
      "|java+constructor:///org/apache/spark/status/api/v1/ApplicationInfo/ApplicationInfo(java.lang.String,java.lang.String,scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/id()|",
      "|java+constructor:///java/util/Date/Date(long)|",
      "|java+constructor:///org/apache/spark/status/api/v1/ApplicationAttemptInfo/ApplicationAttemptInfo(scala.Option,java.util.Date,java.util.Date,java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/name()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/user()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/desc()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/startTime()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/coresGranted()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/endTime()|",
      "|java+constructor:///org/apache/spark/status/api/v1/ApplicationInfo/ApplicationInfo(java.lang.String,java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/maxCores()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/id()|",
      "|java+constructor:///org/apache/spark/status/api/v1/ApplicationAttemptInfo/ApplicationAttemptInfo(scala.Option,java.util.Date,java.util.Date,java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/name()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/user()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/desc()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/startTime()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///java/util/Date/Date(long)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/memoryPerExecutorMB()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/coresPerExecutor()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$53/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$53/apply(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$53/apply(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/shuffle/sort/SortShuffleManager/unregisterShuffle(int)|",
    "called": "|java+method:///scala/Option$/apply(java.lang.Object)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/shuffle/sort/SortShuffleManager$$anonfun$unregisterShuffle$1/SortShuffleManager$$anonfun$unregisterShuffle$1(org.apache.spark.shuffle.sort.SortShuffleManager,int)|",
      "|java+method:///scala/collection/immutable/Range/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/containsKey(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/remove(java.lang.Object)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+method:///scala/collection/immutable/IndexedSeq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleManager/shuffleMapNumber()|"
    ],
    "v2Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/shuffle/sort/SortShuffleManager$$anonfun$unregisterShuffle$1/SortShuffleManager$$anonfun$unregisterShuffle$1(org.apache.spark.shuffle.sort.SortShuffleManager,int)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/remove(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$doKillExecutors$2/apply(java.lang.String)|",
    "called": "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$pendingRemovedSlaveIds()|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/build()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/setValue(java.lang.String)|",
      "|java+method:///scala/collection/mutable/Map/contains(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$doKillExecutors$2$$anonfun$apply$6/CoarseMesosSchedulerBackend$$anonfun$doKillExecutors$2$$anonfun$apply$6(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$doKillExecutors$2,java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$TaskID/newBuilder()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/mesosDriver()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logWarning(scala.Function0)|",
      "|java+method:///org/apache/mesos/SchedulerDriver/killTask(org.apache.mesos.Protos$TaskID)|",
      "|java+method:///org/spark-project/guava/collect/BiMap/get(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$pendingRemovedSlaveIds()|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/build()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///org/spark-project/guava/collect/BiMap/containsKey(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/setValue(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$doKillExecutors$2$$anonfun$apply$7/CoarseMesosSchedulerBackend$$anonfun$doKillExecutors$2$$anonfun$apply$7(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend$$anonfun$doKillExecutors$2,java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$TaskID/newBuilder()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/mesosDriver()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/logWarning(scala.Function0)|",
      "|java+method:///org/apache/mesos/SchedulerDriver/killTask(org.apache.mesos.Protos$TaskID)|",
      "|java+method:///org/spark-project/guava/collect/BiMap/get(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rpc/RpcEnvConfig$/unapply(org.apache.spark.rpc.RpcEnvConfig)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/port()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/conf()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/host()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/name()|",
      "|java+constructor:///scala/Tuple5/Tuple5(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/securityManager()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/port()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/conf()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/host()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/name()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/clientMode()|",
      "|java+constructor:///scala/Tuple6/Tuple6(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/securityManager()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/requestTotalExecutors(int,int,scala.collection.immutable.Map)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$executorsPendingToRemove()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/hostToLocalTaskCount_$eq(scala.collection.immutable.Map)|",
      "|java+method:///scala/collection/mutable/HashSet/size()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$numPendingExecutors_$eq(int)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/localityAwareTasks_$eq(int)|",
      "|java+method:///scala/math/package$/max(int,int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/numExistingExecutors()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/doRequestTotalExecutors(int)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$executorsPendingToRemove()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/hostToLocalTaskCount_$eq(scala.collection.immutable.Map)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$numPendingExecutors_$eq(int)|",
      "|java+method:///scala/collection/mutable/HashMap/size()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/localityAwareTasks_$eq(int)|",
      "|java+method:///scala/math/package$/max(int,int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/numExistingExecutors()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/doRequestTotalExecutors(int)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/taskCompleted(int,int,int,org.apache.spark.TaskEndReason)|",
    "called": "|java+method:///scala/runtime/NonLocalReturnControl/value$mcV$sp()|",
    "v1Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/authorizedCommittersByStage()|",
      "|java+method:///scala/Option/exists(scala.Function1)|",
      "|java+method:///scala/runtime/NonLocalReturnControl/value$mcV$sp()|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/logDebug(scala.Function0)|",
      "|java+method:///scala/collection/mutable/Map/get(java.lang.Object)|",
      "|java+method:///scala/runtime/NonLocalReturnControl/key()|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/logInfo(scala.Function0)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+constructor:///org/apache/spark/scheduler/OutputCommitCoordinator$$anonfun$1/OutputCommitCoordinator$$anonfun$1(org.apache.spark.scheduler.OutputCommitCoordinator,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/OutputCommitCoordinator$$anonfun$taskCompleted$3/OutputCommitCoordinator$$anonfun$taskCompleted$3(org.apache.spark.scheduler.OutputCommitCoordinator,int,int,int)|",
      "|java+constructor:///org/apache/spark/scheduler/OutputCommitCoordinator$$anonfun$taskCompleted$2/OutputCommitCoordinator$$anonfun$taskCompleted$2(org.apache.spark.scheduler.OutputCommitCoordinator,int,int,int)|",
      "|java+method:///scala/collection/mutable/Map/remove(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Map/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/scheduler/OutputCommitCoordinator$$anonfun$taskCompleted$1/OutputCommitCoordinator$$anonfun$taskCompleted$1(org.apache.spark.scheduler.OutputCommitCoordinator,int)|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/authorizedCommittersByStage()|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/NO_AUTHORIZED_COMMITTER()|",
      "|java+method:///scala/runtime/NonLocalReturnControl/value$mcV$sp()|",
      "|java+method:///scala/collection/mutable/Map/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/logDebug(scala.Function0)|",
      "|java+method:///scala/runtime/NonLocalReturnControl/key()|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/logInfo(scala.Function0)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+constructor:///org/apache/spark/scheduler/OutputCommitCoordinator$$anonfun$1/OutputCommitCoordinator$$anonfun$1(org.apache.spark.scheduler.OutputCommitCoordinator,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/OutputCommitCoordinator$$anonfun$taskCompleted$1/OutputCommitCoordinator$$anonfun$taskCompleted$1(org.apache.spark.scheduler.OutputCommitCoordinator,int,int,int)|",
      "|java+constructor:///org/apache/spark/scheduler/OutputCommitCoordinator$$anonfun$taskCompleted$2/OutputCommitCoordinator$$anonfun$taskCompleted$2(org.apache.spark.scheduler.OutputCommitCoordinator,int,int,int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/SparkContext$$anonfun$26/SparkContext$$anonfun$26(org.apache.spark.SparkContext)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$taskEndReasonFromJson$1/JsonProtocol$$anonfun$taskEndReasonFromJson$1()|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0$mcZ$sp/AbstractFunction0$mcZ$sp()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0$mcZ$sp/AbstractFunction0$mcZ$sp()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/r/SerDe$/writeObject(java.io.DataOutputStream,java.lang.Object)|",
    "called": "|java+method:///scala/Predef$/Long2long(java.lang.Long)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeTime(java.io.DataOutputStream,java.sql.Timestamp)|",
      "|java+method:///scala/math/BigDecimal/toDouble()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToFloat(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeDoubleArr(java.io.DataOutputStream,double%5B%5D)|",
      "|java+method:///scala/reflect/ClassTag$/Double()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToDouble(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeType(java.io.DataOutputStream,java.lang.String)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeBytes(java.io.DataOutputStream,byte%5B%5D)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeDate(java.io.DataOutputStream,java.sql.Date)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeTime(java.io.DataOutputStream,java.sql.Time)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeString(java.io.DataOutputStream,java.lang.String)|",
      "|java+method:///java/io/DataOutputStream/writeInt(int)|",
      "|java+method:///scala/math/BigDecimal$/apply(java.math.BigDecimal)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/Predef$/longArrayOps(long%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/api/r/SerDe$$anonfun$writeObject$2/SerDe$$anonfun$writeObject$2(java.io.DataOutputStream)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeIntArr(java.io.DataOutputStream,int%5B%5D)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeInt(java.io.DataOutputStream,int)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeBoolean(java.io.DataOutputStream,boolean)|",
      "|java+method:///java/lang/Class/getName()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeBytesArr(java.io.DataOutputStream,byte%5B%5D%5B%5D)|",
      "|java+constructor:///org/apache/spark/api/r/SerDe$$anonfun$writeObject$1/SerDe$$anonfun$writeObject$1()|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeDouble(java.io.DataOutputStream,double)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeJObj(java.io.DataOutputStream,java.lang.Object)|",
      "|java+method:///java/lang/String/startsWith(java.lang.String)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeBooleanArr(java.io.DataOutputStream,boolean%5B%5D)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeStringArr(java.io.DataOutputStream,java.lang.String%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/api/r/SerDe$/writeTime(java.io.DataOutputStream,java.sql.Timestamp)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///java/util/Iterator/next()|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeDoubleArr(java.io.DataOutputStream,double%5B%5D)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeType(java.io.DataOutputStream,java.lang.String)|",
      "|java+method:///java/util/Map$Entry/getValue()|",
      "|java+method:///scala/Predef$/Integer2int(java.lang.Integer)|",
      "|java+method:///scala/Predef$/Byte2byte(java.lang.Byte)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///java/lang/Character/toString()|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeDate(java.io.DataOutputStream,java.sql.Date)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeTime(java.io.DataOutputStream,java.sql.Time)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeString(java.io.DataOutputStream,java.lang.String)|",
      "|java+method:///scala/math/BigDecimal$/apply(java.math.BigDecimal)|",
      "|java+method:///scala/reflect/ClassTag$/Int()|",
      "|java+method:///java/util/Iterator/hasNext()|",
      "|java+method:///scala/math/BigDecimal/toDouble()|",
      "|java+method:///scala/Predef$/charArrayOps(char%5B%5D)|",
      "|java+method:///scala/Predef$/longArrayOps(long%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/Long2long(java.lang.Long)|",
      "|java+method:///java/util/Map$Entry/getKey()|",
      "|java+method:///scala/collection/Map/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/org$apache$spark$api$r$SerDe$$writeKeyValue(java.io.DataOutputStream,java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Predef$/shortArrayOps(short%5B%5D)|",
      "|java+method:///scala/Predef$/floatArrayOps(float%5B%5D)|",
      "|java+method:///java/util/Map/size()|",
      "|java+method:///scala/Predef$/Boolean2boolean(java.lang.Boolean)|",
      "|java+method:///scala/Function2/apply(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/api/r/SerDe$$anonfun$writeObject$6/SerDe$$anonfun$writeObject$6(java.io.DataOutputStream)|",
      "|java+constructor:///org/apache/spark/api/r/SerDe$$anonfun$writeObject$5/SerDe$$anonfun$writeObject$5(java.io.DataOutputStream)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeIntArr(java.io.DataOutputStream,int%5B%5D)|",
      "|java+method:///scala/Predef$/Float2float(java.lang.Float)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeInt(java.io.DataOutputStream,int)|",
      "|java+method:///scala/Predef$/Short2short(java.lang.Short)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeBoolean(java.io.DataOutputStream,boolean)|",
      "|java+method:///scala/Predef$/Double2double(java.lang.Double)|",
      "|java+method:///scala/reflect/ClassTag$/Double()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/WrappedArray/toArray(scala.reflect.ClassTag)|",
      "|java+method:///java/util/Set/iterator()|",
      "|java+constructor:///org/apache/spark/api/r/SerDe$$anonfun$writeObject$4/SerDe$$anonfun$writeObject$4()|",
      "|java+constructor:///org/apache/spark/api/r/SerDe$$anonfun$writeObject$3/SerDe$$anonfun$writeObject$3()|",
      "|java+constructor:///org/apache/spark/api/r/SerDe$$anonfun$writeObject$2/SerDe$$anonfun$writeObject$2()|",
      "|java+constructor:///org/apache/spark/api/r/SerDe$$anonfun$writeObject$1/SerDe$$anonfun$writeObject$1()|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeDouble(java.io.DataOutputStream,double)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeJObj(java.io.DataOutputStream,java.lang.Object)|",
      "|java+method:///scala/collection/Map/size()|",
      "|java+method:///java/util/Map/entrySet()|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeBooleanArr(java.io.DataOutputStream,boolean%5B%5D)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeStringArr(java.io.DataOutputStream,java.lang.String%5B%5D)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeBytes(java.io.DataOutputStream,byte%5B%5D)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/sqlSerDe()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/collection/ExternalSorter/stop()|",
    "called": "|java+method:///scala/collection/mutable/ArrayBuffer/clear()|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/ArrayBuffer/clear()|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/spills()|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalSorter$$anonfun$stop$1/ExternalSorter$$anonfun$stop$1(org.apache.spark.util.collection.ExternalSorter)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/foreach(scala.Function1)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/spills()|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalSorter$$anonfun$stop$1/ExternalSorter$$anonfun$stop$1(org.apache.spark.util.collection.ExternalSorter)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/map_$eq(org.apache.spark.util.collection.PartitionedAppendOnlyMap)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/releaseMemory()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/buffer_$eq(org.apache.spark.util.collection.PartitionedPairBuffer)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/clear()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/scope/RDDOperationGraph$$anonfun$org$apache$spark$ui$scope$RDDOperationGraph$$makeDotSubgraph$1/apply(org.apache.spark.ui.scope.RDDOperationNode)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationGraph$/org$apache$spark$ui$scope$RDDOperationGraph$$makeDotNode(org.apache.spark.ui.scope.RDDOperationNode)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationGraph$/org$apache$spark$ui$scope$RDDOperationGraph$$makeDotNode(org.apache.spark.ui.scope.RDDOperationNode)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$4/CoarseMesosSchedulerBackend$$anonfun$4(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$14/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$14/apply(org.apache.spark.scheduler.ActiveJob)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$14/apply(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$68/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$68/apply(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$68/apply(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$12/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$12/apply()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$12/apply()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$/rddInfoFromJson(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/json4s/JsonAST$JValue/toSome()|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/storageLevelFromJson(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/storage/RDDInfo/RDDInfo(int,java.lang.String,int,org.apache.spark.storage.StorageLevel,scala.collection.Seq,scala.Option)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$81/JsonProtocol$$anonfun$81(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/apache/spark/storage/RDDInfo/externalBlockStoreSize_$eq(long)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|",
      "|java+method:///org/apache/spark/storage/RDDInfo/numCachedPartitions_$eq(int)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$79/JsonProtocol$$anonfun$79()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$80/JsonProtocol$$anonfun$80()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$78/JsonProtocol$$anonfun$78()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$77/JsonProtocol$$anonfun$77()|",
      "|java+method:///org/apache/spark/storage/RDDInfo/memSize_$eq(long)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/apache/spark/storage/RDDInfo/diskSize_$eq(long)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/json4s/JsonAST$JValue/toSome()|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/storageLevelFromJson(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$88/JsonProtocol$$anonfun$88(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/apache/spark/storage/RDDInfo/externalBlockStoreSize_$eq(long)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|",
      "|java+constructor:///org/apache/spark/storage/RDDInfo/RDDInfo(int,java.lang.String,int,org.apache.spark.storage.StorageLevel,scala.collection.Seq,java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/storage/RDDInfo/numCachedPartitions_$eq(int)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$83/JsonProtocol$$anonfun$83()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$87/JsonProtocol$$anonfun$87()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$82/JsonProtocol$$anonfun$82()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$86/JsonProtocol$$anonfun$86()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$85/JsonProtocol$$anonfun$85()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$84/JsonProtocol$$anonfun$84()|",
      "|java+method:///org/apache/spark/storage/RDDInfo/memSize_$eq(long)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/apache/spark/storage/RDDInfo/diskSize_$eq(long)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/scope/RDDOperationNode$/unapply(org.apache.spark.ui.scope.RDDOperationNode)|",
    "called": "|java+constructor:///scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/id()|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/cached()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/name()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/id()|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/cached()|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/callsite()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/name()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rpc/RpcEnvConfig/hashCode()|",
    "called": "|java+method:///scala/runtime/Statics/finalizeHash(int,int)|",
    "v1Body": [
      "|java+method:///scala/runtime/Statics/finalizeHash(int,int)|",
      "|java+method:///scala/runtime/Statics/mix(int,int)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/port()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/conf()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/host()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/securityManager()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/name()|",
      "|java+method:///scala/runtime/Statics/anyHash(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/Statics/finalizeHash(int,int)|",
      "|java+method:///scala/runtime/Statics/mix(int,int)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/port()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/conf()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/host()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/securityManager()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/name()|",
      "|java+method:///scala/runtime/Statics/anyHash(java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/clientMode()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$55/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$55/apply(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$55/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$apply$mcVI$sp$2/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$apply$mcVI$sp$2(org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1,int)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$12/apply(java.lang.String)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/Utils$/memoryStringToMb(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/toBoolean()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$$anonfun$fn$6$1/apply(scala.collection.Iterator)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/asScalaIterator(java.util.Iterator)|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterator(scala.collection.Iterator)|",
      "|java+method:///org/apache/spark/api/java/function/DoubleFlatMapFunction/call(java.lang.Object)|",
      "|java+method:///java/lang/Iterable/iterator()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaIteratorConverter(java.util.Iterator)|",
      "|java+method:///org/apache/spark/api/java/function/DoubleFlatMapFunction/call(java.lang.Object)|",
      "|java+method:///java/lang/Iterable/iterator()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1/applyOrElse(java.lang.Object,scala.Function1)|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/BlockManager/getStatus(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveBroadcast/broadcastId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveRdd/rddId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerSlaveEndpoint/org$apache$spark$storage$BlockManagerSlaveEndpoint$$doAsync(java.lang.String,org.apache.spark.rpc.RpcCallContext,scala.Function0)|",
      "|java+method:///org/apache/spark/storage/BlockManager/getMatchingBlockIds(scala.Function1)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4/BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4(org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1,long)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3/BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3(org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1,int)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveShuffle/shuffleId()|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1/BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1(org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1,org.apache.spark.storage.BlockId)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetMatchingBlockIds/filter()|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/reply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveBlock/blockId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetBlockStatus/blockId()|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2/BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2(org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1,int)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/BlockManager/getStatus(org.apache.spark.storage.BlockId)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveBroadcast/broadcastId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveRdd/rddId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerSlaveEndpoint/org$apache$spark$storage$BlockManagerSlaveEndpoint$$doAsync(java.lang.String,org.apache.spark.rpc.RpcCallContext,scala.Function0)|",
      "|java+method:///org/apache/spark/storage/BlockManager/getMatchingBlockIds(scala.Function1)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4/BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4(org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1,long)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveShuffle/shuffleId()|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1/BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1(org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1,org.apache.spark.storage.BlockId)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetMatchingBlockIds/filter()|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/reply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveBlock/blockId()|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3/BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3(org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1,int)|",
      "|java+method:///org/apache/spark/util/Utils$/getThreadDump()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetBlockStatus/blockId()|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2/BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2(org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1,int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$takeSample$1/apply()|",
    "called": "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$takeSample$1/apply()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/RDD/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/rdd/RDD/collect()|",
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Utils$/randomizeInPlace(java.lang.Object,java.util.Random)|",
      "|java+method:///org/apache/spark/rdd/RDD/count()|",
      "|java+method:///java/util/Random/nextInt()|",
      "|java+method:///org/apache/spark/rdd/RDD/sample(boolean,double,long)|",
      "|java+method:///org/apache/spark/util/random/SamplingUtils$/computeFractionForSampleSize(int,long,boolean)|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/collection/mutable/ArrayOps/take(int)|",
      "|java+constructor:///scala/runtime/IntRef/IntRef(int)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$takeSample$1$$anonfun$apply$13/RDD$$anonfun$takeSample$1$$anonfun$apply$13(org.apache.spark.rdd.RDD$$anonfun$takeSample$1,double)|",
      "|java+constructor:///java/util/Random/Random(long)|",
      "|java+method:///scala/math/package$/sqrt(double)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$takeSample$1$$anonfun$apply$12/RDD$$anonfun$takeSample$1$$anonfun$apply$12(org.apache.spark.rdd.RDD$$anonfun$takeSample$1)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$takeSample$1$$anonfun$apply$14/RDD$$anonfun$takeSample$1$$anonfun$apply$14(org.apache.spark.rdd.RDD$$anonfun$takeSample$1,scala.runtime.IntRef)|",
      "|java+method:///scala/runtime/ScalaRunTime$/array_length(java.lang.Object)|",
      "|java+method:///scala/reflect/ClassTag/newArray(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkEnv$/environmentDetails(org.apache.spark.SparkConf,java.lang.String,scala.collection.Seq,scala.collection.Seq)|",
    "called": "|java+method:///scala/util/Properties$/javaHome()|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/util/Properties$/javaVendor()|",
      "|java+method:///scala/collection/Seq/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkConf/contains(java.lang.String)|",
      "|java+method:///scala/util/Properties$/javaVersion()|",
      "|java+method:///scala/Predef$/any2ArrowAssoc(java.lang.Object)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/util/Properties$/javaClassPath()|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$6/SparkEnv$$anonfun$6()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$7/SparkEnv$$anonfun$7()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/util/Utils$/getSystemProperties()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///scala/util/Properties$/versionString()|",
      "|java+method:///scala/collection/Map/toSeq()|",
      "|java+method:///scala/collection/mutable/ArrayOps/filterNot(scala.Function1)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/math/Ordering$/Tuple2(scala.math.Ordering,scala.math.Ordering)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayOps/sorted(scala.math.Ordering)|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$8/SparkEnv$$anonfun$8()|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$9/SparkEnv$$anonfun$9()|",
      "|java+method:///scala/collection/SeqLike/sorted(scala.math.Ordering)|",
      "|java+method:///org/apache/spark/SparkConf/getAll()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/util/Properties$/javaHome()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/util/Properties$/javaVendor()|",
      "|java+method:///scala/collection/Seq/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkConf/contains(java.lang.String)|",
      "|java+method:///scala/util/Properties$/javaVersion()|",
      "|java+method:///scala/Predef$/any2ArrowAssoc(java.lang.Object)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$5/SparkEnv$$anonfun$5()|",
      "|java+method:///scala/util/Properties$/javaClassPath()|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$6/SparkEnv$$anonfun$6()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$7/SparkEnv$$anonfun$7()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/SeqLike/sorted(scala.math.Ordering)|",
      "|java+method:///org/apache/spark/util/Utils$/getSystemProperties()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///scala/util/Properties$/versionString()|",
      "|java+method:///scala/collection/Map/toSeq()|",
      "|java+method:///scala/collection/mutable/ArrayOps/filterNot(scala.Function1)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/math/Ordering$/Tuple2(scala.math.Ordering,scala.math.Ordering)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayOps/sorted(scala.math.Ordering)|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$8/SparkEnv$$anonfun$8()|",
      "|java+method:///org/apache/spark/SparkConf/getAll()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/util/Properties$/javaHome()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$105/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$105/apply(org.apache.spark.executor.ShuffleWriteMetrics)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$105/apply(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$9/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$9/apply(org.apache.mesos.Protos$Offer)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$9/apply(org.apache.mesos.Protos$Offer)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$1/CoGroupedRDD$$anonfun$1(org.apache.spark.rdd.CoGroupedRDD,int)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction2/AbstractFunction2()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/JettyUtils$/addFilters(scala.collection.Seq,org.apache.spark.SparkConf)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/ui/JettyUtils$$anonfun$addFilters$1/JettyUtils$$anonfun$addFilters$1(scala.collection.Seq,org.apache.spark.SparkConf)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/ui/JettyUtils$$anonfun$1/JettyUtils$$anonfun$1()|",
      "|java+method:///scala/collection/immutable/StringOps/split(char)|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/ui/JettyUtils$$anonfun$addFilters$1/JettyUtils$$anonfun$addFilters$1(scala.collection.Seq,org.apache.spark.SparkConf)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/split(char)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/ui/JettyUtils$$anonfun$3/JettyUtils$$anonfun$3()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/Pool/getSchedulableByName(java.lang.String)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+method:///scala/collection/JavaConversions$/collectionAsScalaIterable(java.util.Collection)|",
      "|java+method:///org/apache/spark/scheduler/Pool/schedulableNameToSchedulable()|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/containsKey(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/Pool/schedulableQueue()|",
      "|java+method:///scala/runtime/NonLocalReturnControl/key()|",
      "|java+constructor:///org/apache/spark/scheduler/Pool$$anonfun$getSchedulableByName$1/Pool$$anonfun$getSchedulableByName$1(org.apache.spark.scheduler.Pool,java.lang.String,java.lang.Object)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///scala/runtime/NonLocalReturnControl/value()|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/get(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/Pool/schedulableNameToSchedulable()|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/containsKey(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/Pool/schedulableQueue()|",
      "|java+method:///scala/runtime/NonLocalReturnControl/key()|",
      "|java+constructor:///org/apache/spark/scheduler/Pool$$anonfun$getSchedulableByName$1/Pool$$anonfun$getSchedulableByName$1(org.apache.spark.scheduler.Pool,java.lang.String,java.lang.Object)|",
      "|java+method:///scala/runtime/NonLocalReturnControl/value()|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/get(java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+method:///scala/collection/JavaConverters$/collectionAsScalaIterableConverter(java.util.Collection)|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/DAGScheduler/DAGScheduler(org.apache.spark.SparkContext,org.apache.spark.scheduler.TaskScheduler,org.apache.spark.scheduler.LiveListenerBus,org.apache.spark.MapOutputTrackerMaster,org.apache.spark.storage.BlockManagerMaster,org.apache.spark.SparkEnv,org.apache.spark.util.Clock)|",
    "called": "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkContext/getConf()|",
      "|java+method:///org/apache/spark/serializer/Serializer/newInstance()|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonSingleThreadScheduledExecutor(java.lang.String)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/registerSource(org.apache.spark.metrics.source.Source)|",
      "|java+method:///org/apache/spark/SparkEnv/closureSerializer()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/scheduler/TaskScheduler/setDAGScheduler(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGSchedulerEventProcessLoop/DAGSchedulerEventProcessLoop(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/eventProcessLoop()|",
      "|java+method:///org/apache/spark/scheduler/DAGSchedulerEventProcessLoop/start()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGSchedulerSource/DAGSchedulerSource(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///org/apache/spark/SparkEnv/metricsSystem()|",
      "|java+constructor:///java/util/concurrent/atomic/AtomicInteger/AtomicInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/metricsSource()|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///org/apache/spark/SparkEnv/outputCommitCoordinator()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkContext/getConf()|",
      "|java+method:///org/apache/spark/serializer/Serializer/newInstance()|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonSingleThreadScheduledExecutor(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkEnv/closureSerializer()|",
      "|java+constructor:///java/util/concurrent/atomic/AtomicInteger/AtomicInteger(int)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/scheduler/TaskScheduler/setDAGScheduler(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGSchedulerEventProcessLoop/DAGSchedulerEventProcessLoop(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/eventProcessLoop()|",
      "|java+method:///org/apache/spark/scheduler/DAGSchedulerEventProcessLoop/start()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGSchedulerSource/DAGSchedulerSource(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///org/apache/spark/SparkEnv/outputCommitCoordinator()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$75/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$75/apply(org.apache.spark.scheduler.AccumulableInfo)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$75/apply(org.apache.spark.executor.TaskMetrics)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$78/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$78/apply(org.apache.spark.scheduler.AccumulableInfo)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$78/apply(org.apache.spark.executor.TaskMetrics)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext$$anonfun$binaryRecords$1/apply()|",
    "called": "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///org/apache/hadoop/conf/Configuration/setInt(java.lang.String,int)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$assertNotStopped()|",
      "|java+method:///org/apache/spark/SparkContext$$anonfun$binaryRecords$1/apply()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$binaryRecords$1$$anonfun$31/SparkContext$$anonfun$binaryRecords$1$$anonfun$31(org.apache.spark.SparkContext$$anonfun$binaryRecords$1)|",
      "|java+method:///org/apache/spark/input/FixedLengthBinaryInputFormat$/RECORD_LENGTH_PROPERTY()|",
      "|java+method:///org/apache/spark/SparkContext/newAPIHadoopFile(java.lang.String,java.lang.Class,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///org/apache/hadoop/conf/Configuration/setInt(java.lang.String,int)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$assertNotStopped()|",
      "|java+method:///org/apache/spark/input/FixedLengthBinaryInputFormat$/RECORD_LENGTH_PROPERTY()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$binaryRecords$1$$anonfun$32/SparkContext$$anonfun$binaryRecords$1$$anonfun$32(org.apache.spark.SparkContext$$anonfun$binaryRecords$1)|",
      "|java+method:///org/apache/spark/SparkContext$$anonfun$binaryRecords$1/apply()|",
      "|java+method:///org/apache/spark/SparkContext/newAPIHadoopFile(java.lang.String,java.lang.Class,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleTaskCompletion(org.apache.spark.scheduler.CompletionEvent)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$1/DAGScheduler$$anonfun$handleTaskCompletion$1(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.ResultTask)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/numFinished_$eq(int)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/removeOutputLoc(int,org.apache.spark.storage.BlockManagerId)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/JobListener/taskSucceeded(int,java.lang.Object)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$16/DAGScheduler$$anonfun$handleTaskCompletion$16(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Task,org.apache.spark.scheduler.Stage)|",
      "|java+method:///org/apache/spark/storage/BlockManagerId/executorId()|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///java/util/concurrent/ScheduledExecutorService/schedule(java.lang.Runnable,long,java.util.concurrent.TimeUnit)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/outputLocs()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/Stage/pendingTasks()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$12/DAGScheduler$$anonfun$handleTaskCompletion$12(org.apache.spark.scheduler.DAGScheduler,scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$minus$eq(scala.collection.TraversableOnce)|",
      "|java+method:///org/apache/spark/FetchFailed/message()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$/RESUBMIT_TIMEOUT()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$11/DAGScheduler$$anonfun$handleTaskCompletion$11(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerTaskEnd/SparkListenerTaskEnd(int,int,java.lang.String,org.apache.spark.TaskEndReason,org.apache.spark.scheduler.TaskInfo,org.apache.spark.executor.TaskMetrics)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/shuffleDep()|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/finished()|",
      "|java+method:///org/apache/spark/FetchFailed/bmAddress()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$10/DAGScheduler$$anonfun$handleTaskCompletion$10(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/attemptId()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleExecutorLost(java.lang.String,boolean,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/pendingTasks()|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/taskCompleted(int,int,int,org.apache.spark.TaskEndReason)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/clearCacheLocs()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/submitWaitingStages()|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/listener()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$18/DAGScheduler$$anonfun$handleTaskCompletion$18(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Task,org.apache.spark.scheduler.Stage)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/failedEpoch()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/stageIdToStage()|",
      "|java+constructor:///org/apache/spark/SparkDriverExecutionException/SparkDriverExecutionException(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/scheduler/CompletionEvent/result()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$15/DAGScheduler$$anonfun$handleTaskCompletion$15(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Task)|",
      "|java+method:///org/apache/spark/scheduler/Task/stageId()|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$markStageAsFinished(org.apache.spark.scheduler.Stage,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$submitStage(org.apache.spark.scheduler.Stage)|",
      "|java+method:///org/apache/spark/scheduler/ResultTask/outputId()|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/scheduler/CompletionEvent/taskMetrics()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logDebug(scala.Function0)|",
      "|java+method:///scala/collection/mutable/ArrayOps/contains(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerJobEnd/SparkListenerJobEnd(int,long,org.apache.spark.scheduler.JobResult)|",
      "|java+method:///org/apache/spark/util/Utils$/getFormattedClassName(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/CompletionEvent/reason()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/failedStages()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/abortStage(org.apache.spark.scheduler.Stage,java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/FetchFailed/shuffleId()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/updateAccumulators(org.apache.spark.scheduler.CompletionEvent)|",
      "|java+method:///org/apache/spark/scheduler/MapStatus/location()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/runningStages()|",
      "|java+method:///org/apache/spark/scheduler/CompletionEvent/task()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$2/DAGScheduler$$anonfun$handleTaskCompletion$2(org.apache.spark.scheduler.DAGScheduler,java.lang.String)|",
      "|java+method:///org/apache/spark/MapOutputTrackerMaster/registerMapOutputs(int,org.apache.spark.scheduler.MapStatus%5B%5D,boolean)|",
      "|java+method:///scala/collection/mutable/HashSet/withFilter(scala.Function1)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/messageScheduler()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/disallowStageRetryForTest()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/shuffleToMapStage()|",
      "|java+method:///scala/collection/mutable/HashSet/isEmpty()|",
      "|java+method:///org/apache/spark/scheduler/JobListener/jobFailed(java.lang.Exception)|",
      "|java+method:///org/apache/spark/FetchFailed/mapId()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/outputCommitCoordinator()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$9/DAGScheduler$$anonfun$handleTaskCompletion$9(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.ShuffleMapStage)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/attemptNumber()|",
      "|java+method:///org/apache/spark/scheduler/Task/stageAttemptId()|",
      "|java+method:///org/apache/spark/scheduler/Task/partitionId()|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$19/DAGScheduler$$anonfun$handleTaskCompletion$19(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Stage,org.apache.spark.scheduler.ShuffleMapStage)|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$4/DAGScheduler$$anonfun$handleTaskCompletion$4(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$6/DAGScheduler$$anonfun$handleTaskCompletion$6(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/numPartitions()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$8/DAGScheduler$$anonfun$handleTaskCompletion$8(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$5/DAGScheduler$$anonfun$handleTaskCompletion$5(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anon$1/DAGScheduler$$anon$1(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$7/DAGScheduler$$anonfun$handleTaskCompletion$7(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapTask/partitionId()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/Task/epoch()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/sortBy(scala.Function1,scala.math.Ordering)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/cleanupStateForJobAndIndependentStages(org.apache.spark.scheduler.ActiveJob)|",
      "|java+method:///org/apache/spark/MapOutputTrackerMaster/unregisterMapOutput(int,int,org.apache.spark.storage.BlockManagerId)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$17/DAGScheduler$$anonfun$handleTaskCompletion$17(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Stage,org.apache.spark.scheduler.ShuffleMapStage)|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$plus$eq(scala.collection.TraversableOnce)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$3/DAGScheduler$$anonfun$handleTaskCompletion$3(org.apache.spark.scheduler.DAGScheduler,java.lang.String,org.apache.spark.scheduler.ShuffleMapTask)|",
      "|java+method:///org/apache/spark/scheduler/ResultStage/resultOfJob()|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/addOutputLoc(int,org.apache.spark.scheduler.MapStatus)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapTask/epoch()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/markStageAsFinished$default$2()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$14/DAGScheduler$$anonfun$handleTaskCompletion$14(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///org/apache/spark/ShuffleDependency/shuffleId()|",
      "|java+method:///org/apache/spark/scheduler/CompletionEvent/taskInfo()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/waitingStages()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/HashSet/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$13/DAGScheduler$$anonfun$handleTaskCompletion$13(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/jobId()|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/numFinished()|",
      "|java+method:///org/apache/spark/scheduler/Stage/latestInfo()|",
      "|java+method:///scala/collection/mutable/ResizableArray/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/ActiveJob/numFinished_$eq(int)|",
      "|java+method:///org/apache/spark/scheduler/JobListener/taskSucceeded(int,java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/Task/epoch()|",
      "|java+method:///org/apache/spark/ShuffleDependency/shuffleId()|",
      "|java+method:///org/apache/spark/scheduler/CompletionEvent/taskInfo()|",
      "|java+method:///org/apache/spark/storage/BlockManagerId/executorId()|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///java/util/concurrent/ScheduledExecutorService/schedule(java.lang.Runnable,long,java.util.concurrent.TimeUnit)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/outputLocInMapOutputTrackerFormat()|",
      "|java+method:///org/apache/spark/FetchFailed/message()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$/RESUBMIT_TIMEOUT()|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerTaskEnd/SparkListenerTaskEnd(int,int,java.lang.String,org.apache.spark.TaskEndReason,org.apache.spark.scheduler.TaskInfo,org.apache.spark.executor.TaskMetrics)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/shuffleDep()|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/finished()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$12/DAGScheduler$$anonfun$handleTaskCompletion$12(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Stage,org.apache.spark.scheduler.ShuffleMapStage)|",
      "|java+method:///org/apache/spark/FetchFailed/bmAddress()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/attemptId()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleExecutorLost(java.lang.String,boolean,scala.Option)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/taskCompleted(int,int,int,org.apache.spark.TaskEndReason)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/clearCacheLocs()|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/mapStageJobs()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/submitWaitingStages()|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/listener()|",
      "|java+method:///org/apache/spark/scheduler/Stage$/MAX_CONSECUTIVE_FETCH_FAILURES()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$1/DAGScheduler$$anonfun$handleTaskCompletion$1(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.ResultTask)|",
      "|java+method:///org/apache/spark/scheduler/Stage/failedOnFetchAndShouldAbort(int)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/failedEpoch()|",
      "|java+method:///org/apache/spark/scheduler/CompletionEvent/result()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/stageIdToStage()|",
      "|java+constructor:///org/apache/spark/SparkDriverExecutionException/SparkDriverExecutionException(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/scheduler/Task/stageId()|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$markStageAsFinished(org.apache.spark.scheduler.Stage,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$submitStage(org.apache.spark.scheduler.Stage)|",
      "|java+method:///org/apache/spark/scheduler/ResultTask/outputId()|",
      "|java+method:///org/apache/spark/scheduler/ResultStage/activeJob()|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/removeOutputLoc(int,org.apache.spark.storage.BlockManagerId)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/isAvailable()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/pendingPartitions()|",
      "|java+method:///org/apache/spark/scheduler/CompletionEvent/taskMetrics()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerJobEnd/SparkListenerJobEnd(int,long,org.apache.spark.scheduler.JobResult)|",
      "|java+method:///org/apache/spark/scheduler/Stage/pendingPartitions()|",
      "|java+method:///org/apache/spark/util/Utils$/getFormattedClassName(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/CompletionEvent/reason()|",
      "|java+method:///org/apache/spark/MapOutputTrackerMaster/getStatistics(org.apache.spark.ShuffleDependency)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/failedStages()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/abortStage(org.apache.spark.scheduler.Stage,java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/FetchFailed/shuffleId()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/updateAccumulators(org.apache.spark.scheduler.CompletionEvent)|",
      "|java+method:///org/apache/spark/scheduler/MapStatus/location()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/runningStages()|",
      "|java+method:///org/apache/spark/scheduler/CompletionEvent/task()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$2/DAGScheduler$$anonfun$handleTaskCompletion$2(org.apache.spark.scheduler.DAGScheduler,java.lang.String)|",
      "|java+method:///org/apache/spark/MapOutputTrackerMaster/registerMapOutputs(int,org.apache.spark.scheduler.MapStatus%5B%5D,boolean)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/messageScheduler()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$10/DAGScheduler$$anonfun$handleTaskCompletion$10(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Task)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/disallowStageRetryForTest()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/shuffleToMapStage()|",
      "|java+method:///scala/collection/mutable/HashSet/isEmpty()|",
      "|java+method:///org/apache/spark/scheduler/JobListener/jobFailed(java.lang.Exception)|",
      "|java+method:///org/apache/spark/FetchFailed/mapId()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/outputCommitCoordinator()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$8/DAGScheduler$$anonfun$handleTaskCompletion$8(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.ShuffleMapStage)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/attemptNumber()|",
      "|java+method:///org/apache/spark/scheduler/Task/stageAttemptId()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/Task/partitionId()|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+method:///org/apache/spark/scheduler/Stage/name()|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$4/DAGScheduler$$anonfun$handleTaskCompletion$4(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$6/DAGScheduler$$anonfun$handleTaskCompletion$6(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/numPartitions()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$5/DAGScheduler$$anonfun$handleTaskCompletion$5(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anon$1/DAGScheduler$$anon$1(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$7/DAGScheduler$$anonfun$handleTaskCompletion$7(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$11/DAGScheduler$$anonfun$handleTaskCompletion$11(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Task,org.apache.spark.scheduler.Stage)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapTask/partitionId()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/cleanupStateForJobAndIndependentStages(org.apache.spark.scheduler.ActiveJob)|",
      "|java+method:///org/apache/spark/MapOutputTrackerMaster/unregisterMapOutput(int,int,org.apache.spark.storage.BlockManagerId)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$3/DAGScheduler$$anonfun$handleTaskCompletion$3(org.apache.spark.scheduler.DAGScheduler,java.lang.String,org.apache.spark.scheduler.ShuffleMapTask)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/addOutputLoc(int,org.apache.spark.scheduler.MapStatus)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$9/DAGScheduler$$anonfun$handleTaskCompletion$9(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.MapOutputStatistics)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$13/DAGScheduler$$anonfun$handleTaskCompletion$13(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Task,org.apache.spark.scheduler.Stage)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapTask/epoch()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/markStageAsFinished$default$2()|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/jobId()|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/numFinished()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleTaskCompletion$14/DAGScheduler$$anonfun$handleTaskCompletion$14(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Stage,org.apache.spark.scheduler.ShuffleMapStage)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/scheduler/Stage/latestInfo()|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/Utils$$anonfun$findLocalInetAddress$1/apply(java.net.NetworkInterface)|",
    "called": "|java+method:///scala/collection/JavaConverters$/enumerationAsScalaIteratorConverter(java.util.Enumeration)|",
    "v1Body": [
      "|java+method:///scala/collection/immutable/List/find(scala.Function1)|",
      "|java+method:///java/net/InetAddress/getByAddress(byte%5B%5D)|",
      "|java+method:///java/net/InetAddress/getAddress()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$findLocalInetAddress$1$$anonfun$6/Utils$$anonfun$findLocalInetAddress$1$$anonfun$6(org.apache.spark.util.Utils$$anonfun$findLocalInetAddress$1,scala.collection.immutable.List)|",
      "|java+method:///org/apache/spark/util/Utils$/logWarning(scala.Function0)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$findLocalInetAddress$1$$anonfun$apply$4/Utils$$anonfun$findLocalInetAddress$1$$anonfun$apply$4(org.apache.spark.util.Utils$$anonfun$findLocalInetAddress$1,java.net.NetworkInterface,java.net.InetAddress)|",
      "|java+method:///java/net/NetworkInterface/getInetAddresses()|",
      "|java+method:///scala/collection/immutable/List/filterNot(scala.Function1)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/Iterator/toList()|",
      "|java+method:///scala/collection/JavaConversions$/enumerationAsScalaIterator(java.util.Enumeration)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$findLocalInetAddress$1$$anonfun$4/Utils$$anonfun$findLocalInetAddress$1$$anonfun$4(org.apache.spark.util.Utils$$anonfun$findLocalInetAddress$1)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$findLocalInetAddress$1$$anonfun$apply$5/Utils$$anonfun$findLocalInetAddress$1$$anonfun$apply$5(org.apache.spark.util.Utils$$anonfun$findLocalInetAddress$1)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$findLocalInetAddress$1$$anonfun$5/Utils$$anonfun$findLocalInetAddress$1$$anonfun$5(org.apache.spark.util.Utils$$anonfun$findLocalInetAddress$1)|",
      "|java+constructor:///scala/runtime/NonLocalReturnControl/NonLocalReturnControl(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/immutable/List/nonEmpty()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$findLocalInetAddress$1$$anonfun$6/Utils$$anonfun$findLocalInetAddress$1$$anonfun$6(org.apache.spark.util.Utils$$anonfun$findLocalInetAddress$1,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/util/Utils$/logWarning(scala.Function0)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$findLocalInetAddress$1$$anonfun$apply$4/Utils$$anonfun$findLocalInetAddress$1$$anonfun$apply$4(org.apache.spark.util.Utils$$anonfun$findLocalInetAddress$1,java.net.NetworkInterface,java.net.InetAddress)|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+constructor:///scala/runtime/NonLocalReturnControl/NonLocalReturnControl(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///java/net/InetAddress/getByAddress(byte%5B%5D)|",
      "|java+method:///scala/collection/Iterator/toSeq()|",
      "|java+method:///scala/collection/Seq/find(scala.Function1)|",
      "|java+method:///java/net/InetAddress/getAddress()|",
      "|java+method:///scala/collection/JavaConverters$/enumerationAsScalaIteratorConverter(java.util.Enumeration)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$findLocalInetAddress$1$$anonfun$4/Utils$$anonfun$findLocalInetAddress$1$$anonfun$4(org.apache.spark.util.Utils$$anonfun$findLocalInetAddress$1)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$findLocalInetAddress$1$$anonfun$apply$5/Utils$$anonfun$findLocalInetAddress$1$$anonfun$apply$5(org.apache.spark.util.Utils$$anonfun$findLocalInetAddress$1)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$findLocalInetAddress$1$$anonfun$5/Utils$$anonfun$findLocalInetAddress$1$$anonfun$5(org.apache.spark.util.Utils$$anonfun$findLocalInetAddress$1)|",
      "|java+method:///scala/collection/Iterator/filterNot(scala.Function1)|",
      "|java+method:///java/net/NetworkInterface/getInetAddresses()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$53/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/executor/TaskMetrics/memoryBytesSpilled()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$53$$anonfun$apply$21/StagePage$$anonfun$53$$anonfun$apply$21(org.apache.spark.ui.jobs.StagePage$$anonfun$53)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/shuffleReadMetrics()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$53$$anonfun$apply$8/StagePage$$anonfun$53$$anonfun$apply$8(org.apache.spark.ui.jobs.StagePage$$anonfun$53)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$5/MesosSchedulerBackend$$anonfun$5(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/Pool/checkSpeculatableTasks()|",
    "called": "|java+constructor:///scala/runtime/BooleanRef/BooleanRef(boolean)|",
    "v1Body": [
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+method:///scala/collection/JavaConversions$/collectionAsScalaIterable(java.util.Collection)|",
      "|java+constructor:///org/apache/spark/scheduler/Pool$$anonfun$checkSpeculatableTasks$1/Pool$$anonfun$checkSpeculatableTasks$1(org.apache.spark.scheduler.Pool,scala.runtime.BooleanRef)|",
      "|java+method:///org/apache/spark/scheduler/Pool/schedulableQueue()|",
      "|java+constructor:///scala/runtime/BooleanRef/BooleanRef(boolean)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/scheduler/Pool$$anonfun$checkSpeculatableTasks$1/Pool$$anonfun$checkSpeculatableTasks$1(org.apache.spark.scheduler.Pool,scala.runtime.BooleanRef)|",
      "|java+method:///scala/collection/JavaConverters$/collectionAsScalaIterableConverter(java.util.Collection)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/Pool/schedulableQueue()|",
      "|java+constructor:///scala/runtime/BooleanRef/BooleanRef(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext$$anonfun$sequenceFile$3/apply()|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+method:///scala/Function0/apply()|",
      "|java+method:///org/apache/spark/SparkContext$$anonfun$sequenceFile$3/apply()|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/hadoopFile(java.lang.String,java.lang.Class,java.lang.Class,java.lang.Class,int)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$assertNotStopped()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$sequenceFile$3$$anonfun$apply$11/SparkContext$$anonfun$sequenceFile$3$$anonfun$apply$11(org.apache.spark.SparkContext$$anonfun$sequenceFile$3,org.apache.spark.WritableConverter,org.apache.spark.WritableConverter)|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///org/apache/spark/WritableConverter/writableClass()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+method:///scala/Function0/apply()|",
      "|java+method:///org/apache/spark/SparkContext$$anonfun$sequenceFile$3/apply()|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/hadoopFile(java.lang.String,java.lang.Class,java.lang.Class,java.lang.Class,int)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$assertNotStopped()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$sequenceFile$3$$anonfun$apply$12/SparkContext$$anonfun$sequenceFile$3$$anonfun$apply$12(org.apache.spark.SparkContext$$anonfun$sequenceFile$3,org.apache.spark.WritableConverter,org.apache.spark.WritableConverter)|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///org/apache/spark/WritableConverter/writableClass()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$onDisconnected$2/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$onDisconnected$2(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint,org.apache.spark.rpc.RpcAddress)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$executorLost$1/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$executorLost$1/apply(int)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$executorLost$1/apply(scala.Tuple2)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$/mapToJson(scala.collection.Map)|",
    "called": "|java+method:///scala/collection/Map$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///scala/collection/Map$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$48/JsonProtocol$$anonfun$48()|",
      "|java+constructor:///org/json4s/JsonAST$JObject/JsonAST$JObject(scala.collection.immutable.List)|",
      "|java+method:///scala/collection/Map/toList()|",
      "|java+method:///scala/collection/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Map$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$51/JsonProtocol$$anonfun$51()|",
      "|java+constructor:///org/json4s/JsonAST$JObject/JsonAST$JObject(scala.collection.immutable.List)|",
      "|java+method:///scala/collection/Map/toList()|",
      "|java+method:///scala/collection/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaSparkContext/union(org.apache.spark.api.java.JavaDoubleRDD,java.util.List)|",
    "called": "|java+method:///scala/collection/Seq$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkContext/union(scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/api/java/JavaSparkContext$$anonfun$3/JavaSparkContext$$anonfun$3(org.apache.spark.api.java.JavaSparkContext)|",
      "|java+constructor:///org/apache/spark/api/java/JavaDoubleRDD/JavaDoubleRDD(org.apache.spark.rdd.RDD)|",
      "|java+method:///scala/reflect/ClassTag$/Double()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext/sc()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkContext/union(scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/api/java/JavaSparkContext$$anonfun$3/JavaSparkContext$$anonfun$3(org.apache.spark.api.java.JavaSparkContext)|",
      "|java+constructor:///org/apache/spark/api/java/JavaDoubleRDD/JavaDoubleRDD(org.apache.spark.rdd.RDD)|",
      "|java+method:///scala/reflect/ClassTag$/Double()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext/sc()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$class/partitionResources(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils,java.util.List,java.lang.String,double)|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/Buffer/filter(scala.Function1)|",
      "|java+constructor:///scala/runtime/DoubleRef/DoubleRef(double)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$1/MesosSchedulerUtils$$anonfun$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils,scala.runtime.DoubleRef,scala.runtime.ObjectRef,java.lang.String)|",
      "|java+method:///scala/collection/mutable/Buffer/toList()|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$2/MesosSchedulerUtils$$anonfun$2(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/toList()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Buffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///scala/collection/mutable/Buffer/filter(scala.Function1)|",
      "|java+constructor:///scala/runtime/DoubleRef/DoubleRef(double)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$1/MesosSchedulerUtils$$anonfun$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils,scala.runtime.DoubleRef,scala.runtime.ObjectRef,java.lang.String)|",
      "|java+method:///scala/collection/mutable/Buffer/toList()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/toList()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$2/MesosSchedulerUtils$$anonfun$2(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anon$1/FileShuffleBlockResolver$$anon$1(org.apache.spark.shuffle.FileShuffleBlockResolver,int,int,int,org.apache.spark.serializer.Serializer,org.apache.spark.executor.ShuffleWriteMetrics)|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/serializer/Serializer/newInstance()|",
      "|java+method:///java/lang/System/nanoTime()|",
      "|java+method:///org/apache/spark/executor/ShuffleWriteMetrics/incShuffleWriteTime(long)|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/putIfAbsent(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anon$1$$anonfun$3/FileShuffleBlockResolver$$anon$1$$anonfun$3(org.apache.spark.shuffle.FileShuffleBlockResolver$$anon$1)|",
      "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anon$1$$anonfun$2/FileShuffleBlockResolver$$anon$1$$anonfun$2(org.apache.spark.shuffle.FileShuffleBlockResolver$$anon$1)|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver/org$apache$spark$shuffle$FileShuffleBlockResolver$$shuffleStates()|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anon$1/org$apache$spark$shuffle$FileShuffleBlockResolver$$anon$$fileGroup_$eq(org.apache.spark.shuffle.FileShuffleBlockResolver$ShuffleFileGroup)|",
      "|java+method:///scala/Array$/tabulate(int,scala.Function1,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anon$1/getUnusedFileGroup()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver/org$apache$spark$shuffle$FileShuffleBlockResolver$$consolidateShuffleFiles()|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anon$1/openStartTime()|",
      "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$ShuffleState/FileShuffleBlockResolver$ShuffleState(org.apache.spark.shuffle.FileShuffleBlockResolver,int)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/serializer/Serializer/newInstance()|",
      "|java+method:///java/lang/System/nanoTime()|",
      "|java+method:///scala/Array$/tabulate(int,scala.Function1,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/executor/ShuffleWriteMetrics/incShuffleWriteTime(long)|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/putIfAbsent(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anon$1$$anonfun$2/FileShuffleBlockResolver$$anon$1$$anonfun$2(org.apache.spark.shuffle.FileShuffleBlockResolver$$anon$1)|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver/org$apache$spark$shuffle$FileShuffleBlockResolver$$shuffleStates()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/apache/spark/shuffle/FileShuffleBlockResolver$$anon$1/openStartTime()|",
      "|java+constructor:///org/apache/spark/shuffle/FileShuffleBlockResolver$ShuffleState/FileShuffleBlockResolver$ShuffleState(org.apache.spark.shuffle.FileShuffleBlockResolver,int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/SparkShutdownHookManager/runAll()|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/util/Try$/apply(scala.Function0)|",
      "|java+constructor:///org/apache/spark/util/SparkShutdownHookManager$$anonfun$runAll$1/SparkShutdownHookManager$$anonfun$runAll$1(org.apache.spark.util.SparkShutdownHookManager)|",
      "|java+method:///org/apache/spark/util/SparkShutdownHookManager/shuttingDown_$eq(boolean)|",
      "|java+method:///org/apache/spark/util/SparkShutdownHookManager/org$apache$spark$util$SparkShutdownHookManager$$hooks()|",
      "|java+method:///java/util/PriorityQueue/isEmpty()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/util/SparkShutdownHookManager$$anonfun$runAll$1/SparkShutdownHookManager$$anonfun$runAll$1(org.apache.spark.util.SparkShutdownHookManager,scala.runtime.ObjectRef)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///scala/util/Try$/apply(scala.Function0)|",
      "|java+method:///org/apache/spark/util/SparkShutdownHookManager/hooks()|",
      "|java+method:///org/apache/spark/util/SparkShutdownHookManager/shuttingDown_$eq(boolean)|",
      "|java+method:///java/util/PriorityQueue/poll()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/Pool/getSortedTaskSetQueue()|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/scheduler/Pool$$anonfun$1/Pool$$anonfun$1(org.apache.spark.scheduler.Pool,org.apache.spark.scheduler.SchedulingAlgorithm)|",
      "|java+method:///scala/collection/JavaConversions$/collectionAsScalaIterable(java.util.Collection)|",
      "|java+method:///scala/collection/Iterable/toSeq()|",
      "|java+method:///org/apache/spark/scheduler/Pool/taskSetSchedulingAlgorithm()|",
      "|java+constructor:///org/apache/spark/scheduler/Pool$$anonfun$getSortedTaskSetQueue$1/Pool$$anonfun$getSortedTaskSetQueue$1(org.apache.spark.scheduler.Pool,scala.runtime.ObjectRef)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///org/apache/spark/scheduler/Pool/schedulableQueue()|",
      "|java+method:///scala/collection/Seq/sortWith(scala.Function2)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/scheduler/Pool$$anonfun$1/Pool$$anonfun$1(org.apache.spark.scheduler.Pool,org.apache.spark.scheduler.SchedulingAlgorithm)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///org/apache/spark/scheduler/Pool/taskSetSchedulingAlgorithm()|",
      "|java+constructor:///org/apache/spark/scheduler/Pool$$anonfun$getSortedTaskSetQueue$1/Pool$$anonfun$getSortedTaskSetQueue$1(org.apache.spark.scheduler.Pool,scala.runtime.ObjectRef)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/collection/TraversableOnce/toSeq()|",
      "|java+method:///org/apache/spark/scheduler/Pool/schedulableQueue()|",
      "|java+method:///scala/collection/Seq/sortWith(scala.Function2)|",
      "|java+method:///scala/collection/JavaConverters$/collectionAsScalaIterableConverter(java.util.Collection)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/SparkCuratorUtil$/deleteRecursive(org.apache.curator.framework.CuratorFramework,java.lang.String)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///org/apache/curator/framework/CuratorFramework/getChildren()|",
      "|java+method:///org/apache/curator/framework/CuratorFramework/delete()|",
      "|java+method:///scala/collection/mutable/Buffer/foreach(scala.Function1)|",
      "|java+method:///org/apache/curator/framework/api/DeleteBuilder/forPath(java.lang.String)|",
      "|java+method:///org/apache/curator/framework/api/GetChildrenBuilder/forPath(java.lang.String)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///org/apache/curator/framework/api/ExistsBuilder/forPath(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/SparkCuratorUtil$$anonfun$deleteRecursive$1/SparkCuratorUtil$$anonfun$deleteRecursive$1(org.apache.curator.framework.CuratorFramework,java.lang.String)|",
      "|java+method:///org/apache/curator/framework/CuratorFramework/checkExists()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/curator/framework/CuratorFramework/getChildren()|",
      "|java+method:///org/apache/curator/framework/CuratorFramework/delete()|",
      "|java+method:///org/apache/curator/framework/api/DeleteBuilder/forPath(java.lang.String)|",
      "|java+method:///org/apache/curator/framework/api/GetChildrenBuilder/forPath(java.lang.String)|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///org/apache/curator/framework/api/ExistsBuilder/forPath(java.lang.String)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/deploy/SparkCuratorUtil$$anonfun$deleteRecursive$1/SparkCuratorUtil$$anonfun$deleteRecursive$1(org.apache.curator.framework.CuratorFramework,java.lang.String)|",
      "|java+method:///org/apache/curator/framework/CuratorFramework/checkExists()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$82/TaskDataSource$$anonfun$82(org.apache.spark.ui.jobs.TaskDataSource)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$countByValue$1/apply()|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/reflect/ClassTag$/Null()|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$countByValue$1/apply()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$countByValue$1$$anonfun$apply$41/RDD$$anonfun$countByValue$1$$anonfun$apply$41(org.apache.spark.rdd.RDD$$anonfun$countByValue$1)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/countByKey()|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/reflect/ClassTag$/Null()|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$countByValue$1/apply()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/countByKey()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$countByValue$1$$anonfun$apply$45/RDD$$anonfun$countByValue$1$$anonfun$apply$45(org.apache.spark.rdd.RDD$$anonfun$countByValue$1)|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ExecutorLostFailure/toErrorString()|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/ExecutorLostFailure/execId()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ExecutorLostFailure/reason()|",
      "|java+method:///org/apache/spark/ExecutorLostFailure/exitCausedByApp()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/ExecutorLostFailure/execId()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ExecutorLostFailure$$anonfun$toErrorString$2/ExecutorLostFailure$$anonfun$toErrorString$2(org.apache.spark.ExecutorLostFailure)|",
      "|java+constructor:///org/apache/spark/ExecutorLostFailure$$anonfun$toErrorString$1/ExecutorLostFailure$$anonfun$toErrorString$1(org.apache.spark.ExecutorLostFailure)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/ReliableRDDCheckpointData/doCheckpoint()|",
    "called": "|java+method:///scala/Option/foreach(scala.Function1)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/ReliableRDDCheckpointData/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/rdd/ReliableCheckpointRDD$/writeCheckpointFile$default$3()|",
      "|java+method:///scala/reflect/ClassTag$/Unit()|",
      "|java+method:///org/apache/spark/SparkContext/broadcast(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/rdd/ReliableRDDCheckpointData$$anonfun$doCheckpoint$1/ReliableRDDCheckpointData$$anonfun$doCheckpoint$1(org.apache.spark.rdd.ReliableRDDCheckpointData,org.apache.spark.broadcast.Broadcast,int)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/rdd/ReliableRDDCheckpointData$$anonfun$doCheckpoint$3/ReliableRDDCheckpointData$$anonfun$doCheckpoint$3(org.apache.spark.rdd.ReliableRDDCheckpointData,org.apache.spark.rdd.ReliableCheckpointRDD)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/mkdirs(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/rdd/RDD/conf()|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///org/apache/spark/rdd/RDD/context()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function2,scala.reflect.ClassTag)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/ReliableRDDCheckpointData/org$apache$spark$rdd$ReliableRDDCheckpointData$$cpDir()|",
      "|java+method:///org/apache/spark/SparkContext/hadoopConfiguration()|",
      "|java+constructor:///org/apache/spark/rdd/ReliableCheckpointRDD/ReliableCheckpointRDD(org.apache.spark.SparkContext,java.lang.String,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/ReliableCheckpointRDD/partitions()|",
      "|java+method:///org/apache/spark/SparkContext/cleaner()|",
      "|java+constructor:///org/apache/spark/rdd/ReliableRDDCheckpointData$$anonfun$doCheckpoint$2/ReliableRDDCheckpointData$$anonfun$doCheckpoint$2(org.apache.spark.rdd.ReliableRDDCheckpointData,org.apache.spark.rdd.ReliableCheckpointRDD)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/rdd/ReliableRDDCheckpointData/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/rdd/ReliableCheckpointRDD$/writeRDDToCheckpointDirectory(org.apache.spark.rdd.RDD,java.lang.String,int,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/ReliableRDDCheckpointData/org$apache$spark$rdd$ReliableRDDCheckpointData$$cpDir()|",
      "|java+method:///org/apache/spark/rdd/ReliableCheckpointRDD$/writeRDDToCheckpointDirectory$default$3()|",
      "|java+constructor:///org/apache/spark/rdd/ReliableRDDCheckpointData$$anonfun$doCheckpoint$1/ReliableRDDCheckpointData$$anonfun$doCheckpoint$1(org.apache.spark.rdd.ReliableRDDCheckpointData,org.apache.spark.rdd.ReliableCheckpointRDD)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/rdd/RDD/conf()|",
      "|java+method:///org/apache/spark/rdd/ReliableRDDCheckpointData/org$apache$spark$rdd$ReliableRDDCheckpointData$$rdd()|",
      "|java+method:///org/apache/spark/rdd/RDD/context()|",
      "|java+method:///org/apache/spark/SparkContext/cleaner()|",
      "|java+constructor:///org/apache/spark/rdd/ReliableRDDCheckpointData$$anonfun$doCheckpoint$2/ReliableRDDCheckpointData$$anonfun$doCheckpoint$2(org.apache.spark.rdd.ReliableRDDCheckpointData,org.apache.spark.rdd.ReliableCheckpointRDD)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$47/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$47$$anonfun$apply$6/StagePage$$anonfun$47$$anonfun$apply$6(org.apache.spark.ui.jobs.StagePage$$anonfun$47)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/shuffleReadMetrics()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Option/get()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$47$$anonfun$apply$19/StagePage$$anonfun$47$$anonfun$apply$19(org.apache.spark.ui.jobs.StagePage$$anonfun$47)|",
      "|java+method:///scala/Option/map(scala.Function1)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$47$$anonfun$apply$15/StagePage$$anonfun$47$$anonfun$apply$15(org.apache.spark.ui.jobs.StagePage$$anonfun$47)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$47$$anonfun$apply$2/StagePage$$anonfun$47$$anonfun$apply$2(org.apache.spark.ui.jobs.StagePage$$anonfun$47)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/inputMetrics()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1/applyOrElse(java.lang.Throwable,scala.Function1)|",
    "called": "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
    "v1Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1/org$apache$spark$scheduler$cluster$YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$$$outer()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1$$anonfun$applyOrElse$6/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1$$anonfun$applyOrElse$6(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/sendFailure(java.lang.Throwable)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/logError(scala.Function0,java.lang.Throwable)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/sendFailure(java.lang.Throwable)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1/org$apache$spark$scheduler$cluster$YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$$$outer()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1$$anonfun$applyOrElse$8/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1$$anonfun$applyOrElse$8(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/logError(scala.Function0,java.lang.Throwable)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/InputFormatInfo/prefLocsFromMapreduceInputFormat()|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///org/apache/hadoop/mapred/FileInputFormat/setInputPaths(org.apache.hadoop.mapred.JobConf,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/addCredentials(org.apache.hadoop.mapred.JobConf)|",
      "|java+method:///org/apache/hadoop/mapreduce/InputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)|",
      "|java+method:///scala/collection/mutable/Buffer/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/Job/Job(org.apache.hadoop.conf.Configuration)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/toSet()|",
      "|java+method:///org/apache/spark/scheduler/InputFormatInfo/path()|",
      "|java+method:///org/apache/hadoop/util/ReflectionUtils/newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)|",
      "|java+constructor:///org/apache/spark/scheduler/InputFormatInfo$$anonfun$prefLocsFromMapreduceInputFormat$1/InputFormatInfo$$anonfun$prefLocsFromMapreduceInputFormat$1(org.apache.spark.scheduler.InputFormatInfo,scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///org/apache/spark/scheduler/InputFormatInfo/inputFormatClazz()|",
      "|java+constructor:///org/apache/hadoop/mapred/JobConf/JobConf(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/scheduler/InputFormatInfo/configuration()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/hadoop/mapred/FileInputFormat/setInputPaths(org.apache.hadoop.mapred.JobConf,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/addCredentials(org.apache.hadoop.mapred.JobConf)|",
      "|java+method:///org/apache/hadoop/mapreduce/InputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/Job/Job(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///org/apache/spark/scheduler/InputFormatInfo/configuration()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/toSet()|",
      "|java+method:///org/apache/spark/scheduler/InputFormatInfo/path()|",
      "|java+method:///org/apache/hadoop/util/ReflectionUtils/newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)|",
      "|java+constructor:///org/apache/spark/scheduler/InputFormatInfo$$anonfun$prefLocsFromMapreduceInputFormat$1/InputFormatInfo$$anonfun$prefLocsFromMapreduceInputFormat$1(org.apache.spark.scheduler.InputFormatInfo,scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/InputFormatInfo/inputFormatClazz()|",
      "|java+constructor:///org/apache/hadoop/mapred/JobConf/JobConf(org.apache.hadoop.conf.Configuration)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$11/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$11/apply(org.apache.spark.scheduler.Stage)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$11/apply(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$2$$anonfun$apply$mcV$sp$7/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$2$$anonfun$apply$mcV$sp$7/apply(java.io.File)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$2$$anonfun$apply$mcV$sp$7/apply(java.io.File)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$16/apply(int)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/getPreferredLocs(org.apache.spark.rdd.RDD,int)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/partitions()|",
      "|java+method:///org/apache/spark/scheduler/Stage/rdd()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/latestInfo()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/attemptId()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/internalAccumulators()|",
      "|java+constructor:///org/apache/spark/scheduler/ShuffleMapTask/ShuffleMapTask(int,int,org.apache.spark.broadcast.Broadcast,org.apache.spark.Partition,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///scala/collection/Map/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/rdd()|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/id()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$cleanLogs$2$$anonfun$apply$15/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/history/FsApplicationAttemptInfo/logPath()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$cleanLogs$2$$anonfun$apply$15/apply()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$cleanLogs$2$$anonfun$apply$15/apply()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$57/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$57/apply(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$57/apply(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/serializer/KryoSerializer$/KryoSerializer$()|",
    "called": "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
    "v1Body": [
      "|java+method:///java/nio/ByteBuffer/allocate(int)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Predef$/any2ArrowAssoc(java.lang.Object)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+constructor:///org/apache/spark/serializer/KryoSerializer$$anon$1/KryoSerializer$$anon$1()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///java/nio/ByteBuffer/allocate(int)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$78/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$78/apply(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$78/apply(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/handleMessage(org.apache.spark.network.shuffle.protocol.BlockTransferMessage,org.apache.spark.network.client.TransportClient,org.apache.spark.network.client.RpcResponseCallback)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/connectedApps()|",
      "|java+constructor:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$1/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$1(org.apache.spark.deploy.mesos.MesosExternalShuffleBlockHandler,java.lang.String,java.net.SocketAddress)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/RegisterDriverParam()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///java/lang/String/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/network/client/RpcResponseCallback/onSuccess(byte%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$2/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$2(org.apache.spark.deploy.mesos.MesosExternalShuffleBlockHandler,java.lang.String,java.net.SocketAddress,java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/network/client/TransportClient/getSocketAddress()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$RegisterDriverParam$/unapply(org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver)|",
      "|java+method:///org/apache/spark/network/shuffle/ExternalShuffleBlockHandler/handleMessage(org.apache.spark.network.shuffle.protocol.BlockTransferMessage,org.apache.spark.network.client.TransportClient,org.apache.spark.network.client.RpcResponseCallback)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/applicationRemoved(java.lang.String,boolean)|"
    ],
    "v2Body": [
      "|java+method:///java/nio/ByteBuffer/allocate(int)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/connectedApps()|",
      "|java+constructor:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$1/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$1(org.apache.spark.deploy.mesos.MesosExternalShuffleBlockHandler,java.lang.String,java.net.SocketAddress)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/RegisterDriverParam()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///java/lang/String/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/network/client/RpcResponseCallback/onSuccess(java.nio.ByteBuffer)|",
      "|java+constructor:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$2/MesosExternalShuffleBlockHandler$$anonfun$handleMessage$2(org.apache.spark.deploy.mesos.MesosExternalShuffleBlockHandler,java.lang.String,java.net.SocketAddress,java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/network/client/TransportClient/getSocketAddress()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler$RegisterDriverParam$/unapply(org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver)|",
      "|java+method:///org/apache/spark/network/shuffle/ExternalShuffleBlockHandler/handleMessage(org.apache.spark.network.shuffle.protocol.BlockTransferMessage,org.apache.spark.network.client.TransportClient,org.apache.spark.network.client.RpcResponseCallback)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosExternalShuffleBlockHandler/applicationRemoved(java.lang.String,boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$52/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/shuffleWriteMetrics()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$52$$anonfun$apply$11/StagePage$$anonfun$52$$anonfun$apply$11(org.apache.spark.ui.jobs.StagePage$$anonfun$52)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$52$$anonfun$apply$24/StagePage$$anonfun$52$$anonfun$apply$24(org.apache.spark.ui.jobs.StagePage$$anonfun$52)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/shuffleReadMetrics()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$52$$anonfun$apply$7/StagePage$$anonfun$52$$anonfun$apply$7(org.apache.spark.ui.jobs.StagePage$$anonfun$52)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Option/get()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$52$$anonfun$apply$20/StagePage$$anonfun$52$$anonfun$apply$20(org.apache.spark.ui.jobs.StagePage$$anonfun$52)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/Utils$/getPropertiesFromFile(java.lang.String)|",
    "called": "|java+method:///scala/collection/JavaConverters$/asScalaSetConverter(java.util.Set)|",
    "v1Body": [
      "|java+method:///java/util/Properties/stringPropertyNames()|",
      "|java+method:///scala/collection/mutable/Set/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+constructor:///java/util/Properties/Properties()|",
      "|java+method:///java/io/InputStreamReader/close()|",
      "|java+method:///java/io/File/isFile()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$getPropertiesFromFile$3/Utils$$anonfun$getPropertiesFromFile$3(java.util.Properties)|",
      "|java+constructor:///java/io/FileInputStream/FileInputStream(java.io.File)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///java/util/Properties/load(java.io.Reader)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+constructor:///java/io/InputStreamReader/InputStreamReader(java.io.InputStream,java.lang.String)|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$getPropertiesFromFile$1/Utils$$anonfun$getPropertiesFromFile$1(java.io.File)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$getPropertiesFromFile$2/Utils$$anonfun$getPropertiesFromFile$2(java.io.File)|",
      "|java+method:///scala/collection/mutable/Set$/canBuildFrom()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaSet(java.util.Set)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/SetLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+constructor:///java/util/Properties/Properties()|",
      "|java+method:///java/io/InputStreamReader/close()|",
      "|java+method:///java/io/File/isFile()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$getPropertiesFromFile$3/Utils$$anonfun$getPropertiesFromFile$3(java.util.Properties)|",
      "|java+constructor:///java/io/FileInputStream/FileInputStream(java.io.File)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///java/util/Properties/load(java.io.Reader)|",
      "|java+method:///java/util/Properties/stringPropertyNames()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaSetConverter(java.util.Set)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+constructor:///java/io/InputStreamReader/InputStreamReader(java.io.InputStream,java.lang.String)|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$getPropertiesFromFile$1/Utils$$anonfun$getPropertiesFromFile$1(java.io.File)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$getPropertiesFromFile$2/Utils$$anonfun$getPropertiesFromFile$2(java.io.File)|",
      "|java+method:///scala/collection/mutable/Set$/canBuildFrom()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/r/RRDD$$anonfun$createSparkContext$2/apply(scala.Tuple2)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/SparkConf/set(java.lang.String,java.lang.String)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/SparkConf/set(java.lang.String,java.lang.String)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/shuffle/sort/SortShuffleManager/getWriter(org.apache.spark.shuffle.ShuffleHandle,int,org.apache.spark.TaskContext)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleManager/shuffleBlockResolver()|",
      "|java+constructor:///org/apache/spark/shuffle/sort/SortShuffleWriter/SortShuffleWriter(org.apache.spark.shuffle.IndexShuffleBlockResolver,org.apache.spark.shuffle.BaseShuffleHandle,int,org.apache.spark.TaskContext)|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/putIfAbsent(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/shuffle/BaseShuffleHandle/shuffleId()|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleManager/shuffleMapNumber()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/shuffle/BaseShuffleHandle/numMaps()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleManager/shuffleBlockResolver()|",
      "|java+constructor:///org/apache/spark/shuffle/sort/SortShuffleWriter/SortShuffleWriter(org.apache.spark.shuffle.IndexShuffleBlockResolver,org.apache.spark.shuffle.BaseShuffleHandle,int,org.apache.spark.TaskContext)|",
      "|java+method:///org/apache/spark/shuffle/ShuffleHandle/shuffleId()|",
      "|java+constructor:///org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter/BypassMergeSortShuffleWriter(org.apache.spark.storage.BlockManager,org.apache.spark.shuffle.IndexShuffleBlockResolver,org.apache.spark.shuffle.sort.BypassMergeSortShuffleHandle,int,org.apache.spark.TaskContext,org.apache.spark.SparkConf)|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/putIfAbsent(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/TaskContext/taskMemoryManager()|",
      "|java+method:///org/apache/spark/SparkEnv/conf()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/shuffle/BaseShuffleHandle/numMaps()|",
      "|java+constructor:///org/apache/spark/shuffle/sort/UnsafeShuffleWriter/UnsafeShuffleWriter(org.apache.spark.storage.BlockManager,org.apache.spark.shuffle.IndexShuffleBlockResolver,org.apache.spark.memory.TaskMemoryManager,org.apache.spark.shuffle.sort.SerializedShuffleHandle,int,org.apache.spark.TaskContext,org.apache.spark.SparkConf)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/Utils$/fetchFile(java.lang.String,java.io.File,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,org.apache.hadoop.conf.Configuration,long,boolean)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/util/Utils$/copyFile(java.lang.String,java.io.File,java.io.File,boolean,boolean)|",
      "|java+method:///org/apache/spark/util/Utils$/executeAndGetOutput(scala.collection.Seq,java.io.File,scala.collection.Map,boolean)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///org/apache/spark/util/Utils$/copyFile$default$5()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///java/io/RandomAccessFile/getChannel()|",
      "|java+method:///org/apache/spark/util/Utils$/isWindows()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///java/lang/String/endsWith(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/util/Utils$/getLocalDir(org.apache.spark.SparkConf)|",
      "|java+method:///java/lang/String/hashCode()|",
      "|java+method:///org/apache/hadoop/fs/FileUtil/chmod(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/executeAndGetOutput$default$3()|",
      "|java+method:///java/nio/channels/FileChannel/lock()|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+constructor:///java/io/RandomAccessFile/RandomAccessFile(java.io.File,java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/executeAndGetOutput$default$4()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$fetchFile$1/Utils$$anonfun$fetchFile$1(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$fetchFile$2/Utils$$anonfun$fetchFile$2(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/util/Utils$/doFetchFile(java.lang.String,java.io.File,java.lang.String,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///java/nio/channels/FileLock/release()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/last()|",
      "|java+method:///java/io/File/getAbsolutePath()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///java/nio/channels/FileChannel/close()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/util/Utils$/copyFile(java.lang.String,java.io.File,java.io.File,boolean,boolean)|",
      "|java+method:///org/apache/spark/util/Utils$/executeAndGetOutput(scala.collection.Seq,java.io.File,scala.collection.Map,boolean)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///org/apache/spark/util/Utils$/copyFile$default$5()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///java/io/RandomAccessFile/getChannel()|",
      "|java+method:///org/apache/spark/util/Utils$/isWindows()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///java/lang/String/endsWith(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/util/Utils$/getLocalDir(org.apache.spark.SparkConf)|",
      "|java+method:///java/lang/String/hashCode()|",
      "|java+method:///org/apache/hadoop/fs/FileUtil/chmod(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/executeAndGetOutput$default$3()|",
      "|java+method:///java/nio/channels/FileChannel/lock()|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+constructor:///java/io/RandomAccessFile/RandomAccessFile(java.io.File,java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/executeAndGetOutput$default$4()|",
      "|java+method:///org/apache/spark/util/Utils$/decodeFileNameInURI(java.net.URI)|",
      "|java+constructor:///java/net/URI/URI(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$fetchFile$1/Utils$$anonfun$fetchFile$1(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$fetchFile$2/Utils$$anonfun$fetchFile$2(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/util/Utils$/doFetchFile(java.lang.String,java.io.File,java.lang.String,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///java/nio/channels/FileLock/release()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///java/io/File/getAbsolutePath()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///java/nio/channels/FileChannel/close()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$6/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$6/apply(long)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$6/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/client/AppClient/killExecutors(scala.collection.Seq)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$$anonfun$killExecutors$1/AppClient$$anonfun$killExecutors$1(org.apache.spark.deploy.client.AppClient)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$KillExecutors/DeployMessages$KillExecutors(java.lang.String,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/endpoint()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/logWarning(scala.Function0)|",
      "|java+method:///scala/reflect/ClassTag$/Boolean()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/org$apache$spark$deploy$client$AppClient$$appId()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$$anonfun$killExecutors$1/AppClient$$anonfun$killExecutors$1(org.apache.spark.deploy.client.AppClient)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$KillExecutors/DeployMessages$KillExecutors(java.lang.String,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///java/util/concurrent/atomic/AtomicReference/get()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/endpoint()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/logWarning(scala.Function0)|",
      "|java+method:///scala/reflect/ClassTag$/Boolean()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/org$apache$spark$deploy$client$AppClient$$appId()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/r/RUtils$/sparkRPackagePath(boolean)|",
    "called": "|java+method:///scala/Option/getOrElse(scala.Function0)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///scala/sys/SystemProperties/apply(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///java/lang/String/contains(java.lang.CharSequence)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/api/r/RUtils$$anonfun$sparkRPackagePath$1/RUtils$$anonfun$sparkRPackagePath$1()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/sys/package$/props()|",
      "|java+method:///org/apache/spark/api/r/RUtils$/localSparkRPackagePath()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///java/io/File/getAbsolutePath()|",
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkEnv/conf()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///scala/sys/SystemProperties/apply(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///java/lang/String/contains(java.lang.CharSequence)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/api/r/RUtils$$anonfun$2/RUtils$$anonfun$2()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/sys/package$/props()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/api/r/RUtils$/localSparkRPackagePath()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///org/apache/spark/api/r/RUtils$/rPackages()|",
      "|java+method:///java/io/File/getAbsolutePath()|",
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkEnv/conf()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$removeApplication$2/apply(org.apache.spark.deploy.master.ApplicationInfo)|",
    "called": "|java+method:///scala/Option$/apply(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$appIdToUI()|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/removeSource(org.apache.spark.metrics.source.Source)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$removeApplication$2$$anonfun$apply$17/Master$$anonfun$removeApplication$2$$anonfun$apply$17(org.apache.spark.deploy.master.Master$$anonfun$removeApplication$2)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/id()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/appSource()|",
      "|java+method:///scala/collection/mutable/HashMap/remove(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$applicationMetricsSystem()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$appIdToUI()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/removeSource(org.apache.spark.metrics.source.Source)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$removeApplication$2$$anonfun$apply$17/Master$$anonfun$removeApplication$2$$anonfun$apply$17(org.apache.spark.deploy.master.Master$$anonfun$removeApplication$2)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/id()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/appSource()|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/remove(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$applicationMetricsSystem()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$56/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$56/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$56/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$apply$mcVI$sp$2/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$apply$mcVI$sp$2/apply()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/jobId()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$apply$mcVI$sp$2/apply()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1/apply(org.apache.spark.deploy.mesos.MesosDriverDescription)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/mesos/Protos$TaskInfo/newBuilder()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/cores()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/mem()|",
      "|java+method:///scala/collection/immutable/List/find(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$launchedDriversState()|",
      "|java+constructor:///java/util/Date/Date()|",
      "|java+method:///org/apache/mesos/Protos$TaskID/newBuilder()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/build()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$8/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$8(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1,org.apache.spark.deploy.mesos.MesosDriverDescription,double,int)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$buildDriverCommand(org.apache.spark.deploy.mesos.MesosDriverDescription)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterPersistenceEngine/persist(java.lang.String,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$8/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$8(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1,double,int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/logTrace(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$ResourceOffer/mem()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$ResourceOffer/mem_$eq(double)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setSlaveId(org.apache.mesos.Protos$SlaveID)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setCommand(org.apache.mesos.Protos$CommandInfo)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/setValue(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$10/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$10(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1,org.apache.spark.deploy.mesos.MesosDriverDescription,org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$ResourceOffer)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElseUpdate(java.lang.Object,scala.Function0)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/submissionId()|",
      "|java+method:///scala/collection/immutable/Map/apply(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/addResources(org.apache.mesos.Protos$Resource)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$ResourceOffer/offer()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setName(java.lang.String)|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$11/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$11(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/build()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setTaskId(org.apache.mesos.Protos$TaskID)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$launchedDrivers()|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/createResource(java.lang.String,double)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$7/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$7(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1,double,int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$9/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$9(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1,org.apache.spark.deploy.mesos.MesosDriverDescription,org.apache.mesos.Protos$TaskInfo$Builder)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterSubmissionState/MesosClusterSubmissionState(org.apache.spark.deploy.mesos.MesosDriverDescription,org.apache.mesos.Protos$TaskID,org.apache.mesos.Protos$SlaveID,scala.Option,java.util.Date,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$ResourceOffer/cpu()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$ResourceOffer/cpu_$eq(double)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getSlaveId()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/schedulerProperties()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/mesos/Protos$TaskInfo/newBuilder()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/cores()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/mem()|",
      "|java+method:///scala/collection/immutable/List/find(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$launchedDriversState()|",
      "|java+constructor:///java/util/Date/Date()|",
      "|java+method:///org/apache/mesos/Protos$TaskID/newBuilder()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/build()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$8/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$8(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1,org.apache.spark.deploy.mesos.MesosDriverDescription,double,int)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$buildDriverCommand(org.apache.spark.deploy.mesos.MesosDriverDescription)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterPersistenceEngine/persist(java.lang.String,java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/logTrace(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$ResourceOffer/mem()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$ResourceOffer/mem_$eq(double)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setSlaveId(org.apache.mesos.Protos$SlaveID)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setCommand(org.apache.mesos.Protos$CommandInfo)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$9/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$9(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1,double,int)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/mesos/Protos$TaskID$Builder/setValue(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$10/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$10(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1,org.apache.spark.deploy.mesos.MesosDriverDescription,org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$ResourceOffer)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElseUpdate(java.lang.Object,scala.Function0)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/submissionId()|",
      "|java+method:///scala/collection/immutable/Map/apply(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/addResources(org.apache.mesos.Protos$Resource)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$ResourceOffer/offer()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setName(java.lang.String)|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$12/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$12(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1)|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/build()|",
      "|java+method:///org/apache/mesos/Protos$TaskInfo$Builder/setTaskId(org.apache.mesos.Protos$TaskID)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$launchedDrivers()|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/createResource(java.lang.String,double)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$7/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$7(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1,double,int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$9/MesosClusterScheduler$$anonfun$scheduleTasks$1$$anonfun$apply$9(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1,org.apache.spark.deploy.mesos.MesosDriverDescription,org.apache.mesos.Protos$TaskInfo$Builder)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterSubmissionState/MesosClusterSubmissionState(org.apache.spark.deploy.mesos.MesosDriverDescription,org.apache.mesos.Protos$TaskID,org.apache.mesos.Protos$SlaveID,scala.Option,java.util.Date,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$ResourceOffer/cpu()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$ResourceOffer/cpu_$eq(double)|",
      "|java+method:///org/apache/mesos/Protos$Offer/getSlaveId()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/schedulerProperties()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/PythonRunner/compute(scala.collection.Iterator,int,org.apache.spark.TaskContext)|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/ArrayOps/mkString(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/BlockManager/diskBlockManager()|",
      "|java+method:///java/net/Socket/getInputStream()|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anonfun$compute$1/PythonRunner$$anonfun$compute$1(org.apache.spark.api.python.PythonRunner,java.net.Socket,scala.runtime.VolatileBooleanRef,org.apache.spark.api.python.PythonRunner$WriterThread)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///org/apache/spark/TaskContext/addTaskCompletionListener(scala.Function1)|",
      "|java+constructor:///java/io/BufferedInputStream/BufferedInputStream(java.io.InputStream,int)|",
      "|java+method:///org/apache/spark/storage/DiskBlockManager/localDirs()|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$MonitorThread/PythonRunner$MonitorThread(org.apache.spark.api.python.PythonRunner,org.apache.spark.SparkEnv,java.net.Socket,org.apache.spark.TaskContext)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$WriterThread/PythonRunner$WriterThread(org.apache.spark.api.python.PythonRunner,org.apache.spark.SparkEnv,java.net.Socket,scala.collection.Iterator,int,org.apache.spark.TaskContext)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/collection/mutable/Map/$plus$eq(scala.Tuple2)|",
      "|java+constructor:///org/apache/spark/InterruptibleIterator/InterruptibleIterator(org.apache.spark.TaskContext,scala.collection.Iterator)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anonfun$1/PythonRunner$$anonfun$1(org.apache.spark.api.python.PythonRunner)|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anon$1/PythonRunner$$anon$1(org.apache.spark.api.python.PythonRunner,org.apache.spark.TaskContext,long,org.apache.spark.SparkEnv,java.net.Socket,scala.runtime.VolatileBooleanRef,org.apache.spark.api.python.PythonRunner$WriterThread,java.io.DataInputStream)|",
      "|java+constructor:///java/io/DataInputStream/DataInputStream(java.io.InputStream)|",
      "|java+method:///org/apache/spark/api/python/PythonRunner$WriterThread/start()|",
      "|java+method:///scala/Predef$/any2ArrowAssoc(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/python/PythonRunner$MonitorThread/start()|",
      "|java+method:///org/apache/spark/SparkEnv/createPythonWorker(java.lang.String,scala.collection.immutable.Map)|",
      "|java+method:///scala/collection/mutable/Map/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+constructor:///scala/runtime/VolatileBooleanRef/VolatileBooleanRef(boolean)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anon$1/PythonRunner$$anon$1(org.apache.spark.api.python.PythonRunner,org.apache.spark.TaskContext,long,org.apache.spark.SparkEnv,java.net.Socket,scala.runtime.VolatileBooleanRef,org.apache.spark.api.python.PythonRunner$WriterThread,java.io.DataInputStream)|",
      "|java+method:///scala/collection/mutable/ArrayOps/mkString(java.lang.String)|",
      "|java+method:///java/net/Socket/getInputStream()|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anonfun$compute$1/PythonRunner$$anonfun$compute$1(org.apache.spark.api.python.PythonRunner,java.net.Socket,scala.runtime.VolatileBooleanRef,org.apache.spark.api.python.PythonRunner$WriterThread)|",
      "|java+method:///org/apache/spark/api/python/PythonRunner$MonitorThread/start()|",
      "|java+method:///org/apache/spark/SparkEnv/createPythonWorker(java.lang.String,scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|",
      "|java+method:///org/apache/spark/TaskContext/addTaskCompletionListener(scala.Function1)|",
      "|java+method:///org/apache/spark/storage/DiskBlockManager/localDirs()|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$MonitorThread/PythonRunner$MonitorThread(org.apache.spark.api.python.PythonRunner,org.apache.spark.SparkEnv,java.net.Socket,org.apache.spark.TaskContext)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$WriterThread/PythonRunner$WriterThread(org.apache.spark.api.python.PythonRunner,org.apache.spark.SparkEnv,java.net.Socket,scala.collection.Iterator,int,org.apache.spark.TaskContext)|",
      "|java+constructor:///org/apache/spark/InterruptibleIterator/InterruptibleIterator(org.apache.spark.TaskContext,scala.collection.Iterator)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anonfun$1/PythonRunner$$anonfun$1(org.apache.spark.api.python.PythonRunner)|",
      "|java+method:///org/apache/spark/storage/BlockManager/diskBlockManager()|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+constructor:///java/io/DataInputStream/DataInputStream(java.io.InputStream)|",
      "|java+method:///org/apache/spark/api/python/PythonRunner$WriterThread/start()|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+constructor:///java/io/BufferedInputStream/BufferedInputStream(java.io.InputStream,int)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+constructor:///scala/runtime/VolatileBooleanRef/VolatileBooleanRef(boolean)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/r/SerDe$/readTypedObject(java.io.DataInputStream,char)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/r/SerDe$/readList(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readBytes(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readBoolean(java.io.DataInputStream)|",
      "|java+constructor:///java/lang/Integer/Integer(int)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readTime(java.io.DataInputStream)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///java/lang/Boolean/Boolean(boolean)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readInt(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/JVMObjectTracker$/getObject(java.lang.String)|",
      "|java+constructor:///java/lang/Double/Double(double)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readString(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readDouble(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readDate(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readMap(java.io.DataInputStream)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToCharacter(char)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/api/r/SerDe$/readList(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readBytes(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readBoolean(java.io.DataInputStream)|",
      "|java+constructor:///java/lang/Integer/Integer(int)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Function2/apply(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readTime(java.io.DataInputStream)|",
      "|java+constructor:///java/lang/Boolean/Boolean(boolean)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/api/r/JVMObjectTracker$/getObject(java.lang.String)|",
      "|java+constructor:///java/lang/Double/Double(double)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readString(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readDouble(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readDate(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readMap(java.io.DataInputStream)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readArray(java.io.DataInputStream)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToCharacter(char)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/sqlSerDe()|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readInt(java.io.DataInputStream)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(org.apache.spark.scheduler.ActiveJob,java.lang.String,scala.Option)|",
    "called": "|java+constructor:///scala/runtime/BooleanRef/BooleanRef(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/jobIdToStageIds()|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerJobEnd/SparkListenerJobEnd(int,long,org.apache.spark.scheduler.JobResult)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$21/DAGScheduler$$anonfun$21(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/cleanupStateForJobAndIndependentStages(org.apache.spark.scheduler.ActiveJob)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/mutable/HashSet/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/jobId()|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/properties()|",
      "|java+method:///scala/collection/immutable/StringOps/toBoolean()|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/listener()|",
      "|java+method:///org/apache/spark/SparkContext$/SPARK_JOB_INTERRUPT_ON_CANCEL()|",
      "|java+constructor:///org/apache/spark/scheduler/JobFailed/JobFailed(java.lang.Exception)|",
      "|java+method:///scala/collection/mutable/HashSet/isEmpty()|",
      "|java+method:///java/util/Properties/getProperty(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/JobListener/jobFailed(java.lang.Exception)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.ActiveJob,java.lang.String,scala.runtime.BooleanRef,boolean)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$2/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$2(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.ActiveJob)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logError(scala.Function0)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+constructor:///scala/runtime/BooleanRef/BooleanRef(boolean)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/jobIdToStageIds()|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerJobEnd/SparkListenerJobEnd(int,long,org.apache.spark.scheduler.JobResult)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$19/DAGScheduler$$anonfun$19(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/cleanupStateForJobAndIndependentStages(org.apache.spark.scheduler.ActiveJob)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/mutable/HashSet/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/jobId()|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/properties()|",
      "|java+method:///scala/collection/immutable/StringOps/toBoolean()|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/listener()|",
      "|java+method:///org/apache/spark/SparkContext$/SPARK_JOB_INTERRUPT_ON_CANCEL()|",
      "|java+constructor:///org/apache/spark/scheduler/JobFailed/JobFailed(java.lang.Exception)|",
      "|java+method:///scala/collection/mutable/HashSet/isEmpty()|",
      "|java+method:///java/util/Properties/getProperty(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/JobListener/jobFailed(java.lang.Exception)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.ActiveJob,java.lang.String,scala.runtime.BooleanRef,boolean)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$2/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$2(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.ActiveJob)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logError(scala.Function0)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+constructor:///scala/runtime/BooleanRef/BooleanRef(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$60/apply(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$/rddInfoFromJson(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/api/java/JavaSparkContext/JavaSparkContext(java.lang.String,java.lang.String,java.lang.String,java.lang.String%5B%5D,java.util.Map)|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+constructor:///org/apache/spark/SparkContext/SparkContext(java.lang.String,java.lang.String,java.lang.String,scala.collection.Seq,scala.collection.Map,scala.collection.Map)|",
      "|java+constructor:///org/apache/spark/api/java/JavaSparkContext/JavaSparkContext(org.apache.spark.SparkContext)|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+constructor:///org/apache/spark/SparkContext/SparkContext(java.lang.String,java.lang.String,java.lang.String,scala.collection.Seq,scala.collection.Map,scala.collection.Map)|",
      "|java+constructor:///org/apache/spark/api/java/JavaSparkContext/JavaSparkContext(org.apache.spark.SparkContext)|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/deploy/master/Master/Master(org.apache.spark.rpc.RpcEnv,org.apache.spark.rpc.RpcAddress,int,org.apache.spark.SecurityManager,org.apache.spark.SparkConf)|",
    "called": "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getenv(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/checkHost(java.lang.String,java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonSingleThreadScheduledExecutor(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/RecoveryState$/STANDBY()|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/host()|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/newConfiguration(org.apache.spark.SparkConf)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+constructor:///org/apache/spark/deploy/master/MasterSource/MasterSource(org.apache.spark.deploy.master.Master)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///org/apache/spark/SparkConf/getLong(java.lang.String,long)|",
      "|java+method:///org/apache/spark/deploy/master/Master/defaultCores()|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/toSparkURL()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpoint$class/$init$(org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem$/createMetricsSystem(java.lang.String,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getenv(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/checkHost(java.lang.String,java.lang.String)|",
      "|java+constructor:///java/util/concurrent/ConcurrentHashMap/ConcurrentHashMap()|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonSingleThreadScheduledExecutor(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/RecoveryState$/STANDBY()|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/host()|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonSingleThreadExecutor(java.lang.String)|",
      "|java+method:///scala/concurrent/ExecutionContext$/fromExecutor(java.util.concurrent.Executor)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/newConfiguration(org.apache.spark.SparkConf)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+constructor:///org/apache/spark/deploy/master/MasterSource/MasterSource(org.apache.spark.deploy.master.Master)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///org/apache/spark/SparkConf/getLong(java.lang.String,long)|",
      "|java+method:///org/apache/spark/deploy/master/Master/defaultCores()|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/toSparkURL()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpoint$class/$init$(org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///org/apache/spark/deploy/master/Master/rebuildUIThread()|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem$/createMetricsSystem(java.lang.String,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$10/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$10(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/CoarseMesosSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.SparkContext,java.lang.String,org.apache.spark.SecurityManager)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/CoarseGrainedSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.rpc.RpcEnv)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/create()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/conf()|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/shuffleServiceEnabled()|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$class/$init$(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|",
      "|java+constructor:///java/util/concurrent/locks/ReentrantLock/ReentrantLock()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/network/netty/SparkTransportConf$/fromSparkConf(org.apache.spark.SparkConf,int)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+constructor:///org/apache/spark/network/shuffle/mesos/MesosExternalShuffleClient/MesosExternalShuffleClient(org.apache.spark.network.util.TransportConf,org.apache.spark.network.sasl.SecretKeyHolder,boolean,boolean)|",
      "|java+method:///org/apache/spark/network/netty/SparkTransportConf$/fromSparkConf$default$2()|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/SecurityManager/isAuthenticationEnabled()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/parseConstraintString(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/SecurityManager/isSaslEncryptionEnabled()|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/CoarseGrainedSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.rpc.RpcEnv)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/create()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/conf()|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/shuffleServiceEnabled()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$class/$init$(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|",
      "|java+constructor:///java/util/concurrent/locks/ReentrantLock/ReentrantLock()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+constructor:///org/apache/spark/network/shuffle/mesos/MesosExternalShuffleClient/MesosExternalShuffleClient(org.apache.spark.network.util.TransportConf,org.apache.spark.network.sasl.SecretKeyHolder,boolean,boolean)|",
      "|java+method:///org/apache/spark/network/netty/SparkTransportConf$/fromSparkConf$default$3()|",
      "|java+method:///org/apache/spark/network/netty/SparkTransportConf$/fromSparkConf(org.apache.spark.SparkConf,java.lang.String,int)|",
      "|java+method:///org/apache/spark/SecurityManager/isAuthenticationEnabled()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/parseConstraintString(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/getRejectOfferDurationForUnmetConstraints(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///org/apache/spark/SecurityManager/isSaslEncryptionEnabled()|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$116/TaskDataSource$$anonfun$116(org.apache.spark.ui.jobs.TaskDataSource)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1$$anon$1/run()|",
    "called": "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/worker/Worker/rpcEnv()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/self()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1$$anon$1$$anonfun$run$6/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1$$anon$1$$anonfun$run$6(org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1$$anon$1)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master$/ENDPOINT_NAME()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1/org$apache$spark$deploy$worker$Worker$$anonfun$$$outer()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$workerId()|",
      "|java+method:///org/apache/spark/deploy/master/Master$/SYSTEM_NAME()|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$RegisterWorker/DeployMessages$RegisterWorker(java.lang.String,java.lang.String,int,org.apache.spark.rpc.RpcEndpointRef,int,int,int,java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpointRef(java.lang.String,org.apache.spark.rpc.RpcAddress,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$webUi()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$port()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$publicAddress()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$host()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1$$anon$1$$anonfun$run$5/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1$$anon$1$$anonfun$run$5(org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1$$anon$1)|",
      "|java+method:///org/apache/spark/deploy/worker/ui/WorkerWebUI/boundPort()|",
      "|java+method:///scala/Option/isEmpty()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/worker/Worker/rpcEnv()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1$$anon$1$$anonfun$run$6/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1$$anon$1$$anonfun$run$6(org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1$$anon$1)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$registerWithMaster(org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///org/apache/spark/deploy/master/Master$/ENDPOINT_NAME()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1/org$apache$spark$deploy$worker$Worker$$anonfun$$$outer()|",
      "|java+method:///org/apache/spark/deploy/master/Master$/SYSTEM_NAME()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpointRef(java.lang.String,org.apache.spark.rpc.RpcAddress,java.lang.String)|",
      "|java+method:///scala/Option/get()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1$$anon$1$$anonfun$run$5/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1$$anon$1$$anonfun$run$5(org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1$$anon$1)|",
      "|java+method:///scala/Option/isEmpty()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$40/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$/getGettingResultTime(org.apache.spark.scheduler.TaskInfo,long)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskInfo()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/executorDeserializeTime()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anon$4$$anonfun$run$2/apply$mcV$sp()|",
    "called": "|java+method:///scala/Option$/apply(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/worker/Worker/self()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anon$4/org$apache$spark$deploy$worker$Worker$$anon$$$outer()|"
    ],
    "v2Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/self()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anon$4$$anonfun$run$2$$anonfun$apply$mcV$sp$6/Worker$$anon$4$$anonfun$run$2$$anonfun$apply$mcV$sp$6(org.apache.spark.deploy.worker.Worker$$anon$4$$anonfun$run$2)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anon$4/org$apache$spark$deploy$worker$Worker$$anon$$$outer()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$$anonfun$fn$10$1/apply(scala.collection.Iterator,scala.collection.Iterator)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/java/function/FlatMapFunction2/call(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaIterator(java.util.Iterator)|",
      "|java+method:///java/lang/Iterable/iterator()|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterator(scala.collection.Iterator)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///org/apache/spark/api/java/function/FlatMapFunction2/call(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/lang/Iterable/iterator()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaIteratorConverter(java.util.Iterator)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleBeginEvent(org.apache.spark.scheduler.Task,org.apache.spark.scheduler.TaskInfo)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/submitWaitingStages()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/stageIdToStage()|",
      "|java+method:///org/apache/spark/scheduler/Task/stageId()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerTaskStart/SparkListenerTaskStart(int,int,org.apache.spark.scheduler.TaskInfo)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$1/DAGScheduler$$anonfun$1(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$11/DAGScheduler$$anonfun$11(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/submitWaitingStages()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/stageIdToStage()|",
      "|java+method:///org/apache/spark/scheduler/Task/stageId()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerTaskStart/SparkListenerTaskStart(int,int,org.apache.spark.scheduler.TaskInfo)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$10/DAGScheduler$$anonfun$10(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$1/DAGScheduler$$anonfun$1(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$class/iterator(org.apache.spark.api.java.JavaRDDLike,org.apache.spark.Partition,org.apache.spark.TaskContext)|",
    "called": "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
    "v1Body": [
      "|java+method:///org/apache/spark/rdd/RDD/iterator(org.apache.spark.Partition,org.apache.spark.TaskContext)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterator(scala.collection.Iterator)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///org/apache/spark/rdd/RDD/iterator(org.apache.spark.Partition,org.apache.spark.TaskContext)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1$$anon$1/run()|",
    "called": "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
    "v1Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$RegisterApplication/DeployMessages$RegisterApplication(org.apache.spark.deploy.ApplicationDescription,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$anonfun$$$outer()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/org$apache$spark$deploy$client$AppClient$$registered()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/self()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpointRef(java.lang.String,org.apache.spark.rpc.RpcAddress,java.lang.String)|",
      "|java+method:///scala/Option/get()|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1$$anon$1$$anonfun$run$2/AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1$$anon$1$$anonfun$run$2(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1$$anon$1)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1$$anon$1$$anonfun$run$3/AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1$$anon$1$$anonfun$run$3(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1$$anon$1)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$$outer()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/rpcEnv()|",
      "|java+method:///org/apache/spark/deploy/master/Master$/ENDPOINT_NAME()|",
      "|java+method:///org/apache/spark/deploy/master/Master$/SYSTEM_NAME()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/logInfo(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$RegisterApplication/DeployMessages$RegisterApplication(org.apache.spark.deploy.ApplicationDescription,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///java/util/concurrent/atomic/AtomicBoolean/get()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$anonfun$$$outer()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/org$apache$spark$deploy$client$AppClient$$registered()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/self()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpointRef(java.lang.String,org.apache.spark.rpc.RpcAddress,java.lang.String)|",
      "|java+method:///scala/Option/get()|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1$$anon$1$$anonfun$run$2/AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1$$anon$1$$anonfun$run$2(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1$$anon$1)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1$$anon$1$$anonfun$run$3/AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1$$anon$1$$anonfun$run$3(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$tryRegisterAllMasters$1$$anon$1)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$$outer()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/rpcEnv()|",
      "|java+method:///org/apache/spark/deploy/master/Master$/ENDPOINT_NAME()|",
      "|java+method:///org/apache/spark/deploy/master/Master$/SYSTEM_NAME()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/logInfo(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/ParallelCollectionRDD/getPartitions()|",
    "called": "|java+method:///scala/collection/immutable/IndexedSeq$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/collection/Seq/toArray(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/immutable/Range/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/rdd/ParallelCollectionRDD$$anonfun$getPartitions$1/ParallelCollectionRDD$$anonfun$getPartitions$1(org.apache.spark.rdd.ParallelCollectionRDD,scala.collection.Seq%5B%5D)|",
      "|java+method:///scala/collection/immutable/IndexedSeq$/canBuildFrom()|",
      "|java+method:///scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/indices()|",
      "|java+method:///org/apache/spark/rdd/ParallelCollectionRDD$/slice(scala.collection.Seq,int,scala.reflect.ClassTag)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/collection/Seq/toArray(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/immutable/Range/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/rdd/ParallelCollectionRDD/data()|",
      "|java+constructor:///org/apache/spark/rdd/ParallelCollectionRDD$$anonfun$getPartitions$1/ParallelCollectionRDD$$anonfun$getPartitions$1(org.apache.spark.rdd.ParallelCollectionRDD,scala.collection.Seq%5B%5D)|",
      "|java+method:///scala/collection/immutable/IndexedSeq$/canBuildFrom()|",
      "|java+method:///scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/indices()|",
      "|java+method:///org/apache/spark/rdd/ParallelCollectionRDD$/slice(scala.collection.Seq,int,scala.reflect.ClassTag)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSetManager/handleFailedTask(long,scala.Enumeration$Value,org.apache.spark.TaskEndReason)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/TaskState$/KILLED()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/tasks()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/taskEnded(org.apache.spark.scheduler.Task,org.apache.spark.TaskEndReason,java.lang.Object,scala.collection.Map,org.apache.spark.scheduler.TaskInfo,org.apache.spark.executor.TaskMetrics)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/org$apache$spark$scheduler$TaskSetManager$$addPendingTask(int,boolean)|",
      "|java+method:///java/lang/Class/getName()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/abort$default$2()|",
      "|java+constructor:///scala/Tuple2$mcZI$sp/Tuple2$mcZI$sp(boolean,int)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/isZombie_$eq(boolean)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/tasksSuccessful()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/dagScheduler()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/host()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/EXCEPTION_PRINT_INTERVAL()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/copiesRunning()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/tasksSuccessful_$eq(int)|",
      "|java+method:///org/apache/spark/scheduler/TaskSet/id()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/executorId()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/numFailures()|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/markFailed(long)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/maxTaskFailures()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElseUpdate(java.lang.Object,scala.Function0)|",
      "|java+method:///org/apache/spark/ExceptionFailure/description()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$12/TaskSetManager$$anonfun$12(org.apache.spark.scheduler.TaskSetManager,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/isZombie()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$16/TaskSetManager$$anonfun$16(org.apache.spark.scheduler.TaskSetManager,java.lang.String)|",
      "|java+method:///scala/Tuple2/_1$mcZ$sp()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$14/TaskSetManager$$anonfun$14(org.apache.spark.scheduler.TaskSetManager,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/markFailed$default$1()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/index()|",
      "|java+method:///scala/Option/orNull(scala.Predef$$less$colon$less)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/Tuple2$mcIJ$sp/Tuple2$mcIJ$sp(int,long)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/ExceptionFailure/className()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/ExceptionFailure/metrics()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$handleFailedTask$2/TaskSetManager$$anonfun$handleFailedTask$2(org.apache.spark.scheduler.TaskSetManager,int)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/id()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/org$apache$spark$scheduler$TaskSetManager$$addPendingTask$default$2()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logError(scala.Function0)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/taskSet()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$handleFailedTask$1/TaskSetManager$$anonfun$handleFailedTask$1(org.apache.spark.scheduler.TaskSetManager)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/failed()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/successful()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/TaskFailedReason/toErrorString()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/abort(java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/taskInfos()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$15/TaskSetManager$$anonfun$15(org.apache.spark.scheduler.TaskSetManager,long,org.apache.spark.scheduler.TaskInfo,int,org.apache.spark.ExceptionFailure)|",
      "|java+method:///scala/Tuple2/_2$mcJ$sp()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/recentExceptions()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/failedExecutors()|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/removeRunningTask(long)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/Tuple2/_1$mcI$sp()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$17/TaskSetManager$$anonfun$17(org.apache.spark.scheduler.TaskSetManager,org.apache.spark.TaskEndReason)|",
      "|java+method:///org/apache/spark/ExceptionFailure/exception()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$13/TaskSetManager$$anonfun$13(org.apache.spark.scheduler.TaskSetManager,long,org.apache.spark.scheduler.TaskInfo,org.apache.spark.ExceptionFailure)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/maybeFinishTaskSet()|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/TaskState$/KILLED()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/taskEnded(org.apache.spark.scheduler.Task,org.apache.spark.TaskEndReason,java.lang.Object,scala.collection.Map,org.apache.spark.scheduler.TaskInfo,org.apache.spark.executor.TaskMetrics)|",
      "|java+method:///java/lang/Class/getName()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/abort$default$2()|",
      "|java+constructor:///scala/Tuple2$mcZI$sp/Tuple2$mcZI$sp(boolean,int)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/isZombie_$eq(boolean)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/tasksSuccessful()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/dagScheduler()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/host()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/EXCEPTION_PRINT_INTERVAL()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/copiesRunning()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/tasksSuccessful_$eq(int)|",
      "|java+method:///org/apache/spark/scheduler/TaskSet/id()|",
      "|java+method:///org/apache/spark/ExecutorLostFailure/exitCausedByApp()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/tasks()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/executorId()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/numFailures()|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/markFailed(long)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/maxTaskFailures()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElseUpdate(java.lang.Object,scala.Function0)|",
      "|java+method:///org/apache/spark/ExceptionFailure/description()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$12/TaskSetManager$$anonfun$12(org.apache.spark.scheduler.TaskSetManager,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/isZombie()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$17/TaskSetManager$$anonfun$17(org.apache.spark.scheduler.TaskSetManager,java.lang.String)|",
      "|java+method:///scala/Tuple2/_1$mcZ$sp()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$14/TaskSetManager$$anonfun$14(org.apache.spark.scheduler.TaskSetManager,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/org$apache$spark$scheduler$TaskSetManager$$addPendingTask(int)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/markFailed$default$1()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/index()|",
      "|java+method:///scala/Option/orNull(scala.Predef$$less$colon$less)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/Tuple2$mcIJ$sp/Tuple2$mcIJ$sp(int,long)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/ExceptionFailure/className()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/ExceptionFailure/metrics()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$handleFailedTask$2/TaskSetManager$$anonfun$handleFailedTask$2(org.apache.spark.scheduler.TaskSetManager,int)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/id()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logError(scala.Function0)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$16/TaskSetManager$$anonfun$16(org.apache.spark.scheduler.TaskSetManager,long)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/taskSet()|",
      "|java+method:///org/apache/spark/TaskFailedReason/countTowardsTaskFailures()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$handleFailedTask$1/TaskSetManager$$anonfun$handleFailedTask$1(org.apache.spark.scheduler.TaskSetManager)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/failed()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/successful()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/TaskFailedReason/toErrorString()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/abort(java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/taskInfos()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$15/TaskSetManager$$anonfun$15(org.apache.spark.scheduler.TaskSetManager,long,org.apache.spark.scheduler.TaskInfo,int,org.apache.spark.ExceptionFailure)|",
      "|java+method:///scala/Tuple2/_2$mcJ$sp()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/recentExceptions()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/failedExecutors()|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/removeRunningTask(long)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/Tuple2/_1$mcI$sp()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$18/TaskSetManager$$anonfun$18(org.apache.spark.scheduler.TaskSetManager,org.apache.spark.TaskEndReason)|",
      "|java+method:///org/apache/spark/ExceptionFailure/exception()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$13/TaskSetManager$$anonfun$13(org.apache.spark.scheduler.TaskSetManager,long,org.apache.spark.scheduler.TaskInfo,org.apache.spark.ExceptionFailure)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/maybeFinishTaskSet()|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/SubtractedRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///scala/collection/TraversableOnce$/flattenTraversableOnce(scala.collection.TraversableOnce,scala.Function1)|",
      "|java+method:///scala/collection/mutable/Map/iterator()|",
      "|java+constructor:///org/apache/spark/rdd/SubtractedRDD$$anonfun$compute$3/SubtractedRDD$$anonfun$compute$3(org.apache.spark.rdd.SubtractedRDD)|",
      "|java+constructor:///java/util/HashMap/HashMap()|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+method:///org/apache/spark/rdd/SubtractedRDD/integrate$1(int,scala.Function1,org.apache.spark.TaskContext,org.apache.spark.rdd.CoGroupPartition)|",
      "|java+constructor:///org/apache/spark/rdd/SubtractedRDD$$anonfun$compute$1/SubtractedRDD$$anonfun$compute$1(org.apache.spark.rdd.SubtractedRDD,java.util.HashMap)|",
      "|java+constructor:///org/apache/spark/rdd/SubtractedRDD$$anonfun$compute$2/SubtractedRDD$$anonfun$compute$2(org.apache.spark.rdd.SubtractedRDD,java.util.HashMap)|",
      "|java+method:///scala/collection/TraversableOnce$FlattenOps/flatten()|",
      "|java+method:///scala/Predef$/conforms()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/MapLike/iterator()|",
      "|java+constructor:///org/apache/spark/rdd/SubtractedRDD$$anonfun$compute$3/SubtractedRDD$$anonfun$compute$3(org.apache.spark.rdd.SubtractedRDD)|",
      "|java+constructor:///java/util/HashMap/HashMap()|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+method:///org/apache/spark/rdd/SubtractedRDD/integrate$1(int,scala.Function1,org.apache.spark.TaskContext,org.apache.spark.rdd.CoGroupPartition)|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///scala/collection/TraversableOnce$/flattenTraversableOnce(scala.collection.TraversableOnce,scala.Function1)|",
      "|java+constructor:///org/apache/spark/rdd/SubtractedRDD$$anonfun$compute$1/SubtractedRDD$$anonfun$compute$1(org.apache.spark.rdd.SubtractedRDD,java.util.HashMap)|",
      "|java+constructor:///org/apache/spark/rdd/SubtractedRDD$$anonfun$compute$2/SubtractedRDD$$anonfun$compute$2(org.apache.spark.rdd.SubtractedRDD,java.util.HashMap)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/TraversableOnce$FlattenOps/flatten()|",
      "|java+method:///scala/Predef$/conforms()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$ExternalIterator/ExternalAppendOnlyMap$ExternalIterator(org.apache.spark.util.collection.ExternalAppendOnlyMap)|",
    "called": "|java+method:///scala/collection/Seq$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$spilledMaps()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///scala/collection/mutable/PriorityQueue/PriorityQueue(scala.math.Ordering)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$ExternalIterator/sortedMap()|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$ExternalIterator$$anonfun$4/ExternalAppendOnlyMap$ExternalIterator$$anonfun$4(org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$keyComparator()|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$currentMap()|",
      "|java+method:///scala/collection/TraversableOnce$class/$init$(scala.collection.TraversableOnce)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/collection/GenTraversableOnce$class/$init$(scala.collection.GenTraversableOnce)|",
      "|java+method:///scala/collection/Iterator$class/$init$(scala.collection.Iterator)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/math/Ordering$/ordered(scala.Function1)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$ExternalIterator$$anonfun$3/ExternalAppendOnlyMap$ExternalIterator$$anonfun$3(org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$ExternalIterator/inputStreams()|",
      "|java+method:///org/apache/spark/util/collection/SizeTrackingAppendOnlyMap/destructiveSortedIterator(java.util.Comparator)|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$spilledMaps()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///scala/collection/mutable/PriorityQueue/PriorityQueue(scala.math.Ordering)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$ExternalIterator/sortedMap()|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$ExternalIterator$$anonfun$5/ExternalAppendOnlyMap$ExternalIterator$$anonfun$5(org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator)|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$ExternalIterator$$anonfun$4/ExternalAppendOnlyMap$ExternalIterator$$anonfun$4(org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$currentMap()|",
      "|java+method:///scala/collection/TraversableOnce$class/$init$(scala.collection.TraversableOnce)|",
      "|java+method:///org/apache/spark/util/CompletionIterator$/apply(scala.collection.Iterator,scala.Function0)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/collection/GenTraversableOnce$class/$init$(scala.collection.GenTraversableOnce)|",
      "|java+method:///scala/collection/Iterator$class/$init$(scala.collection.Iterator)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/math/Ordering$/ordered(scala.Function1)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$ExternalIterator/inputStreams()|",
      "|java+method:///org/apache/spark/util/collection/SizeTrackingAppendOnlyMap/destructiveSortedIterator(java.util.Comparator)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/org$apache$spark$util$collection$ExternalAppendOnlyMap$$keyComparator()|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$ExternalIterator$$anonfun$1/ExternalAppendOnlyMap$ExternalIterator$$anonfun$1(org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator)|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/serializer/KryoSerializer$$anonfun$newKryo$2/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/serializer/KryoSerializer$$anonfun$newKryo$2/apply(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/serializer/KryoSerializer$$anonfun$newKryo$2/apply(scala.Tuple2)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/DiskStore/putIterator(org.apache.spark.storage.BlockId,scala.collection.Iterator,org.apache.spark.storage.StorageLevel,boolean)|",
    "called": "|java+method:///scala/util/Right$/apply(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/DiskStore/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/storage/DiskStore$$anonfun$putIterator$2/DiskStore$$anonfun$putIterator$2(org.apache.spark.storage.DiskStore,java.io.FileOutputStream)|",
      "|java+constructor:///org/apache/spark/storage/DiskStore$$anonfun$putIterator$4/DiskStore$$anonfun$putIterator$4(org.apache.spark.storage.DiskStore,java.io.File,long,long)|",
      "|java+constructor:///org/apache/spark/storage/DiskStore$$anonfun$putIterator$1/DiskStore$$anonfun$putIterator$1(org.apache.spark.storage.DiskStore,org.apache.spark.storage.BlockId,scala.collection.Iterator,java.io.FileOutputStream)|",
      "|java+method:///scala/package$/Right()|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///scala/util/Right$/apply(java.lang.Object)|",
      "|java+method:///java/io/File/length()|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///org/apache/spark/storage/PutResult$/apply$default$3()|",
      "|java+constructor:///org/apache/spark/storage/PutResult/PutResult(long,scala.util.Either,scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/storage/DiskStore/getBytes(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/util/Utils$/tryWithSafeFinally(scala.Function0,scala.Function0)|",
      "|java+method:///org/apache/spark/storage/DiskBlockManager/getFile(org.apache.spark.storage.BlockId)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+constructor:///org/apache/spark/storage/DiskStore$$anonfun$putIterator$3/DiskStore$$anonfun$putIterator$3(org.apache.spark.storage.DiskStore,org.apache.spark.storage.BlockId)|",
      "|java+method:///java/io/File/delete()|",
      "|java+constructor:///java/io/FileOutputStream/FileOutputStream(java.io.File)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/DiskStore/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/storage/DiskStore$$anonfun$putIterator$2/DiskStore$$anonfun$putIterator$2(org.apache.spark.storage.DiskStore,java.io.FileOutputStream)|",
      "|java+constructor:///org/apache/spark/storage/DiskStore$$anonfun$putIterator$1/DiskStore$$anonfun$putIterator$1(org.apache.spark.storage.DiskStore,org.apache.spark.storage.BlockId,scala.collection.Iterator,java.io.FileOutputStream)|",
      "|java+method:///scala/package$/Right()|",
      "|java+method:///java/io/File/exists()|",
      "|java+constructor:///org/apache/spark/storage/DiskStore$$anonfun$putIterator$3/DiskStore$$anonfun$putIterator$3(org.apache.spark.storage.DiskStore,org.apache.spark.storage.BlockId)|",
      "|java+method:///scala/util/Right$/apply(java.lang.Object)|",
      "|java+method:///java/io/File/length()|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+constructor:///org/apache/spark/storage/PutResult/PutResult(long,scala.util.Either,scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|",
      "|java+constructor:///org/apache/spark/storage/DiskStore$$anonfun$putIterator$5/DiskStore$$anonfun$putIterator$5(org.apache.spark.storage.DiskStore,java.io.File,long,long)|",
      "|java+constructor:///org/apache/spark/storage/DiskStore$$anonfun$putIterator$4/DiskStore$$anonfun$putIterator$4(org.apache.spark.storage.DiskStore,java.io.File)|",
      "|java+method:///org/apache/spark/storage/DiskStore/getBytes(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/util/Utils$/tryWithSafeFinally(scala.Function0,scala.Function0)|",
      "|java+method:///org/apache/spark/storage/DiskBlockManager/getFile(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/PutResult$/apply$default$3()|",
      "|java+method:///org/apache/spark/storage/DiskStore/logWarning(scala.Function0)|",
      "|java+method:///java/io/File/delete()|",
      "|java+constructor:///java/io/FileOutputStream/FileOutputStream(java.io.File)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/close()|",
    "called": "|java+method:///scala/Option/isDefined()|",
    "v1Body": [
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/inputMetrics()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/split()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/reader()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/reader_$eq(org.apache.hadoop.mapred.RecordReader)|",
      "|java+method:///org/apache/spark/executor/InputMetrics/updateBytesRead()|",
      "|java+method:///org/apache/spark/util/ShutdownHookManager$/inShutdown()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/bytesReadCallback()|",
      "|java+method:///org/apache/spark/executor/InputMetrics/incBytesRead(long)|",
      "|java+method:///org/apache/spark/SerializableWritable/value()|",
      "|java+constructor:///org/apache/spark/rdd/HadoopRDD$$anon$1$$anonfun$close$1/HadoopRDD$$anon$1$$anonfun$close$1(org.apache.spark.rdd.HadoopRDD$$anon$1)|",
      "|java+constructor:///org/apache/spark/rdd/HadoopRDD$$anon$1$$anonfun$close$2/HadoopRDD$$anon$1$$anonfun$close$2(org.apache.spark.rdd.HadoopRDD$$anon$1)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/rdd/HadoopPartition/inputSplit()|",
      "|java+method:///org/apache/hadoop/mapred/InputSplit/getLength()|",
      "|java+method:///org/apache/hadoop/mapred/RecordReader/close()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/inputMetrics()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/split()|",
      "|java+method:///org/apache/spark/rdd/SqlNewHadoopRDDState$/unsetInputFileName()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/reader()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/reader_$eq(org.apache.hadoop.mapred.RecordReader)|",
      "|java+method:///org/apache/spark/executor/InputMetrics/updateBytesRead()|",
      "|java+method:///org/apache/spark/util/ShutdownHookManager$/inShutdown()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/bytesReadCallback()|",
      "|java+method:///org/apache/spark/executor/InputMetrics/incBytesRead(long)|",
      "|java+method:///org/apache/spark/SerializableWritable/value()|",
      "|java+constructor:///org/apache/spark/rdd/HadoopRDD$$anon$1$$anonfun$close$1/HadoopRDD$$anon$1$$anonfun$close$1(org.apache.spark.rdd.HadoopRDD$$anon$1)|",
      "|java+constructor:///org/apache/spark/rdd/HadoopRDD$$anon$1$$anonfun$close$2/HadoopRDD$$anon$1$$anonfun$close$2(org.apache.spark.rdd.HadoopRDD$$anon$1)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/rdd/HadoopPartition/inputSplit()|",
      "|java+method:///org/apache/hadoop/mapred/InputSplit/getLength()|",
      "|java+method:///org/apache/hadoop/mapred/RecordReader/close()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$markStageAsFinished(org.apache.spark.scheduler.Stage,scala.Option)|",
    "called": "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/Stage/id()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/stageEnd(int)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/runningStages()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/stageFailed(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/outputCommitCoordinator()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/completionTime_$eq(scala.Option)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/Stage/latestInfo()|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerStageCompleted/SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/submissionTime()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$markStageAsFinished$2/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$markStageAsFinished$2(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Stage,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$markStageAsFinished$1/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$markStageAsFinished$1(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Stage,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/Stage/id()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/stageEnd(int)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/runningStages()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/stageFailed(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/outputCommitCoordinator()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/completionTime_$eq(scala.Option)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/Stage/latestInfo()|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerStageCompleted/SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/Stage/clearFailures()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/submissionTime()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$markStageAsFinished$2/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$markStageAsFinished$2(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Stage,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$markStageAsFinished$1/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$markStageAsFinished$1(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Stage,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$/stageInfoFromJson(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class,scala.reflect.Manifest,scala.collection.Seq)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extractOpt(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo$/$lessinit$greater$default$8()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/failureReason_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/submissionTime_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$61/JsonProtocol$$anonfun$61()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$65/JsonProtocol$$anonfun$65()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$60/JsonProtocol$$anonfun$60()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$64/JsonProtocol$$anonfun$64()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$63/JsonProtocol$$anonfun$63()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$67/JsonProtocol$$anonfun$67()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$62/JsonProtocol$$anonfun$62()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$66/JsonProtocol$$anonfun$66()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$stageInfoFromJson$1/JsonProtocol$$anonfun$stageInfoFromJson$1(org.apache.spark.scheduler.StageInfo)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/completionTime_$eq(scala.Option)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/StageInfo/StageInfo(int,int,java.lang.String,int,scala.collection.Seq,scala.collection.Seq,java.lang.String,scala.collection.Seq)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$7/JsonProtocol$$anonfun$7()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|"
    ],
    "v2Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class,scala.reflect.Manifest,scala.collection.Seq)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extractOpt(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo$/$lessinit$greater$default$8()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/failureReason_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/submissionTime_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$65/JsonProtocol$$anonfun$65()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$69/JsonProtocol$$anonfun$69()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$64/JsonProtocol$$anonfun$64()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$70/JsonProtocol$$anonfun$70()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$68/JsonProtocol$$anonfun$68()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$63/JsonProtocol$$anonfun$63()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$67/JsonProtocol$$anonfun$67()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$66/JsonProtocol$$anonfun$66()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$stageInfoFromJson$1/JsonProtocol$$anonfun$stageInfoFromJson$1(org.apache.spark.scheduler.StageInfo)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/completionTime_$eq(scala.Option)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/StageInfo/StageInfo(int,int,java.lang.String,int,scala.collection.Seq,scala.collection.Seq,java.lang.String,scala.collection.Seq)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$7/JsonProtocol$$anonfun$7()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$$anonfun$fn$7$1/apply(scala.collection.Iterator)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/asScalaIterator(java.util.Iterator)|",
      "|java+method:///java/lang/Iterable/iterator()|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterator(scala.collection.Iterator)|",
      "|java+method:///org/apache/spark/api/java/function/PairFlatMapFunction/call(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///java/lang/Iterable/iterator()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaIteratorConverter(java.util.Iterator)|",
      "|java+method:///org/apache/spark/api/java/function/PairFlatMapFunction/call(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/scope/RDDOperationGraph$/org$apache$spark$ui$scope$RDDOperationGraph$$makeDotNode(org.apache.spark.ui.scope.RDDOperationNode)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/id()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/name()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/id()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/name()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/callsite()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/SparkContext/SparkContext(org.apache.spark.SparkConf)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/MetadataCleanerType$/SPARK_CONTEXT()|",
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///org/apache/spark/SparkContext/postApplicationStart()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_cleaner_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/util/Utils$/localHostName()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/newConfiguration(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkContext/_hadoopConfiguration()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/ask(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkConf/clone()|",
      "|java+method:///org/apache/spark/SparkContext/_files_$eq(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkConf/setIfMissing(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_heartbeatReceiver()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/ExecutorAllocationClient$class/$init$(org.apache.spark.ExecutorAllocationClient)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_conf()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/_schedulerBackend_$eq(org.apache.spark.scheduler.SchedulerBackend)|",
      "|java+method:///org/apache/spark/SparkContext/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/HeartbeatReceiver$/ENDPOINT_NAME()|",
      "|java+method:///org/apache/spark/SparkContext/executorMemory()|",
      "|java+method:///org/apache/spark/SparkContext/postEnvironmentUpdate()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/start()|",
      "|java+method:///org/apache/spark/SparkContext/allowMultipleContexts()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_env()|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/getServletHandlers()|",
      "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
      "|java+method:///org/apache/spark/util/Utils$/isDynamicAllocationEnabled(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkContext/jobProgressListener()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_executorAllocationManager_$eq(scala.Option)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$9/SparkContext$$anonfun$9(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerSource/BlockManagerSource(org.apache.spark.storage.BlockManager)|",
      "|java+method:///org/apache/spark/SparkContext/_taskScheduler()|",
      "|java+method:///org/apache/spark/SparkContext/_jobProgressListener_$eq(org.apache.spark.ui.jobs.JobProgressListener)|",
      "|java+method:///org/apache/spark/SparkContext/_applicationId_$eq(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/jars()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$8/SparkContext$$anonfun$8(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/io/CompressionCodec$/getCodecName(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkContext/_eventLogDir_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/SparkContext/externalBlockStoreFolderName()|",
      "|java+method:///org/apache/spark/scheduler/TaskScheduler/start()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_ui_$eq(scala.Option)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler/DAGScheduler(org.apache.spark.SparkContext)|",
      "|java+method:///scala/collection/TraversableLike/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/util/Utils$/getCallSite$default$1()|",
      "|java+constructor:///org/apache/spark/util/MetadataCleaner/MetadataCleaner(scala.Enumeration$Value,scala.Function1,org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkContext/isLocal()|",
      "|java+method:///org/apache/spark/SparkContext/_shutdownHookRef_$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/executorEnvs()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$7/SparkContext$$anonfun$7(org.apache.spark.SparkContext)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkContext$/setActiveContext(org.apache.spark.SparkContext,boolean)|",
      "|java+constructor:///org/apache/spark/util/TimeStampedWeakValueHashMap/TimeStampedWeakValueHashMap(boolean)|",
      "|java+method:///org/apache/spark/scheduler/TaskScheduler/applicationAttemptId()|",
      "|java+constructor:///java/util/concurrent/atomic/AtomicBoolean/AtomicBoolean(boolean)|",
      "|java+method:///org/apache/spark/scheduler/TaskScheduler/postStartHook()|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_heartbeatReceiver_$eq(org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/SparkConf/validateSettings()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/start()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(int)|",
      "|java+method:///scala/collection/Iterable/toSeq()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$6/SparkContext$$anonfun$6(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/log()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/mutable/HashMap$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkContext/_executorMemory_$eq(int)|",
      "|java+method:///org/apache/spark/util/Utils$/getCurrentUserName()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/SparkConf/contains(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$5/SparkContext$$anonfun$5(org.apache.spark.SparkContext)|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_cleaner()|",
      "|java+constructor:///org/apache/spark/ui/jobs/JobProgressListener/JobProgressListener(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkContext/_jobProgressListener()|",
      "|java+method:///org/apache/spark/util/Utils$/resolveURI(java.lang.String)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///java/util/UUID/randomUUID()|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener$/DEFAULT_LOG_DIR()|",
      "|java+method:///org/apache/spark/SparkContext/createSparkEnv(org.apache.spark.SparkConf,boolean,org.apache.spark.scheduler.LiveListenerBus)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$4/SparkContext$$anonfun$4(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/registerSource(org.apache.spark.metrics.source.Source)|",
      "|java+method:///scala/collection/Seq/flatten(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkContext/sparkUser()|",
      "|java+method:///org/apache/spark/SparkContext/_statusTracker_$eq(org.apache.spark.SparkStatusTracker)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/TaskScheduler/applicationId()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkContext/_eventLogCodec_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/SparkContext/stop()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_executorAllocationManager()|",
      "|java+method:///scala/Some/map(scala.Function1)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkContext/master()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_progressBar_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/addListener(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$3/SparkContext$$anonfun$3(org.apache.spark.SparkContext)|",
      "|java+method:///scala/collection/mutable/HashMap/$plus$plus$eq(scala.collection.TraversableOnce)|",
      "|java+method:///org/apache/spark/SparkContext/_applicationAttemptId_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/SparkEnv/securityManager()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$18/SparkContext$$anonfun$18(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_ui()|",
      "|java+method:///org/apache/spark/SparkContext/setupAndStartListenerBus()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$20/SparkContext$$anonfun$20(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkStatusTracker/SparkStatusTracker(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$14/SparkContext$$anonfun$14(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$25/SparkContext$$anonfun$25(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$2/SparkContext$$anonfun$2(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/ui/SparkUI$/createLiveUI(org.apache.spark.SparkContext,org.apache.spark.SparkConf,org.apache.spark.scheduler.SparkListenerBus,org.apache.spark.ui.jobs.JobProgressListener,org.apache.spark.SecurityManager,java.lang.String,long)|",
      "|java+method:///org/apache/spark/SparkContext/appName()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$19/SparkContext$$anonfun$19(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$10/SparkContext$$anonfun$10(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$21/SparkContext$$anonfun$21(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/_hadoopConfiguration_$eq(org.apache.hadoop.conf.Configuration)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anon$2/SparkContext$$anon$2(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$26/SparkContext$$anonfun$26(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$15/SparkContext$$anonfun$15(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$22/SparkContext$$anonfun$22(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/ContextCleaner/ContextCleaner(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$11/SparkContext$$anonfun$11(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$16/SparkContext$$anonfun$16(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$27/SparkContext$$anonfun$27(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$12/SparkContext$$anonfun$12(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$23/SparkContext$$anonfun$23(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$17/SparkContext$$anonfun$17(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$28/SparkContext$$anonfun$28(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$13/SparkContext$$anonfun$13(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$24/SparkContext$$anonfun$24(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/_eventLogDir()|",
      "|java+method:///org/apache/spark/SparkContext/listenerBus()|",
      "|java+method:///org/apache/spark/SparkContext/isEventLogEnabled()|",
      "|java+method:///scala/collection/Map$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkContext$/org$apache$spark$SparkContext$$createTaskScheduler(org.apache.spark.SparkContext,java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkConf/set(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/SparkContext$/DRIVER_IDENTIFIER()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$1/SparkContext$$anonfun$1(org.apache.spark.SparkContext)|",
      "|java+method:///java/lang/System/setProperty(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_eventLogger_$eq(scala.Option)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/LiveListenerBus/LiveListenerBus()|",
      "|java+method:///org/apache/spark/SparkEnv$/set(org.apache.spark.SparkEnv)|",
      "|java+method:///org/apache/spark/SparkConf/getExecutorEnv()|",
      "|java+method:///org/apache/spark/SparkContext/startTime()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkEnv/metricsSystem()|",
      "|java+method:///org/apache/spark/util/TimeStampedWeakValueHashMap$/$lessinit$greater$default$1()|",
      "|java+method:///org/apache/spark/util/Utils$/getCallSite(scala.Function1)|",
      "|java+constructor:///java/util/concurrent/atomic/AtomicInteger/AtomicInteger(int)|",
      "|java+constructor:///org/apache/spark/scheduler/EventLoggingListener/EventLoggingListener(java.lang.String,scala.Option,java.net.URI,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+method:///org/apache/spark/storage/BlockManager/initialize(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/_jars_$eq(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/ShutdownHookManager$/addShutdownHook(int,scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext/metricsSystem()|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_dagScheduler_$eq(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_env_$eq(org.apache.spark.SparkEnv)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_conf_$eq(org.apache.spark.SparkConf)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///java/lang/System/getenv(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/_taskScheduler_$eq(org.apache.spark.scheduler.TaskScheduler)|",
      "|java+method:///org/apache/spark/SparkContext/_applicationId()|",
      "|java+method:///org/slf4j/Logger/isInfoEnabled()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/StringOps/stripSuffix(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext$/markPartiallyConstructed(org.apache.spark.SparkContext,boolean)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/HeartbeatReceiver/HeartbeatReceiver(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/util/ShutdownHookManager$/SPARK_CONTEXT_SHUTDOWN_PRIORITY()|",
      "|java+method:///java/util/UUID/toString()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///scala/reflect/ClassTag$/Boolean()|",
      "|java+constructor:///org/apache/spark/ui/ConsoleProgressBar/ConsoleProgressBar(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/_metadataCleaner_$eq(org.apache.spark.util.MetadataCleaner)|",
      "|java+method:///org/apache/spark/SparkContext/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext/taskScheduler()|",
      "|java+constructor:///org/apache/spark/ExecutorAllocationManager/ExecutorAllocationManager(org.apache.spark.ExecutorAllocationClient,org.apache.spark.scheduler.LiveListenerBus,org.apache.spark.SparkConf)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/SparkContext/_applicationAttemptId()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/SparkContext/files()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/MetadataCleanerType$/SPARK_CONTEXT()|",
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///org/apache/spark/SparkContext/postApplicationStart()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_cleaner_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/util/Utils$/localHostName()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/newConfiguration(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkContext/_hadoopConfiguration()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/ask(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkConf/clone()|",
      "|java+method:///org/apache/spark/SparkContext/_files_$eq(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkConf/setIfMissing(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_heartbeatReceiver()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/ExecutorAllocationClient$class/$init$(org.apache.spark.ExecutorAllocationClient)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_applicationId()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_conf()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/_schedulerBackend_$eq(org.apache.spark.scheduler.SchedulerBackend)|",
      "|java+method:///org/apache/spark/SparkContext/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/HeartbeatReceiver$/ENDPOINT_NAME()|",
      "|java+method:///org/apache/spark/SparkContext/executorMemory()|",
      "|java+method:///org/apache/spark/SparkContext/postEnvironmentUpdate()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/start()|",
      "|java+method:///org/apache/spark/SparkContext/allowMultipleContexts()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_env()|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/getServletHandlers()|",
      "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
      "|java+method:///org/apache/spark/util/Utils$/isDynamicAllocationEnabled(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkContext/jobProgressListener()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_executorAllocationManager_$eq(scala.Option)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$9/SparkContext$$anonfun$9(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/storage/BlockManagerSource/BlockManagerSource(org.apache.spark.storage.BlockManager)|",
      "|java+method:///org/apache/spark/SparkContext/_taskScheduler()|",
      "|java+method:///org/apache/spark/SparkContext/_jobProgressListener_$eq(org.apache.spark.ui.jobs.JobProgressListener)|",
      "|java+method:///org/apache/spark/SparkContext/jars()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$8/SparkContext$$anonfun$8(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/io/CompressionCodec$/getCodecName(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkContext/_eventLogDir_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/SparkContext/externalBlockStoreFolderName()|",
      "|java+method:///org/apache/spark/scheduler/TaskScheduler/start()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_ui_$eq(scala.Option)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler/DAGScheduler(org.apache.spark.SparkContext)|",
      "|java+method:///scala/collection/TraversableLike/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/util/Utils$/getCallSite$default$1()|",
      "|java+constructor:///org/apache/spark/util/MetadataCleaner/MetadataCleaner(scala.Enumeration$Value,scala.Function1,org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkContext/isLocal()|",
      "|java+method:///org/apache/spark/SparkContext/_shutdownHookRef_$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/executorEnvs()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$7/SparkContext$$anonfun$7(org.apache.spark.SparkContext)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkContext$/setActiveContext(org.apache.spark.SparkContext,boolean)|",
      "|java+constructor:///org/apache/spark/util/TimeStampedWeakValueHashMap/TimeStampedWeakValueHashMap(boolean)|",
      "|java+method:///org/apache/spark/scheduler/TaskScheduler/applicationAttemptId()|",
      "|java+constructor:///java/util/concurrent/atomic/AtomicBoolean/AtomicBoolean(boolean)|",
      "|java+method:///org/apache/spark/scheduler/TaskScheduler/postStartHook()|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_heartbeatReceiver_$eq(org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/SparkConf/validateSettings()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/start()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(int)|",
      "|java+method:///scala/collection/Iterable/toSeq()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$6/SparkContext$$anonfun$6(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/log()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/mutable/HashMap$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkContext/_executorMemory_$eq(int)|",
      "|java+method:///org/apache/spark/util/Utils$/getCurrentUserName()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/SparkConf/contains(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$5/SparkContext$$anonfun$5(org.apache.spark.SparkContext)|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_cleaner()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_applicationId_$eq(java.lang.String)|",
      "|java+constructor:///org/apache/spark/ui/jobs/JobProgressListener/JobProgressListener(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkContext/_jobProgressListener()|",
      "|java+method:///org/apache/spark/util/Utils$/resolveURI(java.lang.String)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///java/util/UUID/randomUUID()|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener$/DEFAULT_LOG_DIR()|",
      "|java+method:///org/apache/spark/SparkContext/createSparkEnv(org.apache.spark.SparkConf,boolean,org.apache.spark.scheduler.LiveListenerBus)|",
      "|java+method:///org/apache/spark/SparkContext/logWarning(scala.Function0)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$4/SparkContext$$anonfun$4(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/registerSource(org.apache.spark.metrics.source.Source)|",
      "|java+method:///scala/collection/Seq/flatten(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkContext/sparkUser()|",
      "|java+method:///org/apache/spark/SparkContext/_statusTracker_$eq(org.apache.spark.SparkStatusTracker)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/TaskScheduler/applicationId()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_dagScheduler()|",
      "|java+method:///org/apache/spark/SparkContext/_eventLogCodec_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/SparkContext/stop()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_executorAllocationManager()|",
      "|java+method:///scala/Some/map(scala.Function1)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkContext/master()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_progressBar_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/addListener(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$3/SparkContext$$anonfun$3(org.apache.spark.SparkContext)|",
      "|java+method:///scala/collection/mutable/HashMap/$plus$plus$eq(scala.collection.TraversableOnce)|",
      "|java+method:///org/apache/spark/SparkContext/_applicationAttemptId_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/SparkEnv/securityManager()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_ui()|",
      "|java+method:///org/apache/spark/SparkContext/setupAndStartListenerBus()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$18/SparkContext$$anonfun$18(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$29/SparkContext$$anonfun$29(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$20/SparkContext$$anonfun$20(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkStatusTracker/SparkStatusTracker(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$14/SparkContext$$anonfun$14(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$25/SparkContext$$anonfun$25(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$2/SparkContext$$anonfun$2(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/ui/SparkUI$/createLiveUI(org.apache.spark.SparkContext,org.apache.spark.SparkConf,org.apache.spark.scheduler.SparkListenerBus,org.apache.spark.ui.jobs.JobProgressListener,org.apache.spark.SecurityManager,java.lang.String,long)|",
      "|java+method:///org/apache/spark/SparkContext/appName()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$19/SparkContext$$anonfun$19(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$10/SparkContext$$anonfun$10(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$21/SparkContext$$anonfun$21(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/_hadoopConfiguration_$eq(org.apache.hadoop.conf.Configuration)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anon$2/SparkContext$$anon$2(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$26/SparkContext$$anonfun$26(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$15/SparkContext$$anonfun$15(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$22/SparkContext$$anonfun$22(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/ContextCleaner/ContextCleaner(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$11/SparkContext$$anonfun$11(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$16/SparkContext$$anonfun$16(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$27/SparkContext$$anonfun$27(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$12/SparkContext$$anonfun$12(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$23/SparkContext$$anonfun$23(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$17/SparkContext$$anonfun$17(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$28/SparkContext$$anonfun$28(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$13/SparkContext$$anonfun$13(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$24/SparkContext$$anonfun$24(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/_eventLogDir()|",
      "|java+method:///org/apache/spark/SparkContext/listenerBus()|",
      "|java+method:///org/apache/spark/SparkContext/isEventLogEnabled()|",
      "|java+method:///org/apache/spark/SparkContext$/org$apache$spark$SparkContext$$createTaskScheduler(org.apache.spark.SparkContext,java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkConf/set(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/SparkContext$/DRIVER_IDENTIFIER()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$1/SparkContext$$anonfun$1(org.apache.spark.SparkContext)|",
      "|java+method:///java/lang/System/setProperty(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_eventLogger_$eq(scala.Option)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/LiveListenerBus/LiveListenerBus()|",
      "|java+method:///org/apache/spark/SparkEnv$/set(org.apache.spark.SparkEnv)|",
      "|java+method:///org/apache/spark/SparkConf/getExecutorEnv()|",
      "|java+method:///org/apache/spark/SparkContext/startTime()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkEnv/metricsSystem()|",
      "|java+method:///org/apache/spark/util/TimeStampedWeakValueHashMap$/$lessinit$greater$default$1()|",
      "|java+method:///org/apache/spark/util/Utils$/getCallSite(scala.Function1)|",
      "|java+constructor:///java/util/concurrent/atomic/AtomicInteger/AtomicInteger(int)|",
      "|java+constructor:///org/apache/spark/scheduler/EventLoggingListener/EventLoggingListener(java.lang.String,scala.Option,java.net.URI,org.apache.spark.SparkConf,org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/metricsSource()|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+method:///org/apache/spark/storage/BlockManager/initialize(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/_jars_$eq(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/ShutdownHookManager$/addShutdownHook(int,scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext/metricsSystem()|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_dagScheduler_$eq(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_env_$eq(org.apache.spark.SparkEnv)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_conf_$eq(org.apache.spark.SparkConf)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///java/lang/System/getenv(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/_taskScheduler_$eq(org.apache.spark.scheduler.TaskScheduler)|",
      "|java+method:///org/slf4j/Logger/isInfoEnabled()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/StringOps/stripSuffix(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext$/markPartiallyConstructed(org.apache.spark.SparkContext,boolean)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/HeartbeatReceiver/HeartbeatReceiver(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/util/ShutdownHookManager$/SPARK_CONTEXT_SHUTDOWN_PRIORITY()|",
      "|java+method:///java/util/UUID/toString()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///scala/reflect/ClassTag$/Boolean()|",
      "|java+constructor:///org/apache/spark/ui/ConsoleProgressBar/ConsoleProgressBar(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/_metadataCleaner_$eq(org.apache.spark.util.MetadataCleaner)|",
      "|java+method:///org/apache/spark/SparkContext/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext/taskScheduler()|",
      "|java+constructor:///org/apache/spark/ExecutorAllocationManager/ExecutorAllocationManager(org.apache.spark.ExecutorAllocationClient,org.apache.spark.scheduler.LiveListenerBus,org.apache.spark.SparkConf)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/SparkContext/_applicationAttemptId()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/SparkContext/files()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient$/main(java.lang.String%5B%5D)|",
    "called": "|java+method:///scala/sys/package$/env()|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/ArrayOps/size()|",
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient$/run$default$5()|",
      "|java+method:///scala/collection/mutable/ArrayOps/slice(int,int)|",
      "|java+constructor:///org/apache/spark/SparkConf/SparkConf()|",
      "|java+method:///scala/sys/package$/error(java.lang.String)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient$/run(java.lang.String,java.lang.String,java.lang.String%5B%5D,org.apache.spark.SparkConf,scala.collection.immutable.Map)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/ArrayOps/size()|",
      "|java+method:///scala/sys/package$/env()|",
      "|java+method:///scala/collection/mutable/ArrayOps/slice(int,int)|",
      "|java+constructor:///org/apache/spark/SparkConf/SparkConf()|",
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient$/filterSystemEnvironment(scala.collection.immutable.Map)|",
      "|java+method:///scala/sys/package$/error(java.lang.String)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient$/run(java.lang.String,java.lang.String,java.lang.String%5B%5D,org.apache.spark.SparkConf,scala.collection.immutable.Map)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI/getSparkUI(java.lang.String)|",
    "called": "|java+method:///scala/Option/flatMap(scala.Function1)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI/masterPage()|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterPage/getMasterState()|",
      "|java+method:///scala/collection/mutable/ArrayOps/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterStateResponse/completedApps()|",
      "|java+method:///scala/Option/flatMap(scala.Function1)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI$$anonfun$getSparkUI$1/MasterWebUI$$anonfun$getSparkUI$1(org.apache.spark.deploy.master.ui.MasterWebUI,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI$$anonfun$getSparkUI$2/MasterWebUI$$anonfun$getSparkUI$2(org.apache.spark.deploy.master.ui.MasterWebUI)|",
      "|java+method:///scala/collection/mutable/ArrayOps/reverse()|",
      "|java+method:///scala/collection/mutable/ArrayOps/sortBy(scala.Function1,scala.math.Ordering)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterStateResponse/activeApps()|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI$$anonfun$5/MasterWebUI$$anonfun$5(org.apache.spark.deploy.master.ui.MasterWebUI)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI$$anonfun$6/MasterWebUI$$anonfun$6(org.apache.spark.deploy.master.ui.MasterWebUI)|",
      "|java+method:///scala/collection/mutable/ArrayOps/find(scala.Function1)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI/masterPage()|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterPage/getMasterState()|",
      "|java+method:///scala/collection/mutable/ArrayOps/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterStateResponse/completedApps()|",
      "|java+method:///scala/Option/flatMap(scala.Function1)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI$$anonfun$getSparkUI$1/MasterWebUI$$anonfun$getSparkUI$1(org.apache.spark.deploy.master.ui.MasterWebUI,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI$$anonfun$getSparkUI$2/MasterWebUI$$anonfun$getSparkUI$2(org.apache.spark.deploy.master.ui.MasterWebUI)|",
      "|java+method:///scala/collection/mutable/ArrayOps/reverse()|",
      "|java+method:///scala/collection/mutable/ArrayOps/sortBy(scala.Function1,scala.math.Ordering)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterStateResponse/activeApps()|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI$$anonfun$6/MasterWebUI$$anonfun$6(org.apache.spark.deploy.master.ui.MasterWebUI)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI$$anonfun$7/MasterWebUI$$anonfun$7(org.apache.spark.deploy.master.ui.MasterWebUI)|",
      "|java+method:///scala/collection/mutable/ArrayOps/find(scala.Function1)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$partitionBy$1/apply()|",
    "called": "|java+constructor:///scala/Some/Some(java.lang.Object)|",
    "v1Body": [
      "|java+method:///java/lang/Class/isArray()|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/rdd/ShuffledRDD/ShuffledRDD(org.apache.spark.rdd.RDD,org.apache.spark.Partitioner)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$partitionBy$1/apply()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitioner()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/keyClass()|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Class/isArray()|",
      "|java+constructor:///org/apache/spark/rdd/ShuffledRDD/ShuffledRDD(org.apache.spark.rdd.RDD,org.apache.spark.Partitioner,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$partitionBy$1/apply()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitioner()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/keyClass()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/UIUtils$/uiRoot()|",
    "called": "|java+method:///scala/Option/getOrElse(scala.Function0)|",
    "v1Body": [
      "|java+method:///java/lang/System/getProperty(java.lang.String)|",
      "|java+method:///java/lang/System/getenv(java.lang.String)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/ui/UIUtils$$anonfun$uiRoot$1/UIUtils$$anonfun$uiRoot$1()|",
      "|java+method:///scala/sys/package$/props()|",
      "|java+constructor:///org/apache/spark/ui/UIUtils$$anonfun$uiRoot$2/UIUtils$$anonfun$uiRoot$2()|",
      "|java+method:///scala/sys/SystemProperties/get(java.lang.String)|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleJobGroupCancelled(java.lang.String)|",
    "called": "|java+method:///scala/collection/mutable/HashSet$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/submitWaitingStages()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleJobGroupCancelled$1/DAGScheduler$$anonfun$handleJobGroupCancelled$1(org.apache.spark.scheduler.DAGScheduler,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/activeJobs()|",
      "|java+method:///scala/collection/mutable/HashSet/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$9/DAGScheduler$$anonfun$9(org.apache.spark.scheduler.DAGScheduler,java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashSet/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/HashSet$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$10/DAGScheduler$$anonfun$10(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///scala/collection/mutable/HashSet/filter(scala.Function1)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/submitWaitingStages()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$9/DAGScheduler$$anonfun$9(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleJobGroupCancelled$1/DAGScheduler$$anonfun$handleJobGroupCancelled$1(org.apache.spark.scheduler.DAGScheduler,java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashSet/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/activeJobs()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$8/DAGScheduler$$anonfun$8(org.apache.spark.scheduler.DAGScheduler,java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashSet/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/HashSet/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/HashSet$/canBuildFrom()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/TestUtils$/createJarWithClasses(scala.collection.Seq,java.lang.String,scala.collection.Seq,scala.collection.Seq)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/TestUtils$/createJar(scala.collection.Seq,java.io.File)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$2/TestUtils$$anonfun$2()|",
      "|java+method:///org/apache/spark/util/Utils$/createTempDir$default$1()|",
      "|java+method:///org/apache/spark/util/Utils$/createTempDir$default$2()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/collection/generic/FilterMonadic/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$1/TestUtils$$anonfun$1(java.lang.String,scala.collection.Seq,java.io.File)|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$3/TestUtils$$anonfun$3(java.lang.String,scala.collection.Seq,java.io.File)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/Seq/withFilter(scala.Function1)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///org/apache/spark/util/Utils$/createTempDir(java.lang.String,java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$2/TestUtils$$anonfun$2()|",
      "|java+method:///org/apache/spark/util/Utils$/createTempDir$default$1()|",
      "|java+method:///org/apache/spark/util/Utils$/createTempDir$default$2()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/TestUtils$/createJar(scala.collection.Seq,java.io.File,scala.Option)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/collection/generic/FilterMonadic/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$1/TestUtils$$anonfun$1(java.lang.String,scala.collection.Seq,java.io.File)|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$3/TestUtils$$anonfun$3(java.lang.String,scala.collection.Seq,java.io.File)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/Seq/withFilter(scala.Function1)|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///org/apache/spark/TestUtils$/createJar$default$3()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///org/apache/spark/util/Utils$/createTempDir(java.lang.String,java.lang.String)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$58/apply(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$/mapFromJson(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$16/apply()|",
    "called": "|java+method:///scala/concurrent/Future$/successful(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$16/apply()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/appUiUrl_$eq(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/desc()|",
      "|java+constructor:///scala/runtime/NonLocalReturnControl/NonLocalReturnControl(java.lang.Object,java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/appUIUrlAtHistoryServer_$eq(scala.Option)|",
      "|java+method:///scala/concurrent/Future$/successful(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$16/apply()|",
      "|java+constructor:///scala/runtime/NonLocalReturnControl/NonLocalReturnControl(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/input/WholeTextFileInputFormat/setMinPartitions(org.apache.hadoop.mapreduce.JobContext,int)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///java/lang/Math/ceil(double)|",
      "|java+constructor:///org/apache/spark/input/WholeTextFileInputFormat$$anonfun$1/WholeTextFileInputFormat$$anonfun$1(org.apache.spark.input.WholeTextFileInputFormat)|",
      "|java+method:///scala/collection/TraversableOnce/sum(scala.math.Numeric)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///org/apache/spark/input/WholeTextFileInputFormat/listStatus(org.apache.hadoop.mapreduce.JobContext)|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat/setMaxSplitSize(long)|",
      "|java+method:///scala/collection/mutable/Buffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Math/ceil(double)|",
      "|java+constructor:///org/apache/spark/input/WholeTextFileInputFormat$$anonfun$1/WholeTextFileInputFormat$$anonfun$1(org.apache.spark.input.WholeTextFileInputFormat)|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///org/apache/spark/input/WholeTextFileInputFormat/listStatus(org.apache.hadoop.mapreduce.JobContext)|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat/setMaxSplitSize(long)|",
      "|java+method:///scala/collection/mutable/Buffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///scala/collection/TraversableOnce/sum(scala.math.Numeric)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/input/WholeTextFileRecordReader/WholeTextFileRecordReader(org.apache.hadoop.mapreduce.lib.input.CombineFileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,java.lang.Integer)|",
    "called": "|java+method:///scala/Predef$/Integer2int(java.lang.Integer)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///org/apache/spark/input/Configurable$class/$init$(org.apache.spark.input.Configurable)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/getConfigurationFromJobContext(org.apache.hadoop.mapreduce.JobContext)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/RecordReader/RecordReader()|",
      "|java+method:///scala/Predef$/Integer2int(java.lang.Integer)|",
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/CombineFileSplit/getPath(int)|",
      "|java+method:///org/apache/hadoop/fs/Path/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///org/apache/spark/input/Configurable$class/$init$(org.apache.spark.input.Configurable)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/getConfigurationFromJobContext(org.apache.hadoop.mapreduce.JobContext)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/RecordReader/RecordReader()|",
      "|java+constructor:///org/apache/hadoop/io/Text/Text(java.lang.String)|",
      "|java+method:///scala/Predef$/Integer2int(java.lang.Integer)|",
      "|java+method:///org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/CombineFileSplit/getPath(int)|",
      "|java+method:///org/apache/hadoop/fs/Path/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler/cleanupStateForJobAndIndependentStages(org.apache.spark.scheduler.ActiveJob)|",
    "called": "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/jobIdToActiveJob()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/stageIdToStage()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/jobIdToStageIds()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/ResultStage/resultOfJob_$eq(scala.Option)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/mutable/HashMap/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/filterKeys(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/jobId()|",
      "|java+method:///scala/collection/Map/foreach(scala.Function1)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$1/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$1(org.apache.spark.scheduler.DAGScheduler,scala.Option)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.ActiveJob)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$2/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$2(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.ActiveJob)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/activeJobs()|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/finalStage()|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/collection/SetLike/isEmpty()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/ResultStage/removeActiveJob()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/stageIdToStage()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/jobIdToStageIds()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/mutable/HashMap/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/filterKeys(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/jobId()|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/removeActiveJob(org.apache.spark.scheduler.ActiveJob)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/jobIdToActiveJob()|",
      "|java+method:///scala/collection/Map/foreach(scala.Function1)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$1/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$1(org.apache.spark.scheduler.DAGScheduler,scala.Option)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.ActiveJob)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$2/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$2(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.ActiveJob)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/activeJobs()|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/finalStage()|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/collection/SetLike/isEmpty()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/ExternalBlockStore/createBlkManager()|",
    "called": "|java+method:///scala/Option/getOrElse(scala.Function0)|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/ExternalBlockStore/addShutdownHook()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/storage/ExternalBlockStore/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///java/lang/Class/newInstance()|",
      "|java+constructor:///org/apache/spark/storage/ExternalBlockStore$$anonfun$createBlkManager$1/ExternalBlockStore$$anonfun$createBlkManager$1(org.apache.spark.storage.ExternalBlockStore)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/storage/ExternalBlockStore$$anonfun$4/ExternalBlockStore$$anonfun$4(org.apache.spark.storage.ExternalBlockStore)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/classForName(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/BlockManager/conf()|",
      "|java+method:///org/apache/spark/storage/ExternalBlockStore$/BLOCK_MANAGER_NAME()|",
      "|java+method:///org/apache/spark/storage/ExternalBlockManager/init(org.apache.spark.storage.BlockManager,java.lang.String)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/storage/ExternalBlockStore/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///java/lang/Class/newInstance()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/ShutdownHookManager$/addShutdownHook(scala.Function0)|",
      "|java+constructor:///org/apache/spark/storage/ExternalBlockStore$$anonfun$4/ExternalBlockStore$$anonfun$4(org.apache.spark.storage.ExternalBlockStore)|",
      "|java+method:///scala/Option/get()|",
      "|java+constructor:///org/apache/spark/storage/ExternalBlockStore$$anonfun$createBlkManager$1/ExternalBlockStore$$anonfun$createBlkManager$1(org.apache.spark.storage.ExternalBlockStore)|",
      "|java+constructor:///org/apache/spark/storage/ExternalBlockStore$$anonfun$createBlkManager$2/ExternalBlockStore$$anonfun$createBlkManager$2(org.apache.spark.storage.ExternalBlockStore)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/classForName(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/BlockManager/conf()|",
      "|java+method:///org/apache/spark/storage/ExternalBlockStore$/BLOCK_MANAGER_NAME()|",
      "|java+method:///org/apache/spark/storage/ExternalBlockManager/init(org.apache.spark.storage.BlockManager,java.lang.String)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/SparkSubmit$/org$apache$spark$deploy$SparkSubmit$$runMain(scala.collection.Seq,scala.collection.Seq,scala.collection.mutable.Map,java.lang.String,boolean)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/collection/Seq/toArray(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/printWarning(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/printStream()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///java/lang/Thread/currentThread()|",
      "|java+constructor:///org/apache/spark/util/ChildFirstURLClassLoader/ChildFirstURLClassLoader(java.net.URL%5B%5D,java.lang.ClassLoader)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$4/SparkSubmit$$anonfun$4()|",
      "|java+constructor:///org/apache/spark/util/MutableURLClassLoader/MutableURLClassLoader(java.net.URL%5B%5D,java.lang.ClassLoader)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Map/withFilter(scala.Function1)|",
      "|java+method:///java/lang/Thread/getContextClassLoader()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/CLASS_NOT_FOUND_EXIT_STATUS()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$org$apache$spark$deploy$SparkSubmit$$runMain$1/SparkSubmit$$anonfun$org$apache$spark$deploy$SparkSubmit$$runMain$1(org.apache.spark.util.MutableURLClassLoader)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///java/lang/Class/isAssignableFrom(java.lang.Class)|",
      "|java+constructor:///java/lang/IllegalStateException/IllegalStateException(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/findCause$1(java.lang.Throwable)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///java/lang/ClassNotFoundException/printStackTrace(java.io.PrintStream)|",
      "|java+method:///scala/collection/immutable/StringOps/toBoolean()|",
      "|java+method:///java/io/PrintStream/println(java.lang.String)|",
      "|java+method:///java/lang/Thread/setContextClassLoader(java.lang.ClassLoader)|",
      "|java+method:///org/apache/spark/SparkUserAppException/exitCode()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///java/lang/Class/getMethod(java.lang.String,java.lang.Class%5B%5D)|",
      "|java+method:///scala/collection/mutable/Map/mkString(java.lang.String)|",
      "|java+method:///java/lang/reflect/Method/getModifiers()|",
      "|java+method:///org/apache/spark/util/Utils$/classForName(java.lang.String)|",
      "|java+method:///java/lang/System/exit(int)|",
      "|java+method:///scala/collection/mutable/Map/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///java/lang/reflect/Modifier/isStatic(int)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$org$apache$spark$deploy$SparkSubmit$$runMain$2/SparkSubmit$$anonfun$org$apache$spark$deploy$SparkSubmit$$runMain$2()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///java/lang/String/contains(java.lang.CharSequence)|",
      "|java+method:///java/lang/reflect/Method/invoke(java.lang.Object,java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$org$apache$spark$deploy$SparkSubmit$$runMain$3/SparkSubmit$$anonfun$org$apache$spark$deploy$SparkSubmit$$runMain$3()|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Seq/toArray(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/printWarning(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/printStream()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///java/lang/Thread/currentThread()|",
      "|java+constructor:///org/apache/spark/util/ChildFirstURLClassLoader/ChildFirstURLClassLoader(java.net.URL%5B%5D,java.lang.ClassLoader)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$4/SparkSubmit$$anonfun$4()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Map/withFilter(scala.Function1)|",
      "|java+method:///java/lang/NoClassDefFoundError/printStackTrace(java.io.PrintStream)|",
      "|java+method:///java/lang/Thread/getContextClassLoader()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/CLASS_NOT_FOUND_EXIT_STATUS()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$org$apache$spark$deploy$SparkSubmit$$runMain$1/SparkSubmit$$anonfun$org$apache$spark$deploy$SparkSubmit$$runMain$1(org.apache.spark.util.MutableURLClassLoader)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///java/lang/Class/isAssignableFrom(java.lang.Class)|",
      "|java+constructor:///java/lang/IllegalStateException/IllegalStateException(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/findCause$1(java.lang.Throwable)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///java/lang/ClassNotFoundException/printStackTrace(java.io.PrintStream)|",
      "|java+method:///scala/collection/immutable/StringOps/toBoolean()|",
      "|java+method:///java/io/PrintStream/println(java.lang.String)|",
      "|java+method:///java/lang/Thread/setContextClassLoader(java.lang.ClassLoader)|",
      "|java+method:///org/apache/spark/SparkUserAppException/exitCode()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///java/lang/Class/getMethod(java.lang.String,java.lang.Class%5B%5D)|",
      "|java+method:///scala/collection/mutable/Map/mkString(java.lang.String)|",
      "|java+method:///java/lang/reflect/Method/getModifiers()|",
      "|java+method:///org/apache/spark/util/Utils$/classForName(java.lang.String)|",
      "|java+method:///java/lang/System/exit(int)|",
      "|java+method:///scala/collection/mutable/Map/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///java/lang/reflect/Modifier/isStatic(int)|",
      "|java+method:///java/lang/NoClassDefFoundError/getMessage()|",
      "|java+constructor:///org/apache/spark/util/MutableURLClassLoader/MutableURLClassLoader(java.net.URL%5B%5D,java.lang.ClassLoader)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$org$apache$spark$deploy$SparkSubmit$$runMain$2/SparkSubmit$$anonfun$org$apache$spark$deploy$SparkSubmit$$runMain$2()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///java/lang/String/contains(java.lang.CharSequence)|",
      "|java+method:///java/lang/reflect/Method/invoke(java.lang.Object,java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/SparkSubmit$$anonfun$org$apache$spark$deploy$SparkSubmit$$runMain$3/SparkSubmit$$anonfun$org$apache$spark$deploy$SparkSubmit$$runMain$3()|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/PythonUtils$/generateRDDWithNull(org.apache.spark.api.java.JavaSparkContext)|",
    "called": "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext/parallelize(java.util.List)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext$/toSparkContext(org.apache.spark.api.java.JavaSparkContext)|",
      "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/api/java/JavaRDD$/fromRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkContext/parallelize$default$2()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/r/RBackendHandler$$anonfun$handleMethodCall$3/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/api/r/RBackendHandler$$anonfun$handleMethodCall$3/apply()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/api/r/RBackendHandler$$anonfun$handleMethodCall$3/apply()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1/apply(org.apache.spark.scheduler.Stage)|",
    "called": "|java+method:///scala/Option/withFilter(scala.Function1)|",
    "v1Body": [
      "|java+method:///scala/Option/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$6/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$6(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1,org.apache.spark.scheduler.Stage)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/shuffleToMapStage()|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3/org$apache$spark$scheduler$DAGScheduler$$anonfun$$$outer()|",
      "|java+method:///scala/Option$WithFilter/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/waitingStages()|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/find(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/runningStages()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/failedStages()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$5/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$5(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$9/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$9(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$10/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$10(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$7/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$7(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$8/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$8(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1)|"
    ],
    "v2Body": [
      "|java+method:///scala/Option/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$5/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$5(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1,org.apache.spark.scheduler.Stage)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/shuffleToMapStage()|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3/org$apache$spark$scheduler$DAGScheduler$$anonfun$$$outer()|",
      "|java+method:///scala/Option$WithFilter/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/waitingStages()|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/find(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/runningStages()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/failedStages()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$9/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$9(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$6/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$6(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$7/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$7(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$4/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$4(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$8/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$8(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/collection/ExternalSorter/spill(org.apache.spark.util.collection.WritablePartitionedPairCollection)|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/collection/WritablePartitionedIterator/writeNext(org.apache.spark.storage.DiskBlockObjectWriter)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/diskBlockManager()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/util/collection/WritablePartitionedIterator/nextPartition()|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/spills()|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///org/apache/spark/util/collection/WritablePartitionedPairCollection/destructiveSortedWritablePartitionedIterator(scala.Option)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///org/apache/spark/storage/DiskBlockManager/createTempShuffleBlock()|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/flush$1(scala.runtime.LongRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/comparator()|",
      "|java+method:///org/apache/spark/util/collection/WritablePartitionedIterator/hasNext()|",
      "|java+method:///org/apache/spark/storage/DiskBlockObjectWriter/revertPartialWritesAndClose()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/append(scala.collection.Seq)|",
      "|java+constructor:///scala/runtime/LongRef/LongRef(long)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/toArray(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/openWriter$1(org.apache.spark.storage.TempShuffleBlockId,java.io.File,scala.runtime.ObjectRef,scala.runtime.ObjectRef)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/org$apache$spark$util$collection$ExternalSorter$$serializerBatchSize()|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalSorter$SpilledFile/ExternalSorter$SpilledFile(org.apache.spark.util.collection.ExternalSorter,java.io.File,org.apache.spark.storage.BlockId,long%5B%5D,long%5B%5D)|",
      "|java+method:///scala/reflect/ClassTag$/Long()|",
      "|java+method:///java/io/File/delete()|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/org$apache$spark$util$collection$ExternalSorter$$numPartitions()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/collection/WritablePartitionedIterator/writeNext(org.apache.spark.storage.DiskBlockObjectWriter)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/diskBlockManager()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/util/collection/WritablePartitionedIterator/nextPartition()|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/spills()|",
      "|java+method:///org/apache/spark/util/collection/WritablePartitionedPairCollection/destructiveSortedWritablePartitionedIterator(scala.Option)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///org/apache/spark/storage/DiskBlockManager/createTempShuffleBlock()|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/flush$1(scala.runtime.LongRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/comparator()|",
      "|java+method:///org/apache/spark/util/collection/WritablePartitionedIterator/hasNext()|",
      "|java+method:///org/apache/spark/storage/DiskBlockObjectWriter/revertPartialWritesAndClose()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/append(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalSorter$$anonfun$spill$1/ExternalSorter$$anonfun$spill$1(org.apache.spark.util.collection.ExternalSorter,int)|",
      "|java+constructor:///scala/runtime/LongRef/LongRef(long)|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalSorter$$anonfun$spill$2/ExternalSorter$$anonfun$spill$2(org.apache.spark.util.collection.ExternalSorter,java.io.File)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/toArray(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/openWriter$1(org.apache.spark.storage.TempShuffleBlockId,java.io.File,scala.runtime.ObjectRef,scala.runtime.ObjectRef)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/org$apache$spark$util$collection$ExternalSorter$$serializerBatchSize()|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalSorter$SpilledFile/ExternalSorter$SpilledFile(org.apache.spark.util.collection.ExternalSorter,java.io.File,org.apache.spark.storage.BlockId,long%5B%5D,long%5B%5D)|",
      "|java+method:///scala/reflect/ClassTag$/Long()|",
      "|java+method:///java/io/File/delete()|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/org$apache$spark$util$collection$ExternalSorter$$numPartitions()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$removeFromQueuedDrivers(java.lang.String)|",
    "called": "|java+method:///scala/collection/mutable/ArrayBuffer/remove(int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/queuedDriversState()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$16/MesosClusterScheduler$$anonfun$16(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/remove(int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$queuedDrivers()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/indexWhere(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterPersistenceEngine/expunge(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/queuedDriversState()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$17/MesosClusterScheduler$$anonfun$17(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/remove(int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$queuedDrivers()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/indexWhere(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterPersistenceEngine/expunge(java.lang.String)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/TimeStampedHashMap/filter(scala.Function1)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/concurrent/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/internalMap()|",
      "|java+method:///scala/collection/TraversableLike/filter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/TimeStampedHashMap$$anonfun$filter$1/TimeStampedHashMap$$anonfun$filter$1(org.apache.spark.util.TimeStampedHashMap)|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/filter(scala.Function1)|",
      "|java+method:///scala/collection/mutable/Map$/canBuildFrom()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaConcurrentMap(java.util.concurrent.ConcurrentMap)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/internalMap()|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaConcurrentMapConverter(java.util.concurrent.ConcurrentMap)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/TraversableLike/filter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/TimeStampedHashMap$$anonfun$filter$1/TimeStampedHashMap$$anonfun$filter$1(org.apache.spark.util.TimeStampedHashMap)|",
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/filter(scala.Function1)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/Map$/canBuildFrom()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkConf$$anonfun$7/apply(java.lang.String)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$7$$anonfun$apply$11/SparkConf$$anonfun$7$$anonfun$apply$11(org.apache.spark.SparkConf$$anonfun$7,java.lang.String)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/immutable/Map/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkConf$/org$apache$spark$SparkConf$$configsWithAlternatives()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/immutable/StringOps/toDouble()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$35/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$35/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$35/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$liftedTree2$1$2/apply()|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$liftedTree2$1$2/apply()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$liftedTree2$1$2/apply()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/TestUtils$/createCompiledClass(java.lang.String,java.io.File,java.lang.String,java.lang.String,scala.collection.Seq)|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/TestUtils$JavaSourceFromString/TestUtils$JavaSourceFromString(java.lang.String,java.lang.String)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$7/TestUtils$$anonfun$7()|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$6/TestUtils$$anonfun$6()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/TestUtils$/createCompiledClass(java.lang.String,java.io.File,org.apache.spark.TestUtils$JavaSourceFromString,scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/TestUtils$JavaSourceFromString/TestUtils$JavaSourceFromString(java.lang.String,java.lang.String)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$8/TestUtils$$anonfun$8()|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$7/TestUtils$$anonfun$7()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/TestUtils$/createCompiledClass(java.lang.String,java.io.File,org.apache.spark.TestUtils$JavaSourceFromString,scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$41/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ListBuffer/find(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/accumulables()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$41$$anonfun$apply$1/StagePage$$anonfun$41$$anonfun$apply$1(org.apache.spark.ui.jobs.StagePage$$anonfun$41)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$41$$anonfun$apply$12/StagePage$$anonfun$41$$anonfun$apply$12(org.apache.spark.ui.jobs.StagePage$$anonfun$41)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$41$$anonfun$apply$13/StagePage$$anonfun$41$$anonfun$apply$13(org.apache.spark.ui.jobs.StagePage$$anonfun$41)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskInfo()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/executorRunTime()|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/ProcessBuilderLike$$anon$3/command()|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///java/lang/ProcessBuilder/command()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///java/lang/ProcessBuilder/command()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/executorRemoved(java.lang.String,java.lang.String,scala.Option)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/SlaveLost/SlaveLost(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$executorRemoved$1/SparkDeploySchedulerBackend$$anonfun$executorRemoved$1(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend,java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/ExecutorExited/ExecutorExited(int)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/ExecutorLossReason/toString()|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/removeExecutor(java.lang.String,java.lang.String)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/SlaveLost/SlaveLost(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/removeExecutor(java.lang.String,org.apache.spark.scheduler.ExecutorLossReason)|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/ExecutorExited/ExecutorExited(int,boolean,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$executorRemoved$1/SparkDeploySchedulerBackend$$anonfun$executorRemoved$1(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend,java.lang.String,java.lang.String)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/handleExtraArgs(java.util.List)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/childArgs()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$plus$eq(scala.collection.TraversableOnce)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/childArgs()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$plus$eq(scala.collection.TraversableOnce)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2/apply(scala.Tuple2)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///org/apache/spark/MapOutputTracker$/logError(scala.Function0)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/scheduler/MapStatus/location()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2$$anonfun$apply$1/MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2$$anonfun$apply$1(org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2,java.lang.String)|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/storage/ShuffleBlockId/ShuffleBlockId(int,int,int)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElseUpdate(java.lang.Object,scala.Function0)|",
      "|java+constructor:///org/apache/spark/MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2$$anonfun$apply$2/MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2$$anonfun$apply$2(org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/shuffle/MetadataFetchFailedException/MetadataFetchFailedException(int,int,java.lang.String)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/MapStatus/getSizeForBlock(int)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2$$anonfun$apply$3/MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2$$anonfun$apply$3(org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2,org.apache.spark.scheduler.MapStatus,int)|",
      "|java+method:///org/apache/spark/MapOutputTracker$/logError(scala.Function0)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2$$anonfun$apply$2/MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2$$anonfun$apply$2(org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2,java.lang.String)|",
      "|java+method:///scala/collection/immutable/Range/foreach(scala.Function1)|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/shuffle/MetadataFetchFailedException/MetadataFetchFailedException(int,int,java.lang.String)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/ZooKeeperPersistenceEngine$$anonfun$read$1/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/master/ZooKeeperPersistenceEngine$$anonfun$read$1/apply(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/master/ZooKeeperPersistenceEngine$$anonfun$read$1/apply(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$5/RestSubmissionClient$$anonfun$5()|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$2/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$2/apply(scala.Tuple2)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$2/apply(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/metrics/MetricsConfig$$anonfun$subProperties$1/apply(scala.Tuple2)|",
    "called": "|java+method:///scala/util/matching/Regex/findPrefixOf(java.lang.CharSequence)|",
    "v1Body": [
      "|java+method:///java/util/Properties/setProperty(java.lang.String,java.lang.String)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$subProperties$1$$anonfun$apply$4/MetricsConfig$$anonfun$subProperties$1$$anonfun$apply$4(org.apache.spark.metrics.MetricsConfig$$anonfun$subProperties$1)|",
      "|java+method:///scala/util/matching/Regex/findPrefixOf(java.lang.CharSequence)|",
      "|java+method:///scala/collection/LinearSeqOptimized/apply(int)|",
      "|java+method:///scala/util/matching/Regex/unapplySeq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElseUpdate(java.lang.Object,scala.Function0)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/LinearSeqOptimized/lengthCompare(int)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///java/util/Properties/setProperty(java.lang.String,java.lang.String)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///java/lang/String/toString()|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$subProperties$1$$anonfun$apply$4/MetricsConfig$$anonfun$subProperties$1$$anonfun$apply$4(org.apache.spark.metrics.MetricsConfig$$anonfun$subProperties$1)|",
      "|java+method:///scala/util/matching/Regex/findPrefixOf(java.lang.CharSequence)|",
      "|java+method:///scala/collection/LinearSeqOptimized/apply(int)|",
      "|java+method:///scala/util/matching/Regex/unapplySeq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElseUpdate(java.lang.Object,scala.Function0)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/LinearSeqOptimized/lengthCompare(int)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/TestUtils$/createCompiledClass(java.lang.String,java.io.File,org.apache.spark.TestUtils$JavaSourceFromString,scala.collection.Seq)|",
    "called": "|java+method:///scala/collection/Seq$/canBuildFrom()|",
    "v1Body": [
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///javax/tools/ToolProvider/getSystemJavaCompiler()|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$5/TestUtils$$anonfun$5()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///javax/tools/JavaCompiler/getTask(java.io.Writer,javax.tools.JavaFileManager,javax.tools.DiagnosticListener,java.lang.Iterable,java.lang.Iterable,java.lang.Iterable)|",
      "|java+method:///org/spark-project/guava/io/Files/move(java.io.File,java.io.File)|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///javax/tools/JavaCompiler$CompilationTask/call()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/Predef$/assert(boolean,scala.Function0)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$createCompiledClass$1/TestUtils$$anonfun$createCompiledClass$1(java.io.File)|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$createCompiledClass$2/TestUtils$$anonfun$createCompiledClass$2(java.io.File)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///javax/tools/ToolProvider/getSystemJavaCompiler()|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$6/TestUtils$$anonfun$6()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///java/util/Arrays/asList(java.lang.Object%5B%5D)|",
      "|java+method:///javax/tools/JavaCompiler/getTask(java.io.Writer,javax.tools.JavaFileManager,javax.tools.DiagnosticListener,java.lang.Iterable,java.lang.Iterable,java.lang.Iterable)|",
      "|java+method:///org/spark-project/guava/io/Files/move(java.io.File,java.io.File)|",
      "|java+method:///scala/collection/Seq/nonEmpty()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///javax/tools/JavaCompiler$CompilationTask/call()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/Predef$/assert(boolean,scala.Function0)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$createCompiledClass$1/TestUtils$$anonfun$createCompiledClass$1(java.io.File)|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$createCompiledClass$2/TestUtils$$anonfun$createCompiledClass$2(java.io.File)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$72/apply(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$/taskMetricsFromJson(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient/org$apache$spark$deploy$rest$RestSubmissionClient$$validateMaster(java.lang.String)|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient/supportedMasterPrefixes()|",
      "|java+method:///scala/collection/Seq/exists(scala.Function1)|",
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$1/RestSubmissionClient$$anonfun$1(org.apache.spark.deploy.rest.RestSubmissionClient,java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient/supportedMasterPrefixes()|",
      "|java+method:///scala/collection/Seq/exists(scala.Function1)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$2/RestSubmissionClient$$anonfun$2(org.apache.spark.deploy.rest.RestSubmissionClient,java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$tryToPut$2/apply()|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/freeMemory()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/bytesToString(long)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$tryToPut$2/apply()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$blocksMemoryUsed()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/bytesToString(long)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$tryToPut$2/apply()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$58/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/TaskInfo/launchTime()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskInfo()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/diskBytesSpilled()|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$taskEndReasonFromJson$1/apply()|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$taskEndReasonFromJson$1/apply()|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$taskEndReasonFromJson$1/apply$mcZ$sp()|",
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$taskEndReasonFromJson$1/apply()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaPairRDD/sampleByKey(boolean,java.util.Map,long)|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/kClassTag()|",
      "|java+constructor:///org/apache/spark/api/java/JavaPairRDD/JavaPairRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/sampleByKey(boolean,scala.collection.Map,long)|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/rdd()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/vClassTag()|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions$default$4(org.apache.spark.rdd.RDD)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/kClassTag()|",
      "|java+constructor:///org/apache/spark/api/java/JavaPairRDD/JavaPairRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/sampleByKey(boolean,scala.collection.Map,long)|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/rdd()|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/vClassTag()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions$default$4(org.apache.spark.rdd.RDD)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$100/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$100/apply(org.apache.spark.executor.ShuffleReadMetrics)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$100/apply(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$resourceOffers$3/apply(org.apache.spark.scheduler.TaskSetManager)|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$7/TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$7(org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3,org.apache.spark.scheduler.TaskSetManager)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/myLocalityLevels()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$8/TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$8(org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3,org.apache.spark.scheduler.TaskSetManager)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/myLocalityLevels()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/addOutputLoc(int,org.apache.spark.scheduler.MapStatus)|",
    "called": "|java+method:///scala/collection/immutable/List/$colon$colon(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/outputLocs()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/numAvailableOutputs_$eq(long)|",
      "|java+method:///scala/collection/immutable/List/$colon$colon(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/numAvailableOutputs()|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/List/$colon$colon(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$46/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$46$$anonfun$apply$18/StagePage$$anonfun$46$$anonfun$apply$18(org.apache.spark.ui.jobs.StagePage$$anonfun$46)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$46$$anonfun$apply$5/StagePage$$anonfun$46$$anonfun$apply$5(org.apache.spark.ui.jobs.StagePage$$anonfun$46)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/outputMetrics()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$/getSchedulerDelay(org.apache.spark.scheduler.TaskInfo,org.apache.spark.executor.TaskMetrics,long)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskInfo()|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/ExternalShuffleService/start()|",
    "called": "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/network/sasl/SaslServerBootstrap/SaslServerBootstrap(org.apache.spark.network.util.TransportConf,org.apache.spark.network.sasl.SecretKeyHolder)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/server_$eq(org.apache.spark.network.server.TransportServer)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/server()|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/transportContext()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/org$apache$spark$deploy$ExternalShuffleService$$port()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/org$apache$spark$deploy$ExternalShuffleService$$useSasl()|",
      "|java+constructor:///org/apache/spark/deploy/ExternalShuffleService$$anonfun$start$1/ExternalShuffleService$$anonfun$start$1(org.apache.spark.deploy.ExternalShuffleService)|",
      "|java+constructor:///org/apache/spark/deploy/ExternalShuffleService$$anonfun$start$2/ExternalShuffleService$$anonfun$start$2(org.apache.spark.deploy.ExternalShuffleService)|",
      "|java+method:///org/apache/spark/network/TransportContext/createServer(int,java.util.List)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/transportConf()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/network/sasl/SaslServerBootstrap/SaslServerBootstrap(org.apache.spark.network.util.TransportConf,org.apache.spark.network.sasl.SecretKeyHolder)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/server_$eq(org.apache.spark.network.server.TransportServer)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/server()|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/transportContext()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/org$apache$spark$deploy$ExternalShuffleService$$port()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/org$apache$spark$deploy$ExternalShuffleService$$useSasl()|",
      "|java+constructor:///org/apache/spark/deploy/ExternalShuffleService$$anonfun$start$1/ExternalShuffleService$$anonfun$start$1(org.apache.spark.deploy.ExternalShuffleService)|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/deploy/ExternalShuffleService$$anonfun$start$2/ExternalShuffleService$$anonfun$start$2(org.apache.spark.deploy.ExternalShuffleService)|",
      "|java+method:///org/apache/spark/network/TransportContext/createServer(int,java.util.List)|",
      "|java+method:///org/apache/spark/deploy/ExternalShuffleService/transportConf()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$/jobEndFromJson(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$4/JsonProtocol$$anonfun$4()|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/jobResultFromJson(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerJobEnd/SparkListenerJobEnd(int,long,org.apache.spark.scheduler.JobResult)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$53/JsonProtocol$$anonfun$53()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$4/JsonProtocol$$anonfun$4()|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/jobResultFromJson(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerJobEnd/SparkListenerJobEnd(int,long,org.apache.spark.scheduler.JobResult)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$56/JsonProtocol$$anonfun$56()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$79/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$79/apply(org.apache.spark.executor.TaskMetrics)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$79/apply(org.apache.spark.scheduler.AccumulableInfo)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1/applyOrElse(java.lang.Object,scala.Function1)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RemoveExecutor/reason()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisterExecutor/logUrls()|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/reply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1,java.lang.String,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/totalRegisteredExecutors()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/ExecutorData/ExecutorData(org.apache.spark.rpc.RpcEndpointRef,org.apache.spark.rpc.RpcAddress,java.lang.String,int,int,scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/stop()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$$outer()|",
      "|java+method:///java/util/concurrent/atomic/AtomicInteger/addAndGet(int)|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/totalCoreCount()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisterExecutor/hostPort()|",
      "|java+method:///org/apache/spark/util/Utils$/checkHostPort(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerExecutorAdded/SparkListenerExecutorAdded(long,java.lang.String,org.apache.spark.scheduler.cluster.ExecutorInfo)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/removeExecutor(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$executorDataMap()|",
      "|java+method:///scala/collection/mutable/HashMap/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RemoveExecutor/executorId()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisterExecutor/executorId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$addressToExecutorId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/address()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$numPendingExecutors_$eq(int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$makeOffers()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisterExecutor/cores()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisterExecutorFailed/CoarseGrainedClusterMessages$RegisterExecutorFailed(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$listenerBus()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$7/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$7(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$6/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$6(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$5/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$5(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisterExecutor/executorRef()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$numPendingExecutors()|",
      "|java+method:///org/apache/spark/util/Utils$/parseHostPort(java.lang.String)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RemoveExecutor/reason()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisterExecutor/logUrls()|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/reply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/removeExecutor(java.lang.String,org.apache.spark.scheduler.ExecutorLossReason)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerExecutorAdded/SparkListenerExecutorAdded(long,java.lang.String,org.apache.spark.scheduler.cluster.ExecutorInfo)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisterExecutor/executorId()|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/totalRegisteredExecutors()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$listenerBus()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1,java.lang.String,org.apache.spark.rpc.RpcEndpointRef,org.apache.spark.rpc.RpcAddress)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/ExecutorData/ExecutorData(org.apache.spark.rpc.RpcEndpointRef,org.apache.spark.rpc.RpcAddress,java.lang.String,int,int,scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/stop()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$$outer()|",
      "|java+method:///java/util/concurrent/atomic/AtomicInteger/addAndGet(int)|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/totalCoreCount()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$executorDataMap()|",
      "|java+method:///scala/collection/mutable/HashMap/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RemoveExecutor/executorId()|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/senderAddress()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/address()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$numPendingExecutors_$eq(int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$makeOffers()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisterExecutor/cores()|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/host()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisterExecutorFailed/CoarseGrainedClusterMessages$RegisterExecutorFailed(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$7/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$7(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$6/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$6(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/addressToExecutorId()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisteredExecutor/CoarseGrainedClusterMessages$RegisteredExecutor(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$5/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$5(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisterExecutor/executorRef()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$numPendingExecutors()|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1/apply$mcVI$sp(int)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/TaskScheduler/cancelTasks(int,boolean)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/stageIdToStage()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$markStageAsFinished(org.apache.spark.scheduler.Stage,scala.Option)|",
      "|java+method:///scala/collection/mutable/HashSet/size()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logInfo(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$apply$mcVI$sp$2/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$apply$mcVI$sp$2(org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1,int)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$22/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$22(org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/jobId()|",
      "|java+method:///scala/Option/get()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$apply$mcVI$sp$4/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$apply$mcVI$sp$4(org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1,int)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/runningStages()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/taskScheduler()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$apply$mcVI$sp$3/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$apply$mcVI$sp$3(org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1,int)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logError(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/TaskScheduler/cancelTasks(int,boolean)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/stageIdToStage()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$markStageAsFinished(org.apache.spark.scheduler.Stage,scala.Option)|",
      "|java+method:///scala/collection/mutable/HashSet/size()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logInfo(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/runningStages()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$apply$mcVI$sp$2/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$apply$mcVI$sp$2(org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1,int)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$20/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$20(org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/jobId()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/taskScheduler()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$apply$mcVI$sp$3/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$apply$mcVI$sp$3(org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1,int)|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$apply$mcVI$sp$1/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1$$anonfun$apply$mcVI$sp$1(org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1,int)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logError(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$74/TaskDataSource$$anonfun$74(org.apache.spark.ui.jobs.TaskDataSource)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/Utils$/portMaxRetries(org.apache.spark.SparkConf)|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/contains(java.lang.String)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$portMaxRetries$1/Utils$$anonfun$portMaxRetries$1()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$portMaxRetries$2/Utils$$anonfun$portMaxRetries$2()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$14/Utils$$anonfun$14()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/contains(java.lang.String)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$portMaxRetries$1/Utils$$anonfun$portMaxRetries$1()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$portMaxRetries$2/Utils$$anonfun$portMaxRetries$2()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$13/Utils$$anonfun$13()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$treeReduce$1/apply()|",
    "called": "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD/context()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeReduce$1$$anonfun$16/RDD$$anonfun$treeReduce$1$$anonfun$16(org.apache.spark.rdd.RDD$$anonfun$treeReduce$1,scala.Function2)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeReduce$1$$anonfun$17/RDD$$anonfun$treeReduce$1$$anonfun$17(org.apache.spark.rdd.RDD$$anonfun$treeReduce$1,scala.Function1)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeReduce$1$$anonfun$18/RDD$$anonfun$treeReduce$1$$anonfun$18(org.apache.spark.rdd.RDD$$anonfun$treeReduce$1,scala.Function2)|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeReduce$1$$anonfun$apply$37/RDD$$anonfun$treeReduce$1$$anonfun$apply$37(org.apache.spark.rdd.RDD$$anonfun$treeReduce$1)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeReduce$1$$anonfun$apply$38/RDD$$anonfun$treeReduce$1$$anonfun$apply$38(org.apache.spark.rdd.RDD$$anonfun$treeReduce$1)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Option$/empty()|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions$default$2()|",
      "|java+method:///org/apache/spark/rdd/RDD/treeAggregate(java.lang.Object,scala.Function2,scala.Function2,int,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD/context()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeReduce$1$$anonfun$16/RDD$$anonfun$treeReduce$1$$anonfun$16(org.apache.spark.rdd.RDD$$anonfun$treeReduce$1,scala.Function2)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeReduce$1$$anonfun$apply$41/RDD$$anonfun$treeReduce$1$$anonfun$apply$41(org.apache.spark.rdd.RDD$$anonfun$treeReduce$1)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeReduce$1$$anonfun$17/RDD$$anonfun$treeReduce$1$$anonfun$17(org.apache.spark.rdd.RDD$$anonfun$treeReduce$1,scala.Function1)|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Option$/empty()|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitions$default$2()|",
      "|java+method:///org/apache/spark/rdd/RDD/treeAggregate(java.lang.Object,scala.Function2,scala.Function2,int,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeReduce$1$$anonfun$apply$42/RDD$$anonfun$treeReduce$1$$anonfun$apply$42(org.apache.spark.rdd.RDD$$anonfun$treeReduce$1)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$treeReduce$1$$anonfun$18/RDD$$anonfun$treeReduce$1$$anonfun$18(org.apache.spark.rdd.RDD$$anonfun$treeReduce$1,scala.Function2)|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/executor/Executor$TaskRunner/run()|",
    "called": "|java+method:///scala/Option/orNull(scala.Predef$$less$colon$less)|",
    "v1Body": [
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$maxResultSize()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/reflect/ClassTag$/Any()|",
      "|java+method:///org/apache/spark/scheduler/Task/run(long,int,org.apache.spark.metrics.MetricsSystem)|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$4/Executor$TaskRunner$$anonfun$4(org.apache.spark.executor.Executor$TaskRunner,int)|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$5/Executor$TaskRunner$$anonfun$5(org.apache.spark.executor.Executor$TaskRunner,int)|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$6/Executor$TaskRunner$$anonfun$6(org.apache.spark.executor.Executor$TaskRunner,int)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$computeTotalGcTime()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/shuffle/FetchFailedException/toTaskEndReason()|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/SparkEnv/serializer()|",
      "|java+method:///org/apache/spark/executor/Executor/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/IndirectTaskResult/IndirectTaskResult(org.apache.spark.storage.BlockId,int)|",
      "|java+constructor:///org/apache/spark/scheduler/DirectTaskResult/DirectTaskResult(java.nio.ByteBuffer,scala.collection.Map,org.apache.spark.executor.TaskMetrics)|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$run$6/Executor$TaskRunner$$anonfun$run$6(org.apache.spark.executor.Executor$TaskRunner)|",
      "|java+constructor:///org/apache/spark/TaskKilledException/TaskKilledException()|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$run$5/Executor$TaskRunner$$anonfun$run$5(org.apache.spark.executor.Executor$TaskRunner)|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$run$3/Executor$TaskRunner$$anonfun$run$3(org.apache.spark.executor.Executor$TaskRunner)|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$run$2/Executor$TaskRunner$$anonfun$run$2(org.apache.spark.executor.Executor$TaskRunner)|",
      "|java+method:///org/apache/spark/scheduler/Task/killed()|",
      "|java+method:///org/apache/spark/util/AkkaUtils$/reservedSizeBytes()|",
      "|java+method:///org/apache/spark/TaskState$/FAILED()|",
      "|java+method:///org/apache/spark/TaskState$/KILLED()|",
      "|java+constructor:///org/apache/spark/unsafe/memory/TaskMemoryManager/TaskMemoryManager(org.apache.spark.unsafe.memory.ExecutorMemoryManager)|",
      "|java+constructor:///org/apache/spark/storage/TaskResultBlockId/TaskResultBlockId(long)|",
      "|java+constructor:///scala/runtime/LongRef/LongRef(long)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$replClassLoader()|",
      "|java+method:///org/apache/spark/serializer/Serializer/newInstance()|",
      "|java+method:///java/lang/Thread/setContextClassLoader(java.lang.ClassLoader)|",
      "|java+method:///org/apache/spark/executor/Executor$TaskRunner/org$apache$spark$executor$Executor$TaskRunner$$$outer()|",
      "|java+method:///scala/Option/flatMap(scala.Function1)|",
      "|java+method:///scala/Tuple3/_1()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple3/_2()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/ExceptionFailure/ExceptionFailure(java.lang.Throwable,scala.Option)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/Tuple3/_3()|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/serialize(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkEnv/mapOutputTracker()|",
      "|java+method:///java/lang/Thread/currentThread()|",
      "|java+method:///org/apache/spark/TaskState$/RUNNING()|",
      "|java+method:///org/apache/spark/executor/Executor/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/StorageLevel$/MEMORY_AND_DISK_SER()|",
      "|java+method:///org/apache/spark/executor/Executor/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$conf()|",
      "|java+constructor:///org/apache/spark/ExceptionFailure/ExceptionFailure(java.lang.Throwable,scala.Option,boolean)|",
      "|java+method:///org/apache/spark/unsafe/memory/TaskMemoryManager/cleanUpAllAllocatedMemory()|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$EMPTY_BYTE_BUFFER()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/Option/orNull(scala.Predef$$less$colon$less)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|",
      "|java+method:///org/apache/spark/scheduler/Task/metrics()|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$run$4/Executor$TaskRunner$$anonfun$run$4(org.apache.spark.executor.Executor$TaskRunner,long,scala.runtime.LongRef,long,long,long)|",
      "|java+method:///org/apache/spark/scheduler/Task/setTaskMemoryManager(org.apache.spark.unsafe.memory.TaskMemoryManager)|",
      "|java+method:///org/apache/spark/SparkEnv/closureSerializer()|",
      "|java+method:///org/apache/spark/executor/Executor/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/MapOutputTracker/updateEpoch(long)|",
      "|java+method:///org/apache/spark/executor/Executor$TaskRunner/killed()|",
      "|java+method:///org/apache/spark/executor/Executor$TaskRunner/attemptNumber()|",
      "|java+method:///org/apache/spark/executor/CommitDeniedException/toTaskEndReason()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManager/putBytes$default$4()|",
      "|java+method:///org/apache/spark/executor/Executor$TaskRunner/taskId()|",
      "|java+method:///org/apache/spark/SparkEnv/metricsSystem()|",
      "|java+method:///org/apache/spark/scheduler/Task/epoch()|",
      "|java+method:///org/apache/spark/storage/BlockManager/putBytes(org.apache.spark.storage.BlockId,java.nio.ByteBuffer,org.apache.spark.storage.StorageLevel,boolean,scala.Option)|",
      "|java+method:///org/apache/spark/storage/BlockManager/putBytes$default$5()|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$7/Executor$TaskRunner$$anonfun$7(org.apache.spark.executor.Executor$TaskRunner,scala.runtime.LongRef)|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/deserialize(java.nio.ByteBuffer,java.lang.ClassLoader,scala.reflect.ClassTag)|",
      "|java+method:///java/nio/ByteBuffer/limit()|",
      "|java+method:///org/apache/spark/util/Utils$/isFatalError(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/util/SparkUncaughtExceptionHandler$/uncaughtException(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/executor/ExecutorBackend/statusUpdate(long,scala.Enumeration$Value,java.nio.ByteBuffer)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$akkaFrameSize()|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$runningTasks()|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///org/apache/spark/executor/Executor/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/executor/Executor$TaskRunner/task_$eq(org.apache.spark.scheduler.Task)|",
      "|java+method:///java/lang/Thread/getContextClassLoader()|",
      "|java+method:///org/apache/spark/TaskState$/FINISHED()|",
      "|java+method:///org/apache/spark/scheduler/Task$/deserializeWithDependencies(java.nio.ByteBuffer)|",
      "|java+method:///org/apache/spark/executor/Executor$TaskRunner/task()|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/remove(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$updateDependencies(scala.collection.mutable.HashMap,scala.collection.mutable.HashMap)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///org/apache/spark/SparkEnv/executorMemoryManager()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$3/Executor$TaskRunner$$anonfun$3(org.apache.spark.executor.Executor$TaskRunner,java.lang.String)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/Executor$TaskRunner/startGCTime_$eq(long)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$maxResultSize()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/reflect/ClassTag$/Any()|",
      "|java+method:///org/apache/spark/scheduler/Task/run(long,int,org.apache.spark.metrics.MetricsSystem)|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$4/Executor$TaskRunner$$anonfun$4(org.apache.spark.executor.Executor$TaskRunner,int)|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$5/Executor$TaskRunner$$anonfun$5(org.apache.spark.executor.Executor$TaskRunner,int)|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$6/Executor$TaskRunner$$anonfun$6(org.apache.spark.executor.Executor$TaskRunner,int)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$computeTotalGcTime()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/shuffle/FetchFailedException/toTaskEndReason()|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/SparkEnv/serializer()|",
      "|java+method:///org/apache/spark/executor/Executor/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/IndirectTaskResult/IndirectTaskResult(org.apache.spark.storage.BlockId,int)|",
      "|java+constructor:///org/apache/spark/scheduler/DirectTaskResult/DirectTaskResult(java.nio.ByteBuffer,scala.collection.Map,org.apache.spark.executor.TaskMetrics)|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$run$6/Executor$TaskRunner$$anonfun$run$6(org.apache.spark.executor.Executor$TaskRunner)|",
      "|java+constructor:///org/apache/spark/TaskKilledException/TaskKilledException()|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$run$5/Executor$TaskRunner$$anonfun$run$5(org.apache.spark.executor.Executor$TaskRunner)|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$run$3/Executor$TaskRunner$$anonfun$run$3(org.apache.spark.executor.Executor$TaskRunner)|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$run$2/Executor$TaskRunner$$anonfun$run$2(org.apache.spark.executor.Executor$TaskRunner)|",
      "|java+method:///org/apache/spark/scheduler/Task/killed()|",
      "|java+method:///org/apache/spark/util/AkkaUtils$/reservedSizeBytes()|",
      "|java+method:///org/apache/spark/TaskState$/FAILED()|",
      "|java+method:///org/apache/spark/TaskState$/KILLED()|",
      "|java+constructor:///org/apache/spark/storage/TaskResultBlockId/TaskResultBlockId(long)|",
      "|java+constructor:///scala/runtime/LongRef/LongRef(long)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$replClassLoader()|",
      "|java+method:///org/apache/spark/serializer/Serializer/newInstance()|",
      "|java+method:///java/lang/Thread/setContextClassLoader(java.lang.ClassLoader)|",
      "|java+method:///org/apache/spark/executor/Executor$TaskRunner/org$apache$spark$executor$Executor$TaskRunner$$$outer()|",
      "|java+method:///scala/Option/flatMap(scala.Function1)|",
      "|java+method:///scala/Tuple3/_1()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple3/_2()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/ExceptionFailure/ExceptionFailure(java.lang.Throwable,scala.Option)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/Tuple3/_3()|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/serialize(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkEnv/mapOutputTracker()|",
      "|java+method:///java/lang/Thread/currentThread()|",
      "|java+method:///org/apache/spark/TaskState$/RUNNING()|",
      "|java+method:///org/apache/spark/executor/Executor/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/StorageLevel$/MEMORY_AND_DISK_SER()|",
      "|java+method:///org/apache/spark/executor/Executor/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$conf()|",
      "|java+constructor:///org/apache/spark/ExceptionFailure/ExceptionFailure(java.lang.Throwable,scala.Option,boolean)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$EMPTY_BYTE_BUFFER()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/Option/orNull(scala.Predef$$less$colon$less)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkEnv/memoryManager()|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|",
      "|java+method:///org/apache/spark/scheduler/Task/metrics()|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$run$4/Executor$TaskRunner$$anonfun$run$4(org.apache.spark.executor.Executor$TaskRunner,long,scala.runtime.LongRef,long,long,long)|",
      "|java+method:///org/apache/spark/SparkEnv/closureSerializer()|",
      "|java+method:///org/apache/spark/executor/Executor/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/MapOutputTracker/updateEpoch(long)|",
      "|java+method:///org/apache/spark/executor/Executor$TaskRunner/killed()|",
      "|java+method:///org/apache/spark/executor/Executor$TaskRunner/attemptNumber()|",
      "|java+method:///org/apache/spark/executor/CommitDeniedException/toTaskEndReason()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManager/putBytes$default$4()|",
      "|java+method:///org/apache/spark/executor/Executor$TaskRunner/taskId()|",
      "|java+method:///org/apache/spark/SparkEnv/metricsSystem()|",
      "|java+method:///org/apache/spark/scheduler/Task/epoch()|",
      "|java+method:///org/apache/spark/storage/BlockManager/putBytes(org.apache.spark.storage.BlockId,java.nio.ByteBuffer,org.apache.spark.storage.StorageLevel,boolean,scala.Option)|",
      "|java+method:///org/apache/spark/storage/BlockManager/putBytes$default$5()|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$7/Executor$TaskRunner$$anonfun$7(org.apache.spark.executor.Executor$TaskRunner,scala.runtime.LongRef)|",
      "|java+method:///org/apache/spark/scheduler/Task/setTaskMemoryManager(org.apache.spark.memory.TaskMemoryManager)|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/deserialize(java.nio.ByteBuffer,java.lang.ClassLoader,scala.reflect.ClassTag)|",
      "|java+method:///java/nio/ByteBuffer/limit()|",
      "|java+method:///org/apache/spark/util/Utils$/isFatalError(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/memory/TaskMemoryManager/cleanUpAllAllocatedMemory()|",
      "|java+method:///org/apache/spark/util/SparkUncaughtExceptionHandler$/uncaughtException(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/executor/ExecutorBackend/statusUpdate(long,scala.Enumeration$Value,java.nio.ByteBuffer)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$akkaFrameSize()|",
      "|java+constructor:///org/apache/spark/memory/TaskMemoryManager/TaskMemoryManager(org.apache.spark.memory.MemoryManager,long)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$runningTasks()|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///org/apache/spark/executor/Executor/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/executor/Executor$TaskRunner/task_$eq(org.apache.spark.scheduler.Task)|",
      "|java+method:///java/lang/Thread/getContextClassLoader()|",
      "|java+method:///org/apache/spark/TaskState$/FINISHED()|",
      "|java+method:///org/apache/spark/scheduler/Task$/deserializeWithDependencies(java.nio.ByteBuffer)|",
      "|java+method:///org/apache/spark/executor/Executor$TaskRunner/task()|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/remove(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$updateDependencies(scala.collection.mutable.HashMap,scala.collection.mutable.HashMap)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+constructor:///org/apache/spark/executor/Executor$TaskRunner$$anonfun$3/Executor$TaskRunner$$anonfun$3(org.apache.spark.executor.Executor$TaskRunner,java.lang.String)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/Executor$TaskRunner/startGCTime_$eq(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/PipedRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)|",
    "called": "|java+constructor:///scala/runtime/BooleanRef/BooleanRef(boolean)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$$anonfun$compute$4/PipedRDD$$anonfun$compute$4(org.apache.spark.rdd.PipedRDD,java.lang.String,java.io.File)|",
      "|java+method:///scala/collection/Map/foreach(scala.Function1)|",
      "|java+method:///java/util/UUID/randomUUID()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsJavaMap(scala.collection.Map)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///java/io/File/list(java.io.FilenameFilter)|",
      "|java+method:///java/io/File/mkdirs()|",
      "|java+method:///java/util/UUID/toString()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/rdd/PipedRDD/logDebug(scala.Function0)|",
      "|java+method:///java/lang/ProcessBuilder/directory(java.io.File)|",
      "|java+method:///org/apache/spark/rdd/HadoopPartition/getPipeEnvVars()|",
      "|java+constructor:///java/lang/ProcessBuilder/ProcessBuilder(java.util.List)|",
      "|java+method:///scala/io/Source$/fromInputStream(java.io.InputStream,scala.io.Codec)|",
      "|java+method:///scala/io/Codec$/fallbackSystemCodec()|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$$anonfun$compute$5/PipedRDD$$anonfun$compute$5(org.apache.spark.rdd.PipedRDD,java.lang.String,java.lang.Exception)|",
      "|java+method:///org/apache/spark/rdd/PipedRDD$$anon$2/start()|",
      "|java+method:///org/apache/spark/rdd/PipedRDD/logError(scala.Function0,java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$$anonfun$compute$1/PipedRDD$$anonfun$compute$1(org.apache.spark.rdd.PipedRDD,java.util.Map)|",
      "|java+method:///org/apache/spark/rdd/PipedRDD$$anon$3/start()|",
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$$anonfun$compute$2/PipedRDD$$anonfun$compute$2(org.apache.spark.rdd.PipedRDD,java.lang.String)|",
      "|java+method:///java/util/Map/putAll(java.util.Map)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$$anon$1/PipedRDD$$anon$1(org.apache.spark.rdd.PipedRDD,java.lang.String,scala.runtime.BooleanRef,java.lang.Process,scala.collection.Iterator)|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+method:///scala/io/BufferedSource/getLines()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$NotEqualsFileNameFilter/PipedRDD$NotEqualsFileNameFilter(org.apache.spark.rdd.PipedRDD,java.lang.String)|",
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$$anonfun$compute$3/PipedRDD$$anonfun$compute$3(org.apache.spark.rdd.PipedRDD,java.io.File)|",
      "|java+constructor:///scala/runtime/BooleanRef/BooleanRef(boolean)|",
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$$anon$3/PipedRDD$$anon$3(org.apache.spark.rdd.PipedRDD,org.apache.spark.Partition,org.apache.spark.TaskContext,java.lang.Process)|",
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$$anon$2/PipedRDD$$anon$2(org.apache.spark.rdd.PipedRDD,java.lang.Process)|",
      "|java+method:///java/lang/ProcessBuilder/environment()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$$anonfun$compute$4/PipedRDD$$anonfun$compute$4(org.apache.spark.rdd.PipedRDD,java.lang.String,java.io.File)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/collection/Map/foreach(scala.Function1)|",
      "|java+method:///java/util/UUID/randomUUID()|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///scala/collection/JavaConverters$/mapAsJavaMapConverter(scala.collection.Map)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///java/io/File/list(java.io.FilenameFilter)|",
      "|java+method:///java/io/File/mkdirs()|",
      "|java+method:///scala/io/BufferedSource/getLines()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/rdd/PipedRDD/logDebug(scala.Function0)|",
      "|java+method:///java/lang/ProcessBuilder/directory(java.io.File)|",
      "|java+method:///org/apache/spark/rdd/HadoopPartition/getPipeEnvVars()|",
      "|java+constructor:///java/lang/ProcessBuilder/ProcessBuilder(java.util.List)|",
      "|java+method:///scala/io/Source$/fromInputStream(java.io.InputStream,scala.io.Codec)|",
      "|java+method:///scala/io/Codec$/fallbackSystemCodec()|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$$anonfun$compute$5/PipedRDD$$anonfun$compute$5(org.apache.spark.rdd.PipedRDD,java.lang.String,java.lang.Exception)|",
      "|java+method:///org/apache/spark/rdd/PipedRDD$$anon$2/start()|",
      "|java+method:///org/apache/spark/rdd/PipedRDD/logError(scala.Function0,java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$$anonfun$compute$1/PipedRDD$$anonfun$compute$1(org.apache.spark.rdd.PipedRDD,java.util.Map)|",
      "|java+method:///org/apache/spark/rdd/PipedRDD$$anon$3/start()|",
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$$anonfun$compute$2/PipedRDD$$anonfun$compute$2(org.apache.spark.rdd.PipedRDD,java.lang.String)|",
      "|java+method:///java/util/Map/putAll(java.util.Map)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$$anon$1/PipedRDD$$anon$1(org.apache.spark.rdd.PipedRDD,java.lang.String,scala.runtime.BooleanRef,java.lang.Process,scala.collection.Iterator)|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$NotEqualsFileNameFilter/PipedRDD$NotEqualsFileNameFilter(org.apache.spark.rdd.PipedRDD,java.lang.String)|",
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$$anonfun$compute$3/PipedRDD$$anonfun$compute$3(org.apache.spark.rdd.PipedRDD,java.io.File)|",
      "|java+constructor:///scala/runtime/BooleanRef/BooleanRef(boolean)|",
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$$anon$3/PipedRDD$$anon$3(org.apache.spark.rdd.PipedRDD,org.apache.spark.Partition,org.apache.spark.TaskContext,java.lang.Process)|",
      "|java+constructor:///org/apache/spark/rdd/PipedRDD$$anon$2/PipedRDD$$anon$2(org.apache.spark.rdd.PipedRDD,java.lang.Process)|",
      "|java+method:///java/util/UUID/toString()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///java/lang/ProcessBuilder/environment()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/BlockManager/removeRdd(int)|",
    "called": "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/keys()|",
      "|java+method:///org/apache/spark/storage/BlockManager/org$apache$spark$storage$BlockManager$$blockInfo()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$12/BlockManager$$anonfun$12(org.apache.spark.storage.BlockManager,int)|",
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$11/BlockManager$$anonfun$11(org.apache.spark.storage.BlockManager)|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$removeRdd$1/BlockManager$$anonfun$removeRdd$1(org.apache.spark.storage.BlockManager,int)|",
      "|java+method:///org/apache/spark/storage/BlockManager/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$removeRdd$2/BlockManager$$anonfun$removeRdd$2(org.apache.spark.storage.BlockManager)|",
      "|java+method:///scala/collection/TraversableLike/filter(scala.Function1)|",
      "|java+method:///scala/collection/Iterable/size()|",
      "|java+method:///scala/collection/Iterable/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/TimeStampedHashMap/keys()|",
      "|java+method:///org/apache/spark/storage/BlockManager/org$apache$spark$storage$BlockManager$$blockInfo()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$13/BlockManager$$anonfun$13(org.apache.spark.storage.BlockManager,int)|",
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$12/BlockManager$$anonfun$12(org.apache.spark.storage.BlockManager)|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$removeRdd$1/BlockManager$$anonfun$removeRdd$1(org.apache.spark.storage.BlockManager,int)|",
      "|java+method:///org/apache/spark/storage/BlockManager/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$removeRdd$2/BlockManager$$anonfun$removeRdd$2(org.apache.spark.storage.BlockManager)|",
      "|java+method:///scala/collection/TraversableLike/filter(scala.Function1)|",
      "|java+method:///scala/collection/Iterable/size()|",
      "|java+method:///scala/collection/Iterable/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/serializer/JavaDeserializationStream$$anon$1/resolveClass(java.io.ObjectStreamClass)|",
    "called": "|java+method:///scala/Option/getOrElse(scala.Function0)|",
    "v1Body": [
      "|java+method:///java/io/ObjectStreamClass/getName()|",
      "|java+method:///java/lang/Class/forName(java.lang.String,boolean,java.lang.ClassLoader)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/serializer/JavaDeserializationStream$/primitiveMappings()|",
      "|java+constructor:///org/apache/spark/serializer/JavaDeserializationStream$$anon$1$$anonfun$resolveClass$1/JavaDeserializationStream$$anon$1$$anonfun$resolveClass$1(org.apache.spark.serializer.JavaDeserializationStream$$anon$1,java.lang.ClassNotFoundException)|",
      "|java+method:///java/io/ObjectStreamClass/getName()|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+method:///java/lang/Class/forName(java.lang.String,boolean,java.lang.ClassLoader)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$class/splits(org.apache.spark.api.java.JavaRDDLike)|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+constructor:///java/util/ArrayList/ArrayList(java.util.Collection)|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$/startRpcEnvAndEndpoint(java.lang.String,int,int,int,int,java.lang.String%5B%5D,java.lang.String,scala.Option,org.apache.spark.SparkConf)|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker/Worker(org.apache.spark.rpc.RpcEnv,int,int,int,org.apache.spark.rpc.RpcAddress%5B%5D,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$/ENDPOINT_NAME()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$/SYSTEM_NAME()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$10/Worker$$anonfun$10()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$12/Worker$$anonfun$12()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$11/Worker$$anonfun$11()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create$default$6()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,boolean)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker/Worker(org.apache.spark.rpc.RpcEnv,int,int,int,org.apache.spark.rpc.RpcAddress%5B%5D,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$/ENDPOINT_NAME()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$/SYSTEM_NAME()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$10/Worker$$anonfun$10()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$12/Worker$$anonfun$12()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$11/Worker$$anonfun$11()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$11/DAGScheduler$$anonfun$11(org.apache.spark.scheduler.DAGScheduler)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$77/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$77/apply(org.apache.spark.scheduler.AccumulableInfo)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$77/apply(org.apache.spark.executor.TaskMetrics)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$48/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$48$$anonfun$apply$7/StagePage$$anonfun$48$$anonfun$apply$7(org.apache.spark.ui.jobs.StagePage$$anonfun$48)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/shuffleReadMetrics()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$48$$anonfun$apply$20/StagePage$$anonfun$48$$anonfun$apply$20(org.apache.spark.ui.jobs.StagePage$$anonfun$48)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$48$$anonfun$apply$16/StagePage$$anonfun$48$$anonfun$apply$16(org.apache.spark.ui.jobs.StagePage$$anonfun$48)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$48$$anonfun$apply$3/StagePage$$anonfun$48$$anonfun$apply$3(org.apache.spark.ui.jobs.StagePage$$anonfun$48)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/inputMetrics()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ExecutorLostFailure$/unapply(org.apache.spark.ExecutorLostFailure)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ExecutorLostFailure/execId()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ExecutorLostFailure/reason()|",
      "|java+method:///org/apache/spark/ExecutorLostFailure/exitCausedByApp()|",
      "|java+method:///org/apache/spark/ExecutorLostFailure/execId()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+constructor:///scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/HadoopRDD/getPartitions()|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/ArrayOps/size()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///org/apache/hadoop/conf/Configurable/setConf(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/addCredentials(org.apache.hadoop.mapred.JobConf)|",
      "|java+method:///scala/collection/immutable/Range/foreach$mVc$sp(scala.Function1)|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/getInputFormat(org.apache.hadoop.mapred.JobConf)|",
      "|java+method:///org/apache/hadoop/mapred/InputFormat/getSplits(org.apache.hadoop.mapred.JobConf,int)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+constructor:///org/apache/spark/rdd/HadoopRDD$$anonfun$getPartitions$1/HadoopRDD$$anonfun$getPartitions$1(org.apache.spark.rdd.HadoopRDD,org.apache.hadoop.mapred.InputSplit%5B%5D,org.apache.spark.Partition%5B%5D)|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/getJobConf()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/ArrayOps/size()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/addCredentials(org.apache.hadoop.mapred.JobConf)|",
      "|java+method:///scala/collection/immutable/Range/foreach$mVc$sp(scala.Function1)|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/getInputFormat(org.apache.hadoop.mapred.JobConf)|",
      "|java+method:///org/apache/hadoop/mapred/InputFormat/getSplits(org.apache.hadoop.mapred.JobConf,int)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+constructor:///org/apache/spark/rdd/HadoopRDD$$anonfun$getPartitions$1/HadoopRDD$$anonfun$getPartitions$1(org.apache.spark.rdd.HadoopRDD,org.apache.hadoop.mapred.InputSplit%5B%5D,org.apache.spark.Partition%5B%5D)|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/getJobConf()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/TachyonBlockManager/putBytes(org.apache.spark.storage.BlockId,java.nio.ByteBuffer)|",
    "called": "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
    "v1Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///org/apache/spark/storage/TachyonBlockManager$$anonfun$putBytes$1/TachyonBlockManager$$anonfun$putBytes$1(org.apache.spark.storage.TachyonBlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///tachyon/client/OutStream/cancel()|",
      "|java+method:///tachyon/client/OutStream/write(byte%5B%5D)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/getFile(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///tachyon/client/TachyonFile/getOutStream(tachyon.client.WriteType)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///tachyon/client/OutStream/close()|",
      "|java+method:///java/nio/ByteBuffer/array()|"
    ],
    "v2Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///tachyon/client/file/FileOutStream/cancel()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///org/apache/spark/storage/TachyonBlockManager$$anonfun$putBytes$1/TachyonBlockManager$$anonfun$putBytes$1(org.apache.spark.storage.TachyonBlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///java/nio/ByteBuffer/array()|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/getFile(org.apache.spark.storage.BlockId)|",
      "|java+method:///tachyon/client/file/FileOutStream/write(byte%5B%5D)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///tachyon/client/TachyonFile/getOutStream(tachyon.client.WriteType)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///tachyon/client/file/FileOutStream/close()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$/taskEndReasonToJson(org.apache.spark.TaskEndReason)|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+method:///org/apache/spark/TaskCommitDenied/attemptNumber()|",
      "|java+method:///org/apache/spark/TaskCommitDenied/jobID()|",
      "|java+method:///org/json4s/JsonDSL$JsonAssoc/$tilde(scala.Tuple2,scala.Function1)|",
      "|java+method:///org/json4s/JsonDSL$JsonListAssoc/$tilde(scala.Tuple2)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/ExceptionFailure/fullStackTrace()|",
      "|java+method:///org/apache/spark/util/Utils$/emptyJson()|",
      "|java+method:///org/json4s/JsonDSL$JsonListAssoc/$tilde(org.json4s.JsonAST$JObject)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/json4s/JsonDSL$JsonAssoc/$tilde(org.json4s.JsonAST$JObject)|",
      "|java+method:///org/apache/spark/FetchFailed/message()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$taskEndReasonToJson$1/JsonProtocol$$anonfun$taskEndReasonToJson$1()|",
      "|java+method:///org/json4s/JsonDSL$/pair2jvalue(scala.Tuple2,scala.Function1)|",
      "|java+method:///org/json4s/JsonDSL$/pair2Assoc(scala.Tuple2,scala.Function1)|",
      "|java+method:///org/apache/spark/FetchFailed/bmAddress()|",
      "|java+method:///org/json4s/JsonDSL$/jobject2assoc(org.json4s.JsonAST$JObject)|",
      "|java+method:///org/apache/spark/ExceptionFailure/stackTrace()|",
      "|java+method:///org/apache/spark/ExceptionFailure/description()|",
      "|java+method:///org/apache/spark/ExecutorLostFailure/execId()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$37/JsonProtocol$$anonfun$37()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$36/JsonProtocol$$anonfun$36()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$42/JsonProtocol$$anonfun$42()|",
      "|java+method:///org/apache/spark/util/Utils$/getFormattedClassName(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$41/JsonProtocol$$anonfun$41()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$39/JsonProtocol$$anonfun$39()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$40/JsonProtocol$$anonfun$40()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$38/JsonProtocol$$anonfun$38()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$43/JsonProtocol$$anonfun$43()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$32/JsonProtocol$$anonfun$32()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$46/JsonProtocol$$anonfun$46()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$35/JsonProtocol$$anonfun$35()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$45/JsonProtocol$$anonfun$45()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$34/JsonProtocol$$anonfun$34()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$44/JsonProtocol$$anonfun$44()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$33/JsonProtocol$$anonfun$33()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/FetchFailed/shuffleId()|",
      "|java+method:///scala/Predef$/any2ArrowAssoc(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/stackTraceToJson(java.lang.StackTraceElement%5B%5D)|",
      "|java+method:///org/apache/spark/TaskCommitDenied/partitionID()|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/FetchFailed/mapId()|",
      "|java+method:///org/apache/spark/ExceptionFailure/className()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/ExceptionFailure/metrics()|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///org/apache/spark/FetchFailed/reduceId()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/TaskCommitDenied/attemptNumber()|",
      "|java+method:///org/json4s/JsonDSL$JsonListAssoc/$tilde(scala.Tuple2)|",
      "|java+method:///org/apache/spark/ExceptionFailure/fullStackTrace()|",
      "|java+method:///org/apache/spark/util/Utils$/emptyJson()|",
      "|java+method:///org/json4s/JsonDSL$JsonListAssoc/$tilde(org.json4s.JsonAST$JObject)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/json4s/JsonDSL$JsonAssoc/$tilde(org.json4s.JsonAST$JObject)|",
      "|java+method:///org/apache/spark/FetchFailed/message()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$taskEndReasonToJson$1/JsonProtocol$$anonfun$taskEndReasonToJson$1()|",
      "|java+method:///org/json4s/JsonDSL$/pair2jvalue(scala.Tuple2,scala.Function1)|",
      "|java+method:///org/json4s/JsonDSL$/pair2Assoc(scala.Tuple2,scala.Function1)|",
      "|java+method:///org/apache/spark/FetchFailed/bmAddress()|",
      "|java+method:///org/json4s/JsonDSL$/jobject2assoc(org.json4s.JsonAST$JObject)|",
      "|java+method:///org/apache/spark/ExceptionFailure/stackTrace()|",
      "|java+method:///org/apache/spark/TaskCommitDenied/jobID()|",
      "|java+method:///org/apache/spark/ExecutorLostFailure/reason()|",
      "|java+method:///org/apache/spark/ExecutorLostFailure/exitCausedByApp()|",
      "|java+method:///org/json4s/JsonDSL$JsonAssoc/$tilde(scala.Tuple2,scala.Function1)|",
      "|java+method:///org/apache/spark/ExceptionFailure/description()|",
      "|java+method:///org/apache/spark/ExecutorLostFailure/execId()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$42/JsonProtocol$$anonfun$42()|",
      "|java+method:///org/apache/spark/util/Utils$/getFormattedClassName(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$41/JsonProtocol$$anonfun$41()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$39/JsonProtocol$$anonfun$39()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$40/JsonProtocol$$anonfun$40()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$37/JsonProtocol$$anonfun$37()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$48/JsonProtocol$$anonfun$48()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$43/JsonProtocol$$anonfun$43()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$32/JsonProtocol$$anonfun$32()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$47/JsonProtocol$$anonfun$47()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$36/JsonProtocol$$anonfun$36()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$46/JsonProtocol$$anonfun$46()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$35/JsonProtocol$$anonfun$35()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$45/JsonProtocol$$anonfun$45()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$34/JsonProtocol$$anonfun$34()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$38/JsonProtocol$$anonfun$38()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$49/JsonProtocol$$anonfun$49()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$44/JsonProtocol$$anonfun$44()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$33/JsonProtocol$$anonfun$33()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/FetchFailed/shuffleId()|",
      "|java+method:///scala/Predef$/any2ArrowAssoc(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/stackTraceToJson(java.lang.StackTraceElement%5B%5D)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/TaskCommitDenied/partitionID()|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/FetchFailed/mapId()|",
      "|java+method:///org/apache/spark/ExceptionFailure/className()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/ExceptionFailure/metrics()|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///org/apache/spark/FetchFailed/reduceId()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/DriverRunner/org$apache$spark$deploy$worker$DriverRunner$$initialize$1(java.lang.Process,java.lang.ProcessBuilder,java.io.File)|",
    "called": "|java+method:///scala/collection/immutable/StringOps/$times(int)|",
    "v1Body": [
      "|java+method:///java/lang/Process/getErrorStream()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/CommandUtils$/redirectStream(java.io.InputStream,java.io.File)|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/spark-project/guava/io/Files/append(java.lang.CharSequence,java.io.File,java.nio.charset.Charset)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///scala/collection/mutable/Buffer/mkString(java.lang.String,java.lang.String,java.lang.String)|",
      "|java+method:///java/lang/ProcessBuilder/command()|",
      "|java+method:///scala/collection/immutable/StringOps/$times(int)|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Process/getErrorStream()|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/CommandUtils$/redirectStream(java.io.InputStream,java.io.File)|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String,java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/spark-project/guava/io/Files/append(java.lang.CharSequence,java.io.File,java.nio.charset.Charset)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///java/lang/ProcessBuilder/command()|",
      "|java+method:///scala/collection/immutable/StringOps/$times(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$19/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$19/apply()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$19/apply()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/storage/BlockManager$$anonfun$9/BlockManager$$anonfun$9(org.apache.spark.storage.BlockManager,org.apache.spark.storage.BlockId)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/ShuffleMapStage$$anonfun$1/ShuffleMapStage$$anonfun$1(org.apache.spark.scheduler.ShuffleMapStage)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1$mcZI$sp/AbstractFunction1$mcZI$sp()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1$mcZI$sp/AbstractFunction1$mcZI$sp()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receiveAndReply$1/applyOrElse(java.lang.Object,scala.Function1)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/reply(java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$sendToMaster(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$UnregisterApplication/DeployMessages$UnregisterApplication(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/stop()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/markDead(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$$outer()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/logWarning(scala.Function0)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$5/AppClient$ClientEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$5(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$6/AppClient$ClientEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$6(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+method:///scala/reflect/ClassTag$/Boolean()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$master()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/org$apache$spark$deploy$client$AppClient$$appId()|"
    ],
    "v2Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/reply(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$sendToMaster(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$UnregisterApplication/DeployMessages$UnregisterApplication(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/stop()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///java/util/concurrent/atomic/AtomicReference/get()|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/markDead(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$askAndReplyAsync(org.apache.spark.rpc.RpcEndpointRef,org.apache.spark.rpc.RpcCallContext,java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$$outer()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/logWarning(scala.Function0)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$5/AppClient$ClientEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$5(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$6/AppClient$ClientEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$6(org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$master()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/org$apache$spark$deploy$client$AppClient$$appId()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/ChildFirstURLClassLoader/getResources(java.lang.String)|",
    "called": "|java+method:///scala/collection/JavaConverters$/enumerationAsScalaIteratorConverter(java.util.Enumeration)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/ChildFirstURLClassLoader/parentClassLoader()|",
      "|java+method:///java/net/URLClassLoader/findResources(java.lang.String)|",
      "|java+method:///org/apache/spark/util/ParentClassLoader/getResources(java.lang.String)|",
      "|java+method:///java/util/Enumeration/hasMoreElements()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/ChildFirstURLClassLoader/parentClassLoader()|",
      "|java+method:///scala/collection/Iterator/$plus$plus(scala.Function0)|",
      "|java+method:///scala/collection/JavaConverters$/enumerationAsScalaIteratorConverter(java.util.Enumeration)|",
      "|java+method:///scala/collection/convert/Decorators$AsJavaEnumeration/asJavaEnumeration()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+constructor:///org/apache/spark/util/ChildFirstURLClassLoader$$anonfun$getResources$1/ChildFirstURLClassLoader$$anonfun$getResources$1(org.apache.spark.util.ChildFirstURLClassLoader,scala.collection.Iterator)|",
      "|java+method:///java/net/URLClassLoader/findResources(java.lang.String)|",
      "|java+method:///org/apache/spark/util/ParentClassLoader/getResources(java.lang.String)|",
      "|java+method:///scala/collection/JavaConverters$/asJavaEnumerationConverter(scala.collection.Iterator)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$makeOffers(java.lang.String)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$executorsPendingToRemove()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$executorDataMap()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/scheduler/WorkerOffer/WorkerOffer(java.lang.String,java.lang.String,int)|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/ExecutorData/executorHost()|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/ExecutorData/freeCores()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$$outer()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/resourceOffers(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/launchTasks(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$executorDataMap()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/scheduler/WorkerOffer/WorkerOffer(java.lang.String,java.lang.String,int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/ExecutorData/executorHost()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$executorIsAlive(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/ExecutorData/freeCores()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$$outer()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/resourceOffers(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/launchTasks(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/HttpFileServer/addFile(java.io.File)|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///java/io/File/getName()|",
      "|java+method:///org/apache/spark/HttpFileServer/serverUri()|",
      "|java+method:///org/apache/spark/HttpFileServer/addFileToDir(java.io.File,java.io.File)|",
      "|java+method:///org/apache/spark/HttpFileServer/fileDir()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///java/io/File/getName()|",
      "|java+method:///org/apache/spark/HttpFileServer/serverUri()|",
      "|java+method:///org/apache/spark/util/Utils$/encodeFileNameToURIRawPath(java.lang.String)|",
      "|java+method:///org/apache/spark/HttpFileServer/addFileToDir(java.io.File,java.io.File)|",
      "|java+method:///org/apache/spark/HttpFileServer/fileDir()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$buildDriverCommand(org.apache.spark.deploy.mesos.MesosDriverDescription)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///org/apache/mesos/Protos$Environment/newBuilder()|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/addUris(org.apache.mesos.Protos$CommandInfo$URI)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$URI$Builder/build()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$buildDriverCommand$1/MesosClusterScheduler$$anonfun$org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$buildDriverCommand$1(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,org.apache.mesos.Protos$Environment$Builder)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/generateCmdOption(org.apache.spark.deploy.mesos.MesosDriverDescription)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/immutable/List/isEmpty()|",
      "|java+method:///scala/Option/toList()|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable/newBuilder()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable$Builder/setName(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/setEnvironment(org.apache.mesos.Protos$Environment)|",
      "|java+method:///org/apache/spark/deploy/Command/environment()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/Map/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$3/MesosClusterScheduler$$anonfun$3(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$5/MesosClusterScheduler$$anonfun$5(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$6/MesosClusterScheduler$$anonfun$6(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$7/MesosClusterScheduler$$anonfun$7(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+method:///scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$URI$Builder/setValue(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/Command/arguments()|",
      "|java+method:///org/apache/spark/deploy/Command/libraryPathEntries()|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String)|",
      "|java+method:///scala/collection/immutable/List/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/mesos/Protos$Environment$Builder/addVariables(org.apache.mesos.Protos$Environment$Variable$Builder)|",
      "|java+method:///scala/collection/immutable/Map/contains(java.lang.Object)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/mesos/Protos$Environment$Builder/build()|",
      "|java+constructor:///java/io/File/File(java.lang.String,java.lang.String)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayOps/last()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/setValue(java.lang.String)|",
      "|java+method:///scala/collection/immutable/Iterable$/canBuildFrom()|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$URI/newBuilder()|",
      "|java+method:///java/io/File/getCanonicalPath()|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/build()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$4/MesosClusterScheduler$$anonfun$4(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,org.apache.spark.deploy.mesos.MesosDriverDescription)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/jarUrl()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/command()|",
      "|java+method:///scala/collection/immutable/StringOps/split(char)|",
      "|java+method:///org/apache/spark/util/Utils$/libraryPathEnvPrefix(scala.collection.Seq)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$buildDriverCommand$2/MesosClusterScheduler$$anonfun$org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$buildDriverCommand$2(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,org.apache.mesos.Protos$CommandInfo$Builder)|",
      "|java+method:///scala/collection/immutable/StringOps/stripPrefix(java.lang.String)|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo/newBuilder()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/schedulerProperties()|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable$Builder/setValue(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/head()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///org/apache/mesos/Protos$Environment/newBuilder()|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/addUris(org.apache.mesos.Protos$CommandInfo$URI)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$URI$Builder/build()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$buildDriverCommand$1/MesosClusterScheduler$$anonfun$org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$buildDriverCommand$1(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,org.apache.mesos.Protos$Environment$Builder)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$buildDriverCommand$4/MesosClusterScheduler$$anonfun$org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$buildDriverCommand$4(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,org.apache.mesos.Protos$CommandInfo$Builder)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/immutable/List/isEmpty()|",
      "|java+method:///scala/Option/toList()|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable/newBuilder()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable$Builder/setName(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/setEnvironment(org.apache.mesos.Protos$Environment)|",
      "|java+method:///org/apache/spark/deploy/Command/environment()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/Map/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$3/MesosClusterScheduler$$anonfun$3(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$5/MesosClusterScheduler$$anonfun$5(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$6/MesosClusterScheduler$$anonfun$6(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$7/MesosClusterScheduler$$anonfun$7(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
      "|java+method:///scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$URI$Builder/setValue(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/Command/arguments()|",
      "|java+method:///org/apache/spark/deploy/Command/libraryPathEntries()|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String)|",
      "|java+method:///scala/collection/immutable/List/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/mesos/Protos$Environment$Builder/addVariables(org.apache.mesos.Protos$Environment$Variable$Builder)|",
      "|java+method:///scala/collection/immutable/Map/contains(java.lang.Object)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/mesos/Protos$Environment$Builder/build()|",
      "|java+constructor:///java/io/File/File(java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayOps/last()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/generateCmdOption(org.apache.spark.deploy.mesos.MesosDriverDescription,java.lang.String)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/setValue(java.lang.String)|",
      "|java+method:///scala/collection/immutable/Iterable$/canBuildFrom()|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$URI/newBuilder()|",
      "|java+method:///java/io/File/getCanonicalPath()|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo$Builder/build()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$4/MesosClusterScheduler$$anonfun$4(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,org.apache.spark.deploy.mesos.MesosDriverDescription)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/jarUrl()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/command()|",
      "|java+method:///scala/collection/immutable/StringOps/split(char)|",
      "|java+method:///org/apache/spark/util/Utils$/libraryPathEnvPrefix(scala.collection.Seq)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$buildDriverCommand$2/MesosClusterScheduler$$anonfun$org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$buildDriverCommand$2(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,org.apache.mesos.Protos$CommandInfo$Builder)|",
      "|java+method:///scala/collection/immutable/StringOps/stripPrefix(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/mesos/Protos$CommandInfo/newBuilder()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription/schedulerProperties()|",
      "|java+method:///org/apache/mesos/Protos$Environment$Variable$Builder/setValue(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$buildDriverCommand$3/MesosClusterScheduler$$anonfun$org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$buildDriverCommand$3(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,org.apache.mesos.Protos$CommandInfo$Builder)|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///java/io/File/toString()|",
      "|java+method:///scala/collection/mutable/ArrayOps/head()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1/apply(scala.Tuple2)|",
    "called": "|java+method:///scala/collection/immutable/Set$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/Set/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/mesos/Protos$Value$Text/getValue()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///org/apache/mesos/Protos$Value$Range/getEnd()|",
      "|java+method:///scala/collection/immutable/Set$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1$$anonfun$apply$3/MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1$$anonfun$apply$3(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1$$anonfun$apply$4/MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1$$anonfun$apply$4(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1)|",
      "|java+method:///scala/Some/x()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1$$anonfun$apply$2/MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1$$anonfun$apply$2(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1,scala.collection.immutable.NumericRange$Inclusive)|",
      "|java+method:///scala/collection/IterableLike/exists(scala.Function1)|",
      "|java+method:///scala/Predef$/longWrapper(long)|",
      "|java+method:///org/apache/mesos/Protos$Value$Range/getBegin()|",
      "|java+method:///scala/collection/mutable/Buffer/toSet()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/collection/immutable/Set/contains(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/mesos/Protos$Value$Set/getItemList()|",
      "|java+method:///scala/collection/immutable/Set/subsetOf(scala.collection.GenSet)|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1$$anonfun$apply$1/MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1$$anonfun$apply$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1,org.apache.mesos.Protos$Value$Scalar)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///scala/runtime/RichLong/RichLong(long)|",
      "|java+method:///scala/collection/immutable/Set/isEmpty()|",
      "|java+method:///scala/runtime/RichLong/to(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$Value$Range/getBegin()|",
      "|java+method:///scala/collection/immutable/Set/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///org/apache/mesos/Protos$Value$Text/getValue()|",
      "|java+method:///org/apache/mesos/Protos$Value$Range/getEnd()|",
      "|java+method:///scala/collection/TraversableOnce/toSet()|",
      "|java+method:///scala/collection/immutable/Set$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1$$anonfun$apply$3/MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1$$anonfun$apply$3(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1$$anonfun$apply$4/MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1$$anonfun$apply$4(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1)|",
      "|java+method:///scala/Some/x()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1$$anonfun$apply$2/MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1$$anonfun$apply$2(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1,scala.collection.immutable.NumericRange$Inclusive)|",
      "|java+method:///scala/collection/IterableLike/exists(scala.Function1)|",
      "|java+method:///scala/Predef$/longWrapper(long)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/collection/immutable/Set/contains(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/mesos/Protos$Value$Set/getItemList()|",
      "|java+method:///scala/collection/immutable/Set/subsetOf(scala.collection.GenSet)|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1$$anonfun$apply$1/MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1$$anonfun$apply$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils$$anonfun$matchesAttributeRequirements$1,org.apache.mesos.Protos$Value$Scalar)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///scala/runtime/RichLong/RichLong(long)|",
      "|java+method:///scala/collection/immutable/Set/isEmpty()|",
      "|java+method:///scala/runtime/RichLong/to(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/ShuffledRDD/getDependencies()|",
    "called": "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/rdd/ShuffledRDD/keyOrdering()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/rdd/ShuffledRDD/aggregator()|",
      "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/rdd/ShuffledRDD/mapSideCombine()|",
      "|java+constructor:///org/apache/spark/ShuffleDependency/ShuffleDependency(org.apache.spark.rdd.RDD,org.apache.spark.Partitioner,scala.Option,scala.Option,scala.Option,boolean)|",
      "|java+method:///org/apache/spark/rdd/ShuffledRDD/serializer()|",
      "|java+method:///org/apache/spark/rdd/ShuffledRDD/prev()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/ShuffledRDD/keyOrdering()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/rdd/ShuffledRDD/aggregator()|",
      "|java+method:///org/apache/spark/rdd/ShuffledRDD/mapSideCombine()|",
      "|java+method:///org/apache/spark/rdd/ShuffledRDD/serializer()|",
      "|java+method:///org/apache/spark/rdd/ShuffledRDD/prev()|",
      "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/ShuffleDependency/ShuffleDependency(org.apache.spark.rdd.RDD,org.apache.spark.Partitioner,scala.Option,scala.Option,scala.Option,boolean,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.reflect.ClassTag)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/EventLoggingListener/stop()|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener$/IN_PROGRESS()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///java/io/IOException/IOException(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/EventLoggingListener$$anonfun$stop$2/EventLoggingListener$$anonfun$stop$2(org.apache.spark.scheduler.EventLoggingListener,org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/logPath()|",
      "|java+constructor:///org/apache/spark/scheduler/EventLoggingListener$$anonfun$stop$1/EventLoggingListener$$anonfun$stop$1(org.apache.spark.scheduler.EventLoggingListener)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/delete(org.apache.hadoop.fs.Path,boolean)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/writer()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/shouldOverwrite()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/fileSystem()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener$/IN_PROGRESS()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///java/io/IOException/IOException(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/logPath()|",
      "|java+constructor:///org/apache/spark/scheduler/EventLoggingListener$$anonfun$stop$1/EventLoggingListener$$anonfun$stop$1(org.apache.spark.scheduler.EventLoggingListener)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/delete(org.apache.hadoop.fs.Path,boolean)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/EventLoggingListener$$anonfun$stop$3/EventLoggingListener$$anonfun$stop$3(org.apache.spark.scheduler.EventLoggingListener,org.apache.hadoop.fs.Path)|",
      "|java+constructor:///org/apache/spark/scheduler/EventLoggingListener$$anonfun$stop$2/EventLoggingListener$$anonfun$stop$2(org.apache.spark.scheduler.EventLoggingListener,org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/writer()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/shouldOverwrite()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/fileSystem()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/productElement(int)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/id()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/cached()|",
      "|java+constructor:///java/lang/IndexOutOfBoundsException/IndexOutOfBoundsException(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/name()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/id()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/cached()|",
      "|java+constructor:///java/lang/IndexOutOfBoundsException/IndexOutOfBoundsException(java.lang.String)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/callsite()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationNode/name()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$removeFromPendingRetryDrivers(java.lang.String)|",
    "called": "|java+method:///scala/collection/mutable/ArrayBuffer/remove(int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/pendingRetryDriversState()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$17/MesosClusterScheduler$$anonfun$17(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/remove(int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$pendingRetryDrivers()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/indexWhere(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterPersistenceEngine/expunge(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/pendingRetryDriversState()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$18/MesosClusterScheduler$$anonfun$18(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler,java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/remove(int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler/org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$pendingRetryDrivers()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/indexWhere(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterPersistenceEngine/expunge(java.lang.String)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$110/TaskDataSource$$anonfun$110(org.apache.spark.ui.jobs.TaskDataSource)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$11/MesosSubmitRequestServlet$$anonfun$11(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/serializer/KryoSerializer/newKryo()|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/serializer/JavaIterableWrapperSerializer/JavaIterableWrapperSerializer()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///com/twitter/chill/KryoBase/setReferences(boolean)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/serializer/GenericAvroSerializer/GenericAvroSerializer(scala.collection.immutable.Map)|",
      "|java+constructor:///com/esotericsoftware/kryo/serializers/JavaSerializer/JavaSerializer()|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/serializer/KryoSerializer$$anonfun$newKryo$1/KryoSerializer$$anonfun$newKryo$1(org.apache.spark.serializer.KryoSerializer,com.twitter.chill.KryoBase)|",
      "|java+method:///com/twitter/chill/AllScalaRegistrar/apply(com.esotericsoftware.kryo.Kryo)|",
      "|java+method:///com/twitter/chill/KryoBase/register(java.lang.Class,com.esotericsoftware.kryo.Serializer)|",
      "|java+method:///org/apache/spark/serializer/JavaIterableWrapperSerializer$/wrapperClass()|",
      "|java+constructor:///org/apache/spark/serializer/KryoSerializer$$anonfun$newKryo$3/KryoSerializer$$anonfun$newKryo$3(org.apache.spark.serializer.KryoSerializer,java.lang.ClassLoader)|",
      "|java+method:///java/lang/Thread/setContextClassLoader(java.lang.ClassLoader)|",
      "|java+method:///org/apache/spark/serializer/KryoSerializer$/org$apache$spark$serializer$KryoSerializer$$toRegister()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/serializer/KryoSerializer$$anonfun$newKryo$2/KryoSerializer$$anonfun$newKryo$2(org.apache.spark.serializer.KryoSerializer,com.twitter.chill.KryoBase,java.lang.ClassLoader)|",
      "|java+method:///java/lang/Thread/currentThread()|",
      "|java+method:///org/apache/spark/serializer/KryoSerializer/avroSchemas()|",
      "|java+constructor:///com/twitter/chill/AllScalaRegistrar/AllScalaRegistrar()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///com/twitter/chill/EmptyScalaKryoInstantiator/newKryo()|",
      "|java+method:///org/apache/spark/serializer/KryoSerializer/defaultClassLoader()|",
      "|java+method:///org/apache/spark/serializer/KryoSerializer/referenceTracking()|",
      "|java+method:///org/apache/spark/serializer/KryoSerializer/classesToRegister()|",
      "|java+constructor:///org/apache/spark/serializer/KryoSerializer$$anonfun$newKryo$4/KryoSerializer$$anonfun$newKryo$4(org.apache.spark.serializer.KryoSerializer,com.twitter.chill.KryoBase)|",
      "|java+method:///org/apache/spark/serializer/KryoSerializer/userRegistrator()|",
      "|java+method:///com/twitter/chill/KryoBase/setRegistrationRequired(boolean)|",
      "|java+method:///java/lang/Thread/getContextClassLoader()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///com/twitter/chill/EmptyScalaKryoInstantiator/EmptyScalaKryoInstantiator()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///com/twitter/chill/KryoBase/setClassLoader(java.lang.ClassLoader)|",
      "|java+method:///org/apache/spark/serializer/KryoSerializer/registrationRequired()|",
      "|java+constructor:///org/apache/spark/serializer/KryoSerializer$$anonfun$2/KryoSerializer$$anonfun$2(org.apache.spark.serializer.KryoSerializer)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/serializer/JavaIterableWrapperSerializer/JavaIterableWrapperSerializer()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///com/twitter/chill/KryoBase/setReferences(boolean)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/util/Utils$/classForName(java.lang.String)|",
      "|java+constructor:///org/apache/spark/serializer/GenericAvroSerializer/GenericAvroSerializer(scala.collection.immutable.Map)|",
      "|java+constructor:///com/esotericsoftware/kryo/serializers/JavaSerializer/JavaSerializer()|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/serializer/KryoSerializer$$anonfun$newKryo$3/KryoSerializer$$anonfun$newKryo$3(org.apache.spark.serializer.KryoSerializer,com.twitter.chill.KryoBase)|",
      "|java+method:///com/twitter/chill/KryoBase/register(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/serializer/KryoSerializer$$anonfun$newKryo$1/KryoSerializer$$anonfun$newKryo$1(org.apache.spark.serializer.KryoSerializer,com.twitter.chill.KryoBase)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+method:///com/twitter/chill/AllScalaRegistrar/apply(com.esotericsoftware.kryo.Kryo)|",
      "|java+method:///com/twitter/chill/KryoBase/register(java.lang.Class,com.esotericsoftware.kryo.Serializer)|",
      "|java+method:///org/apache/spark/serializer/JavaIterableWrapperSerializer$/wrapperClass()|",
      "|java+method:///java/lang/Thread/setContextClassLoader(java.lang.ClassLoader)|",
      "|java+constructor:///org/apache/spark/serializer/KryoSerializer$$anonfun$newKryo$4/KryoSerializer$$anonfun$newKryo$4(org.apache.spark.serializer.KryoSerializer,com.twitter.chill.KryoBase,java.lang.ClassLoader)|",
      "|java+method:///org/apache/spark/serializer/KryoSerializer$/org$apache$spark$serializer$KryoSerializer$$toRegister()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/serializer/KryoSerializer$$anonfun$newKryo$5/KryoSerializer$$anonfun$newKryo$5(org.apache.spark.serializer.KryoSerializer,java.lang.ClassLoader)|",
      "|java+method:///java/lang/Thread/currentThread()|",
      "|java+method:///org/apache/spark/serializer/KryoSerializer/avroSchemas()|",
      "|java+constructor:///com/twitter/chill/AllScalaRegistrar/AllScalaRegistrar()|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/serializer/KryoSerializer$$anonfun$newKryo$6/KryoSerializer$$anonfun$newKryo$6(org.apache.spark.serializer.KryoSerializer,com.twitter.chill.KryoBase)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///com/twitter/chill/EmptyScalaKryoInstantiator/newKryo()|",
      "|java+constructor:///org/apache/spark/serializer/KryoSerializer$$anonfun$newKryo$2/KryoSerializer$$anonfun$newKryo$2(org.apache.spark.serializer.KryoSerializer)|",
      "|java+method:///org/apache/spark/serializer/KryoSerializer/defaultClassLoader()|",
      "|java+method:///org/apache/spark/serializer/KryoSerializer/referenceTracking()|",
      "|java+method:///scala/collection/immutable/Map/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/serializer/KryoSerializer/classesToRegister()|",
      "|java+method:///org/apache/spark/serializer/KryoSerializer/userRegistrator()|",
      "|java+method:///com/twitter/chill/KryoBase/setRegistrationRequired(boolean)|",
      "|java+method:///java/lang/Thread/getContextClassLoader()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/serializer/KryoSerializer$/org$apache$spark$serializer$KryoSerializer$$toRegisterSerializer()|",
      "|java+constructor:///com/twitter/chill/EmptyScalaKryoInstantiator/EmptyScalaKryoInstantiator()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///com/twitter/chill/KryoBase/setClassLoader(java.lang.ClassLoader)|",
      "|java+method:///org/apache/spark/serializer/KryoSerializer/registrationRequired()|",
      "|java+constructor:///org/apache/spark/serializer/KryoSerializer$$anonfun$2/KryoSerializer$$anonfun$2(org.apache.spark.serializer.KryoSerializer)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore/getValues(org.apache.spark.storage.BlockId)|",
    "called": "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///java/nio/ByteBuffer/duplicate()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataDeserialize$default$3()|",
      "|java+method:///scala/collection/mutable/ArrayOps/iterator()|",
      "|java+method:///org/apache/spark/storage/MemoryEntry/value()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockStore/blockManager()|",
      "|java+method:///org/apache/spark/storage/MemoryEntry/deserialized()|",
      "|java+method:///java/util/LinkedHashMap/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataDeserialize(org.apache.spark.storage.BlockId,java.nio.ByteBuffer,org.apache.spark.serializer.Serializer)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$entries()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///java/nio/ByteBuffer/duplicate()|",
      "|java+method:///scala/collection/mutable/ArrayOps/iterator()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataDeserialize(org.apache.spark.storage.BlockId,java.nio.ByteBuffer)|",
      "|java+method:///org/apache/spark/storage/MemoryEntry/value()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockStore/blockManager()|",
      "|java+method:///org/apache/spark/storage/MemoryEntry/deserialized()|",
      "|java+method:///java/util/LinkedHashMap/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$entries()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$executorLost$1/TaskSetManager$$anonfun$executorLost$1(org.apache.spark.scheduler.TaskSetManager)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1$mcVI$sp/AbstractFunction1$mcVI$sp()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/ExecutorExited/hashCode()|",
    "called": "|java+method:///scala/runtime/Statics/finalizeHash(int,int)|",
    "v1Body": [
      "|java+method:///scala/runtime/Statics/finalizeHash(int,int)|",
      "|java+method:///scala/runtime/Statics/mix(int,int)|",
      "|java+method:///org/apache/spark/scheduler/ExecutorExited/exitCode()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/ExecutorExited/reason()|",
      "|java+method:///scala/runtime/Statics/finalizeHash(int,int)|",
      "|java+method:///scala/runtime/Statics/mix(int,int)|",
      "|java+method:///org/apache/spark/scheduler/ExecutorExited/exitCode()|",
      "|java+method:///org/apache/spark/scheduler/ExecutorExited/exitCausedByApp()|",
      "|java+method:///scala/runtime/Statics/anyHash(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/metrics/MetricsConfig$$anonfun$initialize$4$$anonfun$apply$3/apply(scala.Tuple2)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///java/util/Properties/setProperty(java.lang.String,java.lang.String)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///java/util/Properties/put(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$zipWithUniqueId$1/apply()|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$zipWithUniqueId$1$$anonfun$apply$43/RDD$$anonfun$zipWithUniqueId$1$$anonfun$apply$43(org.apache.spark.rdd.RDD$$anonfun$zipWithUniqueId$1,long)|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitionsWithIndex(scala.Function2,boolean,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitionsWithIndex$default$2()|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$zipWithUniqueId$1/apply()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$zipWithUniqueId$1$$anonfun$apply$47/RDD$$anonfun$zipWithUniqueId$1$$anonfun$apply$47(org.apache.spark.rdd.RDD$$anonfun$zipWithUniqueId$1,long)|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitionsWithIndex(scala.Function2,boolean,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitionsWithIndex$default$2()|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$zipWithUniqueId$1/apply()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/history/HistoryServer$$anon$1/doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)|",
    "called": "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///javax/servlet/http/HttpServletResponse/sendError(int,java.lang.String)|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer$$anon$1$$anonfun$2/HistoryServer$$anon$1$$anonfun$2(org.apache.spark.deploy.history.HistoryServer$$anon$1)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServer/org$apache$spark$deploy$history$HistoryServer$$loadAppUi(java.lang.String,scala.Option)|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer$$anon$1$$anonfun$doGet$1/HistoryServer$$anon$1$$anonfun$doGet$1(org.apache.spark.deploy.history.HistoryServer$$anon$1,scala.xml.Elem)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+method:///javax/servlet/http/HttpServletResponse/encodeRedirectURL(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer$$anon$1$$anonfun$doGet$2/HistoryServer$$anon$1$$anonfun$doGet$2(org.apache.spark.deploy.history.HistoryServer$$anon$1,javax.servlet.http.HttpServletResponse)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+method:///javax/servlet/http/HttpServletRequest/getPathInfo()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/basicSparkPage(scala.Function0,java.lang.String)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///javax/servlet/http/HttpServletResponse/sendRedirect(java.lang.String)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+method:///javax/servlet/http/HttpServletRequest/getRequestURI()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///javax/servlet/http/HttpServletResponse/setStatus(int)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///javax/servlet/http/HttpServletResponse/sendError(int,java.lang.String)|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServer/org$apache$spark$deploy$history$HistoryServer$$loadAppUi(java.lang.String,scala.Option)|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer$$anon$1$$anonfun$doGet$1/HistoryServer$$anon$1$$anonfun$doGet$1(org.apache.spark.deploy.history.HistoryServer$$anon$1,scala.xml.Elem)|",
      "|java+method:///javax/servlet/http/HttpServletResponse/encodeRedirectURL(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer$$anon$1$$anonfun$4/HistoryServer$$anon$1$$anonfun$4(org.apache.spark.deploy.history.HistoryServer$$anon$1)|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer$$anon$1$$anonfun$3/HistoryServer$$anon$1$$anonfun$3(org.apache.spark.deploy.history.HistoryServer$$anon$1)|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer$$anon$1$$anonfun$2/HistoryServer$$anon$1$$anonfun$2(org.apache.spark.deploy.history.HistoryServer$$anon$1)|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer$$anon$1$$anonfun$doGet$2/HistoryServer$$anon$1$$anonfun$doGet$2(org.apache.spark.deploy.history.HistoryServer$$anon$1,javax.servlet.http.HttpServletResponse)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+method:///javax/servlet/http/HttpServletRequest/getPathInfo()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/basicSparkPage(scala.Function0,java.lang.String)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///javax/servlet/http/HttpServletResponse/sendRedirect(java.lang.String)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///javax/servlet/http/HttpServletRequest/getRequestURI()|",
      "|java+method:///javax/servlet/http/HttpServletRequest/getQueryString()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///javax/servlet/http/HttpServletResponse/setStatus(int)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkConf/validateSettings()|",
    "called": "|java+method:///scala/collection/JavaConverters$/enumerationAsScalaIteratorConverter(java.util.Enumeration)|",
    "v1Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$1/SparkConf$$anonfun$validateSettings$1(org.apache.spark.SparkConf,java.lang.String)|",
      "|java+method:///scala/sys/package$/props()|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$4/SparkConf$$anonfun$validateSettings$4(org.apache.spark.SparkConf)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$5/SparkConf$$anonfun$validateSettings$5(org.apache.spark.SparkConf,java.lang.String,java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$3/SparkConf$$anonfun$validateSettings$3(org.apache.spark.SparkConf,java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$2/SparkConf$$anonfun$validateSettings$2(org.apache.spark.SparkConf,java.lang.String)|",
      "|java+method:///scala/sys/package$/env()|",
      "|java+method:///scala/sys/SystemProperties/get(java.lang.String)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$6/SparkConf$$anonfun$validateSettings$6(org.apache.spark.SparkConf,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$7/SparkConf$$anonfun$validateSettings$7(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkConf/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkConf/contains(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///java/util/concurrent/ConcurrentHashMap/keys()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$4/SparkConf$$anonfun$validateSettings$4(org.apache.spark.SparkConf)|",
      "|java+method:///scala/collection/JavaConverters$/enumerationAsScalaIteratorConverter(java.util.Enumeration)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$8/SparkConf$$anonfun$validateSettings$8(org.apache.spark.SparkConf)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/collection/Iterator/nonEmpty()|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$3/SparkConf$$anonfun$validateSettings$3(org.apache.spark.SparkConf,java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$2/SparkConf$$anonfun$validateSettings$2(org.apache.spark.SparkConf,java.lang.String)|",
      "|java+method:///scala/sys/package$/env()|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$1/SparkConf$$anonfun$validateSettings$1(org.apache.spark.SparkConf,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/sys/package$/props()|",
      "|java+method:///scala/sys/SystemProperties/get(java.lang.String)|",
      "|java+method:///scala/collection/Seq/toSet()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/collection/Iterator/filter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$6/SparkConf$$anonfun$validateSettings$6(org.apache.spark.SparkConf,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$4/SparkConf$$anonfun$4(org.apache.spark.SparkConf,scala.collection.immutable.Set)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$7/SparkConf$$anonfun$validateSettings$7(org.apache.spark.SparkConf,java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkConf/contains(java.lang.String)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$5/SparkConf$$anonfun$validateSettings$5(org.apache.spark.SparkConf,java.lang.String,scala.collection.Iterator)|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkConf/settings()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl/TaskSchedulerImpl(org.apache.spark.SparkContext,int,boolean)|",
    "called": "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+constructor:///java/util/concurrent/atomic/AtomicLong/AtomicLong(long)|",
      "|java+method:///org/apache/spark/SparkEnv/mapOutputTracker()|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///org/apache/spark/scheduler/TaskScheduler$class/$init$(org.apache.spark.scheduler.TaskScheduler)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/conf()|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskResultGetter/TaskResultGetter(org.apache.spark.SparkEnv,org.apache.spark.scheduler.TaskSchedulerImpl)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+constructor:///java/util/Timer/Timer(boolean)|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonSingleThreadScheduledExecutor(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/liftedTree1$1()|",
      "|java+method:///org/apache/spark/SparkConf/getTimeAsMs(java.lang.String,java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+constructor:///java/util/concurrent/atomic/AtomicLong/AtomicLong(long)|",
      "|java+method:///org/apache/spark/SparkEnv/mapOutputTracker()|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///org/apache/spark/scheduler/TaskScheduler$class/$init$(org.apache.spark.scheduler.TaskScheduler)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/conf()|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskResultGetter/TaskResultGetter(org.apache.spark.SparkEnv,org.apache.spark.scheduler.TaskSchedulerImpl)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+constructor:///java/util/Timer/Timer(boolean)|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonSingleThreadScheduledExecutor(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/liftedTree1$1()|",
      "|java+method:///org/apache/spark/SparkConf/getTimeAsMs(java.lang.String,java.lang.String)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$groupBy$3/apply()|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$groupBy$3/apply()|",
      "|java+method:///org/apache/spark/rdd/RDD/org$apache$spark$rdd$RDD$$sc()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/groupByKey(org.apache.spark.Partitioner)|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$groupBy$3$$anonfun$apply$16/RDD$$anonfun$groupBy$3$$anonfun$apply$16(org.apache.spark.rdd.RDD$$anonfun$groupBy$3,scala.Function1)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$groupBy$3/apply()|",
      "|java+method:///org/apache/spark/rdd/RDD/org$apache$spark$rdd$RDD$$sc()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/groupByKey(org.apache.spark.Partitioner)|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$groupBy$3$$anonfun$apply$19/RDD$$anonfun$groupBy$3$$anonfun$apply$19(org.apache.spark.rdd.RDD$$anonfun$groupBy$3,scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/TestUtils$$anonfun$createJar$1/apply(java.io.File)|",
    "called": "|java+method:///scala/Option/getOrElse(scala.Function0)|",
    "v1Body": [
      "|java+method:///org/spark-project/guava/io/ByteStreams/copy(java.io.InputStream,java.io.OutputStream)|",
      "|java+method:///java/io/File/getName()|",
      "|java+method:///java/util/jar/JarOutputStream/putNextEntry(java.util.zip.ZipEntry)|",
      "|java+method:///java/io/FileInputStream/close()|",
      "|java+constructor:///java/util/jar/JarEntry/JarEntry(java.lang.String)|",
      "|java+constructor:///java/io/FileInputStream/FileInputStream(java.io.File)|"
    ],
    "v2Body": [
      "|java+method:///org/spark-project/guava/io/ByteStreams/copy(java.io.InputStream,java.io.OutputStream)|",
      "|java+method:///java/nio/file/Paths/get(java.lang.String,java.lang.String%5B%5D)|",
      "|java+method:///java/util/jar/JarOutputStream/putNextEntry(java.util.zip.ZipEntry)|",
      "|java+constructor:///java/util/jar/JarEntry/JarEntry(java.lang.String)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///java/io/FileInputStream/FileInputStream(java.io.File)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/io/File/getName()|",
      "|java+constructor:///org/apache/spark/TestUtils$$anonfun$createJar$1$$anonfun$5/TestUtils$$anonfun$createJar$1$$anonfun$5(org.apache.spark.TestUtils$$anonfun$createJar$1)|",
      "|java+method:///java/io/FileInputStream/close()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$$anonfun$fn$5$1/apply(scala.collection.Iterator)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/java/function/FlatMapFunction/call(java.lang.Object)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaIterator(java.util.Iterator)|",
      "|java+method:///java/lang/Iterable/iterator()|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterator(scala.collection.Iterator)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
      "|java+method:///org/apache/spark/api/java/function/FlatMapFunction/call(java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///java/lang/Iterable/iterator()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaIteratorConverter(java.util.Iterator)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$2/MesosSchedulerBackend$$anonfun$2(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/SparkUI/getSparkUI(java.lang.String)|",
    "called": "|java+constructor:///scala/Some/Some(java.lang.Object)|",
    "v1Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/SparkUI/appName()|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/SparkUI/appId()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaNewHadoopRDD$$anonfun$mapPartitionsWithInputSplit$1/apply(org.apache.hadoop.mapreduce.InputSplit,scala.collection.Iterator)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/java/function/Function2/call(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaIterator(java.util.Iterator)|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterator(scala.collection.Iterator)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///org/apache/spark/api/java/function/Function2/call(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaIteratorConverter(java.util.Iterator)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore/releaseUnrollMemoryForThisTask(long)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/storage/MemoryStore$$anonfun$releaseUnrollMemoryForThisTask$1/MemoryStore$$anonfun$releaseUnrollMemoryForThisTask$1(org.apache.spark.storage.MemoryStore,long)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/accountingLock()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/currentTaskAttemptId()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/unrollMemoryMap()|",
      "|java+method:///scala/collection/mutable/HashMap/remove(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/currentTaskAttemptId()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/unrollMemoryMap()|",
      "|java+method:///org/apache/spark/memory/MemoryManager/releaseUnrollMemory(long)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/remove(java.lang.Object)|",
      "|java+method:///scala/math/package$/min(long,long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1/applyOrElse(java.lang.Object,scala.Function1)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/rpc/RpcCallContext/reply(java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/Predef$/byteArrayOps(byte%5B%5D)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/address()|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/sendFailure(java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3/MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3(org.apache.spark.MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2/MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2(org.apache.spark.MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1,java.lang.String)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/MapOutputTrackerMasterEndpoint/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/sender()|",
      "|java+method:///scala/collection/mutable/ArrayOps/size()|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/MapOutputTrackerMasterEndpoint/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/GetMapOutputStatuses/shuffleId()|",
      "|java+method:///org/apache/spark/MapOutputTrackerMaster/getSerializedMapOutputStatuses(int)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/hostPort()|",
      "|java+constructor:///org/apache/spark/MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1/MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1(org.apache.spark.MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1,java.lang.String,int)|",
      "|java+method:///org/apache/spark/MapOutputTrackerMasterEndpoint/stop()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/MapOutputTrackerMasterEndpoint/maxAkkaFrameSize()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rpc/RpcCallContext/reply(java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/senderAddress()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/sendFailure(java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3/MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3(org.apache.spark.MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2/MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2(org.apache.spark.MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1,java.lang.String)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/MapOutputTrackerMasterEndpoint/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/MapOutputTrackerMasterEndpoint/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/GetMapOutputStatuses/shuffleId()|",
      "|java+method:///org/apache/spark/MapOutputTrackerMaster/getSerializedMapOutputStatuses(int)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/hostPort()|",
      "|java+constructor:///org/apache/spark/MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1/MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1(org.apache.spark.MapOutputTrackerMasterEndpoint$$anonfun$receiveAndReply$1,java.lang.String,int)|",
      "|java+method:///org/apache/spark/MapOutputTrackerMasterEndpoint/stop()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/MapOutputTrackerMasterEndpoint/maxAkkaFrameSize()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext/addJar(java.lang.String)|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/logWarning(scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/SparkContext/liftedTree2$1(java.lang.String,java.net.URI)|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/isYarnMode()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/HttpFileServer/addJar(java.io.File)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/SparkContext/addedJars()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///org/apache/hadoop/fs/Path/getName()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/postEnvironmentUpdate()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$addJar$2/SparkContext$$anonfun$addJar$2(org.apache.spark.SparkContext,java.lang.String,scala.runtime.ObjectRef)|",
      "|java+constructor:///java/net/URI/URI(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/master()|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///java/net/URI/getPath()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$addJar$1/SparkContext$$anonfun$addJar$1(org.apache.spark.SparkContext)|",
      "|java+method:///java/net/URI/getScheme()|",
      "|java+method:///org/apache/spark/SparkEnv/httpFileServer()|",
      "|java+method:///org/apache/spark/SparkContext/liftedTree1$1(java.lang.String)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///java/lang/String/contains(java.lang.CharSequence)|",
      "|java+method:///org/apache/spark/SparkContext/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/fileServer()|",
      "|java+method:///org/apache/spark/SparkContext/logWarning(scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/isYarnMode()|",
      "|java+method:///org/apache/spark/SparkContext/liftedTree1$1(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/SparkContext/addedJars()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///org/apache/hadoop/fs/Path/getName()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/postEnvironmentUpdate()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$addJar$2/SparkContext$$anonfun$addJar$2(org.apache.spark.SparkContext,java.lang.String,scala.runtime.ObjectRef)|",
      "|java+constructor:///java/net/URI/URI(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/master()|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvFileServer/addJar(java.io.File)|",
      "|java+method:///org/apache/spark/SparkContext/liftedTree2$1(java.lang.String,java.net.URI)|",
      "|java+method:///java/net/URI/getPath()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$addJar$1/SparkContext$$anonfun$addJar$1(org.apache.spark.SparkContext)|",
      "|java+method:///java/net/URI/getScheme()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///java/lang/String/contains(java.lang.CharSequence)|",
      "|java+method:///org/apache/spark/SparkContext/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$receive$1/applyOrElse(java.lang.Object,scala.Function1)|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$2/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$2(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend/stop()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/Executor/stop()|",
      "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend/rpcEnv()|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/deserialize(java.nio.ByteBuffer,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$KillTask/interruptThread()|",
      "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend/executor()|",
      "|java+method:///org/apache/spark/executor/Executor/launchTask(org.apache.spark.executor.ExecutorBackend,long,int,java.lang.String,java.nio.ByteBuffer)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/serializedTask()|",
      "|java+method:///org/apache/spark/util/Utils$/parseHostPort(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/name()|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/attemptNumber()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisterExecutorFailed/message()|",
      "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend/executor_$eq(org.apache.spark.executor.Executor)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/util/SerializableBuffer/value()|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$5/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$5(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$3/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$3(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$6/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$6(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$1/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$1(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$LaunchTask/data()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/shutdown()|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$4/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$4(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1,org.apache.spark.scheduler.TaskDescription)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/executor/Executor/killTask(long,boolean)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$KillTask/taskId()|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/taskId()|",
      "|java+method:///java/lang/System/exit(int)|",
      "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend/logError(scala.Function0)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/executor/Executor/Executor(java.lang.String,java.lang.String,org.apache.spark.SparkEnv,scala.collection.Seq,boolean)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$2/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$2(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend/stop()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/Executor/stop()|",
      "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend/rpcEnv()|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$4/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$4(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1,org.apache.spark.scheduler.TaskDescription)|",
      "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend/executor()|",
      "|java+method:///org/apache/spark/executor/Executor/launchTask(org.apache.spark.executor.ExecutorBackend,long,int,java.lang.String,java.nio.ByteBuffer)|",
      "|java+constructor:///org/apache/spark/executor/Executor/Executor(java.lang.String,java.lang.String,org.apache.spark.SparkEnv,scala.collection.Seq,boolean)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/serializedTask()|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/name()|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/attemptNumber()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisterExecutorFailed/message()|",
      "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend/executor_$eq(org.apache.spark.executor.Executor)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/util/SerializableBuffer/value()|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$5/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$5(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$3/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$3(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$6/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$6(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$1/CoarseGrainedExecutorBackend$$anonfun$receive$1$$anonfun$applyOrElse$1(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$LaunchTask/data()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/shutdown()|",
      "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend/self()|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/deserialize(java.nio.ByteBuffer,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/executor/Executor/killTask(long,boolean)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$KillTask/taskId()|",
      "|java+method:///org/apache/spark/scheduler/TaskDescription/taskId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisteredExecutor/hostname()|",
      "|java+method:///java/lang/System/exit(int)|",
      "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$KillTask/interruptThread()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1/applyOrElse(java.lang.Object,scala.Function1)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RegisterBlockManager/sender()|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetBlockStatus/askSlaves()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$storageStatus()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$removeShuffle(int)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveRdd/rddId()|",
      "|java+method:///org/apache/spark/storage/BlockUpdatedInfo$/apply(org.apache.spark.storage.BlockManagerMessages$UpdateBlockInfo)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$UpdateBlockInfo/externalBlockStoreSize()|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$heartbeatReceived(org.apache.spark.storage.BlockManagerId)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$UpdateBlockInfo/storageLevel()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetMatchingBlockIds/filter()|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/reply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$getMatchingBlockIds(scala.Function1,boolean)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$blockManagerIdByExecutor()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveBlock/blockId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$BlockManagerHeartbeat/blockManagerId()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetMatchingBlockIds/askSlaves()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveExecutor/execId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RegisterBlockManager/blockManagerId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerInfo/cachedBlocks()|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerBlockUpdated/SparkListenerBlockUpdated(org.apache.spark.storage.BlockUpdatedInfo)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveBroadcast/broadcastId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$blockManagerInfo()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$UpdateBlockInfo/blockId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$blockStatus(org.apache.spark.storage.BlockId,boolean)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$UpdateBlockInfo/blockManagerId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$updateBlockInfo(org.apache.spark.storage.BlockManagerId,org.apache.spark.storage.BlockId,org.apache.spark.storage.StorageLevel,long,long,long)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$removeExecutor(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetRpcHostPortForExecutor/executorId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetPeers/blockManagerId()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$register(org.apache.spark.storage.BlockManagerId,long,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$memoryStatus()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$removeRdd(int)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$getLocationsMultipleBlockIds(org.apache.spark.storage.BlockId%5B%5D)|",
      "|java+method:///scala/collection/Set/nonEmpty()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBlockFromWorkers(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$UpdateBlockInfo/diskSize()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$HasCachedBlocks/executorId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$getLocations(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetLocations/blockId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetLocationsMultipleBlockIds/blockIds()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$getRpcHostPortForExecutor(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/stop()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetBlockStatus/blockId()|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$UpdateBlockInfo/memSize()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$getPeers(org.apache.spark.storage.BlockManagerId)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveShuffle/shuffleId()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBroadcast(long,boolean)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RegisterBlockManager/maxMemSize()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveBroadcast/removeFromDriver()|"
    ],
    "v2Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RegisterBlockManager/sender()|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetBlockStatus/askSlaves()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$storageStatus()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetExecutorEndpointRef/executorId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$removeShuffle(int)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveRdd/rddId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$UpdateBlockInfo/externalBlockStoreSize()|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$heartbeatReceived(org.apache.spark.storage.BlockManagerId)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$UpdateBlockInfo/storageLevel()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetMatchingBlockIds/filter()|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/reply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$getMatchingBlockIds(scala.Function1,boolean)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$blockManagerIdByExecutor()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveBlock/blockId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$BlockManagerHeartbeat/blockManagerId()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetMatchingBlockIds/askSlaves()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveExecutor/execId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RegisterBlockManager/blockManagerId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerInfo/cachedBlocks()|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerBlockUpdated/SparkListenerBlockUpdated(org.apache.spark.storage.BlockUpdatedInfo)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveBroadcast/broadcastId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$blockManagerInfo()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$UpdateBlockInfo/blockId()|",
      "|java+method:///org/apache/spark/storage/BlockUpdatedInfo$/apply(org.apache.spark.storage.BlockManagerMessages$UpdateBlockInfo)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$getExecutorEndpointRef(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$blockStatus(org.apache.spark.storage.BlockId,boolean)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$UpdateBlockInfo/blockManagerId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$updateBlockInfo(org.apache.spark.storage.BlockManagerId,org.apache.spark.storage.BlockId,org.apache.spark.storage.StorageLevel,long,long,long)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$removeExecutor(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetPeers/blockManagerId()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$register(org.apache.spark.storage.BlockManagerId,long,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$memoryStatus()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$removeRdd(int)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$getLocationsMultipleBlockIds(org.apache.spark.storage.BlockId%5B%5D)|",
      "|java+method:///scala/collection/Set/nonEmpty()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBlockFromWorkers(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$UpdateBlockInfo/diskSize()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$HasCachedBlocks/executorId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$getLocations(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetLocations/blockId()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetLocationsMultipleBlockIds/blockIds()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/stop()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$GetBlockStatus/blockId()|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$UpdateBlockInfo/memSize()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$getPeers(org.apache.spark.storage.BlockManagerId)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveShuffle/shuffleId()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint/org$apache$spark$storage$BlockManagerMasterEndpoint$$removeBroadcast(long,boolean)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RegisterBlockManager/maxMemSize()|",
      "|java+method:///org/apache/spark/storage/BlockManagerMessages$RemoveBroadcast/removeFromDriver()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/history/HistoryPage/org$apache$spark$deploy$history$HistoryPage$$makePageLink(int,boolean)|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/ArrayOps/mkString(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/prependBaseUri(java.lang.String,java.lang.String)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/prependBaseUri$default$2()|",
      "|java+method:///scala/collection/mutable/ArrayOps/mkString(java.lang.String)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/UIUtils$/makeProgressBar(int,int,int,int,int)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|",
    "v1Body": [
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+method:///scala/math/package$/min(int,int)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$15/MesosClusterScheduler$$anonfun$15(org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext/setupAndStartListenerBus()|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkContext/stop()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_listenerBusStarted_$eq(boolean)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/split(char)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/start(org.apache.spark.SparkContext)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$38/SparkContext$$anonfun$38(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$39/SparkContext$$anonfun$39(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$setupAndStartListenerBus$1/SparkContext$$anonfun$setupAndStartListenerBus$1(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/listenerBus()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/ArrayOps/filter(scala.Function1)|"
    ],
    "v2Body": [
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkContext/stop()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_listenerBusStarted_$eq(boolean)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/split(char)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/start(org.apache.spark.SparkContext)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$40/SparkContext$$anonfun$40(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$39/SparkContext$$anonfun$39(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$setupAndStartListenerBus$1/SparkContext$$anonfun$setupAndStartListenerBus$1(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/listenerBus()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/ArrayOps/filter(scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkEnv$$anonfun$7/apply(java.lang.String)|",
    "called": "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
    "v1Body": [
      "|java+method:///java/lang/String/isEmpty()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$class/partitions(org.apache.spark.api.java.JavaRDDLike)|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+constructor:///java/util/ArrayList/ArrayList(java.util.Collection)|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/random/SamplingUtils$/reservoirSampleAndCount(scala.collection.Iterator,int,long,scala.reflect.ClassTag)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
    "v1Body": [
      "|java+method:///scala/collection/Iterator/next()|",
      "|java+constructor:///org/apache/spark/util/random/XORShiftRandom/XORShiftRandom(long)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/runtime/ScalaRunTime$/array_update(java.lang.Object,int,java.lang.Object)|",
      "|java+method:///java/lang/System/arraycopy(java.lang.Object,int,java.lang.Object,int,int)|",
      "|java+method:///org/apache/spark/util/random/XORShiftRandom/nextInt(int)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+method:///scala/reflect/ClassTag/newArray(int)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Iterator/next()|",
      "|java+constructor:///org/apache/spark/util/random/XORShiftRandom/XORShiftRandom(long)|",
      "|java+method:///scala/runtime/ScalaRunTime$/array_update(java.lang.Object,int,java.lang.Object)|",
      "|java+method:///java/lang/System/arraycopy(java.lang.Object,int,java.lang.Object,int,int)|",
      "|java+method:///org/apache/spark/util/random/XORShiftRandom/nextDouble()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/Iterator/hasNext()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/reflect/ClassTag/newArray(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/TestOutputValueConverter/convert(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToDouble(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/python/TestOutputValueConverter/convert(java.lang.Object)|",
      "|java+constructor:///org/apache/hadoop/io/DoubleWritable/DoubleWritable(double)|",
      "|java+method:///java/util/Map/keySet()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToDouble(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Set/head()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaSet(java.util.Set)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/api/python/TestOutputValueConverter/convert(java.lang.Object)|",
      "|java+constructor:///org/apache/hadoop/io/DoubleWritable/DoubleWritable(double)|",
      "|java+method:///java/util/Iterator/next()|",
      "|java+method:///java/util/Map/keySet()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToDouble(java.lang.Object)|",
      "|java+method:///java/util/Set/iterator()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$90/TaskDataSource$$anonfun$90(org.apache.spark.ui.jobs.TaskDataSource)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ShuffleDependency$$anonfun$1/ShuffleDependency$$anonfun$1(org.apache.spark.ShuffleDependency)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/SparkContext$$anonfun$23/SparkContext$$anonfun$23(org.apache.spark.SparkContext)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$97/TaskDataSource$$anonfun$97(org.apache.spark.ui.jobs.TaskDataSource)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$$anonfun$foreachPartition$1/apply(scala.collection.Iterator)|",
    "called": "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/java/function/VoidFunction/call(java.lang.Object)|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterator(scala.collection.Iterator)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
      "|java+method:///org/apache/spark/api/java/function/VoidFunction/call(java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$1/apply()|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$1$$anonfun$apply$5/Worker$$anonfun$handleExecutorStateChanged$1$$anonfun$apply$5(org.apache.spark.deploy.worker.Worker$$anonfun$handleExecutorStateChanged$1)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$1/apply()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$1$$anonfun$apply$7/Worker$$anonfun$handleExecutorStateChanged$1$$anonfun$apply$7(org.apache.spark.deploy.worker.Worker$$anonfun$handleExecutorStateChanged$1)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$1$$anonfun$apply$6/Worker$$anonfun$handleExecutorStateChanged$1$$anonfun$apply$6(org.apache.spark.deploy.worker.Worker$$anonfun$handleExecutorStateChanged$1)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$1$$anonfun$apply$8/Worker$$anonfun$handleExecutorStateChanged$1$$anonfun$apply$8(org.apache.spark.deploy.worker.Worker$$anonfun$handleExecutorStateChanged$1)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$1/apply()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$1$$anonfun$apply$10/Worker$$anonfun$handleExecutorStateChanged$1$$anonfun$apply$10(org.apache.spark.deploy.worker.Worker$$anonfun$handleExecutorStateChanged$1)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$1$$anonfun$apply$7/Worker$$anonfun$handleExecutorStateChanged$1$$anonfun$apply$7(org.apache.spark.deploy.worker.Worker$$anonfun$handleExecutorStateChanged$1)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$1$$anonfun$apply$9/Worker$$anonfun$handleExecutorStateChanged$1$$anonfun$apply$9(org.apache.spark.deploy.worker.Worker$$anonfun$handleExecutorStateChanged$1)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$1$$anonfun$apply$8/Worker$$anonfun$handleExecutorStateChanged$1$$anonfun$apply$8(org.apache.spark.deploy.worker.Worker$$anonfun$handleExecutorStateChanged$1)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/ui/MasterPage/org$apache$spark$deploy$master$ui$MasterPage$$appRow(org.apache.spark.deploy.master.ApplicationInfo)|",
    "called": "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/coresGranted()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/state()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///java/lang/String/toString()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/submitDate()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationState$/RUNNING()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/formatDuration(long)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI/killEnabled()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/formatDate(java.util.Date)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/name()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/user()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/desc()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/megabytesToString(long)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/duration()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationState$/WAITING()|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/id()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/appUiUrl()|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/memoryPerExecutorMB()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/coresGranted()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/state()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///java/lang/String/toString()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/curAppUIUrl()|",
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/submitDate()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationState$/RUNNING()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/formatDuration(long)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI/killEnabled()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/formatDate(java.util.Date)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/name()|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/user()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/desc()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/megabytesToString(long)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/duration()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationState$/WAITING()|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/id()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/memoryPerExecutorMB()|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$remove$1/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/freeMemory()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryEntry/size()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$remove$1/apply()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$blocksMemoryUsed()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/storage/MemoryEntry/size()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$maxMemory()|",
      "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$remove$1/apply()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/RDDInfo$/fromRdd(org.apache.spark.rdd.RDD)|",
    "called": "|java+method:///scala/Option/getOrElse(scala.Function0)|",
    "v1Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/storage/RDDInfo$$anonfun$1/RDDInfo$$anonfun$1(org.apache.spark.rdd.RDD)|",
      "|java+method:///org/apache/spark/rdd/RDD/name()|",
      "|java+method:///org/apache/spark/rdd/RDD/scope()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/RDD/getStorageLevel()|",
      "|java+method:///org/apache/spark/rdd/RDD/id()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/rdd/RDD/dependencies()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+constructor:///org/apache/spark/storage/RDDInfo$$anonfun$2/RDDInfo$$anonfun$2()|",
      "|java+constructor:///org/apache/spark/storage/RDDInfo/RDDInfo(int,java.lang.String,int,org.apache.spark.storage.StorageLevel,scala.collection.Seq,scala.Option)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/storage/RDDInfo$$anonfun$1/RDDInfo$$anonfun$1(org.apache.spark.rdd.RDD)|",
      "|java+method:///org/apache/spark/rdd/RDD/name()|",
      "|java+method:///org/apache/spark/rdd/RDD/scope()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/storage/RDDInfo$$anonfun$2/RDDInfo$$anonfun$2()|",
      "|java+method:///org/apache/spark/util/CallSite/shortForm()|",
      "|java+method:///org/apache/spark/rdd/RDD/getStorageLevel()|",
      "|java+method:///org/apache/spark/rdd/RDD/id()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/storage/RDDInfo/RDDInfo(int,java.lang.String,int,org.apache.spark.storage.StorageLevel,scala.collection.Seq,java.lang.String,scala.Option)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/rdd/RDD/creationSite()|",
      "|java+method:///org/apache/spark/rdd/RDD/dependencies()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$getMatchingBlockIds$1$$anonfun$5/apply()|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/BlockManagerInfo/blocks()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$getMatchingBlockIds$1$$anonfun$5/apply()|",
      "|java+method:///scala/collection/Iterable/filter(scala.Function1)|",
      "|java+method:///scala/collection/mutable/Map/keys()|",
      "|java+method:///scala/collection/TraversableOnce/toSeq()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/BlockManagerInfo/blocks()|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$getMatchingBlockIds$1$$anonfun$5/apply()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/Iterable/filter(scala.Function1)|",
      "|java+method:///scala/collection/MapLike/keys()|",
      "|java+method:///scala/collection/TraversableOnce/toSeq()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$101/TaskDataSource$$anonfun$101(org.apache.spark.ui.jobs.TaskDataSource)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$22/apply()|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$22/apply()|",
      "|java+method:///org/apache/spark/deploy/DriverDescription/command()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/deploy/Command/mainClass()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$22/apply()|",
      "|java+method:///org/apache/spark/util/Utils$/megabytesToString(long)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$class/getResource(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils,java.util.List,java.lang.String)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///scala/collection/mutable/Buffer/filter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$getResource$1/MesosSchedulerUtils$$anonfun$getResource$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils,java.lang.String)|",
      "|java+method:///scala/collection/TraversableOnce/sum(scala.math.Numeric)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToDouble(java.lang.Object)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$getResource$2/MesosSchedulerUtils$$anonfun$getResource$2(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$getResource$1/MesosSchedulerUtils$$anonfun$getResource$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils,java.lang.String)|",
      "|java+method:///scala/collection/TraversableLike/filter(scala.Function1)|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToDouble(java.lang.Object)|",
      "|java+method:///scala/collection/TraversableOnce/sum(scala.math.Numeric)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$getResource$2/MesosSchedulerUtils$$anonfun$getResource$2(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/CommandUtils$/buildCommandSeq(org.apache.spark.deploy.Command,int,java.lang.String)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/Buffer/toSeq()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/Command/arguments()|",
      "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/buildCommand()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/deploy/Command/mainClass()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/launcher/WorkerCommandBuilder/WorkerCommandBuilder(java.lang.String,int,org.apache.spark.deploy.Command)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/launcher/WorkerCommandBuilder/WorkerCommandBuilder(java.lang.String,int,org.apache.spark.deploy.Command)|",
      "|java+method:///scala/collection/mutable/Buffer/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///org/apache/spark/deploy/Command/arguments()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/buildCommand()|",
      "|java+method:///org/apache/spark/deploy/Command/mainClass()|",
      "|java+method:///scala/collection/mutable/BufferLike/$plus$plus(scala.collection.GenTraversableOnce)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/partial/GroupedCountEvaluator/currentResult()|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///org/apache/spark/partial/GroupedCountEvaluator/currentResult()|",
      "|java+method:///org/apache/spark/partial/GroupedCountEvaluator/sums()|",
      "|java+method:///org/apache/spark/partial/GroupedCountEvaluator/outputsMerged()|",
      "|java+method:///org/apache/spark/util/collection/OpenHashMap/size()|",
      "|java+constructor:///java/util/HashMap/HashMap(int)|",
      "|java+method:///org/apache/commons/math3/distribution/NormalDistribution/inverseCumulativeProbability(double)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///org/apache/spark/util/collection/OpenHashMap/foreach(scala.Function1)|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+constructor:///org/apache/commons/math3/distribution/NormalDistribution/NormalDistribution()|",
      "|java+constructor:///org/apache/spark/partial/GroupedCountEvaluator$$anonfun$currentResult$2/GroupedCountEvaluator$$anonfun$currentResult$2(org.apache.spark.partial.GroupedCountEvaluator,double,double,java.util.HashMap)|",
      "|java+constructor:///org/apache/spark/partial/GroupedCountEvaluator$$anonfun$currentResult$1/GroupedCountEvaluator$$anonfun$currentResult$1(org.apache.spark.partial.GroupedCountEvaluator,java.util.HashMap)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/partial/GroupedCountEvaluator/currentResult()|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///org/apache/spark/partial/GroupedCountEvaluator/sums()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///org/apache/spark/partial/GroupedCountEvaluator/outputsMerged()|",
      "|java+method:///org/apache/spark/util/collection/OpenHashMap/size()|",
      "|java+constructor:///java/util/HashMap/HashMap(int)|",
      "|java+method:///org/apache/commons/math3/distribution/NormalDistribution/inverseCumulativeProbability(double)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///org/apache/spark/util/collection/OpenHashMap/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/commons/math3/distribution/NormalDistribution/NormalDistribution()|",
      "|java+constructor:///org/apache/spark/partial/GroupedCountEvaluator$$anonfun$currentResult$2/GroupedCountEvaluator$$anonfun$currentResult$2(org.apache.spark.partial.GroupedCountEvaluator,double,double,java.util.HashMap)|",
      "|java+constructor:///org/apache/spark/partial/GroupedCountEvaluator$$anonfun$currentResult$1/GroupedCountEvaluator$$anonfun$currentResult$1(org.apache.spark.partial.GroupedCountEvaluator,java.util.HashMap)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkEnv$$anonfun$6/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/SparkEnv$$anonfun$6/apply(scala.Tuple2)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkEnv$$anonfun$6/apply(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGSchedulerEventProcessLoop/doOnReceive(org.apache.spark.scheduler.DAGSchedulerEvent)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleJobSubmitted(int,org.apache.spark.rdd.RDD,scala.Function2,int%5B%5D,org.apache.spark.util.CallSite,org.apache.spark.scheduler.JobListener,java.util.Properties)|",
      "|java+method:///org/apache/spark/scheduler/JobSubmitted/finalRDD()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/doCancelAllJobs()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleJobCancellation(int,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/JobSubmitted/func()|",
      "|java+method:///org/apache/spark/scheduler/JobSubmitted/callSite()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/resubmitFailedStages()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleExecutorAdded(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleJobGroupCancelled(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleJobCancellation$default$2()|",
      "|java+method:///org/apache/spark/scheduler/JobSubmitted/jobId()|",
      "|java+method:///org/apache/spark/scheduler/BeginEvent/task()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleTaskCompletion(org.apache.spark.scheduler.CompletionEvent)|",
      "|java+method:///org/apache/spark/scheduler/ExecutorAdded/host()|",
      "|java+method:///org/apache/spark/scheduler/ExecutorLost/execId()|",
      "|java+method:///org/apache/spark/scheduler/JobSubmitted/properties()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleExecutorLost(java.lang.String,boolean,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/JobSubmitted/listener()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleStageCancellation(int)|",
      "|java+method:///org/apache/spark/scheduler/ExecutorAdded/execId()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetFailed/taskSet()|",
      "|java+method:///org/apache/spark/scheduler/GettingResultEvent/taskInfo()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleExecutorLost$default$3()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleGetTaskResult(org.apache.spark.scheduler.TaskInfo)|",
      "|java+method:///org/apache/spark/scheduler/BeginEvent/taskInfo()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleTaskSetFailed(org.apache.spark.scheduler.TaskSet,java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetFailed/reason()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleBeginEvent(org.apache.spark.scheduler.Task,org.apache.spark.scheduler.TaskInfo)|",
      "|java+method:///org/apache/spark/scheduler/StageCancelled/stageId()|",
      "|java+method:///org/apache/spark/scheduler/JobCancelled/jobId()|",
      "|java+method:///org/apache/spark/scheduler/JobGroupCancelled/groupId()|",
      "|java+method:///org/apache/spark/scheduler/JobSubmitted/partitions()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetFailed/exception()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/MapStageSubmitted/jobId()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleJobSubmitted(int,org.apache.spark.rdd.RDD,scala.Function2,int%5B%5D,org.apache.spark.util.CallSite,org.apache.spark.scheduler.JobListener,java.util.Properties)|",
      "|java+method:///org/apache/spark/scheduler/JobSubmitted/finalRDD()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/doCancelAllJobs()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleJobCancellation(int,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/JobSubmitted/callSite()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleExecutorAdded(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleJobGroupCancelled(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleJobCancellation$default$2()|",
      "|java+method:///org/apache/spark/scheduler/JobSubmitted/jobId()|",
      "|java+method:///org/apache/spark/scheduler/BeginEvent/task()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleTaskCompletion(org.apache.spark.scheduler.CompletionEvent)|",
      "|java+method:///org/apache/spark/scheduler/MapStageSubmitted/properties()|",
      "|java+method:///org/apache/spark/scheduler/ExecutorAdded/host()|",
      "|java+method:///org/apache/spark/scheduler/ExecutorLost/execId()|",
      "|java+method:///org/apache/spark/scheduler/JobSubmitted/properties()|",
      "|java+method:///org/apache/spark/scheduler/MapStageSubmitted/listener()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleMapStageSubmitted(int,org.apache.spark.ShuffleDependency,org.apache.spark.util.CallSite,org.apache.spark.scheduler.JobListener,java.util.Properties)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleExecutorLost(java.lang.String,boolean,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/JobSubmitted/listener()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleStageCancellation(int)|",
      "|java+method:///org/apache/spark/scheduler/ExecutorAdded/execId()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetFailed/taskSet()|",
      "|java+method:///org/apache/spark/scheduler/GettingResultEvent/taskInfo()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleExecutorLost$default$3()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleGetTaskResult(org.apache.spark.scheduler.TaskInfo)|",
      "|java+method:///org/apache/spark/scheduler/BeginEvent/taskInfo()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleTaskSetFailed(org.apache.spark.scheduler.TaskSet,java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetFailed/reason()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleBeginEvent(org.apache.spark.scheduler.Task,org.apache.spark.scheduler.TaskInfo)|",
      "|java+method:///org/apache/spark/scheduler/MapStageSubmitted/callSite()|",
      "|java+method:///org/apache/spark/scheduler/JobSubmitted/func()|",
      "|java+method:///org/apache/spark/scheduler/StageCancelled/stageId()|",
      "|java+method:///org/apache/spark/scheduler/JobCancelled/jobId()|",
      "|java+method:///org/apache/spark/scheduler/JobGroupCancelled/groupId()|",
      "|java+method:///org/apache/spark/scheduler/JobSubmitted/partitions()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetFailed/exception()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/resubmitFailedStages()|",
      "|java+method:///org/apache/spark/scheduler/MapStageSubmitted/dependency()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$28/StagePage$$anonfun$28(org.apache.spark.ui.jobs.StagePage)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$43/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$43/apply(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$43/apply(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore/putBytes(org.apache.spark.storage.BlockId,java.nio.ByteBuffer,org.apache.spark.storage.StorageLevel)|",
    "called": "|java+method:///scala/util/Right$/apply(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/MemoryStore/putIterator(org.apache.spark.storage.BlockId,scala.collection.Iterator,org.apache.spark.storage.StorageLevel,boolean)|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataDeserialize(org.apache.spark.storage.BlockId,java.nio.ByteBuffer,org.apache.spark.serializer.Serializer)|",
      "|java+method:///java/nio/ByteBuffer/duplicate()|",
      "|java+method:///org/apache/spark/storage/StorageLevel/deserialized()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataDeserialize$default$3()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/tryToPut(org.apache.spark.storage.BlockId,java.lang.Object,long,boolean)|",
      "|java+method:///scala/package$/Right()|",
      "|java+method:///org/apache/spark/storage/BlockStore/blockManager()|",
      "|java+constructor:///org/apache/spark/storage/PutResult/PutResult(long,scala.util.Either,scala.collection.Seq)|",
      "|java+method:///java/nio/ByteBuffer/rewind()|",
      "|java+method:///scala/util/Right$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/ResultWithDroppedBlocks/droppedBlocks()|",
      "|java+method:///java/nio/ByteBuffer/limit()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/MemoryStore/putIterator(org.apache.spark.storage.BlockId,scala.collection.Iterator,org.apache.spark.storage.StorageLevel,boolean)|",
      "|java+method:///java/nio/ByteBuffer/duplicate()|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///java/nio/ByteBuffer/limit()|",
      "|java+method:///org/apache/spark/storage/BlockStore/blockManager()|",
      "|java+constructor:///org/apache/spark/storage/PutResult/PutResult(long,scala.util.Either,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/deserialized()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/tryToPut(org.apache.spark.storage.BlockId,java.lang.Object,long,boolean,scala.collection.mutable.Buffer)|",
      "|java+method:///scala/package$/Right()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataDeserialize(org.apache.spark.storage.BlockId,java.nio.ByteBuffer)|",
      "|java+method:///java/nio/ByteBuffer/rewind()|",
      "|java+method:///scala/util/Right$/apply(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$98/TaskDataSource$$anonfun$98(org.apache.spark.ui.jobs.TaskDataSource)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$/rddInfoToJson(org.apache.spark.storage.RDDInfo)|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/RDDInfo/numCachedPartitions()|",
      "|java+method:///org/json4s/JsonDSL$/pair2jvalue(scala.Tuple2,scala.Function1)|",
      "|java+method:///org/apache/spark/storage/RDDInfo/id()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/json4s/JsonDSL$JsonListAssoc/$tilde(scala.Tuple2)|",
      "|java+method:///org/apache/spark/storage/RDDInfo/name()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$5/JsonProtocol$$anonfun$rddInfoToJson$5()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$6/JsonProtocol$$anonfun$rddInfoToJson$6()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$7/JsonProtocol$$anonfun$rddInfoToJson$7()|",
      "|java+method:///org/json4s/JsonDSL$JsonListAssoc/$tilde(org.json4s.JsonAST$JObject)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$8/JsonProtocol$$anonfun$rddInfoToJson$8()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/json4s/JsonDSL$/pair2Assoc(scala.Tuple2,scala.Function1)|",
      "|java+method:///org/json4s/JsonDSL$/jobject2assoc(org.json4s.JsonAST$JObject)|",
      "|java+method:///org/apache/spark/storage/RDDInfo/scope()|",
      "|java+method:///org/apache/spark/storage/RDDInfo/diskSize()|",
      "|java+method:///org/apache/spark/storage/RDDInfo/memSize()|",
      "|java+method:///org/apache/spark/storage/RDDInfo/externalBlockStoreSize()|",
      "|java+method:///org/json4s/JsonDSL$JsonAssoc/$tilde(scala.Tuple2,scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$47/JsonProtocol$$anonfun$47()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$1/JsonProtocol$$anonfun$rddInfoToJson$1()|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/storageLevelToJson(org.apache.spark.storage.StorageLevel)|",
      "|java+constructor:///org/json4s/JsonAST$JArray/JsonAST$JArray(scala.collection.immutable.List)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$2/JsonProtocol$$anonfun$rddInfoToJson$2()|",
      "|java+method:///scala/Predef$/any2ArrowAssoc(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/storage/RDDInfo/storageLevel()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$3/JsonProtocol$$anonfun$rddInfoToJson$3()|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$4/JsonProtocol$$anonfun$rddInfoToJson$4()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$9/JsonProtocol$$anonfun$rddInfoToJson$9()|",
      "|java+method:///org/apache/spark/storage/RDDInfo/parentIds()|",
      "|java+method:///org/apache/spark/storage/RDDInfo/numPartitions()|",
      "|java+method:///scala/collection/TraversableOnce/toList()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/RDDInfo/numCachedPartitions()|",
      "|java+method:///org/json4s/JsonDSL$/pair2jvalue(scala.Tuple2,scala.Function1)|",
      "|java+method:///org/apache/spark/storage/RDDInfo/id()|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/json4s/JsonDSL$JsonListAssoc/$tilde(scala.Tuple2)|",
      "|java+method:///org/apache/spark/storage/RDDInfo/callSite()|",
      "|java+method:///org/apache/spark/storage/RDDInfo/name()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$5/JsonProtocol$$anonfun$rddInfoToJson$5()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$6/JsonProtocol$$anonfun$rddInfoToJson$6()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$7/JsonProtocol$$anonfun$rddInfoToJson$7()|",
      "|java+method:///org/json4s/JsonDSL$JsonListAssoc/$tilde(org.json4s.JsonAST$JObject)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$8/JsonProtocol$$anonfun$rddInfoToJson$8()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/json4s/JsonDSL$/pair2Assoc(scala.Tuple2,scala.Function1)|",
      "|java+method:///org/json4s/JsonDSL$/jobject2assoc(org.json4s.JsonAST$JObject)|",
      "|java+method:///org/apache/spark/storage/RDDInfo/scope()|",
      "|java+method:///org/apache/spark/storage/RDDInfo/diskSize()|",
      "|java+method:///org/apache/spark/storage/RDDInfo/memSize()|",
      "|java+method:///org/apache/spark/storage/RDDInfo/externalBlockStoreSize()|",
      "|java+method:///org/json4s/JsonDSL$JsonAssoc/$tilde(scala.Tuple2,scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$50/JsonProtocol$$anonfun$50()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$10/JsonProtocol$$anonfun$rddInfoToJson$10()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$1/JsonProtocol$$anonfun$rddInfoToJson$1()|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/storageLevelToJson(org.apache.spark.storage.StorageLevel)|",
      "|java+constructor:///org/json4s/JsonAST$JArray/JsonAST$JArray(scala.collection.immutable.List)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$2/JsonProtocol$$anonfun$rddInfoToJson$2()|",
      "|java+method:///scala/Predef$/any2ArrowAssoc(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/storage/RDDInfo/storageLevel()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$3/JsonProtocol$$anonfun$rddInfoToJson$3()|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$4/JsonProtocol$$anonfun$rddInfoToJson$4()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$rddInfoToJson$9/JsonProtocol$$anonfun$rddInfoToJson$9()|",
      "|java+method:///org/apache/spark/storage/RDDInfo/parentIds()|",
      "|java+method:///org/apache/spark/storage/RDDInfo/numPartitions()|",
      "|java+method:///scala/collection/TraversableOnce/toList()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1/apply()|",
    "called": "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
    "v1Body": [
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKey$default$5()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKey$default$6()|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$7/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$7(org.apache.spark.rdd.PairRDDFunctions$$anonfun$countApproxDistinctByKey$1)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$6/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$6(org.apache.spark.rdd.PairRDDFunctions$$anonfun$countApproxDistinctByKey$1)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$5/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$5(org.apache.spark.rdd.PairRDDFunctions$$anonfun$countApproxDistinctByKey$1)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$apply$22/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$apply$22(org.apache.spark.rdd.PairRDDFunctions$$anonfun$countApproxDistinctByKey$1)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$apply$21/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$apply$21(org.apache.spark.rdd.PairRDDFunctions$$anonfun$countApproxDistinctByKey$1)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$apply$19/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$apply$19(org.apache.spark.rdd.PairRDDFunctions$$anonfun$countApproxDistinctByKey$1)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$apply$20/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$apply$20(org.apache.spark.rdd.PairRDDFunctions$$anonfun$countApproxDistinctByKey$1)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/mapValues(scala.Function1)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKey(scala.Function1,scala.Function2,scala.Function2,org.apache.spark.Partitioner,boolean,org.apache.spark.serializer.Serializer)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1/apply()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKeyWithClassTag$default$6()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKeyWithClassTag(scala.Function1,scala.Function2,scala.Function2,org.apache.spark.Partitioner,boolean,org.apache.spark.serializer.Serializer,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1/apply()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$7/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$7(org.apache.spark.rdd.PairRDDFunctions$$anonfun$countApproxDistinctByKey$1)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$6/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$6(org.apache.spark.rdd.PairRDDFunctions$$anonfun$countApproxDistinctByKey$1)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$5/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$5(org.apache.spark.rdd.PairRDDFunctions$$anonfun$countApproxDistinctByKey$1)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/combineByKeyWithClassTag$default$5()|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$apply$22/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$apply$22(org.apache.spark.rdd.PairRDDFunctions$$anonfun$countApproxDistinctByKey$1)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$apply$21/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$apply$21(org.apache.spark.rdd.PairRDDFunctions$$anonfun$countApproxDistinctByKey$1)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$apply$19/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$apply$19(org.apache.spark.rdd.PairRDDFunctions$$anonfun$countApproxDistinctByKey$1)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$apply$20/PairRDDFunctions$$anonfun$countApproxDistinctByKey$1$$anonfun$apply$20(org.apache.spark.rdd.PairRDDFunctions$$anonfun$countApproxDistinctByKey$1)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/mapValues(scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/liftedTree2$1(long,scala.Enumeration$Value,java.nio.ByteBuffer,scala.runtime.ObjectRef)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Predef$/Set()|",
      "|java+method:///org/apache/spark/TaskState$/LOST()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/TaskState$/isFinished(scala.Enumeration$Value)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$liftedTree2$1$2/TaskSchedulerImpl$$anonfun$liftedTree2$1$2(org.apache.spark.scheduler.TaskSchedulerImpl)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/taskIdToTaskSetManager()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/taskResultGetter()|",
      "|java+method:///scala/collection/mutable/HashMap/remove(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskResultGetter/enqueueFailedTask(org.apache.spark.scheduler.TaskSetManager,long,scala.Enumeration$Value,java.nio.ByteBuffer)|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/logError(scala.Function0)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/removeExecutor(java.lang.String)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/activeExecutorIds()|",
      "|java+method:///org/apache/spark/TaskState$/FAILED()|",
      "|java+method:///org/apache/spark/TaskState$/KILLED()|",
      "|java+method:///scala/collection/SetLike/contains(java.lang.Object)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$liftedTree2$1$1/TaskSchedulerImpl$$anonfun$liftedTree2$1$1(org.apache.spark.scheduler.TaskSchedulerImpl,long,scala.Enumeration$Value)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/taskIdToExecutorId()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/TaskState$/FINISHED()|",
      "|java+method:///scala/collection/immutable/Set$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/removeRunningTask(long)|",
      "|java+method:///org/apache/spark/scheduler/TaskResultGetter/enqueueSuccessfulTask(org.apache.spark.scheduler.TaskSetManager,long,java.nio.ByteBuffer)|"
    ],
    "v2Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/SlaveLost/SlaveLost(java.lang.String)|",
      "|java+method:///scala/Predef$/Set()|",
      "|java+method:///org/apache/spark/TaskState$/LOST()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$liftedTree2$1$2/TaskSchedulerImpl$$anonfun$liftedTree2$1$2(org.apache.spark.scheduler.TaskSchedulerImpl,long,scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/taskIdToTaskSetManager()|",
      "|java+method:///scala/collection/mutable/HashMap/remove(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskResultGetter/enqueueFailedTask(org.apache.spark.scheduler.TaskSetManager,long,scala.Enumeration$Value,java.nio.ByteBuffer)|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/TaskState$/FAILED()|",
      "|java+method:///org/apache/spark/TaskState$/KILLED()|",
      "|java+method:///scala/collection/SetLike/contains(java.lang.Object)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/TaskState$/isFinished(scala.Enumeration$Value)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/org$apache$spark$scheduler$TaskSchedulerImpl$$executorIdToTaskCount()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$liftedTree2$1$3/TaskSchedulerImpl$$anonfun$liftedTree2$1$3(org.apache.spark.scheduler.TaskSchedulerImpl)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$liftedTree2$1$1/TaskSchedulerImpl$$anonfun$liftedTree2$1$1(org.apache.spark.scheduler.TaskSchedulerImpl)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/taskIdToExecutorId()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/removeExecutor(java.lang.String,org.apache.spark.scheduler.ExecutorLossReason)|",
      "|java+method:///org/apache/spark/TaskState$/FINISHED()|",
      "|java+method:///scala/collection/immutable/Set$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/removeRunningTask(long)|",
      "|java+method:///org/apache/spark/scheduler/TaskResultGetter/enqueueSuccessfulTask(org.apache.spark.scheduler.TaskSetManager,long,java.nio.ByteBuffer)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/taskResultGetter()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$run$1/apply$mcV$sp()|",
    "called": "|java+method:///scala/Option/orNull(scala.Predef$$less$colon$less)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/shutdown()|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$apply$mcV$sp$5/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$apply$mcV$sp$5(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1,org.apache.spark.SparkEnv)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/stopExecutorDelegationTokenRenewer()|",
      "|java+method:///scala/collection/Seq/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend/CoarseGrainedExecutorBackend(org.apache.spark.rpc.RpcEnv,java.lang.String,java.lang.String,java.lang.String,int,scala.collection.Seq,org.apache.spark.SparkEnv)|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$apply$mcV$sp$3/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$apply$mcV$sp$3(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1,org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/util/Utils$/checkHost(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$apply$mcV$sp$4/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$apply$mcV$sp$4(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1,org.apache.spark.SparkConf)|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$apply$mcV$sp$2/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$apply$mcV$sp$2(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/SparkConf/SparkConf()|",
      "|java+method:///org/apache/spark/SparkConf/contains(java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/startExecutorDelegationTokenRenewer(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkEnv/conf()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpointRefByURI(java.lang.String)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///org/apache/spark/SparkEnv$/createExecutorEnv(org.apache.spark.SparkConf,java.lang.String,java.lang.String,int,int,boolean)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/awaitTermination()|",
      "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend$/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/util/Utils$/checkHost$default$2()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/shutdown()|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,boolean)|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$apply$mcV$sp$5/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$apply$mcV$sp$5(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1,org.apache.spark.SparkEnv)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///org/apache/spark/util/Utils$/checkHost$default$2()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/stopExecutorDelegationTokenRenewer()|",
      "|java+method:///scala/collection/Seq/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend/CoarseGrainedExecutorBackend(org.apache.spark.rpc.RpcEnv,java.lang.String,java.lang.String,java.lang.String,int,scala.collection.Seq,org.apache.spark.SparkEnv)|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$apply$mcV$sp$3/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$apply$mcV$sp$3(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1,org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/util/Utils$/checkHost(java.lang.String,java.lang.String)|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$apply$mcV$sp$4/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$apply$mcV$sp$4(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1,org.apache.spark.SparkConf)|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$apply$mcV$sp$2/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$apply$mcV$sp$2(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1)|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/SparkConf/SparkConf()|",
      "|java+method:///org/apache/spark/SparkConf/contains(java.lang.String)|",
      "|java+method:///scala/Option/orNull(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/startExecutorDelegationTokenRenewer(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkEnv/conf()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpointRefByURI(java.lang.String)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/executor/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$1/CoarseGrainedExecutorBackend$$anonfun$run$1$$anonfun$1(org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1)|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///org/apache/spark/SparkEnv$/createExecutorEnv(org.apache.spark.SparkConf,java.lang.String,java.lang.String,int,int,boolean)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/awaitTermination()|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///org/apache/spark/executor/CoarseGrainedExecutorBackend$/logInfo(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext/stop()|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/ShutdownHookManager$/removeShutdownHook(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_heartbeatReceiver()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_env()|",
      "|java+method:///org/apache/spark/SparkContext/_shutdownHookRef()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$14/SparkContext$$anonfun$stop$14(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/metadataCleaner()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$13/SparkContext$$anonfun$stop$13(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$12/SparkContext$$anonfun$stop$12(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$10/SparkContext$$anonfun$stop$10(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkEnv$/set(org.apache.spark.SparkEnv)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_dagScheduler_$eq(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///org/apache/spark/SparkContext/_taskScheduler_$eq(org.apache.spark.scheduler.TaskScheduler)|",
      "|java+method:///org/apache/spark/util/Utils$/tryLogNonFatalError(scala.Function0)|",
      "|java+method:///java/lang/System/clearProperty(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_dagScheduler()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$1/SparkContext$$anonfun$stop$1(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$2/SparkContext$$anonfun$stop$2(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$3/SparkContext$$anonfun$stop$3(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$4/SparkContext$$anonfun$stop$4(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$5/SparkContext$$anonfun$stop$5(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$6/SparkContext$$anonfun$stop$6(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$7/SparkContext$$anonfun$stop$7(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$11/SparkContext$$anonfun$stop$11(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$8/SparkContext$$anonfun$stop$8(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$9/SparkContext$$anonfun$stop$9(org.apache.spark.SparkContext)|",
      "|java+method:///java/util/concurrent/atomic/AtomicBoolean/compareAndSet(boolean,boolean)|",
      "|java+method:///org/apache/spark/SparkContext/stopped()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_listenerBusStarted()|",
      "|java+method:///org/apache/spark/SparkContext/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext$/clearActiveContext()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/ShutdownHookManager$/removeShutdownHook(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_heartbeatReceiver()|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_env()|",
      "|java+method:///org/apache/spark/SparkContext/_shutdownHookRef()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$14/SparkContext$$anonfun$stop$14(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkContext/metadataCleaner()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$12/SparkContext$$anonfun$stop$12(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$10/SparkContext$$anonfun$stop$10(org.apache.spark.SparkContext)|",
      "|java+method:///org/apache/spark/SparkEnv$/set(org.apache.spark.SparkEnv)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_dagScheduler_$eq(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///org/apache/spark/SparkContext/_taskScheduler_$eq(org.apache.spark.scheduler.TaskScheduler)|",
      "|java+method:///org/apache/spark/util/Utils$/tryLogNonFatalError(scala.Function0)|",
      "|java+method:///java/lang/System/clearProperty(java.lang.String)|",
      "|java+method:///org/apache/spark/util/AsynchronousListenerBus$/withinListenerThread()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$13/SparkContext$$anonfun$stop$13(org.apache.spark.SparkContext)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_dagScheduler()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$1/SparkContext$$anonfun$stop$1(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$2/SparkContext$$anonfun$stop$2(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$3/SparkContext$$anonfun$stop$3(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$4/SparkContext$$anonfun$stop$4(org.apache.spark.SparkContext)|",
      "|java+method:///scala/util/DynamicVariable/value()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$5/SparkContext$$anonfun$stop$5(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$6/SparkContext$$anonfun$stop$6(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$7/SparkContext$$anonfun$stop$7(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$11/SparkContext$$anonfun$stop$11(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$8/SparkContext$$anonfun$stop$8(org.apache.spark.SparkContext)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$stop$9/SparkContext$$anonfun$stop$9(org.apache.spark.SparkContext)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///java/util/concurrent/atomic/AtomicBoolean/compareAndSet(boolean,boolean)|",
      "|java+method:///org/apache/spark/SparkContext/stopped()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$_listenerBusStarted()|",
      "|java+method:///org/apache/spark/SparkContext/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext$/clearActiveContext()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler/handleJobSubmitted(int,org.apache.spark.rdd.RDD,scala.Function2,int%5B%5D,org.apache.spark.util.CallSite,org.apache.spark.scheduler.JobListener,java.util.Properties)|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/newResultStage(org.apache.spark.rdd.RDD,int,int,org.apache.spark.util.CallSite)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/JobListener/jobFailed(java.lang.Exception)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleJobSubmitted$1/DAGScheduler$$anonfun$handleJobSubmitted$1(org.apache.spark.scheduler.DAGScheduler,int)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$12/DAGScheduler$$anonfun$12(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/activeJobs()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleJobSubmitted$2/DAGScheduler$$anonfun$handleJobSubmitted$2(org.apache.spark.scheduler.DAGScheduler,int%5B%5D,org.apache.spark.util.CallSite,org.apache.spark.scheduler.ActiveJob)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/jobId()|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/collection/mutable/ArrayOps/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/jobIdToActiveJob()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///scala/reflect/ClassTag$/Int()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/clearCacheLocs()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/submitWaitingStages()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/jobIdToStageIds()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$submitStage(org.apache.spark.scheduler.Stage)|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///scala/Predef$/intArrayOps(int%5B%5D)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleJobSubmitted$4/DAGScheduler$$anonfun$handleJobSubmitted$4(org.apache.spark.scheduler.DAGScheduler,scala.runtime.ObjectRef)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleJobSubmitted$3/DAGScheduler$$anonfun$handleJobSubmitted$3(org.apache.spark.scheduler.DAGScheduler,scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleJobSubmitted$5/DAGScheduler$$anonfun$handleJobSubmitted$5(org.apache.spark.scheduler.DAGScheduler,scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/scheduler/ActiveJob/ActiveJob(int,org.apache.spark.scheduler.ResultStage,scala.Function2,int%5B%5D,org.apache.spark.util.CallSite,org.apache.spark.scheduler.JobListener,java.util.Properties)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerJobStart/SparkListenerJobStart(int,long,scala.collection.Seq,java.util.Properties)|",
      "|java+method:///org/apache/spark/scheduler/ResultStage/resultOfJob_$eq(scala.Option)|",
      "|java+method:///scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerJobStart/SparkListenerJobStart(int,long,scala.collection.Seq,java.util.Properties)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/newResultStage(org.apache.spark.rdd.RDD,scala.Function2,int%5B%5D,int,org.apache.spark.util.CallSite)|",
      "|java+method:///org/apache/spark/scheduler/JobListener/jobFailed(java.lang.Exception)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleJobSubmitted$1/DAGScheduler$$anonfun$handleJobSubmitted$1(org.apache.spark.scheduler.DAGScheduler,int)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$11/DAGScheduler$$anonfun$11(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/activeJobs()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleJobSubmitted$2/DAGScheduler$$anonfun$handleJobSubmitted$2(org.apache.spark.scheduler.DAGScheduler,int%5B%5D,org.apache.spark.util.CallSite,org.apache.spark.scheduler.ActiveJob)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/collection/mutable/ArrayOps/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/jobIdToActiveJob()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///scala/reflect/ClassTag$/Int()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/clearCacheLocs()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/submitWaitingStages()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/jobIdToStageIds()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$submitStage(org.apache.spark.scheduler.Stage)|",
      "|java+method:///scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)|",
      "|java+method:///scala/Predef$/intArrayOps(int%5B%5D)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleJobSubmitted$4/DAGScheduler$$anonfun$handleJobSubmitted$4(org.apache.spark.scheduler.DAGScheduler,scala.runtime.ObjectRef)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleJobSubmitted$3/DAGScheduler$$anonfun$handleJobSubmitted$3(org.apache.spark.scheduler.DAGScheduler,scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$handleJobSubmitted$5/DAGScheduler$$anonfun$handleJobSubmitted$5(org.apache.spark.scheduler.DAGScheduler,scala.runtime.ObjectRef)|",
      "|java+method:///scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/scheduler/LiveListenerBus/post(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/ActiveJob/jobId()|",
      "|java+method:///org/apache/spark/scheduler/ResultStage/setActiveJob(org.apache.spark.scheduler.ActiveJob)|",
      "|java+constructor:///org/apache/spark/scheduler/ActiveJob/ActiveJob(int,org.apache.spark.scheduler.Stage,org.apache.spark.util.CallSite,org.apache.spark.scheduler.JobListener,java.util.Properties)|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/ClosureCleaner$/getClassReader(java.lang.Class)|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///java/lang/Class/getResourceAsStream(java.lang.String)|",
      "|java+constructor:///java/io/ByteArrayOutputStream/ByteArrayOutputStream(int)|",
      "|java+constructor:///java/io/ByteArrayInputStream/ByteArrayInputStream(byte%5B%5D)|",
      "|java+method:///java/lang/Class/getName()|",
      "|java+method:///java/lang/String/replaceFirst(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/copyStream(java.io.InputStream,java.io.OutputStream,boolean,boolean)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/util/Utils$/copyStream$default$4()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///java/io/ByteArrayOutputStream/toByteArray()|",
      "|java+constructor:///com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/ClassReader/ClassReader(java.io.InputStream)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Class/getResourceAsStream(java.lang.String)|",
      "|java+constructor:///java/io/ByteArrayOutputStream/ByteArrayOutputStream(int)|",
      "|java+constructor:///java/io/ByteArrayInputStream/ByteArrayInputStream(byte%5B%5D)|",
      "|java+method:///java/lang/Class/getName()|",
      "|java+constructor:///org/apache/xbean/asm5/ClassReader/ClassReader(java.io.InputStream)|",
      "|java+method:///java/lang/String/replaceFirst(java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/util/Utils$/copyStream(java.io.InputStream,java.io.OutputStream,boolean,boolean)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/util/Utils$/copyStream$default$4()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///java/io/ByteArrayOutputStream/toByteArray()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$$anonfun$mapPartitionsWithIndex$1/apply(int,scala.collection.Iterator)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/java/JavaPairRDD$/toScalaFunction2(org.apache.spark.api.java.function.Function2)|",
      "|java+method:///scala/Function2/apply(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaIterator(java.util.Iterator)|",
      "|java+method:///scala/Predef$/int2Integer(int)|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterator(scala.collection.Iterator)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///org/apache/spark/api/java/function/Function2/call(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaIteratorConverter(java.util.Iterator)|",
      "|java+method:///scala/Predef$/int2Integer(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext$$anonfun$hadoopFile$1/apply()|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/broadcast(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkContext/hadoopConfiguration()|",
      "|java+constructor:///org/apache/spark/rdd/HadoopRDD/HadoopRDD(org.apache.spark.SparkContext,org.apache.spark.broadcast.Broadcast,scala.Option,java.lang.Class,java.lang.Class,java.lang.Class,int)|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/setName(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$hadoopFile$1$$anonfun$32/SparkContext$$anonfun$hadoopFile$1$$anonfun$32(org.apache.spark.SparkContext$$anonfun$hadoopFile$1)|",
      "|java+method:///org/apache/spark/SparkContext$$anonfun$hadoopFile$1/apply()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$assertNotStopped()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/broadcast(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkContext/hadoopConfiguration()|",
      "|java+constructor:///org/apache/spark/rdd/HadoopRDD/HadoopRDD(org.apache.spark.SparkContext,org.apache.spark.broadcast.Broadcast,scala.Option,java.lang.Class,java.lang.Class,java.lang.Class,int)|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/setName(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$hadoopFile$1$$anonfun$33/SparkContext$$anonfun$hadoopFile$1$$anonfun$33(org.apache.spark.SparkContext$$anonfun$hadoopFile$1)|",
      "|java+method:///org/apache/spark/SparkContext$$anonfun$hadoopFile$1/apply()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$assertNotStopped()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/ClosureCleaner$/getInnerClosureClasses(java.lang.Object)|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/immutable/List/isEmpty()|",
      "|java+method:///scala/collection/immutable/List/tail()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/Set/$minus$minus(scala.collection.GenTraversableOnce)|",
      "|java+method:///scala/collection/mutable/Set/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$getInnerClosureClasses$1/ClosureCleaner$$anonfun$getInnerClosureClasses$1(scala.collection.mutable.Set,scala.runtime.ObjectRef)|",
      "|java+method:///scala/collection/immutable/List/head()|",
      "|java+method:///scala/collection/mutable/Set$/apply(scala.collection.Seq)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+constructor:///org/apache/spark/util/InnerClosureFinder/InnerClosureFinder(scala.collection.mutable.Set)|",
      "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/Set/$minus(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Set/toList()|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getClassReader(java.lang.Class)|",
      "|java+method:///com/esotericsoftware/reflectasm/shaded/org/objectweb/asm/ClassReader/accept(com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassVisitor,int)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/immutable/List/isEmpty()|",
      "|java+method:///scala/collection/immutable/List/tail()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/Set/$minus$minus(scala.collection.GenTraversableOnce)|",
      "|java+method:///scala/collection/mutable/Set/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/ClosureCleaner$$anonfun$getInnerClosureClasses$1/ClosureCleaner$$anonfun$getInnerClosureClasses$1(scala.collection.mutable.Set,scala.runtime.ObjectRef)|",
      "|java+method:///scala/collection/immutable/List/head()|",
      "|java+method:///scala/collection/mutable/Set$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/util/ClosureCleaner$/getClassReader(java.lang.Class)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+constructor:///org/apache/spark/util/InnerClosureFinder/InnerClosureFinder(scala.collection.mutable.Set)|",
      "|java+method:///org/apache/xbean/asm5/ClassReader/accept(org.apache.xbean.asm5.ClassVisitor,int)|",
      "|java+method:///scala/collection/immutable/List$/apply(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/Set/$minus(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Set/toList()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$7/DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1$$anonfun$apply$7(org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanupStateForJobAndIndependentStages$3$$anonfun$removeStage$1$1)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$7/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$7/apply()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/name()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$7/apply()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$84/TaskDataSource$$anonfun$84(org.apache.spark.ui.jobs.TaskDataSource)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$$anonfun$4/apply(java.lang.String)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$executorsPendingToRemove()|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$executorDataMap()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$12/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$12/apply(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$12/apply(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$103/TaskDataSource$$anonfun$103(org.apache.spark.ui.jobs.TaskDataSource)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/JavaToWritableConverter/org$apache$spark$api$python$JavaToWritableConverter$$convertToWritable(java.lang.Object)|",
    "called": "|java+method:///scala/Predef$/Long2long(java.lang.Long)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/hadoop/io/ArrayWritable/ArrayWritable(java.lang.Class)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/hadoop/io/BytesWritable/BytesWritable(byte%5B%5D)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/api/python/JavaToWritableConverter$$anonfun$org$apache$spark$api$python$JavaToWritableConverter$$convertToWritable$1/JavaToWritableConverter$$anonfun$org$apache$spark$api$python$JavaToWritableConverter$$convertToWritable$1(org.apache.spark.api.python.JavaToWritableConverter,org.apache.hadoop.io.MapWritable)|",
      "|java+method:///scala/Predef$/Double2double(java.lang.Double)|",
      "|java+constructor:///org/apache/hadoop/io/Text/Text(java.lang.String)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+method:///scala/Predef$/Integer2int(java.lang.Integer)|",
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+constructor:///org/apache/hadoop/io/FloatWritable/FloatWritable(float)|",
      "|java+method:///scala/Predef$/Long2long(java.lang.Long)|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+constructor:///org/apache/hadoop/io/IntWritable/IntWritable(int)|",
      "|java+constructor:///org/apache/hadoop/io/DoubleWritable/DoubleWritable(double)|",
      "|java+constructor:///org/apache/spark/api/python/JavaToWritableConverter$$anonfun$org$apache$spark$api$python$JavaToWritableConverter$$convertToWritable$2/JavaToWritableConverter$$anonfun$org$apache$spark$api$python$JavaToWritableConverter$$convertToWritable$2(org.apache.spark.api.python.JavaToWritableConverter)|",
      "|java+method:///scala/Predef$/Boolean2boolean(java.lang.Boolean)|",
      "|java+method:///scala/Predef$/Float2float(java.lang.Float)|",
      "|java+method:///java/lang/Class/getName()|",
      "|java+constructor:///org/apache/hadoop/io/MapWritable/MapWritable()|",
      "|java+method:///scala/collection/mutable/Map/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/hadoop/io/ArrayWritable/set(org.apache.hadoop.io.Writable%5B%5D)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/hadoop/io/LongWritable/LongWritable(long)|",
      "|java+constructor:///org/apache/hadoop/io/BooleanWritable/BooleanWritable(boolean)|",
      "|java+method:///org/apache/hadoop/io/NullWritable/get()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/hadoop/io/ArrayWritable/ArrayWritable(java.lang.Class)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/Long2long(java.lang.Long)|",
      "|java+constructor:///org/apache/hadoop/io/BytesWritable/BytesWritable(byte%5B%5D)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/api/python/JavaToWritableConverter$$anonfun$org$apache$spark$api$python$JavaToWritableConverter$$convertToWritable$1/JavaToWritableConverter$$anonfun$org$apache$spark$api$python$JavaToWritableConverter$$convertToWritable$1(org.apache.spark.api.python.JavaToWritableConverter,org.apache.hadoop.io.MapWritable)|",
      "|java+method:///scala/Predef$/Double2double(java.lang.Double)|",
      "|java+constructor:///org/apache/hadoop/io/Text/Text(java.lang.String)|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+method:///scala/Predef$/Integer2int(java.lang.Integer)|",
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+constructor:///org/apache/hadoop/io/FloatWritable/FloatWritable(float)|",
      "|java+constructor:///org/apache/hadoop/io/IntWritable/IntWritable(int)|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+constructor:///org/apache/hadoop/io/DoubleWritable/DoubleWritable(double)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/api/python/JavaToWritableConverter$$anonfun$org$apache$spark$api$python$JavaToWritableConverter$$convertToWritable$2/JavaToWritableConverter$$anonfun$org$apache$spark$api$python$JavaToWritableConverter$$convertToWritable$2(org.apache.spark.api.python.JavaToWritableConverter)|",
      "|java+method:///scala/Predef$/Boolean2boolean(java.lang.Boolean)|",
      "|java+method:///scala/Predef$/Float2float(java.lang.Float)|",
      "|java+method:///java/lang/Class/getName()|",
      "|java+constructor:///org/apache/hadoop/io/MapWritable/MapWritable()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/hadoop/io/ArrayWritable/set(org.apache.hadoop.io.Writable%5B%5D)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/hadoop/io/LongWritable/LongWritable(long)|",
      "|java+constructor:///org/apache/hadoop/io/BooleanWritable/BooleanWritable(boolean)|",
      "|java+method:///org/apache/hadoop/io/NullWritable/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$108/TaskDataSource$$anonfun$108(org.apache.spark.ui.jobs.TaskDataSource)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$ExternalIterator$$anonfun$4/ExternalAppendOnlyMap$ExternalIterator$$anonfun$4(org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaPairRDD$$anonfun$cogroupResultToJava$1/apply(scala.Tuple2)|",
    "called": "|java+method:///scala/collection/JavaConverters$/asJavaIterableConverter(scala.collection.Iterable)|",
    "v1Body": [
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterable(scala.collection.Iterable)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIterableConverter(scala.collection.Iterable)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$toLocalIterator$1/apply()|",
    "called": "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$toLocalIterator$1/apply()|",
      "|java+method:///scala/collection/Iterator/flatMap(scala.Function1)|",
      "|java+method:///scala/collection/immutable/Range/iterator()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$toLocalIterator$1$$anonfun$apply$30/RDD$$anonfun$toLocalIterator$1$$anonfun$apply$30(org.apache.spark.rdd.RDD$$anonfun$toLocalIterator$1)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$toLocalIterator$1/apply()|",
      "|java+method:///scala/collection/Iterator/flatMap(scala.Function1)|",
      "|java+method:///scala/collection/immutable/Range/iterator()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$toLocalIterator$1$$anonfun$apply$34/RDD$$anonfun$toLocalIterator$1$$anonfun$apply$34(org.apache.spark.rdd.RDD$$anonfun$toLocalIterator$1)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$5/DAGScheduler$$anonfun$5(org.apache.spark.scheduler.DAGScheduler)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anon$2/run()|",
    "called": "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
    "v1Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/self()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$workerId()|",
      "|java+method:///org/apache/spark/deploy/master/Master$/SYSTEM_NAME()|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$RegisterWorker/DeployMessages$RegisterWorker(java.lang.String,java.lang.String,int,org.apache.spark.rpc.RpcEndpointRef,int,int,int,java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpointRef(java.lang.String,org.apache.spark.rpc.RpcAddress,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$webUi()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$port()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$publicAddress()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/rpcEnv()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1/org$apache$spark$deploy$worker$Worker$$anonfun$$$outer()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$host()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/worker/ui/WorkerWebUI/boundPort()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/deploy/master/Master$/ENDPOINT_NAME()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anon$2$$anonfun$run$7/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anon$2$$anonfun$run$7(org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anon$2)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anon$2$$anonfun$run$8/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anon$2$$anonfun$run$8(org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anon$2)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/logInfo(scala.Function0)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/org$apache$spark$deploy$worker$Worker$$registerWithMaster(org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///org/apache/spark/deploy/master/Master$/SYSTEM_NAME()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpointRef(java.lang.String,org.apache.spark.rpc.RpcAddress,java.lang.String)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker/rpcEnv()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1/org$apache$spark$deploy$worker$Worker$$anonfun$$$outer()|",
      "|java+method:///org/apache/spark/deploy/master/Master$/ENDPOINT_NAME()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anon$2$$anonfun$run$7/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anon$2$$anonfun$run$7(org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anon$2)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anon$2$$anonfun$run$8/Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anon$2$$anonfun$run$8(org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$reregisterWithMaster$1$$anon$2)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$removeDriver(java.lang.String,scala.Enumeration$Value,scala.Option)|",
    "called": "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/math/package$/max(int,int)|",
      "|java+method:///org/apache/spark/deploy/master/Master/RETAINED_DRIVERS()|",
      "|java+method:///org/apache/spark/deploy/master/DriverInfo/exception_$eq(scala.Option)|",
      "|java+method:///scala/collection/mutable/HashSet/find(scala.Function1)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$org$apache$spark$deploy$master$Master$$removeDriver$2/Master$$anonfun$org$apache$spark$deploy$master$Master$$removeDriver$2(org.apache.spark.deploy.master.Master,org.apache.spark.deploy.master.DriverInfo)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$org$apache$spark$deploy$master$Master$$removeDriver$3/Master$$anonfun$org$apache$spark$deploy$master$Master$$removeDriver$3(org.apache.spark.deploy.master.Master,java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$18/Master$$anonfun$18(org.apache.spark.deploy.master.Master,java.lang.String)|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$completedDrivers()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$drivers()|",
      "|java+method:///org/apache/spark/deploy/master/PersistenceEngine/removeDriver(org.apache.spark.deploy.master.DriverInfo)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/logWarning(scala.Function0)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/size()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/trimStart(int)|",
      "|java+method:///org/apache/spark/deploy/master/DriverInfo/worker()|",
      "|java+method:///org/apache/spark/deploy/master/DriverInfo/state_$eq(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/master/Master/logInfo(scala.Function0)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$org$apache$spark$deploy$master$Master$$removeDriver$1/Master$$anonfun$org$apache$spark$deploy$master$Master$$removeDriver$1(org.apache.spark.deploy.master.Master,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$persistenceEngine()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$schedule()|"
    ],
    "v2Body": [
      "|java+method:///scala/math/package$/max(int,int)|",
      "|java+method:///org/apache/spark/deploy/master/Master/RETAINED_DRIVERS()|",
      "|java+method:///org/apache/spark/deploy/master/DriverInfo/exception_$eq(scala.Option)|",
      "|java+method:///scala/collection/mutable/HashSet/find(scala.Function1)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$org$apache$spark$deploy$master$Master$$removeDriver$2/Master$$anonfun$org$apache$spark$deploy$master$Master$$removeDriver$2(org.apache.spark.deploy.master.Master,org.apache.spark.deploy.master.DriverInfo)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$org$apache$spark$deploy$master$Master$$removeDriver$3/Master$$anonfun$org$apache$spark$deploy$master$Master$$removeDriver$3(org.apache.spark.deploy.master.Master,java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$completedDrivers()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$20/Master$$anonfun$20(org.apache.spark.deploy.master.Master,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$drivers()|",
      "|java+method:///org/apache/spark/deploy/master/PersistenceEngine/removeDriver(org.apache.spark.deploy.master.DriverInfo)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/logWarning(scala.Function0)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/size()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/trimStart(int)|",
      "|java+method:///org/apache/spark/deploy/master/DriverInfo/worker()|",
      "|java+method:///org/apache/spark/deploy/master/DriverInfo/state_$eq(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/master/Master/logInfo(scala.Function0)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$org$apache$spark$deploy$master$Master$$removeDriver$1/Master$$anonfun$org$apache$spark$deploy$master$Master$$removeDriver$1(org.apache.spark.deploy.master.Master,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$persistenceEngine()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$schedule()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master/onStart()|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+method:///java/lang/reflect/Constructor/newInstance(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$applicationMetricsSystem()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$onStart$1/Master$$anonfun$onStart$1(org.apache.spark.deploy.master.Master)|",
      "|java+method:///org/apache/spark/deploy/master/Master/RECOVERY_MODE()|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI/bind()|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/registerSource(org.apache.spark.metrics.source.Source)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$restServerBoundPort_$eq(scala.Option)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anon$1/Master$$anon$1(org.apache.spark.deploy.master.Master)|",
      "|java+constructor:///org/apache/spark/serializer/JavaSerializer/JavaSerializer(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$masterUrl()|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+constructor:///org/apache/spark/deploy/master/BlackHolePersistenceEngine/BlackHolePersistenceEngine()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$onStart$3/Master$$anonfun$onStart$3(org.apache.spark.deploy.master.Master)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI/boundPort()|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/getServletHandlers()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$onStart$2/Master$$anonfun$onStart$2(org.apache.spark.deploy.master.Master)|",
      "|java+method:///org/apache/spark/deploy/master/Master/masterPublicAddress()|",
      "|java+method:///org/apache/spark/deploy/master/Master/restServer_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/deploy/master/ZooKeeperRecoveryModeFactory/createPersistenceEngine()|",
      "|java+method:///org/apache/spark/deploy/master/Master/self()|",
      "|java+method:///org/apache/spark/deploy/master/Master/restServerEnabled()|",
      "|java+constructor:///org/apache/spark/deploy/master/FileSystemRecoveryModeFactory/FileSystemRecoveryModeFactory(org.apache.spark.SparkConf,org.apache.spark.serializer.Serializer)|",
      "|java+method:///org/apache/spark/deploy/master/Master/logInfo(scala.Function0)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/deploy/master/FileSystemRecoveryModeFactory/createPersistenceEngine()|",
      "|java+method:///org/apache/spark/util/Utils$/classForName(java.lang.String)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/start()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$forwardMessageThread()|",
      "|java+method:///org/apache/spark/deploy/master/Master/masterMetricsSystem()|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/host()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/master/StandaloneRecoveryModeFactory/createPersistenceEngine()|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI/MasterWebUI(org.apache.spark.deploy.master.Master,int)|",
      "|java+method:///org/apache/spark/deploy/master/Master/restServer()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/master/ZooKeeperRecoveryModeFactory/createLeaderElectionAgent(org.apache.spark.deploy.master.LeaderElectable)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$onStart$5/Master$$anonfun$onStart$5(org.apache.spark.deploy.master.Master,org.apache.spark.deploy.master.ui.MasterWebUI)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$persistenceEngine_$eq(org.apache.spark.deploy.master.PersistenceEngine)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$onStart$4/Master$$anonfun$onStart$4(org.apache.spark.deploy.master.Master,org.apache.spark.deploy.master.ui.MasterWebUI)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$masterWebUiUrl_$eq(java.lang.String)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///java/util/concurrent/ScheduledExecutorService/scheduleAtFixedRate(java.lang.Runnable,long,long,java.util.concurrent.TimeUnit)|",
      "|java+method:///org/apache/spark/deploy/master/Master/masterSource()|",
      "|java+method:///java/lang/Class/getConstructor(java.lang.Class%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/master/FileSystemRecoveryModeFactory/createLeaderElectionAgent(org.apache.spark.deploy.master.LeaderElectable)|",
      "|java+constructor:///org/apache/spark/deploy/master/ZooKeeperRecoveryModeFactory/ZooKeeperRecoveryModeFactory(org.apache.spark.SparkConf,org.apache.spark.serializer.Serializer)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$7/Master$$anonfun$7(org.apache.spark.deploy.master.Master)|",
      "|java+method:///org/apache/spark/deploy/master/Master/leaderElectionAgent_$eq(org.apache.spark.deploy.master.LeaderElectionAgent)|",
      "|java+method:///org/apache/spark/deploy/master/Master/checkForWorkerTimeOutTask_$eq(java.util.concurrent.ScheduledFuture)|",
      "|java+constructor:///org/apache/spark/deploy/master/MonarchyLeaderAgent/MonarchyLeaderAgent(org.apache.spark.deploy.master.LeaderElectable)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$WORKER_TIMEOUT_MS()|",
      "|java+method:///org/apache/spark/deploy/master/StandaloneRecoveryModeFactory/createLeaderElectionAgent(org.apache.spark.deploy.master.LeaderElectable)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/deploy/master/Master/conf()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$webUi()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/rest/StandaloneRestServer/StandaloneRestServer(java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.rpc.RpcEndpointRef,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$webUi_$eq(org.apache.spark.deploy.master.ui.MasterWebUI)|"
    ],
    "v2Body": [
      "|java+method:///java/lang/reflect/Constructor/newInstance(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$applicationMetricsSystem()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$onStart$1/Master$$anonfun$onStart$1(org.apache.spark.deploy.master.Master)|",
      "|java+method:///org/apache/spark/deploy/master/Master/RECOVERY_MODE()|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI/bind()|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/registerSource(org.apache.spark.metrics.source.Source)|",
      "|java+method:///org/apache/spark/deploy/master/Master/restServer()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$restServerBoundPort_$eq(scala.Option)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI/MasterWebUI(org.apache.spark.deploy.master.Master,int,scala.Option)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anon$1/Master$$anon$1(org.apache.spark.deploy.master.Master)|",
      "|java+constructor:///org/apache/spark/serializer/JavaSerializer/JavaSerializer(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$masterUrl()|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+constructor:///org/apache/spark/deploy/master/BlackHolePersistenceEngine/BlackHolePersistenceEngine()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$onStart$3/Master$$anonfun$onStart$3(org.apache.spark.deploy.master.Master)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI/boundPort()|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/getServletHandlers()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$onStart$2/Master$$anonfun$onStart$2(org.apache.spark.deploy.master.Master)|",
      "|java+method:///org/apache/spark/deploy/master/Master/masterPublicAddress()|",
      "|java+method:///org/apache/spark/deploy/master/Master/restServer_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/deploy/master/ZooKeeperRecoveryModeFactory/createPersistenceEngine()|",
      "|java+method:///org/apache/spark/deploy/master/Master/self()|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI$/$lessinit$greater$default$3()|",
      "|java+method:///org/apache/spark/deploy/master/Master/restServerEnabled()|",
      "|java+constructor:///org/apache/spark/deploy/master/FileSystemRecoveryModeFactory/FileSystemRecoveryModeFactory(org.apache.spark.SparkConf,org.apache.spark.serializer.Serializer)|",
      "|java+method:///org/apache/spark/deploy/master/Master/logInfo(scala.Function0)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/deploy/master/FileSystemRecoveryModeFactory/createPersistenceEngine()|",
      "|java+method:///org/apache/spark/util/Utils$/classForName(java.lang.String)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/start()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$forwardMessageThread()|",
      "|java+method:///org/apache/spark/deploy/master/Master/masterMetricsSystem()|",
      "|java+method:///org/apache/spark/rpc/RpcAddress/host()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/master/StandaloneRecoveryModeFactory/createPersistenceEngine()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/master/ZooKeeperRecoveryModeFactory/createLeaderElectionAgent(org.apache.spark.deploy.master.LeaderElectable)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$onStart$5/Master$$anonfun$onStart$5(org.apache.spark.deploy.master.Master,org.apache.spark.deploy.master.ui.MasterWebUI)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$persistenceEngine_$eq(org.apache.spark.deploy.master.PersistenceEngine)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$onStart$4/Master$$anonfun$onStart$4(org.apache.spark.deploy.master.Master,org.apache.spark.deploy.master.ui.MasterWebUI)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$masterWebUiUrl_$eq(java.lang.String)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///java/util/concurrent/ScheduledExecutorService/scheduleAtFixedRate(java.lang.Runnable,long,long,java.util.concurrent.TimeUnit)|",
      "|java+method:///org/apache/spark/deploy/master/Master/masterSource()|",
      "|java+method:///java/lang/Class/getConstructor(java.lang.Class%5B%5D)|",
      "|java+method:///org/apache/spark/deploy/master/FileSystemRecoveryModeFactory/createLeaderElectionAgent(org.apache.spark.deploy.master.LeaderElectable)|",
      "|java+constructor:///org/apache/spark/deploy/master/ZooKeeperRecoveryModeFactory/ZooKeeperRecoveryModeFactory(org.apache.spark.SparkConf,org.apache.spark.serializer.Serializer)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$7/Master$$anonfun$7(org.apache.spark.deploy.master.Master)|",
      "|java+method:///org/apache/spark/deploy/master/Master/leaderElectionAgent_$eq(org.apache.spark.deploy.master.LeaderElectionAgent)|",
      "|java+method:///org/apache/spark/deploy/master/Master/checkForWorkerTimeOutTask_$eq(java.util.concurrent.ScheduledFuture)|",
      "|java+constructor:///org/apache/spark/deploy/master/MonarchyLeaderAgent/MonarchyLeaderAgent(org.apache.spark.deploy.master.LeaderElectable)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$WORKER_TIMEOUT_MS()|",
      "|java+method:///org/apache/spark/deploy/master/StandaloneRecoveryModeFactory/createLeaderElectionAgent(org.apache.spark.deploy.master.LeaderElectable)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/deploy/master/Master/conf()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$webUi()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/rest/StandaloneRestServer/StandaloneRestServer(java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.rpc.RpcEndpointRef,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$webUi_$eq(org.apache.spark.deploy.master.ui.MasterWebUI)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSetManager/checkSpeculatableTasks()|",
    "called": "|java+constructor:///scala/runtime/BooleanRef/BooleanRef(boolean)|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/ArrayOps/size()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/numTasks()|",
      "|java+method:///scala/Predef$/longArrayOps(long%5B%5D)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logDebug(scala.Function0)|",
      "|java+method:///scala/collection/Iterable/filter(scala.Function1)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+method:///java/util/Arrays/sort(long%5B%5D)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$checkSpeculatableTasks$1/TaskSetManager$$anonfun$checkSpeculatableTasks$1(org.apache.spark.scheduler.TaskSetManager,int)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/SPECULATION_QUANTILE()|",
      "|java+method:///scala/runtime/RichDouble$/round$extension(double)|",
      "|java+method:///scala/reflect/ClassTag$/Long()|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/tasksSuccessful()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$checkSpeculatableTasks$2/TaskSetManager$$anonfun$checkSpeculatableTasks$2(org.apache.spark.scheduler.TaskSetManager,double)|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///scala/collection/mutable/HashMap/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/isZombie()|",
      "|java+method:///scala/math/package$/max(double,double)|",
      "|java+method:///scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$checkSpeculatableTasks$4/TaskSetManager$$anonfun$checkSpeculatableTasks$4(org.apache.spark.scheduler.TaskSetManager,scala.runtime.BooleanRef,long,double)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$checkSpeculatableTasks$3/TaskSetManager$$anonfun$checkSpeculatableTasks$3(org.apache.spark.scheduler.TaskSetManager)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$18/TaskSetManager$$anonfun$18(org.apache.spark.scheduler.TaskSetManager)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$19/TaskSetManager$$anonfun$19(org.apache.spark.scheduler.TaskSetManager)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/doubleWrapper(double)|",
      "|java+method:///scala/math/package$/min(int,int)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/taskInfos()|",
      "|java+method:///scala/runtime/RichDouble$/floor$extension(double)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/SPECULATION_MULTIPLIER()|",
      "|java+constructor:///scala/runtime/BooleanRef/BooleanRef(boolean)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/ArrayOps/size()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/numTasks()|",
      "|java+method:///scala/Predef$/longArrayOps(long%5B%5D)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logDebug(scala.Function0)|",
      "|java+method:///scala/collection/Iterable/filter(scala.Function1)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+method:///java/util/Arrays/sort(long%5B%5D)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$checkSpeculatableTasks$1/TaskSetManager$$anonfun$checkSpeculatableTasks$1(org.apache.spark.scheduler.TaskSetManager,int)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/SPECULATION_QUANTILE()|",
      "|java+method:///scala/runtime/RichDouble$/round$extension(double)|",
      "|java+method:///scala/reflect/ClassTag$/Long()|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/tasksSuccessful()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$checkSpeculatableTasks$2/TaskSetManager$$anonfun$checkSpeculatableTasks$2(org.apache.spark.scheduler.TaskSetManager,double)|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///scala/collection/mutable/HashMap/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/isZombie()|",
      "|java+method:///scala/math/package$/max(double,double)|",
      "|java+method:///scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$checkSpeculatableTasks$4/TaskSetManager$$anonfun$checkSpeculatableTasks$4(org.apache.spark.scheduler.TaskSetManager,scala.runtime.BooleanRef,long,double)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$20/TaskSetManager$$anonfun$20(org.apache.spark.scheduler.TaskSetManager)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$checkSpeculatableTasks$3/TaskSetManager$$anonfun$checkSpeculatableTasks$3(org.apache.spark.scheduler.TaskSetManager)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$19/TaskSetManager$$anonfun$19(org.apache.spark.scheduler.TaskSetManager)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/doubleWrapper(double)|",
      "|java+method:///scala/math/package$/min(int,int)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/taskInfos()|",
      "|java+method:///scala/runtime/RichDouble$/floor$extension(double)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/SPECULATION_MULTIPLIER()|",
      "|java+constructor:///scala/runtime/BooleanRef/BooleanRef(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$42/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$42(org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$76/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$76/apply(org.apache.spark.scheduler.AccumulableInfo)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$76/apply(org.apache.spark.executor.TaskMetrics)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$2/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$2(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/input/StreamFileInputFormat/setMinPartitions(org.apache.hadoop.mapreduce.JobContext,int)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///java/lang/Math/ceil(double)|",
      "|java+constructor:///org/apache/spark/input/StreamFileInputFormat$$anonfun$1/StreamFileInputFormat$$anonfun$1(org.apache.spark.input.StreamFileInputFormat)|",
      "|java+method:///scala/collection/TraversableOnce/sum(scala.math.Numeric)|",
      "|java+method:///org/apache/spark/input/StreamFileInputFormat/listStatus(org.apache.hadoop.mapreduce.JobContext)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/collection/mutable/Buffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///scala/collection/mutable/Buffer/length()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat/setMaxSplitSize(long)|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Math/ceil(double)|",
      "|java+constructor:///org/apache/spark/input/StreamFileInputFormat$$anonfun$1/StreamFileInputFormat$$anonfun$1(org.apache.spark.input.StreamFileInputFormat)|",
      "|java+method:///scala/collection/mutable/Buffer/size()|",
      "|java+method:///org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat/setMaxSplitSize(long)|",
      "|java+method:///scala/collection/mutable/Buffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///scala/collection/TraversableOnce/sum(scala.math.Numeric)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///org/apache/spark/input/StreamFileInputFormat/listStatus(org.apache.hadoop.mapreduce.JobContext)|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/HttpFileServer/addFileToDir(java.io.File,java.io.File)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///java/io/File/getName()|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///java/io/File/isDirectory()|",
      "|java+method:///org/spark-project/guava/io/Files/copy(java.io.File,java.io.File)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Predef$/any2stringadd(java.lang.Object)|",
      "|java+method:///scala/runtime/StringAdd$/$plus$extension(java.lang.Object,java.lang.String)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///java/io/File/getName()|",
      "|java+constructor:///java/io/File/File(java.io.File,java.lang.String)|",
      "|java+method:///java/io/File/isDirectory()|",
      "|java+method:///org/apache/spark/util/Utils$/encodeFileNameToURIRawPath(java.lang.String)|",
      "|java+method:///org/spark-project/guava/io/Files/copy(java.io.File,java.io.File)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Predef$/any2stringadd(java.lang.Object)|",
      "|java+method:///scala/runtime/StringAdd$/$plus$extension(java.lang.Object,java.lang.String)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1/apply$mcV$sp()|",
    "called": "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
    "v1Body": [
      "|java+method:///org/apache/mesos/Protos$Filters$Builder/setRefuseSeconds(double)|",
      "|java+method:///scala/collection/mutable/HashMap/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/HashSet$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$6/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$6(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.mutable.HashSet)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$3/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$3(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.mutable.HashMap)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$Filters$Builder/build()|",
      "|java+method:///org/apache/mesos/Protos$Filters/newBuilder()|",
      "|java+method:///scala/collection/mutable/Buffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$5/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$5(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.immutable.Map,scala.collection.immutable.Map,org.apache.mesos.Protos$Filters)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$10/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$10(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+method:///scala/collection/Seq/filter(scala.Function1)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$4/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$4(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.mutable.HashMap,scala.collection.mutable.HashMap,scala.collection.mutable.HashSet)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$11/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$11(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+method:///scala/collection/mutable/Buffer/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/Buffer/partition(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$9/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$9(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$8/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$8(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$7/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$7(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$2/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$2(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/resourceOffers(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/Buffer/withFilter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$7/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$7(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$8/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$8(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+method:///scala/collection/mutable/HashMap/foreach(scala.Function1)|",
      "|java+method:///scala/collection/TraversableLike/partition(scala.Function1)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$3/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$3(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+method:///org/apache/mesos/Protos$Filters$Builder/build()|",
      "|java+method:///org/apache/mesos/Protos$Filters/newBuilder()|",
      "|java+method:///scala/collection/mutable/Buffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///org/apache/mesos/Protos$Filters$Builder/setRefuseSeconds(double)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$6/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$6(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.immutable.Map,scala.collection.immutable.Map,org.apache.mesos.Protos$Filters)|",
      "|java+method:///scala/collection/mutable/HashSet$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$7/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$7(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.mutable.HashSet)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$10/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$10(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$5/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$5(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.mutable.HashMap,scala.collection.mutable.HashMap,scala.collection.mutable.HashSet)|",
      "|java+method:///scala/collection/Seq/filter(scala.Function1)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$11/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$11(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$12/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$12(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+method:///scala/collection/mutable/Buffer/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/Buffer/partition(scala.Function1)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$13/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$13(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$9/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$9(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$4/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$4(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1,scala.collection.mutable.HashMap)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$8/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$8(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$2/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$2(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend$$anonfun$resourceOffers$1)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/resourceOffers(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/Buffer/withFilter(scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/Stage/Stage(int,org.apache.spark.rdd.RDD,int,scala.collection.immutable.List,int,org.apache.spark.util.CallSite)|",
    "called": "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/ArrayOps/size()|",
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo$/fromStage(org.apache.spark.scheduler.Stage,int,scala.Option,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/util/CallSite/shortForm()|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/apache/spark/util/CallSite/longForm()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo$/fromStage$default$3()|",
      "|java+method:///org/apache/spark/scheduler/Stage/nextAttemptId()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo$/fromStage$default$4()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///scala/collection/Seq$/empty()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/collection/mutable/HashSet/HashSet()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo$/fromStage(org.apache.spark.scheduler.Stage,int,scala.Option,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/util/CallSite/shortForm()|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///org/apache/spark/util/CallSite/longForm()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo$/fromStage$default$3()|",
      "|java+method:///org/apache/spark/scheduler/Stage/nextAttemptId()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo$/fromStage$default$4()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///scala/collection/Seq$/empty()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/HttpFileServer/addJar(java.io.File)|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///java/io/File/getName()|",
      "|java+method:///org/apache/spark/HttpFileServer/serverUri()|",
      "|java+method:///org/apache/spark/HttpFileServer/addFileToDir(java.io.File,java.io.File)|",
      "|java+method:///org/apache/spark/HttpFileServer/jarDir()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///java/io/File/getName()|",
      "|java+method:///org/apache/spark/HttpFileServer/serverUri()|",
      "|java+method:///org/apache/spark/util/Utils$/encodeFileNameToURIRawPath(java.lang.String)|",
      "|java+method:///org/apache/spark/HttpFileServer/addFileToDir(java.io.File,java.io.File)|",
      "|java+method:///org/apache/spark/HttpFileServer/jarDir()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/SerDeUtil$$anonfun$pairRDDToPython$1$$anonfun$apply$2/apply(scala.collection.Seq)|",
    "called": "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///net/razorvine/pickle/Pickler/dumps(java.lang.Object)|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///net/razorvine/pickle/Pickler/dumps(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$17/apply(int)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/latestInfo()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/attemptId()|",
      "|java+method:///scala/collection/immutable/Map/apply(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/internalAccumulators()|",
      "|java+constructor:///org/apache/spark/scheduler/ShuffleMapTask/ShuffleMapTask(int,int,org.apache.spark.broadcast.Broadcast,org.apache.spark.Partition,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/rdd()|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/id()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/ResultStage/id()|",
      "|java+method:///org/apache/spark/scheduler/ResultStage/rdd()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/attemptId()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///org/apache/spark/scheduler/ResultStage/partitions()|",
      "|java+method:///org/apache/spark/scheduler/ResultStage/internalAccumulators()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/Map/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/ResultStage/latestInfo()|",
      "|java+constructor:///org/apache/spark/scheduler/ResultTask/ResultTask(int,int,org.apache.spark.broadcast.Broadcast,org.apache.spark.Partition,scala.collection.Seq,int,scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$54/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/diskBytesSpilled()|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$54$$anonfun$apply$9/StagePage$$anonfun$54$$anonfun$apply$9(org.apache.spark.ui.jobs.StagePage$$anonfun$54)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/shuffleReadMetrics()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$54$$anonfun$apply$22/StagePage$$anonfun$54$$anonfun$apply$22(org.apache.spark.ui.jobs.StagePage$$anonfun$54)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$makeOffers()|",
    "called": "|java+method:///scala/collection/mutable/HashMap/filterKeys(scala.Function1)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$2/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$2(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$1/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$1(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint)|",
      "|java+method:///scala/collection/TraversableOnce/toSeq()|",
      "|java+method:///scala/collection/mutable/HashMap/filterKeys(scala.Function1)|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$executorDataMap()|",
      "|java+method:///scala/collection/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$$outer()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/resourceOffers(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/launchTasks(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$2/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$2(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint)|",
      "|java+method:///scala/collection/TraversableOnce/toSeq()|",
      "|java+method:///scala/collection/mutable/HashMap/filterKeys(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$3/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$3(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint)|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$executorDataMap()|",
      "|java+method:///scala/collection/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$$outer()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/resourceOffers(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/launchTasks(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$3/apply()|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/megabytesToString(long)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$3/apply()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/name()|",
      "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$3/apply()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10/apply()|",
    "called": "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///org/apache/spark/ComplexFutureAction/runJob(org.apache.spark.rdd.RDD,scala.Function1,scala.collection.Seq,scala.Function2,scala.Function0)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer(int)|",
      "|java+constructor:///org/apache/spark/rdd/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10$$anonfun$apply$12/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10$$anonfun$apply$12(org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10,int)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/rdd/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10$$anonfun$apply$11/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10$$anonfun$apply$11(org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/toSeq()|",
      "|java+method:///scala/math/package$/min(int,int)|",
      "|java+method:///java/lang/Math/min(int,int)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/size()|",
      "|java+method:///scala/collection/immutable/Range/size()|",
      "|java+method:///java/lang/Math/max(int,int)|",
      "|java+constructor:///org/apache/spark/rdd/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10$$anonfun$apply$14/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10$$anonfun$apply$14(org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10,scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///scala/reflect/ClassTag/runtimeClass()|",
      "|java+method:///org/apache/spark/rdd/AsyncRDDActions$$anonfun$takeAsync$1/org$apache$spark$rdd$AsyncRDDActions$$anonfun$$$outer()|",
      "|java+method:///org/apache/spark/rdd/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10/apply()|",
      "|java+method:///scala/reflect/ClassTag/newArray(int)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/rdd/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10$$anonfun$apply$13/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10$$anonfun$apply$13(org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10,java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///org/apache/spark/ComplexFutureAction/runJob(org.apache.spark.rdd.RDD,scala.Function1,scala.collection.Seq,scala.Function2,scala.Function0)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer(int)|",
      "|java+constructor:///org/apache/spark/rdd/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10$$anonfun$apply$12/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10$$anonfun$apply$12(org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10,int)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/rdd/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10$$anonfun$apply$11/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10$$anonfun$apply$11(org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/toSeq()|",
      "|java+method:///scala/math/package$/min(int,int)|",
      "|java+method:///java/lang/Math/min(int,int)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/size()|",
      "|java+method:///org/apache/spark/SparkContext/setCallSite(org.apache.spark.util.CallSite)|",
      "|java+method:///scala/collection/immutable/Range/size()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+method:///org/apache/spark/rdd/RDD/context()|",
      "|java+method:///java/lang/Math/max(int,int)|",
      "|java+constructor:///org/apache/spark/rdd/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10$$anonfun$apply$14/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10$$anonfun$apply$14(org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10,scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///scala/reflect/ClassTag/runtimeClass()|",
      "|java+method:///org/apache/spark/rdd/AsyncRDDActions$$anonfun$takeAsync$1/org$apache$spark$rdd$AsyncRDDActions$$anonfun$$$outer()|",
      "|java+method:///org/apache/spark/rdd/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10/apply()|",
      "|java+method:///scala/reflect/ClassTag/newArray(int)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/rdd/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10$$anonfun$apply$13/AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10$$anonfun$apply$13(org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$10,java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1/apply(org.apache.spark.deploy.history.FsApplicationHistoryInfo)|",
    "called": "|java+method:///scala/Option/flatMap(scala.Function1)|",
    "v1Body": [
      "|java+method:///scala/collection/immutable/List/find(scala.Function1)|",
      "|java+method:///scala/Option/flatMap(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/history/FsApplicationHistoryInfo/attempts()|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4(org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$getAppUI$1)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$3/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$3(org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$getAppUI$1)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$3/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$3(org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$getAppUI$1)|",
      "|java+method:///scala/collection/immutable/List/find(scala.Function1)|",
      "|java+method:///scala/Option/flatMap(scala.Function1)|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4(org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$getAppUI$1,org.apache.spark.deploy.history.FsApplicationHistoryInfo)|",
      "|java+method:///org/apache/spark/deploy/history/FsApplicationHistoryInfo/attempts()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/deploy/ApplicationDescription$/ApplicationDescription$()|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction9/AbstractFunction9()|",
    "v1Body": [
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction9/AbstractFunction9()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$3/CoarseMesosSchedulerBackend$$anonfun$3(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$67/apply(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/JsonProtocol$/accumulableInfoFromJson(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/rdd/ReliableCheckpointRDD$$anonfun$3/ReliableCheckpointRDD$$anonfun$3(org.apache.spark.rdd.ReliableCheckpointRDD)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend/doRequestTotalExecutors(int)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend/yarnSchedulerEndpoint()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend/localityAwareTasks()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RequestExecutors/CoarseGrainedClusterMessages$RequestExecutors(int,int,scala.collection.immutable.Map)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///scala/reflect/ClassTag$/Boolean()|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend/hostToLocalTaskCount()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend/yarnSchedulerEndpointRef()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/askWithRetry(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend/localityAwareTasks()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RequestExecutors/CoarseGrainedClusterMessages$RequestExecutors(int,int,scala.collection.immutable.Map)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///scala/reflect/ClassTag$/Boolean()|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend/hostToLocalTaskCount()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/executor/ExecutorSource/org$apache$spark$executor$ExecutorSource$$fileStats(java.lang.String)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/Buffer/find(scala.Function1)|",
      "|java+constructor:///org/apache/spark/executor/ExecutorSource$$anonfun$org$apache$spark$executor$ExecutorSource$$fileStats$1/ExecutorSource$$anonfun$org$apache$spark$executor$ExecutorSource$$fileStats$1(org.apache.spark.executor.ExecutorSource,java.lang.String)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/getAllStatistics()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/IterableLike/find(scala.Function1)|",
      "|java+constructor:///org/apache/spark/executor/ExecutorSource$$anonfun$org$apache$spark$executor$ExecutorSource$$fileStats$1/ExecutorSource$$anonfun$org$apache$spark$executor$ExecutorSource$$fileStats$1(org.apache.spark.executor.ExecutorSource,java.lang.String)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/getAllStatistics()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/SparkConf$/SparkConf$()|",
    "called": "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+constructor:///org/apache/spark/SparkConf$DeprecatedConfig/SparkConf$DeprecatedConfig(java.lang.String,java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/SparkConf$AlternateConfig/SparkConf$AlternateConfig(java.lang.String,java.lang.String,scala.Function1)|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Predef$/any2ArrowAssoc(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///org/apache/spark/SparkConf$AlternateConfig$/apply$default$3()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$7/SparkConf$$anonfun$7()|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$4/SparkConf$$anonfun$4()|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$6/SparkConf$$anonfun$6()|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$5/SparkConf$$anonfun$5()|",
      "|java+method:///org/apache/spark/SparkConf$/org$apache$spark$SparkConf$$configsWithAlternatives()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/immutable/Map/keys()|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+constructor:///org/apache/spark/SparkConf$DeprecatedConfig/SparkConf$DeprecatedConfig(java.lang.String,java.lang.String,java.lang.String)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/SparkConf$AlternateConfig/SparkConf$AlternateConfig(java.lang.String,java.lang.String,scala.Function1)|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Predef$/any2ArrowAssoc(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///org/apache/spark/SparkConf$AlternateConfig$/apply$default$3()|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$7/SparkConf$$anonfun$7()|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$6/SparkConf$$anonfun$6()|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$8/SparkConf$$anonfun$8()|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$5/SparkConf$$anonfun$5()|",
      "|java+method:///org/apache/spark/SparkConf$/org$apache$spark$SparkConf$$configsWithAlternatives()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/immutable/Map/keys()|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StageTableBase/stageRow(org.apache.spark.scheduler.StageInfo)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/formatDate(java.util.Date)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/numActiveTasks()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/prependBaseUri(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/schedulingPool()|",
      "|java+method:///org/apache/spark/util/Utils$/bytesToString(long)|",
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/stageIdToData()|",
      "|java+method:///org/apache/spark/ui/jobs/StageTableBase/missingStageRow(int)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/numTasks()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/attemptId()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$1/StageTableBase$$anonfun$1(org.apache.spark.ui.jobs.StageTableBase)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/completionTime()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$stageRow$2/StageTableBase$$anonfun$stageRow$2(org.apache.spark.ui.jobs.StageTableBase)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$stageRow$1/StageTableBase$$anonfun$stageRow$1(org.apache.spark.ui.jobs.StageTableBase)|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/prependBaseUri$default$2()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/completedIndices()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$2/StageTableBase$$anonfun$2(org.apache.spark.ui.jobs.StageTableBase,long)|",
      "|java+method:///scala/xml/Elem/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/xml/NodeSeq$/canBuildFrom()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+method:///org/apache/spark/util/collection/OpenHashSet/size()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/inputBytes()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/shuffleWriteBytes()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/makeProgressBar(int,int,int,int,int)|",
      "|java+method:///org/apache/spark/ui/jobs/StageTableBase/makeDescription(org.apache.spark.scheduler.StageInfo)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/outputBytes()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/submissionTime()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/stageId()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///java/util/Date/Date(long)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$7/StageTableBase$$anonfun$7(org.apache.spark.ui.jobs.StageTableBase)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/numFailedTasks()|",
      "|java+constructor:///scala/Tuple2$mcII$sp/Tuple2$mcII$sp(int,int)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$6/StageTableBase$$anonfun$6(org.apache.spark.ui.jobs.StageTableBase)|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/shuffleReadTotalBytes()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$2/StageTableBase$$anonfun$2(org.apache.spark.ui.jobs.StageTableBase)|",
      "|java+method:///scala/collection/Seq$/empty()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/formatDate(java.util.Date)|",
      "|java+method:///scala/xml/Elem/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/schedulingPool()|",
      "|java+method:///org/apache/spark/util/Utils$/bytesToString(long)|",
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/ui/jobs/JobProgressListener/stageIdToData()|",
      "|java+method:///org/apache/spark/ui/jobs/StageTableBase/missingStageRow(int)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/numTasks()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/attemptId()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$1/StageTableBase$$anonfun$1(org.apache.spark.ui.jobs.StageTableBase)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/completionTime()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$stageRow$2/StageTableBase$$anonfun$stageRow$2(org.apache.spark.ui.jobs.StageTableBase)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$stageRow$1/StageTableBase$$anonfun$stageRow$1(org.apache.spark.ui.jobs.StageTableBase)|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/prependBaseUri$default$2()|",
      "|java+method:///scala/collection/Iterable$/canBuildFrom()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/completedIndices()|",
      "|java+method:///scala/xml/NodeSeq$/canBuildFrom()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+method:///org/apache/spark/util/collection/OpenHashSet/size()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/inputBytes()|",
      "|java+method:///scala/collection/TraversableLike/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/shuffleWriteBytes()|",
      "|java+method:///org/apache/spark/ui/UIUtils$/makeProgressBar(int,int,int,int,int)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/numActiveTasks()|",
      "|java+method:///scala/collection/Iterable/nonEmpty()|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+method:///org/apache/spark/ui/jobs/StageTableBase/makeDescription(org.apache.spark.scheduler.StageInfo)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/outputBytes()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/submissionTime()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/stageId()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+constructor:///java/util/Date/Date(long)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$8/StageTableBase$$anonfun$8(org.apache.spark.ui.jobs.StageTableBase)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$7/StageTableBase$$anonfun$7(org.apache.spark.ui.jobs.StageTableBase)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/numFailedTasks()|",
      "|java+constructor:///scala/Tuple2$mcII$sp/Tuple2$mcII$sp(int,int)|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StageTableBase$$anonfun$6/StageTableBase$$anonfun$6(org.apache.spark.ui.jobs.StageTableBase)|",
      "|java+method:///scala/collection/Iterable/min(scala.math.Ordering)|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/prependBaseUri(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/taskData()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$StageUIData/shuffleReadTotalBytes()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/shuffle/hash/HashShuffleWriter/commitWritesAndBuildStatus()|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/BlockManager/shuffleServerId()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/shuffle/hash/HashShuffleWriter$$anonfun$3/HashShuffleWriter$$anonfun$3(org.apache.spark.shuffle.hash.HashShuffleWriter)|",
      "|java+method:///org/apache/spark/shuffle/hash/HashShuffleWriter/blockManager()|",
      "|java+method:///org/apache/spark/shuffle/hash/HashShuffleWriter/org$apache$spark$shuffle$hash$HashShuffleWriter$$shuffle()|",
      "|java+method:///org/apache/spark/scheduler/MapStatus$/apply(org.apache.spark.storage.BlockManagerId,long%5B%5D)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/ClassTag$/Long()|",
      "|java+method:///org/apache/spark/shuffle/ShuffleWriterGroup/writers()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/shuffle/hash/HashShuffleWriter$$anonfun$commitWritesAndBuildStatus$1/HashShuffleWriter$$anonfun$commitWritesAndBuildStatus$1(org.apache.spark.shuffle.hash.HashShuffleWriter,long%5B%5D)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/shuffle/hash/HashShuffleWriter$$anonfun$3/HashShuffleWriter$$anonfun$3(org.apache.spark.shuffle.hash.HashShuffleWriter)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayOps/zipWithIndex(scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/shuffle/hash/HashShuffleWriter/org$apache$spark$shuffle$hash$HashShuffleWriter$$shuffle()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/storage/BlockManager/shuffleServerId()|",
      "|java+method:///org/apache/spark/shuffle/hash/HashShuffleWriter/org$apache$spark$shuffle$hash$HashShuffleWriter$$blockManager()|",
      "|java+method:///org/apache/spark/scheduler/MapStatus$/apply(org.apache.spark.storage.BlockManagerId,long%5B%5D)|",
      "|java+method:///scala/reflect/ClassTag$/Long()|",
      "|java+method:///org/apache/spark/shuffle/ShuffleWriterGroup/writers()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaPairRDD/sampleByKeyExact(boolean,java.util.Map,long)|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/kClassTag()|",
      "|java+constructor:///org/apache/spark/api/java/JavaPairRDD/JavaPairRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/rdd()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/vClassTag()|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions$default$4(org.apache.spark.rdd.RDD)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/sampleByKeyExact(boolean,scala.collection.Map,long)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/kClassTag()|",
      "|java+constructor:///org/apache/spark/api/java/JavaPairRDD/JavaPairRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/rdd()|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///org/apache/spark/api/java/JavaPairRDD/vClassTag()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions$default$4(org.apache.spark.rdd.RDD)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/sampleByKeyExact(boolean,scala.collection.Map,long)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/createSimpleWorker()|",
    "called": "|java+method:///scala/collection/mutable/StringBuilder/append(int)|",
    "v1Body": [
      "|java+method:///java/net/InetAddress/getByAddress(byte%5B%5D)|",
      "|java+method:///scala/Array$/apply(scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///java/net/ServerSocket/ServerSocket(int,int,java.net.InetAddress)|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///java/net/ServerSocket/close()|",
      "|java+method:///java/net/ServerSocket/accept()|",
      "|java+method:///java/net/ServerSocket/setSoTimeout(int)|",
      "|java+method:///java/lang/Process/getErrorStream()|",
      "|java+constructor:///java/lang/ProcessBuilder/ProcessBuilder(java.util.List)|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/Predef$/wrapByteArray(byte%5B%5D)|",
      "|java+method:///java/net/ServerSocket/getLocalPort()|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/simpleWorkers()|",
      "|java+method:///scala/reflect/ClassTag$/Byte()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///java/util/Map/putAll(java.util.Map)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(int)|",
      "|java+method:///scala/collection/JavaConversions$/mapAsJavaMap(scala.collection.Map)|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/pythonPath()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///java/io/OutputStreamWriter/write(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/io/OutputStreamWriter/OutputStreamWriter(java.io.OutputStream)|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/redirectStreamsToStderr(java.io.InputStream,java.io.InputStream)|",
      "|java+method:///scala/collection/mutable/WeakHashMap/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/lang/ProcessBuilder/environment()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///java/lang/Process/getOutputStream()|",
      "|java+method:///java/io/OutputStreamWriter/flush()|"
    ],
    "v2Body": [
      "|java+method:///java/net/InetAddress/getByAddress(byte%5B%5D)|",
      "|java+method:///scala/Array$/apply(scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+method:///java/util/Arrays/asList(java.lang.Object%5B%5D)|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/pythonPath()|",
      "|java+method:///scala/collection/JavaConverters$/mapAsJavaMapConverter(scala.collection.Map)|",
      "|java+constructor:///java/net/ServerSocket/ServerSocket(int,int,java.net.InetAddress)|",
      "|java+method:///java/net/ServerSocket/close()|",
      "|java+method:///java/net/ServerSocket/accept()|",
      "|java+method:///java/net/ServerSocket/setSoTimeout(int)|",
      "|java+method:///java/lang/Process/getErrorStream()|",
      "|java+constructor:///java/lang/ProcessBuilder/ProcessBuilder(java.util.List)|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/Predef$/wrapByteArray(byte%5B%5D)|",
      "|java+method:///java/net/ServerSocket/getLocalPort()|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/simpleWorkers()|",
      "|java+method:///scala/reflect/ClassTag$/Byte()|",
      "|java+method:///java/util/Map/putAll(java.util.Map)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(int)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///java/io/OutputStreamWriter/write(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/io/OutputStreamWriter/OutputStreamWriter(java.io.OutputStream)|",
      "|java+method:///org/apache/spark/api/python/PythonWorkerFactory/redirectStreamsToStderr(java.io.InputStream,java.io.InputStream)|",
      "|java+method:///scala/collection/mutable/WeakHashMap/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/lang/ProcessBuilder/environment()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///java/lang/Process/getOutputStream()|",
      "|java+method:///java/io/OutputStreamWriter/flush()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$2/CoarseMesosSchedulerBackend$$anonfun$2(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$59/apply(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/taskMetricsFromJson(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|",
      "|java+constructor:///scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$$anonfun$glom$1/apply(java.lang.Object)|",
    "called": "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/java/JavaRDDLike$$anonfun$glom$1/apply(java.lang.Object)|",
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+constructor:///java/util/ArrayList/ArrayList(java.util.Collection)|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/api/java/JavaRDDLike$$anonfun$glom$1/apply(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/BlockManagerMasterEndpoint$$anonfun$org$apache$spark$storage$BlockManagerMasterEndpoint$$storageStatus$1/apply(scala.Tuple2)|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/BlockManagerInfo/blocks()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/storage/BlockManagerInfo/maxMem()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/storage/StorageStatus/StorageStatus(org.apache.spark.storage.BlockManagerId,long,scala.collection.Map)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/BlockManagerInfo/blocks()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///org/apache/spark/storage/BlockManagerInfo/maxMem()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/storage/StorageStatus/StorageStatus(org.apache.spark.storage.BlockManagerId,long,scala.collection.Map)|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///scala/Tuple2/_1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/Utils$/getSystemProperties()|",
    "called": "|java+method:///scala/collection/JavaConverters$/asScalaSetConverter(java.util.Set)|",
    "v1Body": [
      "|java+method:///java/util/Properties/stringPropertyNames()|",
      "|java+method:///scala/collection/mutable/Set/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/Set/toMap(scala.Predef$$less$colon$less)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$9/Utils$$anonfun$9()|",
      "|java+method:///java/lang/System/getProperties()|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/collection/mutable/Set$/canBuildFrom()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaSet(java.util.Set)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/SetLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///java/lang/System/getProperties()|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/collection/mutable/Set$/canBuildFrom()|",
      "|java+method:///java/util/Properties/stringPropertyNames()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaSetConverter(java.util.Set)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$getSystemProperties$1/Utils$$anonfun$getSystemProperties$1()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/MapOutputTrackerWorker/MapOutputTrackerWorker(org.apache.spark.SparkConf)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+constructor:///java/util/concurrent/ConcurrentHashMap/ConcurrentHashMap()|",
      "|java+constructor:///org/apache/spark/MapOutputTracker/MapOutputTracker(org.apache.spark.SparkConf)|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaConcurrentMap(java.util.concurrent.ConcurrentMap)|"
    ],
    "v2Body": [
      "|java+constructor:///java/util/concurrent/ConcurrentHashMap/ConcurrentHashMap()|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaConcurrentMapConverter(java.util.concurrent.ConcurrentMap)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+constructor:///org/apache/spark/MapOutputTracker/MapOutputTracker(org.apache.spark.SparkConf)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/onDisconnected(org.apache.spark.rpc.RpcAddress)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$onDisconnected$1/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$onDisconnected$1(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$addressToExecutorId()|"
    ],
    "v2Body": [
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$onDisconnected$1/CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$onDisconnected$1(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$DriverEndpoint/addressToExecutorId()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/executor/Executor/org$apache$spark$executor$Executor$$computeTotalGcTime()|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///java/lang/management/ManagementFactory/getGarbageCollectorMXBeans()|",
      "|java+method:///scala/collection/TraversableOnce/sum(scala.math.Numeric)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+constructor:///org/apache/spark/executor/Executor$$anonfun$org$apache$spark$executor$Executor$$computeTotalGcTime$1/Executor$$anonfun$org$apache$spark$executor$Executor$$computeTotalGcTime$1(org.apache.spark.executor.Executor)|",
      "|java+method:///scala/collection/mutable/Buffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|"
    ],
    "v2Body": [
      "|java+method:///java/lang/management/ManagementFactory/getGarbageCollectorMXBeans()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///scala/collection/TraversableOnce/sum(scala.math.Numeric)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/executor/Executor$$anonfun$org$apache$spark$executor$Executor$$computeTotalGcTime$1/Executor$$anonfun$org$apache$spark$executor$Executor$$computeTotalGcTime$1(org.apache.spark.executor.Executor)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/deploy/SparkSubmitArguments/SparkSubmitArguments(scala.collection.Seq,scala.collection.immutable.Map)|",
    "called": "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/parse(java.util.List)|",
      "|java+constructor:///org/apache/spark/launcher/SparkSubmitArgumentsParser/SparkSubmitArgumentsParser()|",
      "|java+method:///scala/collection/Seq/toList()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/printErrorAndExit(java.lang.String)|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/ignoreNonSparkProperties()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/validateArguments()|",
      "|java+method:///java/lang/IllegalArgumentException/getMessage()|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/mergeDefaultSparkProperties()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/loadEnvironmentArguments()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/parse(java.util.List)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+constructor:///org/apache/spark/launcher/SparkSubmitArgumentsParser/SparkSubmitArgumentsParser()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmit$/printErrorAndExit(java.lang.String)|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/ignoreNonSparkProperties()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/validateArguments()|",
      "|java+method:///java/lang/IllegalArgumentException/getMessage()|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/mergeDefaultSparkProperties()|",
      "|java+method:///org/apache/spark/deploy/SparkSubmitArguments/loadEnvironmentArguments()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$6/apply()|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$6/apply()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/name()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$6/apply()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4/apply(org.apache.spark.deploy.history.FsApplicationAttemptInfo)|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/scheduler/ReplayListenerBus/ReplayListenerBus()|",
      "|java+method:///org/apache/spark/SparkConf/clone()|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4$$anonfun$apply$5/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4$$anonfun$apply$5(org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4,org.apache.spark.deploy.history.FsApplicationAttemptInfo,org.apache.spark.ui.SparkUI,org.apache.spark.scheduler.ApplicationEventListener)|",
      "|java+method:///org/apache/spark/deploy/history/FsApplicationAttemptInfo/logPath()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/getFileStatus(org.apache.hadoop.fs.Path)|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/history/FsApplicationAttemptInfo/startTime()|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1/org$apache$spark$deploy$history$FsHistoryProvider$$anonfun$$$outer()|",
      "|java+constructor:///org/apache/spark/scheduler/ApplicationEventListener/ApplicationEventListener()|",
      "|java+method:///org/apache/spark/scheduler/ReplayListenerBus/addListener(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServer$/getAttemptURI(java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$logDir()|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$replay(org.apache.hadoop.fs.FileStatus,org.apache.spark.scheduler.ReplayListenerBus)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$fs()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/history/FsApplicationAttemptInfo/attemptId()|",
      "|java+method:///org/apache/spark/ui/SparkUI$/createHistoryUI(org.apache.spark.SparkConf,org.apache.spark.scheduler.SparkListenerBus,org.apache.spark.SecurityManager,java.lang.String,java.lang.String,long)|",
      "|java+method:///scala/Option/map(scala.Function1)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/scheduler/ReplayListenerBus/ReplayListenerBus()|",
      "|java+method:///org/apache/spark/SparkConf/clone()|",
      "|java+constructor:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4$$anonfun$apply$5/FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4$$anonfun$apply$5(org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$getAppUI$1$$anonfun$apply$4,org.apache.spark.deploy.history.FsApplicationAttemptInfo,org.apache.spark.ui.SparkUI,org.apache.spark.scheduler.ApplicationEventListener)|",
      "|java+method:///org/apache/spark/deploy/history/FsApplicationAttemptInfo/logPath()|",
      "|java+method:///org/apache/hadoop/fs/FileSystem/getFileStatus(org.apache.hadoop.fs.Path)|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/history/FsApplicationAttemptInfo/startTime()|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider$$anonfun$getAppUI$1/org$apache$spark$deploy$history$FsHistoryProvider$$anonfun$$$outer()|",
      "|java+constructor:///org/apache/spark/scheduler/ApplicationEventListener/ApplicationEventListener()|",
      "|java+method:///org/apache/spark/scheduler/ReplayListenerBus/addListener(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServer$/getAttemptURI(java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$logDir()|",
      "|java+method:///org/apache/spark/deploy/history/FsApplicationHistoryInfo/name()|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$replay(org.apache.hadoop.fs.FileStatus,org.apache.spark.scheduler.ReplayListenerBus)|",
      "|java+method:///org/apache/spark/deploy/history/FsHistoryProvider/org$apache$spark$deploy$history$FsHistoryProvider$$fs()|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/history/FsApplicationAttemptInfo/attemptId()|",
      "|java+method:///org/apache/spark/ui/SparkUI$/createHistoryUI(org.apache.spark.SparkConf,org.apache.spark.scheduler.SparkListenerBus,org.apache.spark.SecurityManager,java.lang.String,java.lang.String,long)|",
      "|java+method:///scala/Option/map(scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$57/StagePage$$anonfun$57(org.apache.spark.ui.jobs.StagePage)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$collectPartitions$1/apply()|",
    "called": "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$collectPartitions$1$$anonfun$apply$53/RDD$$anonfun$collectPartitions$1$$anonfun$apply$53(org.apache.spark.rdd.RDD$$anonfun$collectPartitions$1)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///scala/reflect/ClassTag/runtimeClass()|",
      "|java+method:///org/apache/spark/rdd/RDD/org$apache$spark$rdd$RDD$$sc()|",
      "|java+method:///org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function1,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$collectPartitions$1/apply()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$collectPartitions$1$$anonfun$apply$57/RDD$$anonfun$collectPartitions$1$$anonfun$apply$57(org.apache.spark.rdd.RDD$$anonfun$collectPartitions$1)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///scala/reflect/ClassTag/runtimeClass()|",
      "|java+method:///org/apache/spark/rdd/RDD/org$apache$spark$rdd$RDD$$sc()|",
      "|java+method:///org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function1,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$collectPartitions$1/apply()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/requestExecutors(int)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$executorsPendingToRemove()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/numExistingExecutors()|",
      "|java+method:///scala/collection/mutable/HashSet/size()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$numPendingExecutors_$eq(int)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$$anonfun$requestExecutors$1/CoarseGrainedSchedulerBackend$$anonfun$requestExecutors$1(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend,int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/logDebug(scala.Function0)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$$anonfun$requestExecutors$2/CoarseGrainedSchedulerBackend$$anonfun$requestExecutors$2(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/doRequestTotalExecutors(int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$numPendingExecutors()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$executorsPendingToRemove()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/numExistingExecutors()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$numPendingExecutors_$eq(int)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$$anonfun$requestExecutors$1/CoarseGrainedSchedulerBackend$$anonfun$requestExecutors$1(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend,int)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/HashMap/size()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/logDebug(scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$$anonfun$requestExecutors$2/CoarseGrainedSchedulerBackend$$anonfun$requestExecutors$2(org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/doRequestTotalExecutors(int)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$numPendingExecutors()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$coalesce$1/apply()|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/CoalescedRDD$/$lessinit$greater$default$3()|",
      "|java+constructor:///org/apache/spark/HashPartitioner/HashPartitioner(int)|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$coalesce$1/apply()|",
      "|java+constructor:///org/apache/spark/rdd/CoalescedRDD/CoalescedRDD(org.apache.spark.rdd.RDD,int,double,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitionsWithIndex$default$2()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$coalesce$1$$anonfun$7/RDD$$anonfun$coalesce$1$$anonfun$7(org.apache.spark.rdd.RDD$$anonfun$coalesce$1)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/values()|",
      "|java+method:///scala/reflect/ClassTag$/Int()|",
      "|java+constructor:///org/apache/spark/rdd/ShuffledRDD/ShuffledRDD(org.apache.spark.rdd.RDD,org.apache.spark.Partitioner)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitionsWithIndex(scala.Function2,boolean,scala.reflect.ClassTag)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/rdd/ShuffledRDD/ShuffledRDD(org.apache.spark.rdd.RDD,org.apache.spark.Partitioner,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/CoalescedRDD$/$lessinit$greater$default$3()|",
      "|java+constructor:///org/apache/spark/HashPartitioner/HashPartitioner(int)|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$coalesce$1/apply()|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitionsWithIndex(scala.Function2,boolean,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/rdd/CoalescedRDD/CoalescedRDD(org.apache.spark.rdd.RDD,int,double,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitionsWithIndex$default$2()|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$coalesce$1$$anonfun$7/RDD$$anonfun$coalesce$1$$anonfun$7(org.apache.spark.rdd.RDD$$anonfun$coalesce$1)|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/values()|",
      "|java+method:///scala/reflect/ClassTag$/Int()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI/getApplicationInfoList()|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///scala/collection/Iterator/$plus$plus(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterStateResponse/activeApps()|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterPage/getMasterState()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterStateResponse/completedApps()|",
      "|java+method:///scala/collection/mutable/ArrayOps/iterator()|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI$$anonfun$getApplicationInfoList$2/MasterWebUI$$anonfun$getApplicationInfoList$2(org.apache.spark.deploy.master.ui.MasterWebUI,org.apache.spark.deploy.master.ApplicationInfo%5B%5D)|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayOps/reverse()|",
      "|java+method:///scala/collection/mutable/ArrayOps/sortBy(scala.Function1,scala.math.Ordering)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI$$anonfun$getApplicationInfoList$1/MasterWebUI$$anonfun$getApplicationInfoList$1(org.apache.spark.deploy.master.ui.MasterWebUI)|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI/masterPage()|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI$$anonfun$3/MasterWebUI$$anonfun$3(org.apache.spark.deploy.master.ui.MasterWebUI)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI$$anonfun$4/MasterWebUI$$anonfun$4(org.apache.spark.deploy.master.ui.MasterWebUI)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Iterator/$plus$plus(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterStateResponse/activeApps()|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterPage/getMasterState()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterStateResponse/completedApps()|",
      "|java+method:///scala/collection/mutable/ArrayOps/iterator()|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI$$anonfun$getApplicationInfoList$2/MasterWebUI$$anonfun$getApplicationInfoList$2(org.apache.spark.deploy.master.ui.MasterWebUI,org.apache.spark.deploy.master.ApplicationInfo%5B%5D)|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayOps/reverse()|",
      "|java+method:///scala/collection/mutable/ArrayOps/sortBy(scala.Function1,scala.math.Ordering)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI$$anonfun$getApplicationInfoList$1/MasterWebUI$$anonfun$getApplicationInfoList$1(org.apache.spark.deploy.master.ui.MasterWebUI)|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI/masterPage()|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI$$anonfun$5/MasterWebUI$$anonfun$5(org.apache.spark.deploy.master.ui.MasterWebUI)|",
      "|java+constructor:///org/apache/spark/deploy/master/ui/MasterWebUI$$anonfun$4/MasterWebUI$$anonfun$4(org.apache.spark.deploy.master.ui.MasterWebUI)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/EventLoggingListener$$anonfun$start$2/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener/logPath()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener$$anonfun$start$2/apply()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/EventLoggingListener$$anonfun$start$2/apply()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$104/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$104/apply(org.apache.spark.executor.ShuffleWriteMetrics)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$104/apply(org.apache.spark.executor.ShuffleReadMetrics)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/DiskStore$$anonfun$putIterator$4/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///java/io/File/getName()|",
      "|java+method:///org/apache/spark/storage/DiskStore$$anonfun$putIterator$4/apply()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/bytesToString(long)|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/storage/DiskStore$$anonfun$putIterator$4/apply()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext$$anonfun$union$1/apply()|",
    "called": "|java+method:///scala/collection/Seq$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/Seq/forall(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkContext$$anonfun$union$1/apply()|",
      "|java+method:///scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/TraversableOnce/toSet()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$union$1$$anonfun$apply$13/SparkContext$$anonfun$union$1$$anonfun$apply$13(org.apache.spark.SparkContext$$anonfun$union$1)|",
      "|java+method:///scala/collection/immutable/Set/size()|",
      "|java+constructor:///org/apache/spark/rdd/PartitionerAwareUnionRDD/PartitionerAwareUnionRDD(org.apache.spark.SparkContext,scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$union$1$$anonfun$33/SparkContext$$anonfun$union$1$$anonfun$33(org.apache.spark.SparkContext$$anonfun$union$1)|",
      "|java+constructor:///org/apache/spark/rdd/UnionRDD/UnionRDD(org.apache.spark.SparkContext,scala.collection.Seq,scala.reflect.ClassTag)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/Seq/forall(scala.Function1)|",
      "|java+method:///org/apache/spark/SparkContext$$anonfun$union$1/apply()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$union$1$$anonfun$34/SparkContext$$anonfun$union$1$$anonfun$34(org.apache.spark.SparkContext$$anonfun$union$1)|",
      "|java+method:///scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/TraversableOnce/toSet()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$union$1$$anonfun$apply$14/SparkContext$$anonfun$union$1$$anonfun$apply$14(org.apache.spark.SparkContext$$anonfun$union$1)|",
      "|java+method:///scala/collection/immutable/Set/size()|",
      "|java+constructor:///org/apache/spark/rdd/PartitionerAwareUnionRDD/PartitionerAwareUnionRDD(org.apache.spark.SparkContext,scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/rdd/UnionRDD/UnionRDD(org.apache.spark.SparkContext,scala.collection.Seq,scala.reflect.ClassTag)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore/putBytes(org.apache.spark.storage.BlockId,long,scala.Function0)|",
    "called": "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+constructor:///scala/runtime/VolatileByteRef/VolatileByteRef(byte)|",
      "|java+method:///scala/package$/Right()|",
      "|java+method:///org/apache/spark/storage/ResultWithDroppedBlocks/success()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$bytes$1(scala.Function0,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)|",
      "|java+constructor:///org/apache/spark/storage/MemoryStore$$anonfun$4/MemoryStore$$anonfun$4(org.apache.spark.storage.MemoryStore,scala.Function0,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)|",
      "|java+constructor:///org/apache/spark/storage/PutResult/PutResult(long,scala.util.Either,scala.collection.Seq)|",
      "|java+method:///java/nio/ByteBuffer/duplicate()|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/tryToPut(org.apache.spark.storage.BlockId,scala.Function0,long,boolean)|",
      "|java+method:///scala/util/Right$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/ResultWithDroppedBlocks/droppedBlocks()|",
      "|java+method:///java/nio/ByteBuffer/limit()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+constructor:///scala/runtime/VolatileByteRef/VolatileByteRef(byte)|",
      "|java+method:///scala/package$/Right()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/tryToPut(org.apache.spark.storage.BlockId,scala.Function0,long,boolean,scala.collection.mutable.Buffer)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/util/Right$/apply(java.lang.Object)|",
      "|java+method:///java/nio/ByteBuffer/limit()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$bytes$1(scala.Function0,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)|",
      "|java+constructor:///org/apache/spark/storage/MemoryStore$$anonfun$3/MemoryStore$$anonfun$3(org.apache.spark.storage.MemoryStore,scala.Function0,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)|",
      "|java+constructor:///org/apache/spark/storage/PutResult/PutResult(long,scala.util.Either,scala.collection.Seq)|",
      "|java+method:///java/nio/ByteBuffer/duplicate()|",
      "|java+method:///scala/Predef$/assert(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/PythonRunner$$anon$1/read()|",
    "called": "|java+method:///scala/runtime/RichInt$/to$extension0(int,int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/python/SpecialLengths$/PYTHON_EXCEPTION_THROWN()|",
      "|java+constructor:///org/apache/spark/TaskKilledException/TaskKilledException()|",
      "|java+method:///org/apache/spark/TaskContext/isInterrupted()|",
      "|java+method:///org/apache/spark/api/python/PythonRunner/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/SparkEnv/releasePythonWorker(java.lang.String,scala.collection.immutable.Map,java.net.Socket)|",
      "|java+method:///scala/Array$/empty(scala.reflect.ClassTag)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/immutable/Range$Inclusive/foreach$mVc$sp(scala.Function1)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/incMemoryBytesSpilled(long)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/Option/get()|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anon$1$$anonfun$read$6/PythonRunner$$anon$1$$anonfun$read$6(org.apache.spark.api.python.PythonRunner$$anon$1)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anon$1$$anonfun$read$2/PythonRunner$$anon$1$$anonfun$read$2(org.apache.spark.api.python.PythonRunner$$anon$1,long,long,long,long)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anon$1$$anonfun$read$7/PythonRunner$$anon$1$$anonfun$read$7(org.apache.spark.api.python.PythonRunner$$anon$1)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anon$1$$anonfun$read$1/PythonRunner$$anon$1$$anonfun$read$1(org.apache.spark.api.python.PythonRunner$$anon$1)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anon$1$$anonfun$read$3/PythonRunner$$anon$1$$anonfun$read$3(org.apache.spark.api.python.PythonRunner$$anon$1)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anon$1$$anonfun$read$4/PythonRunner$$anon$1$$anonfun$read$4(org.apache.spark.api.python.PythonRunner$$anon$1)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anon$1$$anonfun$read$5/PythonRunner$$anon$1$$anonfun$read$5(org.apache.spark.api.python.PythonRunner$$anon$1)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+method:///org/apache/spark/api/python/SpecialLengths$/TIMING_DATA()|",
      "|java+method:///org/apache/spark/api/python/PythonRunner$$anon$1/read()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///org/apache/spark/SparkEnv/isStopped()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///java/io/DataInputStream/readInt()|",
      "|java+method:///scala/reflect/ClassTag$/Byte()|",
      "|java+method:///org/apache/spark/api/python/SpecialLengths$/END_OF_STREAM()|",
      "|java+method:///org/apache/spark/TaskContext/taskMetrics()|",
      "|java+constructor:///java/lang/String/String(byte%5B%5D,java.nio.charset.Charset)|",
      "|java+constructor:///org/apache/spark/api/python/PythonException/PythonException(java.lang.String,java.lang.Exception)|",
      "|java+method:///scala/collection/mutable/Map/toMap(scala.Predef$$less$colon$less)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/incDiskBytesSpilled(long)|",
      "|java+method:///org/apache/spark/api/python/PythonRunner/logInfo(scala.Function0)|",
      "|java+method:///java/io/DataInputStream/readLong()|",
      "|java+method:///org/apache/spark/api/python/PythonRunner$WriterThread/exception()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/api/python/SpecialLengths$/END_OF_DATA_SECTION()|",
      "|java+method:///java/io/DataInputStream/readFully(byte%5B%5D)|",
      "|java+method:///scala/runtime/RichInt$/to$extension0(int,int)|",
      "|java+method:///org/apache/spark/api/python/PythonRunner/logDebug(scala.Function0,java.lang.Throwable)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/api/python/SpecialLengths$/PYTHON_EXCEPTION_THROWN()|",
      "|java+constructor:///org/apache/spark/TaskKilledException/TaskKilledException()|",
      "|java+method:///org/apache/spark/TaskContext/isInterrupted()|",
      "|java+method:///org/apache/spark/api/python/PythonRunner/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/Array$/empty(scala.reflect.ClassTag)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/collection/immutable/Range$Inclusive/foreach$mVc$sp(scala.Function1)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/incMemoryBytesSpilled(long)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/Option/get()|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anon$1$$anonfun$read$6/PythonRunner$$anon$1$$anonfun$read$6(org.apache.spark.api.python.PythonRunner$$anon$1)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anon$1$$anonfun$read$2/PythonRunner$$anon$1$$anonfun$read$2(org.apache.spark.api.python.PythonRunner$$anon$1,long,long,long,long)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anon$1$$anonfun$read$7/PythonRunner$$anon$1$$anonfun$read$7(org.apache.spark.api.python.PythonRunner$$anon$1)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anon$1$$anonfun$read$1/PythonRunner$$anon$1$$anonfun$read$1(org.apache.spark.api.python.PythonRunner$$anon$1)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anon$1$$anonfun$read$3/PythonRunner$$anon$1$$anonfun$read$3(org.apache.spark.api.python.PythonRunner$$anon$1)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anon$1$$anonfun$read$4/PythonRunner$$anon$1$$anonfun$read$4(org.apache.spark.api.python.PythonRunner$$anon$1)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRunner$$anon$1$$anonfun$read$5/PythonRunner$$anon$1$$anonfun$read$5(org.apache.spark.api.python.PythonRunner$$anon$1)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+method:///org/apache/spark/api/python/SpecialLengths$/TIMING_DATA()|",
      "|java+method:///org/apache/spark/api/python/PythonRunner$$anon$1/read()|",
      "|java+method:///org/apache/spark/SparkEnv/isStopped()|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///java/io/DataInputStream/readInt()|",
      "|java+method:///scala/reflect/ClassTag$/Byte()|",
      "|java+method:///org/apache/spark/api/python/SpecialLengths$/END_OF_STREAM()|",
      "|java+method:///org/apache/spark/TaskContext/taskMetrics()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///org/apache/spark/SparkEnv/releasePythonWorker(java.lang.String,scala.collection.immutable.Map,java.net.Socket)|",
      "|java+constructor:///java/lang/String/String(byte%5B%5D,java.nio.charset.Charset)|",
      "|java+constructor:///org/apache/spark/api/python/PythonException/PythonException(java.lang.String,java.lang.Exception)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/incDiskBytesSpilled(long)|",
      "|java+method:///org/apache/spark/api/python/PythonRunner/logInfo(scala.Function0)|",
      "|java+method:///java/io/DataInputStream/readLong()|",
      "|java+method:///org/apache/spark/api/python/PythonRunner$WriterThread/exception()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/api/python/SpecialLengths$/END_OF_DATA_SECTION()|",
      "|java+method:///java/io/DataInputStream/readFully(byte%5B%5D)|",
      "|java+method:///scala/runtime/RichInt$/to$extension0(int,int)|",
      "|java+method:///org/apache/spark/api/python/PythonRunner/logDebug(scala.Function0,java.lang.Throwable)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaSparkContext/parallelizePairs(java.util.List,int)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/java/JavaPairRDD$/fromRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext/sc()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext$/fakeClassTag()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/api/java/JavaPairRDD$/fromRDD(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext/sc()|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext$/fakeClassTag()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$45/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$45$$anonfun$apply$17/StagePage$$anonfun$45$$anonfun$apply$17(org.apache.spark.ui.jobs.StagePage$$anonfun$45)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/outputMetrics()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Option/get()|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$45$$anonfun$apply$4/StagePage$$anonfun$45$$anonfun$apply$4(org.apache.spark.ui.jobs.StagePage$$anonfun$45)|",
      "|java+method:///scala/Option/map(scala.Function1)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/TaskInfo/accumulables()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$45$$anonfun$apply$12/StagePage$$anonfun$45$$anonfun$apply$12(org.apache.spark.ui.jobs.StagePage$$anonfun$45)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$45$$anonfun$apply$13/StagePage$$anonfun$45$$anonfun$apply$13(org.apache.spark.ui.jobs.StagePage$$anonfun$45)|",
      "|java+method:///scala/collection/mutable/ListBuffer/find(scala.Function1)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$45$$anonfun$apply$1/StagePage$$anonfun$45$$anonfun$apply$1(org.apache.spark.ui.jobs.StagePage$$anonfun$45)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskInfo()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/stop(boolean)|",
    "called": "|java+method:///scala/Option$/apply(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/shuffle/IndexShuffleBlockResolver/removeDataByMap(int,int)|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/sorter_$eq(org.apache.spark.shuffle.sort.SortShuffleFileWriter)|",
      "|java+method:///java/lang/System/nanoTime()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/shuffle/sort/SortShuffleWriter$$anonfun$stop$1/SortShuffleWriter$$anonfun$stop$1(org.apache.spark.shuffle.sort.SortShuffleWriter,long)|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/mapStatus()|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/dep()|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/sorter()|",
      "|java+method:///org/apache/spark/ShuffleDependency/shuffleId()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/stopping()|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/stopping_$eq(boolean)|",
      "|java+method:///org/apache/spark/TaskContext/taskMetrics()|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleFileWriter/stop()|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/shuffleWriteMetrics()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/shuffle/IndexShuffleBlockResolver/removeDataByMap(int,int)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/shuffle/sort/SortShuffleWriter$$anonfun$stop$1/SortShuffleWriter$$anonfun$stop$1(org.apache.spark.shuffle.sort.SortShuffleWriter,long)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/shuffleWriteMetrics()|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/mapStatus()|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/dep()|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/sorter()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/stopping()|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/stop()|",
      "|java+method:///java/lang/System/nanoTime()|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/stopping_$eq(boolean)|",
      "|java+method:///org/apache/spark/TaskContext/taskMetrics()|",
      "|java+method:///org/apache/spark/ShuffleDependency/shuffleId()|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/sorter_$eq(org.apache.spark.util.collection.ExternalSorter)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$14/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$14/apply(org.apache.spark.scheduler.cluster.mesos.MesosClusterRetryState)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosClusterScheduler$$anonfun$14/apply(org.apache.spark.deploy.mesos.MesosDriverDescription)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$/jobStartFromJson(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/propertiesFromJson(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class,scala.reflect.Manifest,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$3/JsonProtocol$$anonfun$3()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$52/JsonProtocol$$anonfun$52(scala.collection.immutable.List)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$50/JsonProtocol$$anonfun$50()|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerJobStart/SparkListenerJobStart(int,long,scala.collection.Seq,java.util.Properties)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$51/JsonProtocol$$anonfun$51()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$49/JsonProtocol$$anonfun$49()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/propertiesFromJson(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class,scala.reflect.Manifest,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$3/JsonProtocol$$anonfun$3()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$55/JsonProtocol$$anonfun$55(scala.collection.immutable.List)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$54/JsonProtocol$$anonfun$54()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$53/JsonProtocol$$anonfun$53()|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerJobStart/SparkListenerJobStart(int,long,scala.collection.Seq,java.util.Properties)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$52/JsonProtocol$$anonfun$52()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler/abortStage(org.apache.spark.scheduler.Stage,java.lang.String,scala.Option)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/collection/SetLike/toSeq()|",
      "|java+method:///org/apache/spark/scheduler/Stage/id()|",
      "|java+method:///scala/collection/Seq/isEmpty()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/stageIdToStage()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logInfo(scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/mutable/HashSet/filter(scala.Function1)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$abortStage$1/DAGScheduler$$anonfun$abortStage$1(org.apache.spark.scheduler.DAGScheduler,java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/Stage/latestInfo()|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$20/DAGScheduler$$anonfun$20(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Stage)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$abortStage$2/DAGScheduler$$anonfun$abortStage$2(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Stage)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/activeJobs()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/completionTime_$eq(scala.Option)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/SetLike/toSeq()|",
      "|java+method:///org/apache/spark/scheduler/Stage/id()|",
      "|java+method:///scala/collection/Seq/isEmpty()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/stageIdToStage()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/logInfo(scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/mutable/HashSet/filter(scala.Function1)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$abortStage$1/DAGScheduler$$anonfun$abortStage$1(org.apache.spark.scheduler.DAGScheduler,java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/Stage/latestInfo()|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$18/DAGScheduler$$anonfun$18(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Stage)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$abortStage$2/DAGScheduler$$anonfun$abortStage$2(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.scheduler.Stage)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/activeJobs()|",
      "|java+method:///org/apache/spark/scheduler/StageInfo/completionTime_$eq(scala.Option)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$$anonfun$foreachPartitionAsync$1/apply(scala.collection.Iterator)|",
    "called": "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/java/function/VoidFunction/call(java.lang.Object)|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterator(scala.collection.Iterator)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
      "|java+method:///org/apache/spark/api/java/function/VoidFunction/call(java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$assertNotStopped()|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+constructor:///java/lang/IllegalStateException/IllegalStateException(java.lang.String)|",
      "|java+method:///java/util/concurrent/atomic/AtomicBoolean/get()|",
      "|java+method:///org/apache/spark/SparkContext/stopped()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/CallSite/longForm()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///java/util/concurrent/atomic/AtomicReference/get()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext$/org$apache$spark$SparkContext$$activeContext()|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$creationSite()|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///java/lang/IllegalStateException/IllegalStateException(java.lang.String)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///java/util/concurrent/atomic/AtomicBoolean/get()|",
      "|java+method:///org/apache/spark/SparkContext/stopped()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$class/toAttributeMap(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils,java.util.List)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/collection/mutable/Buffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$toAttributeMap$1/MesosSchedulerUtils$$anonfun$toAttributeMap$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$toAttributeMap$1/MesosSchedulerUtils$$anonfun$toAttributeMap$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/broadcast/TorrentBroadcast$/unBlockifyObject(java.nio.ByteBuffer%5B%5D,org.apache.spark.serializer.Serializer,scala.Option,scala.reflect.ClassTag)|",
    "called": "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/broadcast/TorrentBroadcast$$anonfun$7/TorrentBroadcast$$anonfun$7(java.io.SequenceInputStream)|",
      "|java+constructor:///java/io/SequenceInputStream/SequenceInputStream(java.util.Enumeration)|",
      "|java+constructor:///org/apache/spark/broadcast/TorrentBroadcast$$anonfun$6/TorrentBroadcast$$anonfun$6()|",
      "|java+method:///org/apache/spark/serializer/Serializer/newInstance()|",
      "|java+method:///org/apache/spark/serializer/DeserializationStream/readObject(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/ArrayOps/iterator()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/deserializeStream(java.io.InputStream)|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/broadcast/TorrentBroadcast$$anonfun$unBlockifyObject$1/TorrentBroadcast$$anonfun$unBlockifyObject$1()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/nonEmpty()|",
      "|java+method:///scala/collection/JavaConversions$/asJavaEnumeration(scala.collection.Iterator)|",
      "|java+method:///org/apache/spark/serializer/DeserializationStream/close()|",
      "|java+constructor:///org/apache/spark/broadcast/TorrentBroadcast$$anonfun$8/TorrentBroadcast$$anonfun$8(java.io.SequenceInputStream)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/broadcast/TorrentBroadcast$$anonfun$7/TorrentBroadcast$$anonfun$7(java.io.SequenceInputStream)|",
      "|java+constructor:///java/io/SequenceInputStream/SequenceInputStream(java.util.Enumeration)|",
      "|java+constructor:///org/apache/spark/broadcast/TorrentBroadcast$$anonfun$6/TorrentBroadcast$$anonfun$6()|",
      "|java+method:///org/apache/spark/serializer/Serializer/newInstance()|",
      "|java+method:///org/apache/spark/serializer/DeserializationStream/readObject(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/convert/Decorators$AsJavaEnumeration/asJavaEnumeration()|",
      "|java+method:///scala/collection/mutable/ArrayOps/iterator()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/deserializeStream(java.io.InputStream)|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/broadcast/TorrentBroadcast$$anonfun$unBlockifyObject$1/TorrentBroadcast$$anonfun$unBlockifyObject$1()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/nonEmpty()|",
      "|java+method:///org/apache/spark/serializer/DeserializationStream/close()|",
      "|java+constructor:///org/apache/spark/broadcast/TorrentBroadcast$$anonfun$8/TorrentBroadcast$$anonfun$8(java.io.SequenceInputStream)|",
      "|java+method:///scala/collection/JavaConverters$/asJavaEnumerationConverter(scala.collection.Iterator)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$unrollSafely$2/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$unrollSafely$2/apply()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$unrollSafely$2/apply()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Utils$/bytesToString(long)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/ShuffleMapStage/ShuffleMapStage(int,org.apache.spark.rdd.RDD,int,scala.collection.immutable.List,int,org.apache.spark.util.CallSite,org.apache.spark.ShuffleDependency)|",
    "called": "|java+method:///scala/Array$/fill(int,scala.Function0,scala.reflect.ClassTag)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/scheduler/Stage/Stage(int,org.apache.spark.rdd.RDD,int,scala.collection.immutable.List,int,org.apache.spark.util.CallSite)|",
      "|java+method:///scala/Array$/fill(int,scala.Function0,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/scheduler/ShuffleMapStage$$anonfun$1/ShuffleMapStage$$anonfun$1(org.apache.spark.scheduler.ShuffleMapStage)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/numPartitions()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/scheduler/Stage/Stage(int,org.apache.spark.rdd.RDD,int,scala.collection.immutable.List,int,org.apache.spark.util.CallSite)|",
      "|java+method:///scala/Array$/fill(int,scala.Function0,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/scheduler/ShuffleMapStage$$anonfun$2/ShuffleMapStage$$anonfun$2(org.apache.spark.scheduler.ShuffleMapStage)|",
      "|java+method:///org/apache/spark/scheduler/ShuffleMapStage/numPartitions()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$79/apply(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class,scala.reflect.Manifest,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$79$$anonfun$apply$16/JsonProtocol$$anonfun$79$$anonfun$apply$16(org.apache.spark.util.JsonProtocol$$anonfun$79)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Boolean()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext$$anonfun$getRDDStorageInfo$1/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkContext$$anonfun$getRDDStorageInfo$1/apply(org.apache.spark.storage.RDDInfo)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/SparkContext$$anonfun$getRDDStorageInfo$1/apply(org.apache.spark.rdd.RDD)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSetManager/resourceOffer(java.lang.String,java.lang.String,scala.Enumeration$Value)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/dagScheduler()|",
      "|java+method:///org/apache/spark/SparkContext/addedFiles()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/tasks()|",
      "|java+method:///scala/Tuple3/_1()|",
      "|java+method:///scala/Tuple3/_2()|",
      "|java+method:///scala/collection/immutable/List/size()|",
      "|java+method:///java/nio/ByteBuffer/limit()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/emittedTaskSizeWarning_$eq(boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskDescription/TaskDescription(long,int,java.lang.String,java.lang.String,int,java.nio.ByteBuffer)|",
      "|java+method:///scala/Some/x()|",
      "|java+constructor:///org/apache/spark/TaskNotSerializableException/TaskNotSerializableException(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/sc()|",
      "|java+method:///scala/Enumeration$Value/$greater(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager$/TASK_SIZE_TO_WARN_KB()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/addRunningTask(long)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/getLocalityIndex(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/copiesRunning()|",
      "|java+method:///org/apache/spark/scheduler/TaskSet/id()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/newTaskId()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/taskAttempts()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/lastLaunchTime_$eq(long)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$resourceOffer$1/TaskSetManager$$anonfun$resourceOffer$1(org.apache.spark.scheduler.TaskSetManager,org.apache.spark.scheduler.Task,java.nio.ByteBuffer)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logInfo(scala.Function0)|",
      "|java+method:///scala/Tuple3/_3()|",
      "|java+method:///org/apache/spark/scheduler/TaskLocality$/NO_PREF()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/dequeueTask(java.lang.String,java.lang.String,scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/getAllowedLocalityLevel(long)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/isZombie()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$10/TaskSetManager$$anonfun$10(org.apache.spark.scheduler.TaskSetManager,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/taskStarted(org.apache.spark.scheduler.Task,org.apache.spark.scheduler.TaskInfo)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/emittedTaskSizeWarning()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/currentLocalityIndex_$eq(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/Task$/serializeWithDependencies(org.apache.spark.scheduler.Task,scala.collection.mutable.HashMap,scala.collection.mutable.HashMap,org.apache.spark.serializer.SerializerInstance)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/abort$default$2()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/id()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/taskSet()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/List/$colon$colon(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/abort(java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/ser()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/taskInfos()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskInfo/TaskInfo(long,int,int,long,java.lang.String,java.lang.String,scala.Enumeration$Value,boolean)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$resourceOffer$2/TaskSetManager$$anonfun$resourceOffer$2(org.apache.spark.scheduler.TaskSetManager,java.lang.String,scala.Enumeration$Value,long,java.nio.ByteBuffer,java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkContext/addedJars()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/dagScheduler()|",
      "|java+method:///org/apache/spark/SparkContext/addedFiles()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/tasks()|",
      "|java+method:///scala/Tuple3/_1()|",
      "|java+method:///scala/Tuple3/_2()|",
      "|java+method:///scala/collection/immutable/List/size()|",
      "|java+method:///java/nio/ByteBuffer/limit()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/emittedTaskSizeWarning_$eq(boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskDescription/TaskDescription(long,int,java.lang.String,java.lang.String,int,java.nio.ByteBuffer)|",
      "|java+method:///scala/Some/x()|",
      "|java+constructor:///org/apache/spark/TaskNotSerializableException/TaskNotSerializableException(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/sc()|",
      "|java+method:///scala/Enumeration$Value/$greater(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager$/TASK_SIZE_TO_WARN_KB()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/addRunningTask(long)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/getLocalityIndex(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/copiesRunning()|",
      "|java+method:///org/apache/spark/scheduler/TaskSet/id()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/newTaskId()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/taskAttempts()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/lastLaunchTime_$eq(long)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$resourceOffer$1/TaskSetManager$$anonfun$resourceOffer$1(org.apache.spark.scheduler.TaskSetManager,org.apache.spark.scheduler.Task,java.nio.ByteBuffer)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logInfo(scala.Function0)|",
      "|java+method:///scala/Tuple3/_3()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$resourceOffer$2/TaskSetManager$$anonfun$resourceOffer$2(org.apache.spark.scheduler.TaskSetManager,java.lang.String,scala.Enumeration$Value,org.apache.spark.scheduler.Task,long,java.nio.ByteBuffer,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/TaskLocality$/NO_PREF()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/dequeueTask(java.lang.String,java.lang.String,scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/getAllowedLocalityLevel(long)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/isZombie()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$10/TaskSetManager$$anonfun$10(org.apache.spark.scheduler.TaskSetManager,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/taskStarted(org.apache.spark.scheduler.Task,org.apache.spark.scheduler.TaskInfo)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/emittedTaskSizeWarning()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/currentLocalityIndex_$eq(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/Task$/serializeWithDependencies(org.apache.spark.scheduler.Task,scala.collection.mutable.HashMap,scala.collection.mutable.HashMap,org.apache.spark.serializer.SerializerInstance)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/abort$default$2()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/id()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/taskSet()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/List/$colon$colon(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/abort(java.lang.String,scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/ser()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/taskInfos()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskInfo/TaskInfo(long,int,int,long,java.lang.String,java.lang.String,scala.Enumeration$Value,boolean)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkContext/addedJars()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/util/Clock/getTimeMillis()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1/applyOrElse(java.lang.Object,scala.Function1)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/rpc/RpcCallContext/reply(java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1,org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages$KillExecutors)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1,org.apache.spark.rpc.RpcEndpointRef,org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages$KillExecutors)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/logWarning(scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/askAmExecutor()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1,org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages$RequestExecutors)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$9/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$9(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+method:///scala/Some/x()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$7/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$7(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+method:///scala/concurrent/Future/onFailure(scala.PartialFunction,scala.concurrent.ExecutionContext)|",
      "|java+method:///scala/concurrent/Future$/apply(scala.Function0,scala.concurrent.ExecutionContext)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1,org.apache.spark.rpc.RpcEndpointRef,org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages$RequestExecutors)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/org$apache$spark$scheduler$cluster$YarnSchedulerBackend$YarnSchedulerEndpoint$$amEndpoint()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rpc/RpcCallContext/reply(java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1,org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages$KillExecutors)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1,org.apache.spark.rpc.RpcEndpointRef,org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages$KillExecutors)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/logWarning(scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$11/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$11(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/askAmExecutor()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$1(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1,org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages$RequestExecutors)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$9/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$9(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1)|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///scala/concurrent/Future/onFailure(scala.PartialFunction,scala.concurrent.ExecutionContext)|",
      "|java+method:///scala/concurrent/Future$/apply(scala.Function0,scala.concurrent.ExecutionContext)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1,org.apache.spark.rpc.RpcEndpointRef,org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages$RequestExecutors)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/org$apache$spark$scheduler$cluster$YarnSchedulerBackend$YarnSchedulerEndpoint$$amEndpoint()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/ExecutorExited/productElement(int)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///org/apache/spark/scheduler/ExecutorExited/exitCode()|",
      "|java+constructor:///java/lang/IndexOutOfBoundsException/IndexOutOfBoundsException(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/ExecutorExited/reason()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///org/apache/spark/scheduler/ExecutorExited/exitCode()|",
      "|java+constructor:///java/lang/IndexOutOfBoundsException/IndexOutOfBoundsException(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/ExecutorExited/exitCausedByApp()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/metrics/MetricsConfig/subProperties(java.util.Properties,scala.util.matching.Regex)|",
    "called": "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/propertiesAsScalaMap(java.util.Properties)|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$subProperties$1/MetricsConfig$$anonfun$subProperties$1(org.apache.spark.metrics.MetricsConfig,scala.util.matching.Regex,scala.collection.mutable.HashMap)|",
      "|java+method:///scala/collection/mutable/Map/foreach(scala.Function1)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/propertiesAsScalaMapConverter(java.util.Properties)|",
      "|java+constructor:///org/apache/spark/metrics/MetricsConfig$$anonfun$subProperties$1/MetricsConfig$$anonfun$subProperties$1(org.apache.spark.metrics.MetricsConfig,scala.util.matching.Regex,scala.collection.mutable.HashMap)|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/rdd/HadoopRDD$$anon$1/HadoopRDD$$anon$1(org.apache.spark.rdd.HadoopRDD,org.apache.spark.Partition,org.apache.spark.TaskContext)|",
    "called": "|java+method:///scala/Option/orElse(scala.Function0)|",
    "v1Body": [
      "|java+method:///org/apache/hadoop/mapred/InputFormat/getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter)|",
      "|java+method:///org/apache/spark/TaskContext/taskMetrics()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/getInputFormat(org.apache.hadoop.mapred.JobConf)|",
      "|java+method:///org/apache/spark/rdd/HadoopPartition/inputSplit()|",
      "|java+method:///org/apache/hadoop/mapred/RecordReader/createValue()|",
      "|java+method:///org/apache/spark/TaskContext/attemptNumber()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/inputMetrics()|",
      "|java+method:///org/apache/spark/executor/InputMetrics/setBytesReadCallback(scala.Option)|",
      "|java+method:///org/apache/spark/SerializableWritable/value()|",
      "|java+method:///org/apache/spark/TaskContext/stageId()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/split()|",
      "|java+method:///java/text/SimpleDateFormat/format(java.util.Date)|",
      "|java+constructor:///org/apache/spark/rdd/HadoopRDD$$anon$1$$anonfun$3/HadoopRDD$$anon$1$$anonfun$3(org.apache.spark.rdd.HadoopRDD$$anon$1)|",
      "|java+constructor:///org/apache/spark/rdd/HadoopRDD$$anon$1$$anonfun$2/HadoopRDD$$anon$1$$anonfun$2(org.apache.spark.rdd.HadoopRDD$$anon$1)|",
      "|java+constructor:///org/apache/spark/rdd/HadoopRDD$$anon$1$$anonfun$1/HadoopRDD$$anon$1$$anonfun$1(org.apache.spark.rdd.HadoopRDD$$anon$1)|",
      "|java+method:///org/apache/hadoop/mapred/RecordReader/createKey()|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/getInputMetricsForReadMethod(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/org$apache$spark$rdd$HadoopRDD$$createTime()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/inputFormat()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/jobConf()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/reader()|",
      "|java+constructor:///org/apache/spark/util/NextIterator/NextIterator()|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/getJobConf()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/reader_$eq(org.apache.hadoop.mapred.RecordReader)|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/bytesReadCallback()|",
      "|java+method:///org/apache/spark/TaskContext/addTaskCompletionListener(scala.Function1)|",
      "|java+constructor:///java/text/SimpleDateFormat/SimpleDateFormat(java.lang.String)|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$/addLocalConfiguration(java.lang.String,int,int,int,org.apache.hadoop.mapred.JobConf)|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|",
      "|java+method:///org/apache/spark/executor/InputMetrics/bytesReadCallback()|",
      "|java+method:///org/apache/spark/executor/DataReadMethod$/Hadoop()|",
      "|java+method:///org/apache/spark/Partition/index()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/hadoop/fs/Path/toString()|",
      "|java+method:///org/apache/hadoop/mapred/InputFormat/getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter)|",
      "|java+method:///org/apache/spark/TaskContext/taskMetrics()|",
      "|java+method:///org/apache/spark/rdd/SqlNewHadoopRDDState$/setInputFileName(java.lang.String)|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/getInputFormat(org.apache.hadoop.mapred.JobConf)|",
      "|java+method:///org/apache/spark/rdd/HadoopPartition/inputSplit()|",
      "|java+method:///org/apache/hadoop/mapred/RecordReader/createValue()|",
      "|java+method:///org/apache/spark/TaskContext/attemptNumber()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/inputMetrics()|",
      "|java+method:///org/apache/spark/executor/InputMetrics/setBytesReadCallback(scala.Option)|",
      "|java+method:///org/apache/spark/SerializableWritable/value()|",
      "|java+method:///org/apache/spark/TaskContext/stageId()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/split()|",
      "|java+method:///java/text/SimpleDateFormat/format(java.util.Date)|",
      "|java+constructor:///org/apache/spark/rdd/HadoopRDD$$anon$1$$anonfun$3/HadoopRDD$$anon$1$$anonfun$3(org.apache.spark.rdd.HadoopRDD$$anon$1)|",
      "|java+constructor:///org/apache/spark/rdd/HadoopRDD$$anon$1$$anonfun$2/HadoopRDD$$anon$1$$anonfun$2(org.apache.spark.rdd.HadoopRDD$$anon$1)|",
      "|java+constructor:///org/apache/spark/rdd/HadoopRDD$$anon$1$$anonfun$1/HadoopRDD$$anon$1$$anonfun$1(org.apache.spark.rdd.HadoopRDD$$anon$1)|",
      "|java+method:///org/apache/hadoop/mapred/RecordReader/createKey()|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/getInputMetricsForReadMethod(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/rdd/SqlNewHadoopRDDState$/unsetInputFileName()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/org$apache$spark$rdd$HadoopRDD$$createTime()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/inputFormat()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/jobConf()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/reader()|",
      "|java+constructor:///org/apache/spark/util/NextIterator/NextIterator()|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD/getJobConf()|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/reader_$eq(org.apache.hadoop.mapred.RecordReader)|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$$anon$1/bytesReadCallback()|",
      "|java+method:///org/apache/spark/TaskContext/addTaskCompletionListener(scala.Function1)|",
      "|java+constructor:///java/text/SimpleDateFormat/SimpleDateFormat(java.lang.String)|",
      "|java+method:///org/apache/spark/rdd/HadoopRDD$/addLocalConfiguration(java.lang.String,int,int,int,org.apache.hadoop.mapred.JobConf)|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|",
      "|java+method:///org/apache/hadoop/mapred/FileSplit/getPath()|",
      "|java+method:///org/apache/spark/executor/InputMetrics/bytesReadCallback()|",
      "|java+method:///org/apache/spark/executor/DataReadMethod$/Hadoop()|",
      "|java+method:///org/apache/spark/Partition/index()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient/readResponse(java.net.HttpURLConnection)|",
    "called": "|java+constructor:///scala/concurrent/duration/package$DurationInt/package$DurationInt(int)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/rest/SubmitRestProtocolMessage/toJson()|",
      "|java+constructor:///org/apache/spark/deploy/rest/SubmitRestProtocolException/SubmitRestProtocolException(java.lang.String,java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$readResponse$1/RestSubmissionClient$$anonfun$readResponse$1(org.apache.spark.deploy.rest.RestSubmissionClient,java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///java/net/HttpURLConnection/getErrorStream()|",
      "|java+method:///org/apache/spark/deploy/rest/SubmitRestProtocolMessage$/fromJson(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/rest/SubmitRestConnectionException/SubmitRestConnectionException(java.lang.String,java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$readResponse$2/RestSubmissionClient$$anonfun$readResponse$2(org.apache.spark.deploy.rest.RestSubmissionClient,org.apache.spark.deploy.rest.ErrorResponse)|",
      "|java+method:///scala/io/Source$/fromInputStream(java.io.InputStream,scala.io.Codec)|",
      "|java+method:///scala/io/Codec$/fallbackSystemCodec()|",
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/rest/SubmitRestProtocolException$/$lessinit$greater$default$2()|",
      "|java+method:///java/net/HttpURLConnection/getResponseCode()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///java/net/HttpURLConnection/getInputStream()|",
      "|java+method:///org/apache/spark/deploy/rest/SubmitRestProtocolMessage/validate()|",
      "|java+method:///scala/io/BufferedSource/mkString()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+constructor:///scala/concurrent/duration/package$DurationInt/package$DurationInt(int)|",
      "|java+method:///scala/concurrent/duration/package$/DurationInt(int)|",
      "|java+constructor:///org/apache/spark/deploy/rest/SubmitRestProtocolException/SubmitRestProtocolException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/concurrent/Future$/apply(scala.Function0,scala.concurrent.ExecutionContext)|",
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$1/RestSubmissionClient$$anonfun$1(org.apache.spark.deploy.rest.RestSubmissionClient,java.net.HttpURLConnection)|",
      "|java+method:///scala/concurrent/Await$/result(scala.concurrent.Awaitable,scala.concurrent.duration.Duration)|",
      "|java+method:///scala/concurrent/ExecutionContext$Implicits$/global()|",
      "|java+constructor:///org/apache/spark/deploy/rest/SubmitRestConnectionException/SubmitRestConnectionException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/concurrent/duration/package$DurationInt/seconds()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$/storageLevelFromJson(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/Option/getOrElse(scala.Function0)|",
    "v1Body": [
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/JsonAST$JValue/toSome()|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/apache/spark/storage/StorageLevel$/apply(boolean,boolean,boolean,boolean,int)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Boolean()|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$82/JsonProtocol$$anonfun$82(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/JsonAST$JValue/toSome()|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/apache/spark/storage/StorageLevel$/apply(boolean,boolean,boolean,boolean,int)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Boolean()|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$89/JsonProtocol$$anonfun$89(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$logMemoryUsage$1/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$logMemoryUsage$1/apply()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/numTasksUnrolling()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Utils$/bytesToString(long)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$blocksMemoryUsed()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$logMemoryUsage$1/apply()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$maxMemory()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$numTasksUnrolling()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Utils$/bytesToString(long)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$memoryUsed()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/currentUnrollMemory()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receive$1/applyOrElse(java.lang.Object,scala.Function1)|",
    "called": "|java+method:///scala/Option$/apply(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RemoveExecutor/reason()|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend/org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$addWebUIFilter(java.lang.String,scala.collection.immutable.Map,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend/removeExecutor(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisterClusterManager/am()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RemoveExecutor/executorId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/org$apache$spark$scheduler$cluster$YarnSchedulerBackend$YarnSchedulerEndpoint$$amEndpoint_$eq(scala.Option)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/org$apache$spark$scheduler$cluster$YarnSchedulerBackend$YarnSchedulerEndpoint$$$outer()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$AddWebUIFilter/filterParams()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$AddWebUIFilter/filterName()|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$AddWebUIFilter/proxyBase()|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/logInfo(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$5/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$5(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receive$1,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RemoveExecutor/reason()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RegisterClusterManager/am()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$RemoveExecutor/executorId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/org$apache$spark$scheduler$cluster$YarnSchedulerBackend$YarnSchedulerEndpoint$$amEndpoint_$eq(scala.Option)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/org$apache$spark$scheduler$cluster$YarnSchedulerBackend$YarnSchedulerEndpoint$$$outer()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$AddWebUIFilter/filterParams()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$6/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$6(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receive$1,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$AddWebUIFilter/filterName()|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessages$AddWebUIFilter/proxyBase()|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend/removeExecutor(java.lang.String,org.apache.spark.scheduler.ExecutorLossReason)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend/org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$addWebUIFilter(java.lang.String,scala.collection.immutable.Map,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/logWarning(scala.Function0)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$7/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receive$1$$anonfun$applyOrElse$7(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receive$1,org.apache.spark.scheduler.ExecutorLossReason)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/executorTerminated(org.apache.mesos.SchedulerDriver,java.lang.String,java.lang.String)|",
    "called": "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/removeExecutor(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$pendingRemovedSlaveIds()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/taskIdToSlaveId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/stateLock()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/remove(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/sparkExecutorId(java.lang.String,java.lang.String)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/inverse()|",
      "|java+method:///org/spark-project/guava/collect/BiMap/get(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/Map/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/slaveIdsWithExecutors()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/org$apache$spark$scheduler$cluster$mesos$CoarseMesosSchedulerBackend$$pendingRemovedSlaveIds()|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+constructor:///org/apache/spark/scheduler/SlaveLost/SlaveLost(java.lang.String)|",
      "|java+method:///org/spark-project/guava/collect/BiMap/containsKey(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/taskIdToSlaveId()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/slaveIdsWithExecutors()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/removeExecutor(java.lang.String,org.apache.spark.scheduler.ExecutorLossReason)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/stateLock()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/remove(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/sparkExecutorId(java.lang.String,java.lang.String)|",
      "|java+method:///org/spark-project/guava/collect/HashBiMap/inverse()|",
      "|java+method:///org/spark-project/guava/collect/BiMap/get(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashSet/$minus$eq(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$10/apply()|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/ApplicationDescription/name()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$10/apply()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$receive$1$$anonfun$applyOrElse$10/apply()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/PythonUtils$/toSeq(java.util.List)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/immutable/List/toSeq()|",
      "|java+method:///scala/collection/mutable/Buffer/toList()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SecurityManager/generateSecretKey()|",
    "called": "|java+method:///scala/Option$/apply(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///akka/util/Crypt$/generateSecureCookie()|",
      "|java+method:///org/apache/spark/SecurityManager$/SPARK_AUTH_SECRET_CONF()|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///org/apache/spark/SecurityManager/isAuthenticationEnabled()|",
      "|java+constructor:///org/apache/hadoop/io/Text/Text(byte%5B%5D)|",
      "|java+method:///org/apache/hadoop/io/Text/toString()|",
      "|java+method:///org/apache/spark/SecurityManager/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/getSecretKeyFromUserCredentials(java.lang.String)|",
      "|java+method:///scala/sys/package$/env()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/addSecretKeyToUserCredentials(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SecurityManager/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/SecurityManager$/ENV_AUTH_SECRET()|",
      "|java+constructor:///org/apache/spark/SecurityManager$$anonfun$9/SecurityManager$$anonfun$9(org.apache.spark.SecurityManager)|",
      "|java+constructor:///org/apache/spark/SecurityManager$$anonfun$8/SecurityManager$$anonfun$8(org.apache.spark.SecurityManager)|",
      "|java+constructor:///org/apache/spark/SecurityManager$$anonfun$7/SecurityManager$$anonfun$7(org.apache.spark.SecurityManager)|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/isYarnMode()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/Exception/Exception(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/SecurityManager/sparkSecretLookupKey()|",
      "|java+constructor:///org/apache/spark/SecurityManager$$anonfun$10/SecurityManager$$anonfun$10(org.apache.spark.SecurityManager)|"
    ],
    "v2Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///org/apache/spark/SecurityManager$/SPARK_AUTH_SECRET_CONF()|",
      "|java+method:///scala/Option/orElse(scala.Function0)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/SecurityManager/isAuthenticationEnabled()|",
      "|java+constructor:///java/security/SecureRandom/SecureRandom()|",
      "|java+constructor:///org/apache/hadoop/io/Text/Text(byte%5B%5D)|",
      "|java+method:///org/apache/hadoop/io/Text/toString()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/getSecretKeyFromUserCredentials(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/addSecretKeyToUserCredentials(java.lang.String,java.lang.String)|",
      "|java+method:///org/apache/spark/SecurityManager/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/SecurityManager$/ENV_AUTH_SECRET()|",
      "|java+constructor:///org/apache/spark/SecurityManager$$anonfun$7/SecurityManager$$anonfun$7(org.apache.spark.SecurityManager)|",
      "|java+constructor:///org/apache/spark/SecurityManager$$anonfun$generateSecretKey$1/SecurityManager$$anonfun$generateSecretKey$1(org.apache.spark.SecurityManager)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkConf/getenv(java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/SecurityManager$/SECRET_LOOKUP_KEY()|",
      "|java+method:///java/security/SecureRandom/nextBytes(byte%5B%5D)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/isYarnMode()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+method:///org/spark-project/guava/hash/HashCode/toString()|",
      "|java+method:///org/spark-project/guava/hash/HashCodes/fromBytes(byte%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/write(scala.collection.Iterator)|",
    "called": "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/BlockManager/shuffleServerId()|",
      "|java+method:///org/apache/spark/ShuffleDependency/serializer()|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/sorter_$eq(org.apache.spark.shuffle.sort.SortShuffleFileWriter)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/Partitioner/numPartitions()|",
      "|java+method:///org/apache/spark/shuffle/IndexShuffleBlockResolver$/NOOP_REDUCE_ID()|",
      "|java+constructor:///org/apache/spark/storage/ShuffleBlockId/ShuffleBlockId(int,int,int)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/blockManager()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/mapStatus_$eq(org.apache.spark.scheduler.MapStatus)|",
      "|java+method:///org/apache/spark/shuffle/IndexShuffleBlockResolver/writeIndexFile(int,int,long%5B%5D)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/dep()|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/sorter()|",
      "|java+method:///org/apache/spark/ShuffleDependency/keyOrdering()|",
      "|java+constructor:///org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter/BypassMergeSortShuffleWriter(org.apache.spark.SparkConf,org.apache.spark.storage.BlockManager,org.apache.spark.Partitioner,org.apache.spark.executor.ShuffleWriteMetrics,org.apache.spark.serializer.Serializer)|",
      "|java+method:///org/apache/spark/ShuffleDependency/partitioner()|",
      "|java+method:///org/apache/spark/shuffle/IndexShuffleBlockResolver/getDataFile(int,int)|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleFileWriter/insertAll(scala.collection.Iterator)|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalSorter/ExternalSorter(scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/writeMetrics()|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter$/shouldBypassMergeSort(org.apache.spark.SparkConf,int,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleFileWriter/writePartitionedFile(org.apache.spark.storage.BlockId,org.apache.spark.TaskContext,java.io.File)|",
      "|java+method:///org/apache/spark/serializer/Serializer$/getSerializer(scala.Option)|",
      "|java+method:///org/apache/spark/SparkEnv/conf()|",
      "|java+method:///org/apache/spark/scheduler/MapStatus$/apply(org.apache.spark.storage.BlockManagerId,long%5B%5D)|",
      "|java+method:///org/apache/spark/ShuffleDependency/aggregator()|",
      "|java+method:///org/apache/spark/ShuffleDependency/mapSideCombine()|",
      "|java+method:///org/apache/spark/ShuffleDependency/shuffleId()|",
      "|java+constructor:///org/apache/spark/shuffle/sort/SortShuffleWriter$$anonfun$write$1/SortShuffleWriter$$anonfun$write$1(org.apache.spark.shuffle.sort.SortShuffleWriter)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/BlockManager/shuffleServerId()|",
      "|java+method:///org/apache/spark/shuffle/IndexShuffleBlockResolver/writeIndexFileAndCommit(int,int,long%5B%5D,java.io.File)|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/writePartitionedFile(org.apache.spark.storage.BlockId,java.io.File)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/shuffle/IndexShuffleBlockResolver$/NOOP_REDUCE_ID()|",
      "|java+constructor:///org/apache/spark/storage/ShuffleBlockId/ShuffleBlockId(int,int,int)|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/blockManager()|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/util/Utils$/tempFileWith(java.io.File)|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/mapStatus_$eq(org.apache.spark.scheduler.MapStatus)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/dep()|",
      "|java+method:///org/apache/spark/ShuffleDependency/keyOrdering()|",
      "|java+method:///org/apache/spark/ShuffleDependency/partitioner()|",
      "|java+method:///org/apache/spark/shuffle/IndexShuffleBlockResolver/getDataFile(int,int)|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalSorter/ExternalSorter(org.apache.spark.TaskContext,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/ShuffleDependency/serializer()|",
      "|java+method:///org/apache/spark/scheduler/MapStatus$/apply(org.apache.spark.storage.BlockManagerId,long%5B%5D)|",
      "|java+method:///org/apache/spark/ShuffleDependency/aggregator()|",
      "|java+method:///org/apache/spark/util/collection/ExternalSorter/insertAll(scala.collection.Iterator)|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/sorter()|",
      "|java+method:///org/apache/spark/ShuffleDependency/mapSideCombine()|",
      "|java+constructor:///org/apache/spark/shuffle/sort/SortShuffleWriter$$anonfun$write$1/SortShuffleWriter$$anonfun$write$1(org.apache.spark.shuffle.sort.SortShuffleWriter)|",
      "|java+method:///org/apache/spark/ShuffleDependency/shuffleId()|",
      "|java+method:///org/apache/spark/shuffle/sort/SortShuffleWriter/sorter_$eq(org.apache.spark.util.collection.ExternalSorter)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$/executorMetricsUpdateFromJson(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$59/JsonProtocol$$anonfun$59()|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class,scala.reflect.Manifest,scala.collection.Seq)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerExecutorMetricsUpdate/SparkListenerExecutorMetricsUpdate(java.lang.String,scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$62/JsonProtocol$$anonfun$62()|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class,scala.reflect.Manifest,scala.collection.Seq)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+constructor:///org/apache/spark/scheduler/SparkListenerExecutorMetricsUpdate/SparkListenerExecutorMetricsUpdate(java.lang.String,scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore/putArray(org.apache.spark.storage.BlockId,java.lang.Object%5B%5D,org.apache.spark.storage.StorageLevel,boolean)|",
    "called": "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///java/nio/ByteBuffer/duplicate()|",
      "|java+method:///org/apache/spark/storage/StorageLevel/deserialized()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataSerialize(org.apache.spark.storage.BlockId,scala.collection.Iterator,org.apache.spark.serializer.Serializer)|",
      "|java+method:///scala/package$/Right()|",
      "|java+method:///scala/util/Right$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/estimate(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockStore/blockManager()|",
      "|java+constructor:///org/apache/spark/storage/PutResult/PutResult(long,scala.util.Either,scala.collection.Seq)|",
      "|java+method:///scala/util/Left$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataSerialize$default$3()|",
      "|java+method:///scala/package$/Left()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/tryToPut(org.apache.spark.storage.BlockId,java.lang.Object,long,boolean)|",
      "|java+method:///scala/collection/mutable/ArrayOps/iterator()|",
      "|java+method:///org/apache/spark/storage/ResultWithDroppedBlocks/droppedBlocks()|",
      "|java+method:///java/nio/ByteBuffer/limit()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataSerialize(org.apache.spark.storage.BlockId,scala.collection.Iterator)|",
      "|java+method:///java/nio/ByteBuffer/duplicate()|",
      "|java+method:///scala/package$/Left()|",
      "|java+method:///scala/collection/mutable/ArrayOps/iterator()|",
      "|java+method:///scala/package$/Right()|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/util/Right$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/SizeEstimator$/estimate(java.lang.Object)|",
      "|java+method:///java/nio/ByteBuffer/limit()|",
      "|java+method:///org/apache/spark/storage/BlockStore/blockManager()|",
      "|java+constructor:///org/apache/spark/storage/PutResult/PutResult(long,scala.util.Either,scala.collection.Seq)|",
      "|java+method:///scala/util/Left$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/deserialized()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/tryToPut(org.apache.spark.storage.BlockId,java.lang.Object,long,boolean,scala.collection.mutable.Buffer)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master/onStop()|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/metrics/MetricsSystem/report()|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/stop()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI/stop()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$recoveryCompletionTask()|",
      "|java+method:///java/util/concurrent/ScheduledExecutorService/shutdownNow()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$webUi()|",
      "|java+method:///org/apache/spark/deploy/master/Master/leaderElectionAgent()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$applicationMetricsSystem()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/master/Master/checkForWorkerTimeOutTask()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$onStop$1/Master$$anonfun$onStop$1(org.apache.spark.deploy.master.Master)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$forwardMessageThread()|",
      "|java+method:///org/apache/spark/deploy/master/Master/masterMetricsSystem()|",
      "|java+method:///java/util/concurrent/ScheduledFuture/cancel(boolean)|",
      "|java+method:///org/apache/spark/deploy/master/Master/restServer()|",
      "|java+method:///org/apache/spark/deploy/master/LeaderElectionAgent/stop()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$persistenceEngine()|",
      "|java+method:///org/apache/spark/deploy/master/PersistenceEngine/close()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/metrics/MetricsSystem/report()|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/stop()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/deploy/master/ui/MasterWebUI/stop()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$recoveryCompletionTask()|",
      "|java+method:///java/util/concurrent/ScheduledExecutorService/shutdownNow()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$webUi()|",
      "|java+method:///org/apache/spark/deploy/master/Master/leaderElectionAgent()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$applicationMetricsSystem()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/master/Master/checkForWorkerTimeOutTask()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$onStop$1/Master$$anonfun$onStop$1(org.apache.spark.deploy.master.Master)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$forwardMessageThread()|",
      "|java+method:///org/apache/spark/deploy/master/Master/masterMetricsSystem()|",
      "|java+method:///java/util/concurrent/ScheduledFuture/cancel(boolean)|",
      "|java+method:///org/apache/spark/deploy/master/Master/restServer()|",
      "|java+method:///java/util/concurrent/ExecutorService/shutdownNow()|",
      "|java+method:///org/apache/spark/deploy/master/Master/rebuildUIThread()|",
      "|java+method:///org/apache/spark/deploy/master/LeaderElectionAgent/stop()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$persistenceEngine()|",
      "|java+method:///org/apache/spark/deploy/master/PersistenceEngine/close()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/RangePartitioner$$anonfun$10/apply(scala.Tuple3)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Tuple3/_2()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/Tuple3/_2()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$3/RestSubmissionClient$$anonfun$3(org.apache.spark.deploy.rest.RestSubmissionClient)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$35/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$35/apply(org.apache.spark.storage.RDDInfo)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$35/apply(org.apache.spark.rdd.RDD)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$39/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+method:///scala/Option/isDefined()|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/resultSerializationTime()|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/Option/isDefined()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskInfo()|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/status()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$unrollSafely$1/apply()|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
    "v1Body": [
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$unrollSafely$1/apply()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Utils$/bytesToString(long)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$unrollSafely$1/apply()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$unrollSafely$1/apply$mcJ$sp()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext$/org$apache$spark$SparkContext$$createTaskScheduler(org.apache.spark.SparkContext,java.lang.String)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkMasterRegex$/MESOS_REGEX()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/SparkDeploySchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.SparkContext,java.lang.String%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/LinearSeqOptimized/lengthCompare(int)|",
      "|java+method:///scala/collection/immutable/StringOps/stripPrefix(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkMasterRegex$/LOCAL_N_REGEX()|",
      "|java+method:///org/apache/spark/SparkContext/getConf()|",
      "|java+method:///java/lang/reflect/Constructor/newInstance(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/SparkContext/executorMemory()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkMasterRegex$/SPARK_REGEX()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$org$apache$spark$SparkContext$$createTaskScheduler$1/SparkContext$$anonfun$org$apache$spark$SparkContext$$createTaskScheduler$1(org.apache.spark.deploy.LocalSparkCluster)|",
      "|java+method:///org/apache/mesos/MesosNativeLibrary/load()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/LinearSeqOptimized/apply(int)|",
      "|java+method:///scala/util/matching/Regex/unapplySeq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$45/SparkContext$$anonfun$45()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$org$apache$spark$SparkContext$$createTaskScheduler$2/SparkContext$$anonfun$org$apache$spark$SparkContext$$createTaskScheduler$2()|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/shutdownCallback_$eq(scala.Function1)|",
      "|java+method:///org/apache/spark/util/Utils$/classForName(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/MesosSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.SparkContext,java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SimrSchedulerBackend/SimrSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.SparkContext,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkMasterRegex$/LOCAL_N_FAILURES_REGEX()|",
      "|java+method:///org/apache/spark/SparkMasterRegex$/LOCAL_CLUSTER_REGEX()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/initialize(org.apache.spark.scheduler.SchedulerBackend)|",
      "|java+method:///org/apache/spark/SparkEnv/securityManager()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/scheduler/local/LocalBackend/LocalBackend(org.apache.spark.SparkConf,org.apache.spark.scheduler.TaskSchedulerImpl,int)|",
      "|java+constructor:///org/apache/spark/deploy/LocalSparkCluster/LocalSparkCluster(int,int,int,org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/SparkContext$/localCpuCount$1()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext$/localCpuCount$2()|",
      "|java+method:///org/apache/spark/deploy/LocalSparkCluster/start()|",
      "|java+method:///java/lang/Class/getConstructor(java.lang.Class%5B%5D)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/CoarseMesosSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.SparkContext,java.lang.String,org.apache.spark.SecurityManager)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl/TaskSchedulerImpl(org.apache.spark.SparkContext)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl/TaskSchedulerImpl(org.apache.spark.SparkContext,int,boolean)|",
      "|java+method:///org/apache/spark/SparkMasterRegex$/SIMR_REGEX()|",
      "|java+method:///org/apache/spark/SparkContext$/logWarning(scala.Function0)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkMasterRegex$/MESOS_REGEX()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/SparkDeploySchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.SparkContext,java.lang.String%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/collection/LinearSeqOptimized/lengthCompare(int)|",
      "|java+method:///java/lang/String/startsWith(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkMasterRegex$/LOCAL_N_REGEX()|",
      "|java+method:///org/apache/spark/SparkContext/getConf()|",
      "|java+method:///java/lang/reflect/Constructor/newInstance(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///org/apache/spark/SparkContext/executorMemory()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/SparkMasterRegex$/SPARK_REGEX()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$org$apache$spark$SparkContext$$createTaskScheduler$1/SparkContext$$anonfun$org$apache$spark$SparkContext$$createTaskScheduler$1(org.apache.spark.deploy.LocalSparkCluster)|",
      "|java+method:///org/apache/mesos/MesosNativeLibrary/load()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/LinearSeqOptimized/apply(int)|",
      "|java+method:///scala/util/matching/Regex/unapplySeq(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$46/SparkContext$$anonfun$46()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$org$apache$spark$SparkContext$$createTaskScheduler$2/SparkContext$$anonfun$org$apache$spark$SparkContext$$createTaskScheduler$2()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$org$apache$spark$SparkContext$$createTaskScheduler$3/SparkContext$$anonfun$org$apache$spark$SparkContext$$createTaskScheduler$3()|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/shutdownCallback_$eq(scala.Function1)|",
      "|java+method:///org/apache/spark/util/Utils$/classForName(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/MesosSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.SparkContext,java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SimrSchedulerBackend/SimrSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.SparkContext,java.lang.String)|",
      "|java+method:///org/apache/spark/SparkMasterRegex$/LOCAL_N_FAILURES_REGEX()|",
      "|java+method:///org/apache/spark/SparkMasterRegex$/LOCAL_CLUSTER_REGEX()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/initialize(org.apache.spark.scheduler.SchedulerBackend)|",
      "|java+method:///org/apache/spark/SparkEnv/securityManager()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/scheduler/local/LocalBackend/LocalBackend(org.apache.spark.SparkConf,org.apache.spark.scheduler.TaskSchedulerImpl,int)|",
      "|java+constructor:///org/apache/spark/deploy/LocalSparkCluster/LocalSparkCluster(int,int,int,org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///org/apache/spark/SparkContext$/localCpuCount$1()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext$/localCpuCount$2()|",
      "|java+method:///org/apache/spark/deploy/LocalSparkCluster/start()|",
      "|java+method:///java/lang/Class/getConstructor(java.lang.Class%5B%5D)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/CoarseMesosSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.SparkContext,java.lang.String,org.apache.spark.SecurityManager)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl/TaskSchedulerImpl(org.apache.spark.SparkContext)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl/TaskSchedulerImpl(org.apache.spark.SparkContext,int,boolean)|",
      "|java+method:///org/apache/spark/SparkMasterRegex$/SIMR_REGEX()|",
      "|java+method:///org/apache/spark/SparkContext$/logWarning(scala.Function0)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$99/TaskDataSource$$anonfun$99(org.apache.spark.ui.jobs.TaskDataSource)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$16/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$16/apply()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$16/apply()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ExecutorLostFailure/hashCode()|",
    "called": "|java+method:///scala/runtime/Statics/finalizeHash(int,int)|",
    "v1Body": [
      "|java+method:///scala/runtime/ScalaRunTime$/_hashCode(scala.Product)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/Statics/mix(int,int)|",
      "|java+method:///org/apache/spark/ExecutorLostFailure/exitCausedByApp()|",
      "|java+method:///org/apache/spark/ExecutorLostFailure/execId()|",
      "|java+method:///scala/runtime/Statics/anyHash(java.lang.Object)|",
      "|java+method:///scala/runtime/Statics/finalizeHash(int,int)|",
      "|java+method:///org/apache/spark/ExecutorLostFailure/reason()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler/getCacheLocs(org.apache.spark.rdd.RDD)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/collection/IndexedSeq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$7/DAGScheduler$$anonfun$7(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///scala/collection/immutable/Range/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$5/DAGScheduler$$anonfun$5(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$6/DAGScheduler$$anonfun$6(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.rdd.RDD)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/cacheLocs()|",
      "|java+method:///scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD/getStorageLevel()|",
      "|java+method:///org/apache/spark/rdd/RDD/id()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///scala/collection/IndexedSeq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/indices()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMaster/getLocations(org.apache.spark.storage.BlockId%5B%5D)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/StorageLevel$/NONE()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/IndexedSeq$/canBuildFrom()|",
      "|java+method:///scala/package$/IndexedSeq()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/IndexedSeq$/fill(int,scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/collection/IndexedSeq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/rdd/RDD/partitions()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$4/DAGScheduler$$anonfun$4(org.apache.spark.scheduler.DAGScheduler,org.apache.spark.rdd.RDD)|",
      "|java+method:///scala/collection/immutable/Range/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$5/DAGScheduler$$anonfun$5(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/cacheLocs()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$3/DAGScheduler$$anonfun$3(org.apache.spark.scheduler.DAGScheduler)|",
      "|java+method:///scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/rdd/RDD/getStorageLevel()|",
      "|java+method:///org/apache/spark/rdd/RDD/id()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///scala/collection/IndexedSeq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/indices()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMaster/getLocations(org.apache.spark.storage.BlockId%5B%5D)|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/StorageLevel$/NONE()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/immutable/IndexedSeq$/canBuildFrom()|",
      "|java+method:///scala/package$/IndexedSeq()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/IndexedSeq$/fill(int,scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkConf$$anonfun$4/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkConf$$anonfun$4/apply(org.apache.spark.SparkConf$DeprecatedConfig)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/SparkConf$$anonfun$4/apply(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/ExternalAppendOnlyMap$DiskMapIterator(org.apache.spark.util.collection.ExternalAppendOnlyMap,java.io.File,org.apache.spark.storage.BlockId,scala.collection.mutable.ArrayBuffer)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/context()|",
      "|java+method:///scala/Predef$/assert(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/TaskContext$/get()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/TaskContext/addTaskCompletionListener(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/scanLeft(java.lang.Object,scala.Function2,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer$/canBuildFrom()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/nextBatchStream()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/org$apache$spark$util$collection$ExternalAppendOnlyMap$DiskMapIterator$$batchOffsets()|",
      "|java+method:///scala/collection/GenTraversableOnce$class/$init$(scala.collection.GenTraversableOnce)|",
      "|java+method:///scala/collection/Iterator$class/$init$(scala.collection.Iterator)|",
      "|java+method:///java/io/File/length()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/last()|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator$$anonfun$6/ExternalAppendOnlyMap$DiskMapIterator$$anonfun$6(org.apache.spark.util.collection.ExternalAppendOnlyMap$DiskMapIterator)|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator$$anonfun$1/ExternalAppendOnlyMap$DiskMapIterator$$anonfun$1(org.apache.spark.util.collection.ExternalAppendOnlyMap$DiskMapIterator)|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator$$anonfun$5/ExternalAppendOnlyMap$DiskMapIterator$$anonfun$5(org.apache.spark.util.collection.ExternalAppendOnlyMap$DiskMapIterator)|",
      "|java+method:///scala/collection/TraversableOnce$class/$init$(scala.collection.TraversableOnce)|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/Predef$/assert(boolean,scala.Function0)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/TaskContext/addTaskCompletionListener(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/scanLeft(java.lang.Object,scala.Function2,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer$/canBuildFrom()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/nextBatchStream()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/org$apache$spark$util$collection$ExternalAppendOnlyMap$DiskMapIterator$$batchOffsets()|",
      "|java+method:///scala/collection/GenTraversableOnce$class/$init$(scala.collection.GenTraversableOnce)|",
      "|java+method:///scala/collection/Iterator$class/$init$(scala.collection.Iterator)|",
      "|java+method:///java/io/File/length()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/last()|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator$$anonfun$2/ExternalAppendOnlyMap$DiskMapIterator$$anonfun$2(org.apache.spark.util.collection.ExternalAppendOnlyMap$DiskMapIterator)|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator$$anonfun$6/ExternalAppendOnlyMap$DiskMapIterator$$anonfun$6(org.apache.spark.util.collection.ExternalAppendOnlyMap$DiskMapIterator)|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator$$anonfun$7/ExternalAppendOnlyMap$DiskMapIterator$$anonfun$7(org.apache.spark.util.collection.ExternalAppendOnlyMap$DiskMapIterator)|",
      "|java+method:///scala/collection/TraversableOnce$class/$init$(scala.collection.TraversableOnce)|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/org$apache$spark$util$collection$ExternalAppendOnlyMap$DiskMapIterator$$cleanup()|",
    "called": "|java+method:///scala/collection/mutable/ArrayBuffer/length()|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/fileStream_$eq(java.io.FileInputStream)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/batchIndex_$eq(int)|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/deserializeStream()|",
      "|java+method:///java/io/FileInputStream/close()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/org$apache$spark$util$collection$ExternalAppendOnlyMap$DiskMapIterator$$batchOffsets()|",
      "|java+method:///java/io/File/delete()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/deserializeStream_$eq(org.apache.spark.serializer.DeserializationStream)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/length()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/fileStream()|",
      "|java+method:///org/apache/spark/serializer/DeserializationStream/close()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/fileStream_$eq(java.io.FileInputStream)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/batchIndex_$eq(int)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/org$apache$spark$util$collection$ExternalAppendOnlyMap$DiskMapIterator$$$outer()|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///java/io/FileInputStream/close()|",
      "|java+constructor:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator$$anonfun$org$apache$spark$util$collection$ExternalAppendOnlyMap$DiskMapIterator$$cleanup$1/ExternalAppendOnlyMap$DiskMapIterator$$anonfun$org$apache$spark$util$collection$ExternalAppendOnlyMap$DiskMapIterator$$cleanup$1(org.apache.spark.util.collection.ExternalAppendOnlyMap$DiskMapIterator)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/org$apache$spark$util$collection$ExternalAppendOnlyMap$DiskMapIterator$$batchOffsets()|",
      "|java+method:///java/io/File/delete()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/deserializeStream_$eq(org.apache.spark.serializer.DeserializationStream)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/length()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/deserializeStream()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap$DiskMapIterator/fileStream()|",
      "|java+method:///org/apache/spark/serializer/DeserializationStream/close()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore/clear()|",
    "called": "|java+method:///scala/collection/mutable/HashMap/clear()|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$currentMemory_$eq(long)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$entries()|",
      "|java+constructor:///org/apache/spark/storage/MemoryStore$$anonfun$clear$1/MemoryStore$$anonfun$clear$1(org.apache.spark.storage.MemoryStore)|",
      "|java+method:///java/util/LinkedHashMap/clear()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/logInfo(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$entries()|",
      "|java+constructor:///org/apache/spark/storage/MemoryStore$$anonfun$clear$1/MemoryStore$$anonfun$clear$1(org.apache.spark.storage.MemoryStore)|",
      "|java+method:///java/util/LinkedHashMap/clear()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/unrollMemoryMap()|",
      "|java+method:///scala/collection/mutable/HashMap/clear()|",
      "|java+method:///org/apache/spark/storage/MemoryStore/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/pendingUnrollMemoryMap()|",
      "|java+method:///org/apache/spark/memory/MemoryManager/releaseAllStorageMemory()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/local/LocalBackend/LocalBackend(org.apache.spark.SparkConf,org.apache.spark.scheduler.TaskSchedulerImpl,int)|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/sc()|",
      "|java+method:///org/apache/spark/scheduler/local/LocalBackend/getUserClasspath(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/scheduler/SchedulerBackend$class/$init$(org.apache.spark.scheduler.SchedulerBackend)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/SparkContext/listenerBus()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/sc()|",
      "|java+method:///org/apache/spark/scheduler/local/LocalBackend/getUserClasspath(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/scheduler/SchedulerBackend$class/$init$(org.apache.spark.scheduler.SchedulerBackend)|",
      "|java+method:///org/apache/spark/launcher/LauncherBackend/connect()|",
      "|java+method:///org/apache/spark/scheduler/local/LocalBackend/launcherBackend()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/SparkContext/listenerBus()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/Logging$class/$init$(org.apache.spark.Logging)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+constructor:///org/apache/spark/scheduler/local/LocalBackend$$anon$1/LocalBackend$$anon$1(org.apache.spark.scheduler.local.LocalBackend)|",
      "|java+constructor:///java/lang/Object/Object()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ExecutorLostFailure/productElement(int)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///java/lang/Object/toString()|",
      "|java+constructor:///java/lang/IndexOutOfBoundsException/IndexOutOfBoundsException(java.lang.String)|",
      "|java+method:///org/apache/spark/ExecutorLostFailure/execId()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ExecutorLostFailure/exitCausedByApp()|",
      "|java+constructor:///java/lang/IndexOutOfBoundsException/IndexOutOfBoundsException(java.lang.String)|",
      "|java+method:///org/apache/spark/ExecutorLostFailure/execId()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///org/apache/spark/ExecutorLostFailure/reason()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$4/MesosSchedulerBackend$$anonfun$4(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/r/RBackendHandler/channelRead0(io.netty.channel.ChannelHandlerContext,byte%5B%5D)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/r/RBackend/close()|",
      "|java+method:///java/io/DataOutputStream/writeInt(int)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readObjectType(java.io.DataInputStream)|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///java/io/ByteArrayInputStream/ByteArrayInputStream(byte%5B%5D)|",
      "|java+constructor:///java/io/DataInputStream/DataInputStream(java.io.InputStream)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readString(java.io.DataInputStream)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeInt(java.io.DataOutputStream,int)|",
      "|java+method:///org/apache/spark/api/r/RBackendHandler/handleMethodCall(boolean,java.lang.String,java.lang.String,int,java.io.DataInputStream,java.io.DataOutputStream)|",
      "|java+method:///org/apache/spark/api/r/JVMObjectTracker$/remove(java.lang.String)|",
      "|java+constructor:///java/io/ByteArrayOutputStream/ByteArrayOutputStream()|",
      "|java+method:///io/netty/channel/ChannelHandlerContext/write(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/api/r/RBackendHandler$$anonfun$channelRead0$1/RBackendHandler$$anonfun$channelRead0$1(org.apache.spark.api.r.RBackendHandler,java.lang.String)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeString(java.io.DataOutputStream,java.lang.String)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeObject(java.io.DataOutputStream,java.lang.Object)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readBoolean(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/RBackendHandler/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeType(java.io.DataOutputStream,java.lang.String)|",
      "|java+constructor:///java/io/DataOutputStream/DataOutputStream(java.io.OutputStream)|",
      "|java+method:///java/io/ByteArrayOutputStream/toByteArray()|",
      "|java+method:///java/lang/Exception/getMessage()|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readInt(java.io.DataInputStream)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/api/r/RBackend/close()|",
      "|java+method:///java/io/DataOutputStream/writeInt(int)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readObjectType(java.io.DataInputStream)|",
      "|java+method:///scala/Predef$/assert(boolean)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///java/io/ByteArrayInputStream/ByteArrayInputStream(byte%5B%5D)|",
      "|java+constructor:///java/io/DataInputStream/DataInputStream(java.io.InputStream)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeInt(java.io.DataOutputStream,int)|",
      "|java+method:///org/apache/spark/api/r/RBackendHandler/handleMethodCall(boolean,java.lang.String,java.lang.String,int,java.io.DataInputStream,java.io.DataOutputStream)|",
      "|java+method:///org/apache/spark/api/r/JVMObjectTracker$/remove(java.lang.String)|",
      "|java+constructor:///java/io/ByteArrayOutputStream/ByteArrayOutputStream()|",
      "|java+method:///io/netty/channel/ChannelHandlerContext/write(java.lang.Object)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/api/r/RBackendHandler$$anonfun$channelRead0$1/RBackendHandler$$anonfun$channelRead0$1(org.apache.spark.api.r.RBackendHandler,java.lang.String)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeString(java.io.DataOutputStream,java.lang.String)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeObject(java.io.DataOutputStream,java.lang.Object)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readBoolean(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/RBackendHandler/readArgs(int,java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readString(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/RBackendHandler/logError(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/writeType(java.io.DataOutputStream,java.lang.String)|",
      "|java+constructor:///java/io/DataOutputStream/DataOutputStream(java.io.OutputStream)|",
      "|java+method:///java/io/ByteArrayOutputStream/toByteArray()|",
      "|java+method:///java/lang/Exception/getMessage()|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readInt(java.io.DataInputStream)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$2/apply()|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$2/apply()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$2$$anonfun$apply$9/Worker$$anonfun$handleExecutorStateChanged$2$$anonfun$apply$9(org.apache.spark.deploy.worker.Worker$$anonfun$handleExecutorStateChanged$2)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$2$$anonfun$apply$10/Worker$$anonfun$handleExecutorStateChanged$2$$anonfun$apply$10(org.apache.spark.deploy.worker.Worker$$anonfun$handleExecutorStateChanged$2)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$2$$anonfun$apply$11/Worker$$anonfun$handleExecutorStateChanged$2$$anonfun$apply$11(org.apache.spark.deploy.worker.Worker$$anonfun$handleExecutorStateChanged$2)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$2$$anonfun$apply$12/Worker$$anonfun$handleExecutorStateChanged$2$$anonfun$apply$12(org.apache.spark.deploy.worker.Worker$$anonfun$handleExecutorStateChanged$2)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$2/apply()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$2$$anonfun$apply$11/Worker$$anonfun$handleExecutorStateChanged$2$$anonfun$apply$11(org.apache.spark.deploy.worker.Worker$$anonfun$handleExecutorStateChanged$2)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$2$$anonfun$apply$12/Worker$$anonfun$handleExecutorStateChanged$2$$anonfun$apply$12(org.apache.spark.deploy.worker.Worker$$anonfun$handleExecutorStateChanged$2)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$2$$anonfun$apply$13/Worker$$anonfun$handleExecutorStateChanged$2$$anonfun$apply$13(org.apache.spark.deploy.worker.Worker$$anonfun$handleExecutorStateChanged$2)|",
      "|java+constructor:///org/apache/spark/deploy/worker/Worker$$anonfun$handleExecutorStateChanged$2$$anonfun$apply$14/Worker$$anonfun$handleExecutorStateChanged$2$$anonfun$apply$14(org.apache.spark.deploy.worker.Worker$$anonfun$handleExecutorStateChanged$2)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$76/apply(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/status/api/v1/ApplicationsListResource$/appHistoryInfoToPublicAppInfo(org.apache.spark.deploy.history.ApplicationHistoryInfo)|",
    "called": "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationHistoryInfo/attempts()|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationHistoryInfo/name()|",
      "|java+constructor:///org/apache/spark/status/api/v1/ApplicationInfo/ApplicationInfo(java.lang.String,java.lang.String,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/status/api/v1/ApplicationsListResource$$anonfun$appHistoryInfoToPublicAppInfo$1/ApplicationsListResource$$anonfun$appHistoryInfoToPublicAppInfo$1()|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationHistoryInfo/id()|",
      "|java+method:///scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationHistoryInfo/attempts()|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationHistoryInfo/name()|",
      "|java+constructor:///org/apache/spark/status/api/v1/ApplicationInfo/ApplicationInfo(java.lang.String,java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/status/api/v1/ApplicationsListResource$$anonfun$appHistoryInfoToPublicAppInfo$1/ApplicationsListResource$$anonfun$appHistoryInfoToPublicAppInfo$1()|",
      "|java+method:///org/apache/spark/deploy/history/ApplicationHistoryInfo/id()|",
      "|java+method:///scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/start()|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/createSchedulerDriver(java.lang.String,org.apache.mesos.Scheduler,java.lang.String,java.lang.String,org.apache.spark.SparkConf,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///java/lang/Thread/currentThread()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/classLoader_$eq(java.lang.ClassLoader)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/createSchedulerDriver$default$6()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/createSchedulerDriver$default$7()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/createSchedulerDriver$default$8()|",
      "|java+method:///org/apache/spark/SparkContext/sparkUser()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/startScheduler(org.apache.mesos.SchedulerDriver)|",
      "|java+method:///org/apache/spark/SparkContext/appName()|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///java/lang/Thread/getContextClassLoader()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/createSchedulerDriver$default$9()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/createSchedulerDriver(java.lang.String,org.apache.mesos.Scheduler,java.lang.String,java.lang.String,org.apache.spark.SparkConf,scala.Option,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/SparkContext/ui()|",
      "|java+method:///java/lang/Thread/currentThread()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/classLoader_$eq(java.lang.ClassLoader)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$1/MesosSchedulerBackend$$anonfun$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerBackend)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/createSchedulerDriver$default$7()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/createSchedulerDriver$default$8()|",
      "|java+method:///org/apache/spark/SparkContext/sparkUser()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/startScheduler(org.apache.mesos.SchedulerDriver)|",
      "|java+method:///org/apache/spark/SparkContext/appName()|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+method:///java/lang/Thread/getContextClassLoader()|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend/createSchedulerDriver$default$9()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/TachyonBlockManager/putValues(org.apache.spark.storage.BlockId,scala.collection.Iterator)|",
    "called": "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/blockManager()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataSerializeStream(org.apache.spark.storage.BlockId,java.io.OutputStream,scala.collection.Iterator,org.apache.spark.serializer.Serializer)|",
      "|java+method:///tachyon/client/OutStream/close()|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/getFile(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///tachyon/client/TachyonFile/getOutStream(tachyon.client.WriteType)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataSerializeStream$default$4()|",
      "|java+constructor:///org/apache/spark/storage/TachyonBlockManager$$anonfun$putValues$1/TachyonBlockManager$$anonfun$putValues$1(org.apache.spark.storage.TachyonBlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///tachyon/client/OutStream/cancel()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/blockManager()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataSerializeStream(org.apache.spark.storage.BlockId,java.io.OutputStream,scala.collection.Iterator)|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///tachyon/client/file/FileOutStream/cancel()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///org/apache/spark/storage/TachyonBlockManager$$anonfun$putValues$1/TachyonBlockManager$$anonfun$putValues$1(org.apache.spark.storage.TachyonBlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/getFile(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///tachyon/client/TachyonFile/getOutStream(tachyon.client.WriteType)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///tachyon/client/file/FileOutStream/close()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/executor/MesosExecutorBackend/registered(org.apache.mesos.ExecutorDriver,org.apache.mesos.Protos$ExecutorInfo,org.apache.mesos.Protos$FrameworkInfo,org.apache.mesos.Protos$SlaveInfo)|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+method:///org/apache/mesos/Protos$ExecutorID/getValue()|",
      "|java+constructor:///org/apache/spark/executor/MesosExecutorBackend$$anonfun$3/MesosExecutorBackend$$anonfun$3(org.apache.spark.executor.MesosExecutorBackend)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/SparkConf/setAll(scala.collection.Traversable)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/executor/MesosExecutorBackend/executor_$eq(org.apache.spark.executor.Executor)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+constructor:///org/apache/spark/executor/Executor/Executor(java.lang.String,java.lang.String,org.apache.spark.SparkEnv,scala.collection.Seq,boolean)|",
      "|java+method:///org/apache/mesos/Protos$FrameworkInfo/getId()|",
      "|java+method:///org/apache/spark/SparkEnv$/createExecutorEnv(org.apache.spark.SparkConf,java.lang.String,java.lang.String,int,int,boolean)|",
      "|java+method:///org/apache/spark/executor/Executor$/$lessinit$greater$default$4()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/SparkConf/SparkConf(boolean)|",
      "|java+method:///org/apache/spark/executor/MesosExecutorBackend/logInfo(scala.Function0)|",
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo/getExecutorId()|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/mutable/Buffer/find(scala.Function1)|",
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo/getResourcesList()|",
      "|java+constructor:///org/apache/spark/executor/MesosExecutorBackend$$anonfun$1/MesosExecutorBackend$$anonfun$1(org.apache.spark.executor.MesosExecutorBackend)|",
      "|java+method:///scala/collection/mutable/ArrayOps/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/executor/MesosExecutorBackend$$anonfun$2/MesosExecutorBackend$$anonfun$2(org.apache.spark.executor.MesosExecutorBackend)|",
      "|java+method:///org/apache/mesos/Protos$SlaveInfo/getHostname()|",
      "|java+method:///org/apache/spark/executor/MesosExecutorBackend/driver_$eq(org.apache.mesos.ExecutorDriver)|",
      "|java+method:///org/apache/spark/util/Utils$/deserialize(byte%5B%5D)|",
      "|java+method:///org/apache/mesos/protobuf/ByteString/toByteArray()|",
      "|java+method:///org/apache/spark/executor/Executor$/$lessinit$greater$default$5()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/mesos/Protos$FrameworkID/getValue()|",
      "|java+constructor:///org/apache/spark/executor/MesosExecutorBackend$$anonfun$registered$1/MesosExecutorBackend$$anonfun$registered$1(org.apache.spark.executor.MesosExecutorBackend,int,java.lang.String)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo/getData()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/mesos/Protos$ExecutorID/getValue()|",
      "|java+constructor:///org/apache/spark/executor/MesosExecutorBackend$$anonfun$3/MesosExecutorBackend$$anonfun$3(org.apache.spark.executor.MesosExecutorBackend)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/executor/Executor/Executor(java.lang.String,java.lang.String,org.apache.spark.SparkEnv,scala.collection.Seq,boolean)|",
      "|java+method:///org/apache/mesos/Protos$FrameworkInfo/getId()|",
      "|java+method:///org/apache/spark/SparkEnv$/createExecutorEnv(org.apache.spark.SparkConf,java.lang.String,java.lang.String,int,int,boolean)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/SparkConf/SparkConf(boolean)|",
      "|java+method:///org/apache/spark/executor/MesosExecutorBackend/logInfo(scala.Function0)|",
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo/getExecutorId()|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/IterableLike/find(scala.Function1)|",
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo/getResourcesList()|",
      "|java+constructor:///org/apache/spark/executor/MesosExecutorBackend$$anonfun$1/MesosExecutorBackend$$anonfun$1(org.apache.spark.executor.MesosExecutorBackend)|",
      "|java+method:///scala/collection/mutable/ArrayOps/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/executor/MesosExecutorBackend$$anonfun$2/MesosExecutorBackend$$anonfun$2(org.apache.spark.executor.MesosExecutorBackend)|",
      "|java+method:///org/apache/mesos/Protos$SlaveInfo/getHostname()|",
      "|java+method:///java/lang/Thread/setContextClassLoader(java.lang.ClassLoader)|",
      "|java+method:///java/lang/Thread/currentThread()|",
      "|java+method:///org/apache/spark/SparkConf/setAll(scala.collection.Traversable)|",
      "|java+method:///org/apache/spark/executor/MesosExecutorBackend/executor_$eq(org.apache.spark.executor.Executor)|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///org/apache/spark/executor/MesosExecutorBackend/driver_$eq(org.apache.mesos.ExecutorDriver)|",
      "|java+method:///org/apache/spark/util/Utils$/deserialize(byte%5B%5D)|",
      "|java+method:///org/apache/mesos/protobuf/ByteString/toByteArray()|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+method:///java/lang/Class/getClassLoader()|",
      "|java+method:///org/apache/spark/executor/Executor$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/executor/Executor$/$lessinit$greater$default$5()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/mesos/Protos$FrameworkID/getValue()|",
      "|java+constructor:///org/apache/spark/executor/MesosExecutorBackend$$anonfun$registered$1/MesosExecutorBackend$$anonfun$registered$1(org.apache.spark.executor.MesosExecutorBackend,int,java.lang.String)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/mesos/Protos$ExecutorInfo/getData()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/UIUtils$/makeDescription(java.lang.String,java.lang.String)|",
    "called": "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/xml/transform/RuleTransformer/RuleTransformer(scala.collection.Seq)|",
      "|java+method:///scala/xml/Elem/$bslash$bslash(java.lang.String)|",
      "|java+method:///scala/collection/immutable/Seq/exists(scala.Function1)|",
      "|java+constructor:///org/apache/spark/ui/UIUtils$$anonfun$makeDescription$3/UIUtils$$anonfun$makeDescription$3(java.lang.String)|",
      "|java+constructor:///org/apache/spark/ui/UIUtils$$anon$1/UIUtils$$anon$1(java.lang.String)|",
      "|java+method:///scala/Predef$/Set()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/immutable/Seq/mkString(java.lang.String)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+method:///org/apache/spark/ui/UIUtils$/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/xml/NodeSeq/nonEmpty()|",
      "|java+method:///scala/xml/NodeSeq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/TraversableLike/filter(scala.Function1)|",
      "|java+method:///scala/xml/NodeSeq$/seqToNodeSeq(scala.collection.Seq)|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String)|",
      "|java+method:///scala/xml/XML$/loadString(java.lang.String)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+method:///scala/xml/NodeSeq/filterNot(scala.Function1)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+method:///scala/xml/transform/RuleTransformer/transform(scala.xml.Node)|",
      "|java+constructor:///org/apache/spark/ui/UIUtils$$anonfun$6/UIUtils$$anonfun$6(scala.collection.immutable.Set)|",
      "|java+method:///scala/xml/NodeSeq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/Seq$/canBuildFrom()|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/collection/immutable/Set$/apply(scala.collection.Seq)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/ui/UIUtils$$anonfun$9/UIUtils$$anonfun$9()|",
      "|java+constructor:///org/apache/spark/ui/UIUtils$$anonfun$makeDescription$1/UIUtils$$anonfun$makeDescription$1()|",
      "|java+constructor:///org/apache/spark/ui/UIUtils$$anonfun$8/UIUtils$$anonfun$8()|",
      "|java+constructor:///org/apache/spark/ui/UIUtils$$anonfun$7/UIUtils$$anonfun$7()|",
      "|java+constructor:///org/apache/spark/ui/UIUtils$$anonfun$makeDescription$2/UIUtils$$anonfun$makeDescription$2()|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/$scope()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///scala/xml/NodeSeq/nonEmpty()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/xml/transform/RuleTransformer/RuleTransformer(scala.collection.Seq)|",
      "|java+method:///scala/xml/Elem/$bslash$bslash(java.lang.String)|",
      "|java+method:///scala/collection/immutable/Seq/exists(scala.Function1)|",
      "|java+constructor:///org/apache/spark/ui/UIUtils$$anon$1/UIUtils$$anon$1(java.lang.String)|",
      "|java+method:///scala/Predef$/Set()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/immutable/Seq/mkString(java.lang.String)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/xml/Text/Text(java.lang.String)|",
      "|java+method:///scala/xml/NodeSeq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/TraversableLike/filter(scala.Function1)|",
      "|java+method:///scala/xml/NodeSeq$/seqToNodeSeq(scala.collection.Seq)|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String)|",
      "|java+method:///scala/xml/XML$/loadString(java.lang.String)|",
      "|java+constructor:///scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)|",
      "|java+method:///scala/xml/NodeSeq/filterNot(scala.Function1)|",
      "|java+constructor:///scala/xml/NodeBuffer/NodeBuffer()|",
      "|java+method:///scala/xml/transform/RuleTransformer/transform(scala.xml.Node)|",
      "|java+constructor:///org/apache/spark/ui/UIUtils$$anonfun$6/UIUtils$$anonfun$6(scala.collection.immutable.Set)|",
      "|java+method:///scala/xml/NodeSeq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/Seq$/canBuildFrom()|",
      "|java+method:///scala/xml/NodeBuffer/$amp$plus(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)|",
      "|java+constructor:///scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)|",
      "|java+method:///scala/collection/immutable/Set$/apply(scala.collection.Seq)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+constructor:///org/apache/spark/ui/UIUtils$$anonfun$9/UIUtils$$anonfun$9()|",
      "|java+constructor:///org/apache/spark/ui/UIUtils$$anonfun$makeDescription$1/UIUtils$$anonfun$makeDescription$1()|",
      "|java+constructor:///org/apache/spark/ui/UIUtils$$anonfun$makeDescription$2/UIUtils$$anonfun$makeDescription$2()|",
      "|java+constructor:///org/apache/spark/ui/UIUtils$$anonfun$8/UIUtils$$anonfun$8()|",
      "|java+constructor:///org/apache/spark/ui/UIUtils$$anonfun$7/UIUtils$$anonfun$7()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$class/top(org.apache.spark.api.java.JavaRDDLike,int,java.util.Comparator)|",
    "called": "|java+method:///scala/math/Ordering$/comparatorToOrdering(java.util.Comparator)|",
    "v1Body": [
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///scala/math/Ordering$/comparatorToOrdering(java.util.Comparator)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///org/apache/spark/rdd/RDD/top(int,scala.math.Ordering)|",
      "|java+constructor:///java/util/ArrayList/ArrayList(java.util.Collection)|",
      "|java+method:///scala/package$/Ordering()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/RDD/top(int,scala.math.Ordering)|",
      "|java+method:///scala/math/Ordering$/comparatorToOrdering(java.util.Comparator)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike/rdd()|",
      "|java+method:///scala/package$/Ordering()|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$36/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$36/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToDouble(double)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$36/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$7/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$7/apply(org.apache.mesos.Protos$Offer)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend$$anonfun$resourceOffers$1$$anonfun$apply$mcV$sp$7/apply(org.apache.mesos.Protos$Offer)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/start()|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/start()|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/rpcEnv()|",
      "|java+method:///org/apache/spark/SparkContext/executorEnvs()|",
      "|java+constructor:///org/apache/spark/deploy/Command/Command(java.lang.String,scala.collection.Seq,scala.collection.Map,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$10/SparkDeploySchedulerBackend$$anonfun$10(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$11/SparkDeploySchedulerBackend$$anonfun$11(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/maxCores()|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/client_$eq(org.apache.spark.deploy.client.AppClient)|",
      "|java+constructor:///org/apache/spark/rpc/RpcAddress/RpcAddress(java.lang.String,int)|",
      "|java+method:///scala/sys/SystemProperties/contains(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/SparkEnv$/driverActorSystemName()|",
      "|java+method:///org/apache/spark/util/Utils$/sparkJavaOpts(org.apache.spark.SparkConf,scala.Function1)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/executorMemory()|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/conf()|",
      "|java+method:///org/apache/spark/SparkContext/ui()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/uriOf(java.lang.String,org.apache.spark.rpc.RpcAddress,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/ApplicationDescription/ApplicationDescription(java.lang.String,scala.Option,int,org.apache.spark.deploy.Command,java.lang.String,scala.Option,scala.Option,scala.Option)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/sys/package$/props()|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/waitForRegistration()|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/client()|",
      "|java+method:///org/apache/spark/SparkContext/eventLogDir()|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$6/SparkDeploySchedulerBackend$$anonfun$6(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$7/SparkDeploySchedulerBackend$$anonfun$7(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$3/SparkDeploySchedulerBackend$$anonfun$3(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$12/SparkDeploySchedulerBackend$$anonfun$12(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$4/SparkDeploySchedulerBackend$$anonfun$4(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$9/SparkDeploySchedulerBackend$$anonfun$9(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$5/SparkDeploySchedulerBackend$$anonfun$5(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$8/SparkDeploySchedulerBackend$$anonfun$8(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkContext/appName()|",
      "|java+method:///scala/sys/SystemProperties/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/eventLogCodec()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$/ENDPOINT_NAME()|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient/AppClient(org.apache.spark.rpc.RpcEnv,java.lang.String%5B%5D,org.apache.spark.deploy.ApplicationDescription,org.apache.spark.deploy.client.AppClientListener,org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/start()|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/start()|",
      "|java+method:///java/lang/String/split(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/rpcEnv()|",
      "|java+method:///org/apache/spark/SparkContext/executorEnvs()|",
      "|java+constructor:///org/apache/spark/deploy/Command/Command(java.lang.String,scala.collection.Seq,scala.collection.Map,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$10/SparkDeploySchedulerBackend$$anonfun$10(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$11/SparkDeploySchedulerBackend$$anonfun$11(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/maxCores()|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/client_$eq(org.apache.spark.deploy.client.AppClient)|",
      "|java+constructor:///org/apache/spark/rpc/RpcAddress/RpcAddress(java.lang.String,int)|",
      "|java+method:///scala/sys/SystemProperties/contains(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/SparkEnv$/driverActorSystemName()|",
      "|java+constructor:///org/apache/spark/deploy/ApplicationDescription/ApplicationDescription(java.lang.String,scala.Option,int,org.apache.spark.deploy.Command,java.lang.String,scala.Option,scala.Option,scala.Option,java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/sparkJavaOpts(org.apache.spark.SparkConf,scala.Function1)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///org/apache/spark/SparkContext/executorMemory()|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/conf()|",
      "|java+method:///org/apache/spark/SparkContext/ui()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/uriOf(java.lang.String,org.apache.spark.rpc.RpcAddress,java.lang.String)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/sys/package$/props()|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/waitForRegistration()|",
      "|java+method:///org/apache/spark/launcher/LauncherBackend/connect()|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/client()|",
      "|java+method:///org/apache/spark/SparkContext/eventLogDir()|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/launcher/LauncherBackend/setState(org.apache.spark.launcher.SparkAppHandle$State)|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/launcherBackend()|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$6/SparkDeploySchedulerBackend$$anonfun$6(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$7/SparkDeploySchedulerBackend$$anonfun$7(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$3/SparkDeploySchedulerBackend$$anonfun$3(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$12/SparkDeploySchedulerBackend$$anonfun$12(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$4/SparkDeploySchedulerBackend$$anonfun$4(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$9/SparkDeploySchedulerBackend$$anonfun$9(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$5/SparkDeploySchedulerBackend$$anonfun$5(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$8/SparkDeploySchedulerBackend$$anonfun$8(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkContext/appName()|",
      "|java+method:///scala/sys/SystemProperties/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/eventLogCodec()|",
      "|java+method:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend$/ENDPOINT_NAME()|",
      "|java+method:///org/apache/spark/SparkContext/conf()|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient/AppClient(org.apache.spark.rpc.RpcEnv,java.lang.String%5B%5D,org.apache.spark.deploy.ApplicationDescription,org.apache.spark.deploy.client.AppClientListener,org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+method:///org/apache/spark/SparkConf/get(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/client/AppClient/start()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/ApplicationDescription$/$lessinit$greater$default$9()|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaSparkContext/parallelizeDoubles(java.util.List,int)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)|",
      "|java+method:///scala/reflect/ClassTag$/Double()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext/sc()|",
      "|java+method:///org/apache/spark/api/java/JavaDoubleRDD$/fromRDD(org.apache.spark.rdd.RDD)|",
      "|java+constructor:///org/apache/spark/api/java/JavaSparkContext$$anonfun$parallelizeDoubles$1/JavaSparkContext$$anonfun$parallelizeDoubles$1(org.apache.spark.api.java.JavaSparkContext)|",
      "|java+method:///scala/collection/mutable/Buffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/Buffer$/canBuildFrom()|",
      "|java+method:///org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///scala/reflect/ClassTag$/Double()|",
      "|java+method:///org/apache/spark/api/java/JavaDoubleRDD$/fromRDD(org.apache.spark.rdd.RDD)|",
      "|java+constructor:///org/apache/spark/api/java/JavaSparkContext$$anonfun$parallelizeDoubles$1/JavaSparkContext$$anonfun$parallelizeDoubles$1(org.apache.spark.api.java.JavaSparkContext)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/api/java/JavaSparkContext/sc()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaPairRDD$$anonfun$groupByResultToJava$1/apply(scala.collection.Iterable)|",
    "called": "|java+method:///scala/collection/JavaConverters$/asJavaIterableConverter(scala.collection.Iterable)|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/asJavaIterable(scala.collection.Iterable)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIterableConverter(scala.collection.Iterable)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/PythonRDD$/runJob(org.apache.spark.SparkContext,org.apache.spark.api.java.JavaRDD,java.util.ArrayList)|",
    "called": "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/api/java/JavaRDD$/toRDD(org.apache.spark.api.java.JavaRDD)|",
      "|java+method:///scala/collection/mutable/Buffer/mkString(java.lang.String)|",
      "|java+method:///org/apache/spark/api/java/JavaRDD/id()|",
      "|java+method:///org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function1,scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///scala/collection/mutable/ArrayOps/iterator()|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/api/python/PythonRDD$/serveIterator(scala.collection.Iterator,java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRDD$$anonfun$3/PythonRDD$$anonfun$3()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Array$/concat(scala.collection.Seq,scala.reflect.ClassTag)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/api/java/JavaRDD$/toRDD(org.apache.spark.api.java.JavaRDD)|",
      "|java+method:///org/apache/spark/api/java/JavaRDD/id()|",
      "|java+method:///org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function1,scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/api/python/PythonRDD$/serveIterator(scala.collection.Iterator,java.lang.String)|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/api/python/PythonRDD$$anonfun$3/PythonRDD$$anonfun$3()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Array$/concat(scala.collection.Seq,scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/mutable/ArrayOps/iterator()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/TachyonBlockManager/getBytes(org.apache.spark.storage.BlockId)|",
    "called": "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
    "v1Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///tachyon/client/TachyonFile/getLocationHosts()|",
      "|java+method:///tachyon/client/InStream/close()|",
      "|java+method:///tachyon/client/TachyonFile/length()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/spark-project/guava/io/ByteStreams/readFully(java.io.InputStream,byte%5B%5D)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/storage/TachyonBlockManager$$anonfun$getBytes$1/TachyonBlockManager$$anonfun$getBytes$1(org.apache.spark.storage.TachyonBlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///java/util/List/size()|",
      "|java+method:///tachyon/client/TachyonFile/getInStream(tachyon.client.ReadType)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/getFile(org.apache.spark.storage.BlockId)|",
      "|java+method:///java/nio/ByteBuffer/wrap(byte%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///tachyon/client/TachyonFile/getLocationHosts()|",
      "|java+method:///tachyon/client/TachyonFile/length()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/spark-project/guava/io/ByteStreams/readFully(java.io.InputStream,byte%5B%5D)|",
      "|java+method:///tachyon/client/file/FileInStream/close()|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/storage/TachyonBlockManager$$anonfun$getBytes$1/TachyonBlockManager$$anonfun$getBytes$1(org.apache.spark.storage.TachyonBlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///java/util/List/size()|",
      "|java+method:///tachyon/client/TachyonFile/getInStream(tachyon.client.ReadType)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/getFile(org.apache.spark.storage.BlockId)|",
      "|java+method:///java/nio/ByteBuffer/wrap(byte%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/TimeStampedHashSet/iterator()|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/util/TimeStampedHashSet$$anonfun$iterator$1/TimeStampedHashSet$$anonfun$iterator$1(org.apache.spark.util.TimeStampedHashSet)|",
      "|java+method:///org/apache/spark/util/TimeStampedHashSet/internalMap()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaIterator(java.util.Iterator)|",
      "|java+method:///java/util/Set/iterator()|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/entrySet()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/util/TimeStampedHashSet$$anonfun$iterator$1/TimeStampedHashSet$$anonfun$iterator$1(org.apache.spark.util.TimeStampedHashSet)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///org/apache/spark/util/TimeStampedHashSet/internalMap()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaIteratorConverter(java.util.Iterator)|",
      "|java+method:///java/util/Set/iterator()|",
      "|java+method:///scala/collection/Iterator/map(scala.Function1)|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/entrySet()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/Client$/main(java.lang.String%5B%5D)|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/util/Utils$/localHostName()|",
      "|java+method:///scala/Predef$/println(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/ClientArguments/ClientArguments(java.lang.String%5B%5D)|",
      "|java+method:///org/apache/log4j/Level/isGreaterOrEqual(org.apache.log4j.Priority)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/deploy/ClientArguments/masters()|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/SparkConf/SparkConf()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+method:///org/apache/log4j/Logger/getRootLogger()|",
      "|java+constructor:///org/apache/spark/deploy/Client$$anonfun$7/Client$$anonfun$7(org.apache.spark.rpc.RpcEnv)|",
      "|java+method:///org/apache/spark/SparkConf/set(java.lang.String,java.lang.String)|",
      "|java+method:///scala/sys/SystemProperties/contains(java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/sys/package$/props()|",
      "|java+constructor:///org/apache/spark/deploy/ClientEndpoint/ClientEndpoint(org.apache.spark.rpc.RpcEnv,org.apache.spark.deploy.ClientArguments,scala.collection.Seq,org.apache.spark.SparkConf)|",
      "|java+constructor:///org/apache/spark/deploy/Client$$anonfun$6/Client$$anonfun$6()|",
      "|java+method:///java/lang/String/replace(java.lang.CharSequence,java.lang.CharSequence)|",
      "|java+method:///org/apache/log4j/Logger/setLevel(org.apache.log4j.Level)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/awaitTermination()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/log4j/Level/toString()|",
      "|java+method:///org/apache/spark/deploy/ClientArguments/logLevel()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/util/Utils$/localHostName()|",
      "|java+method:///scala/Predef$/println(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/ClientArguments/ClientArguments(java.lang.String%5B%5D)|",
      "|java+method:///org/apache/log4j/Level/isGreaterOrEqual(org.apache.log4j.Priority)|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/deploy/ClientArguments/masters()|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/log4j/Logger/getRootLogger()|",
      "|java+method:///org/apache/spark/SparkConf/set(java.lang.String,java.lang.String)|",
      "|java+method:///scala/sys/SystemProperties/contains(java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/sys/package$/props()|",
      "|java+constructor:///org/apache/spark/deploy/ClientEndpoint/ClientEndpoint(org.apache.spark.rpc.RpcEnv,org.apache.spark.deploy.ClientArguments,scala.collection.Seq,org.apache.spark.SparkConf)|",
      "|java+constructor:///org/apache/spark/deploy/Client$$anonfun$6/Client$$anonfun$6()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create$default$6()|",
      "|java+constructor:///org/apache/spark/SparkConf/SparkConf()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,boolean)|",
      "|java+constructor:///org/apache/spark/deploy/Client$$anonfun$7/Client$$anonfun$7(org.apache.spark.rpc.RpcEnv)|",
      "|java+method:///java/lang/String/replace(java.lang.CharSequence,java.lang.CharSequence)|",
      "|java+method:///org/apache/log4j/Logger/setLevel(org.apache.log4j.Level)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/awaitTermination()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/log4j/Level/toString()|",
      "|java+method:///org/apache/spark/deploy/ClientArguments/logLevel()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1/apply(org.apache.spark.scheduler.TaskLocation)|",
    "called": "|java+method:///scala/Option/foreach(scala.Function1)|",
    "v1Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/org$apache$spark$scheduler$TaskSetManager$$pendingTasksForHost()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$6/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$6(org.apache.spark.scheduler.TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1,scala.collection.immutable.Set,org.apache.spark.scheduler.HDFSCacheTaskLocation)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/org$apache$spark$scheduler$TaskSetManager$$addTo$1(scala.collection.mutable.ArrayBuffer,int,boolean)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/getRackForHost(java.lang.String)|",
      "|java+method:///scala/collection/immutable/Set/foreach(scala.Function1)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskLocation/host()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$7/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$7(org.apache.spark.scheduler.TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1,org.apache.spark.scheduler.HDFSCacheTaskLocation)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/getExecutorsAliveOnHost(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElseUpdate(java.lang.Object,scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/ExecutorCacheTaskLocation/executorId()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/org$apache$spark$scheduler$TaskSetManager$$pendingTasksForExecutor()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$8/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$8(org.apache.spark.scheduler.TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$9/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$9(org.apache.spark.scheduler.TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$3/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$3(org.apache.spark.scheduler.TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$4/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$4(org.apache.spark.scheduler.TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1)|"
    ],
    "v2Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/org$apache$spark$scheduler$TaskSetManager$$pendingTasksForHost()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$6/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$6(org.apache.spark.scheduler.TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1,scala.collection.immutable.Set,org.apache.spark.scheduler.HDFSCacheTaskLocation)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logInfo(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/getRackForHost(java.lang.String)|",
      "|java+method:///scala/collection/immutable/Set/foreach(scala.Function1)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskLocation/host()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$7/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$7(org.apache.spark.scheduler.TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1,org.apache.spark.scheduler.HDFSCacheTaskLocation)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/org$apache$spark$scheduler$TaskSetManager$$addTo$1(scala.collection.mutable.ArrayBuffer,int)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/getExecutorsAliveOnHost(java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/getOrElseUpdate(java.lang.Object,scala.Function0)|",
      "|java+method:///org/apache/spark/scheduler/ExecutorCacheTaskLocation/executorId()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/org$apache$spark$scheduler$TaskSetManager$$pendingTasksForExecutor()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$8/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$8(org.apache.spark.scheduler.TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$9/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$9(org.apache.spark.scheduler.TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$3/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$3(org.apache.spark.scheduler.TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1)|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$4/TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1$$anonfun$apply$4(org.apache.spark.scheduler.TaskSetManager$$anonfun$org$apache$spark$scheduler$TaskSetManager$$addPendingTask$1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSetManager$$anonfun$executorLost$6/apply(scala.Tuple2)|",
    "called": "|java+method:///scala/Tuple2/_1$mcJ$sp()|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/tasksSuccessful()|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/dagScheduler()|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/tasks()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/index()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/taskEnded(org.apache.spark.scheduler.Task,org.apache.spark.TaskEndReason,java.lang.Object,scala.collection.Map,org.apache.spark.scheduler.TaskInfo,org.apache.spark.executor.TaskMetrics)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/org$apache$spark$scheduler$TaskSetManager$$addPendingTask(int,boolean)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/org$apache$spark$scheduler$TaskSetManager$$addPendingTask$default$2()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/successful()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/taskInfos()|",
      "|java+method:///scala/Tuple2/_1$mcJ$sp()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/copiesRunning()|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/tasksSuccessful_$eq(int)|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/TaskState$/FAILED()|",
      "|java+method:///org/apache/spark/scheduler/TaskInfo/executorId()|",
      "|java+method:///org/apache/spark/scheduler/ExecutorExited/exitCausedByApp()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/Tuple2/_1$mcJ$sp()|",
      "|java+method:///org/apache/spark/scheduler/ExecutorLossReason/toString()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSetManager/handleFailedTask(long,scala.Enumeration$Value,org.apache.spark.TaskEndReason)|",
      "|java+constructor:///org/apache/spark/ExecutorLostFailure/ExecutorLostFailure(java.lang.String,boolean,scala.Option)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext$$anonfun$range$1/apply()|",
    "called": "|java+method:///scala/math/BigInt/$percent(scala.math.BigInt)|",
    "v1Body": [
      "|java+method:///scala/math/BigInt/$plus(scala.math.BigInt)|",
      "|java+method:///org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$range$1$$anonfun$apply$7/SparkContext$$anonfun$range$1$$anonfun$apply$7(org.apache.spark.SparkContext$$anonfun$range$1)|",
      "|java+method:///scala/math/BigInt$/long2bigInt(long)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitionsWithIndex(scala.Function2,boolean,scala.reflect.ClassTag)|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$assertNotStopped()|",
      "|java+method:///org/apache/spark/SparkContext$$anonfun$range$1/apply()|",
      "|java+method:///scala/math/BigInt/$percent(scala.math.BigInt)|",
      "|java+method:///scala/reflect/ClassTag$/Long()|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+method:///scala/reflect/ClassTag$/Int()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$range$1$$anonfun$29/SparkContext$$anonfun$range$1$$anonfun$29(org.apache.spark.SparkContext$$anonfun$range$1,scala.math.BigInt)|",
      "|java+method:///scala/math/BigInt/$greater(scala.math.BigInt)|",
      "|java+method:///scala/math/BigInt/$div(scala.math.BigInt)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitionsWithIndex$default$2()|",
      "|java+method:///scala/math/BigInt$/int2bigInt(int)|",
      "|java+method:///scala/math/BigInt/$minus(scala.math.BigInt)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/package$/BigInt()|",
      "|java+method:///scala/runtime/BoxesRunTime/equalsNumObject(java.lang.Number,java.lang.Object)|",
      "|java+method:///scala/math/BigInt$/apply(long)|"
    ],
    "v2Body": [
      "|java+method:///scala/math/BigInt/$plus(scala.math.BigInt)|",
      "|java+method:///org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)|",
      "|java+method:///scala/math/BigInt/$greater(scala.math.BigInt)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$range$1$$anonfun$apply$7/SparkContext$$anonfun$range$1$$anonfun$apply$7(org.apache.spark.SparkContext$$anonfun$range$1)|",
      "|java+method:///scala/math/BigInt$/long2bigInt(long)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitionsWithIndex(scala.Function2,boolean,scala.reflect.ClassTag)|",
      "|java+method:///scala/Predef$/require(boolean,scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$assertNotStopped()|",
      "|java+method:///org/apache/spark/SparkContext$$anonfun$range$1/apply()|",
      "|java+method:///scala/math/BigInt/$percent(scala.math.BigInt)|",
      "|java+method:///scala/reflect/ClassTag$/Long()|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+method:///scala/reflect/ClassTag$/Int()|",
      "|java+method:///scala/math/BigInt/$div(scala.math.BigInt)|",
      "|java+method:///org/apache/spark/rdd/RDD/mapPartitionsWithIndex$default$2()|",
      "|java+method:///scala/math/BigInt$/int2bigInt(int)|",
      "|java+method:///scala/math/BigInt/$minus(scala.math.BigInt)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/package$/BigInt()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$range$1$$anonfun$30/SparkContext$$anonfun$range$1$$anonfun$30(org.apache.spark.SparkContext$$anonfun$range$1,scala.math.BigInt)|",
      "|java+method:///scala/runtime/BoxesRunTime/equalsNumObject(java.lang.Number,java.lang.Object)|",
      "|java+method:///scala/math/BigInt$/apply(long)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD$$anonfun$keyBy$1/apply()|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+method:///org/apache/spark/rdd/RDD/org$apache$spark$rdd$RDD$$sc()|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$keyBy$1/apply()|",
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$keyBy$1$$anonfun$apply$52/RDD$$anonfun$keyBy$1$$anonfun$apply$52(org.apache.spark.rdd.RDD$$anonfun$keyBy$1,scala.Function1)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rdd/RDD/org$apache$spark$rdd$RDD$$sc()|",
      "|java+method:///org/apache/spark/SparkContext/clean$default$2()|",
      "|java+method:///org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/rdd/RDD$$anonfun$keyBy$1/apply()|",
      "|java+method:///org/apache/spark/SparkContext/clean(java.lang.Object,boolean)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$keyBy$1$$anonfun$apply$56/RDD$$anonfun$keyBy$1$$anonfun$apply$56(org.apache.spark.rdd.RDD$$anonfun$keyBy$1,scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore/remove(org.apache.spark.storage.BlockId)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/MemoryStore/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$currentMemory_$eq(long)|",
      "|java+constructor:///org/apache/spark/storage/MemoryStore$$anonfun$remove$1/MemoryStore$$anonfun$remove$1(org.apache.spark.storage.MemoryStore,org.apache.spark.storage.BlockId,org.apache.spark.storage.MemoryEntry)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$currentMemory()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryEntry/size()|",
      "|java+method:///java/util/LinkedHashMap/remove(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$entries()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/MemoryStore/logDebug(scala.Function0)|",
      "|java+constructor:///org/apache/spark/storage/MemoryStore$$anonfun$remove$1/MemoryStore$$anonfun$remove$1(org.apache.spark.storage.MemoryStore,org.apache.spark.storage.BlockId,org.apache.spark.storage.MemoryEntry)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryEntry/size()|",
      "|java+method:///org/apache/spark/memory/MemoryManager/releaseStorageMemory(long)|",
      "|java+method:///java/util/LinkedHashMap/remove(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$entries()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1/apply$mcV$sp()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/Int()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/newTaskAttemptContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)|",
      "|java+method:///java/lang/Class/newInstance()|",
      "|java+method:///org/apache/spark/rdd/RDD/context()|",
      "|java+method:///java/text/SimpleDateFormat/format(java.util.Date)|",
      "|java+method:///org/apache/hadoop/mapreduce/OutputCommitter/commitJob(org.apache.hadoop.mapreduce.JobContext)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12/PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12(org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1,java.lang.String,int,org.apache.spark.util.SerializableConfiguration,java.lang.Class)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/getConfiguration()|",
      "|java+constructor:///org/apache/hadoop/mapreduce/Job/Job(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/org$apache$spark$rdd$PairRDDFunctions$$isOutputSpecValidationEnabled()|",
      "|java+constructor:///java/util/Date/Date()|",
      "|java+method:///org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function2,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/getOutputFormatClass()|",
      "|java+constructor:///java/text/SimpleDateFormat/SimpleDateFormat(java.lang.String)|",
      "|java+method:///org/apache/spark/rdd/RDD/id()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/newTaskAttemptID(java.lang.String,int,boolean,int,int)|",
      "|java+constructor:///org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/hadoop/mapreduce/OutputFormat/getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)|",
      "|java+method:///org/apache/hadoop/mapreduce/OutputCommitter/setupJob(org.apache.hadoop.mapreduce.JobContext)|",
      "|java+method:///org/apache/spark/util/SerializableConfiguration/value()|",
      "|java+method:///org/apache/hadoop/mapreduce/OutputFormat/checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil$/get()|",
      "|java+method:///java/lang/Class/getSimpleName()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/newTaskAttemptContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)|",
      "|java+method:///java/lang/Class/newInstance()|",
      "|java+method:///org/apache/spark/rdd/RDD/context()|",
      "|java+method:///java/text/SimpleDateFormat/format(java.util.Date)|",
      "|java+method:///org/apache/hadoop/mapreduce/OutputCommitter/commitJob(org.apache.hadoop.mapreduce.JobContext)|",
      "|java+constructor:///org/apache/hadoop/mapreduce/Job/Job(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/org$apache$spark$rdd$PairRDDFunctions$$isOutputSpecValidationEnabled()|",
      "|java+constructor:///java/util/Date/Date()|",
      "|java+method:///org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function2,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/hadoop/mapreduce/Job/getOutputFormatClass()|",
      "|java+constructor:///java/text/SimpleDateFormat/SimpleDateFormat(java.lang.String)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/logWarning(scala.Function0)|",
      "|java+method:///org/apache/hadoop/mapreduce/OutputFormat/getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$apply$mcV$sp$3/PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$apply$mcV$sp$3(org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1,java.lang.String)|",
      "|java+method:///org/apache/spark/rdd/RDD/conf()|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+method:///scala/reflect/ClassTag$/Int()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12/PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12(org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1,java.lang.String,int,org.apache.spark.util.SerializableConfiguration,java.lang.Class)|",
      "|java+method:///org/apache/hadoop/mapreduce/OutputCommitter/setupJob(org.apache.hadoop.mapreduce.JobContext)|",
      "|java+method:///org/apache/spark/util/SerializableConfiguration/value()|",
      "|java+method:///org/apache/spark/deploy/SparkHadoopUtil/getConfigurationFromJobContext(org.apache.hadoop.mapreduce.JobContext)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/hadoop/mapreduce/OutputFormat/checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/RDD/id()|",
      "|java+method:///java/lang/Object/getClass()|",
      "|java+method:///java/lang/String/contains(java.lang.CharSequence)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/newTaskAttemptID(java.lang.String,int,boolean,int,int)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaPairRDD$$anonfun$cogroupResult3ToJava$1/apply(scala.Tuple4)|",
    "called": "|java+constructor:///scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Tuple4/_1()|",
      "|java+method:///scala/Tuple4/_2()|",
      "|java+method:///scala/Tuple4/_3()|",
      "|java+method:///scala/Tuple4/_4()|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterable(scala.collection.Iterable)|",
      "|java+constructor:///scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIterableConverter(scala.collection.Iterable)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/Tuple4/_1()|",
      "|java+method:///scala/Tuple4/_2()|",
      "|java+method:///scala/Tuple4/_3()|",
      "|java+method:///scala/Tuple4/_4()|",
      "|java+constructor:///scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkConf$$anonfun$validateSettings$6/apply(java.lang.String)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/SparkConf/logWarning(scala.Function0)|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$6$$anonfun$apply$8/SparkConf$$anonfun$validateSettings$6$$anonfun$apply$8(org.apache.spark.SparkConf$$anonfun$validateSettings$6,java.lang.String)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$6$$anonfun$apply$7/SparkConf$$anonfun$validateSettings$6$$anonfun$apply$7(org.apache.spark.SparkConf$$anonfun$validateSettings$6,java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/SparkConf/logWarning(scala.Function0)|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripMargin()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$6$$anonfun$apply$5/SparkConf$$anonfun$validateSettings$6$$anonfun$apply$5(org.apache.spark.SparkConf$$anonfun$validateSettings$6,java.lang.String)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/SparkConf$$anonfun$validateSettings$6$$anonfun$apply$4/SparkConf$$anonfun$validateSettings$6$$anonfun$apply$4(org.apache.spark.SparkConf$$anonfun$validateSettings$6,java.lang.String)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/launcher/WorkerCommandBuilder/WorkerCommandBuilder(java.lang.String,int,org.apache.spark.deploy.Command)|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsJavaMapConverter(scala.collection.Map)|",
    "v1Body": [
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/JavaConversions$/mapAsJavaMap(scala.collection.Map)|",
      "|java+method:///org/apache/spark/deploy/Command/environment()|",
      "|java+method:///java/util/Map/putAll(java.util.Map)|",
      "|java+constructor:///org/apache/spark/launcher/AbstractCommandBuilder/AbstractCommandBuilder()|"
    ],
    "v2Body": [
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/collection/JavaConverters$/mapAsJavaMapConverter(scala.collection.Map)|",
      "|java+method:///org/apache/spark/deploy/Command/environment()|",
      "|java+method:///java/util/Map/putAll(java.util.Map)|",
      "|java+constructor:///org/apache/spark/launcher/AbstractCommandBuilder/AbstractCommandBuilder()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/Utils$/isBindCollision(java.lang.Throwable)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///java/net/BindException/getMessage()|",
      "|java+method:///java/net/BindException/getCause()|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$isBindCollision$1/Utils$$anonfun$isBindCollision$1()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///org/spark-project/jetty/util/MultiException/getThrowables()|",
      "|java+method:///java/lang/Exception/getCause()|",
      "|java+method:///scala/collection/mutable/Buffer/exists(scala.Function1)|"
    ],
    "v2Body": [
      "|java+method:///java/net/BindException/getMessage()|",
      "|java+method:///java/net/BindException/getCause()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///org/spark-project/jetty/util/MultiException/getThrowables()|",
      "|java+method:///java/lang/Exception/getCause()|",
      "|java+method:///scala/collection/IterableLike/exists(scala.Function1)|",
      "|java+constructor:///org/apache/spark/util/Utils$$anonfun$isBindCollision$1/Utils$$anonfun$isBindCollision$1()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/onStop()|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/registrationRetryThread()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/registrationRetryTimer()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$registerMasterThreadPool()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$registerMasterFutures()|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$onStop$1/AppClient$ClientEndpoint$$anonfun$onStop$1(org.apache.spark.deploy.client.AppClient$ClientEndpoint)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///java/util/concurrent/ScheduledFuture/cancel(boolean)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///java/util/concurrent/ScheduledExecutorService/shutdownNow()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///java/util/concurrent/ThreadPoolExecutor/shutdownNow()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/registrationRetryThread()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/registrationRetryTimer()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$registerMasterThreadPool()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/org$apache$spark$deploy$client$AppClient$ClientEndpoint$$registerMasterFutures()|",
      "|java+constructor:///org/apache/spark/deploy/client/AppClient$ClientEndpoint$$anonfun$onStop$1/AppClient$ClientEndpoint$$anonfun$onStop$1(org.apache.spark.deploy.client.AppClient$ClientEndpoint)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///scala/collection/mutable/ArrayOps/foreach(scala.Function1)|",
      "|java+method:///java/util/concurrent/Future/cancel(boolean)|",
      "|java+method:///java/util/concurrent/atomic/AtomicReference/get()|",
      "|java+method:///java/util/concurrent/ScheduledExecutorService/shutdownNow()|",
      "|java+method:///org/apache/spark/deploy/client/AppClient$ClientEndpoint/askAndReplyThreadPool()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///java/util/concurrent/ThreadPoolExecutor/shutdownNow()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/RRunner$/main(java.lang.String%5B%5D)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+constructor:///java/lang/ProcessBuilder/ProcessBuilder(java.util.List)|",
      "|java+method:///java/lang/ProcessBuilder/redirectErrorStream(boolean)|",
      "|java+method:///java/io/PrintStream/println(java.lang.String)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///java/lang/Process/waitFor()|",
      "|java+method:///org/apache/spark/api/r/RBackend/close()|",
      "|java+method:///scala/sys/package$/env()|",
      "|java+method:///org/apache/hadoop/fs/Path/getName()|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///java/util/concurrent/Semaphore/tryAcquire(long,java.util.concurrent.TimeUnit)|",
      "|java+constructor:///org/apache/spark/deploy/RRunner$$anon$1/RRunner$$anon$1(org.apache.spark.api.r.RBackend,scala.runtime.VolatileIntRef,java.util.concurrent.Semaphore)|",
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///java/util/concurrent/Semaphore/Semaphore(int)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/sys/package$/props()|",
      "|java+method:///java/lang/Thread/start()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/slice(int,int)|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPath(java.lang.String,boolean)|",
      "|java+constructor:///org/apache/spark/deploy/RRunner$$anonfun$5/RRunner$$anonfun$5(scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/deploy/RRunner$$anonfun$3/RRunner$$anonfun$3(scala.runtime.ObjectRef)|",
      "|java+method:///scala/collection/immutable/Map/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///java/lang/System/exit(int)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/RRunner$$anonfun$1/RRunner$$anonfun$1()|",
      "|java+method:///org/apache/spark/api/r/RUtils$/sparkRPackagePath(boolean)|",
      "|java+constructor:///org/apache/spark/deploy/RRunner$$anonfun$2/RRunner$$anonfun$2()|",
      "|java+constructor:///org/apache/spark/deploy/RRunner$$anonfun$4/RRunner$$anonfun$4()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/sys/SystemProperties/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+constructor:///scala/runtime/VolatileIntRef/VolatileIntRef(int)|",
      "|java+constructor:///org/apache/spark/util/RedirectThread/RedirectThread(java.io.InputStream,java.io.OutputStream,java.lang.String,boolean)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+constructor:///org/apache/spark/api/r/RBackend/RBackend()|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/util/RedirectThread$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/util/RedirectThread/start()|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPath$default$2()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///java/lang/ProcessBuilder/environment()|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+constructor:///java/lang/ProcessBuilder/ProcessBuilder(java.util.List)|",
      "|java+method:///java/io/PrintStream/println(java.lang.String)|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/api/r/RBackend/close()|",
      "|java+method:///scala/sys/package$/env()|",
      "|java+method:///org/apache/hadoop/fs/Path/getName()|",
      "|java+method:///java/lang/Process/getInputStream()|",
      "|java+method:///java/util/concurrent/Semaphore/tryAcquire(long,java.util.concurrent.TimeUnit)|",
      "|java+constructor:///org/apache/spark/deploy/RRunner$$anon$1/RRunner$$anon$1(org.apache.spark.api.r.RBackend,scala.runtime.VolatileIntRef,java.util.concurrent.Semaphore)|",
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+constructor:///java/util/concurrent/Semaphore/Semaphore(int)|",
      "|java+method:///java/lang/Object/toString()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/sys/package$/props()|",
      "|java+method:///scala/collection/Seq/apply(int)|",
      "|java+method:///java/lang/ProcessBuilder/redirectErrorStream(boolean)|",
      "|java+method:///java/lang/Thread/start()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/slice(int,int)|",
      "|java+method:///scala/collection/immutable/StringOps/toInt()|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPath(java.lang.String,boolean)|",
      "|java+constructor:///org/apache/spark/deploy/RRunner$$anonfun$5/RRunner$$anonfun$5(scala.runtime.ObjectRef)|",
      "|java+constructor:///org/apache/spark/deploy/RRunner$$anonfun$3/RRunner$$anonfun$3(scala.runtime.ObjectRef)|",
      "|java+method:///scala/collection/immutable/Map/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///java/lang/ProcessBuilder/start()|",
      "|java+method:///java/io/File/exists()|",
      "|java+method:///scala/collection/TraversableOnce/mkString(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkUserAppException/SparkUserAppException(int)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/RRunner$$anonfun$1/RRunner$$anonfun$1()|",
      "|java+method:///org/apache/spark/api/r/RUtils$/sparkRPackagePath(boolean)|",
      "|java+constructor:///org/apache/spark/deploy/RRunner$$anonfun$2/RRunner$$anonfun$2()|",
      "|java+constructor:///org/apache/spark/deploy/RRunner$$anonfun$4/RRunner$$anonfun$4()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/sys/SystemProperties/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+constructor:///scala/runtime/VolatileIntRef/VolatileIntRef(int)|",
      "|java+constructor:///org/apache/spark/util/RedirectThread/RedirectThread(java.io.InputStream,java.io.OutputStream,java.lang.String,boolean)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/api/r/RBackend/RBackend()|",
      "|java+method:///scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///org/apache/spark/util/RedirectThread$/$lessinit$greater$default$4()|",
      "|java+method:///org/apache/spark/util/RedirectThread/start()|",
      "|java+method:///org/apache/spark/deploy/PythonRunner$/formatPath$default$2()|",
      "|java+method:///java/lang/Process/waitFor()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/hadoop/fs/Path/Path(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///java/lang/ProcessBuilder/environment()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/worker/DriverWrapper$/main(java.lang.String%5B%5D)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/util/Utils$/localHostName()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/shutdown()|",
      "|java+method:///scala/collection/immutable/$colon$colon/hd$1()|",
      "|java+method:///java/lang/Class/getMethod(java.lang.String,java.lang.Class%5B%5D)|",
      "|java+method:///scala/sys/SystemProperties/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///scala/collection/mutable/ArrayOps/toList()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///java/lang/Thread/getContextClassLoader()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/worker/DriverWrapper$$anonfun$1/DriverWrapper$$anonfun$1()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/immutable/$colon$colon/tl$1()|",
      "|java+method:///scala/collection/immutable/List/toArray(scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/deploy/worker/WorkerWatcher/WorkerWatcher(org.apache.spark.rpc.RpcEnv,java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/toBoolean()|",
      "|java+method:///scala/sys/package$/props()|",
      "|java+method:///java/io/PrintStream/println(java.lang.String)|",
      "|java+method:///java/lang/Thread/setContextClassLoader(java.lang.ClassLoader)|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/util/Utils$/classForName(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkConf/SparkConf()|",
      "|java+method:///java/lang/Thread/currentThread()|",
      "|java+method:///java/net/URI/toURL()|",
      "|java+constructor:///org/apache/spark/util/ChildFirstURLClassLoader/ChildFirstURLClassLoader(java.net.URL%5B%5D,java.lang.ClassLoader)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+method:///java/lang/System/exit(int)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///java/io/File/toURI()|",
      "|java+constructor:///org/apache/spark/util/MutableURLClassLoader/MutableURLClassLoader(java.net.URL%5B%5D,java.lang.ClassLoader)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///java/lang/reflect/Method/invoke(java.lang.Object,java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/util/Utils$/localHostName()|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/shutdown()|",
      "|java+method:///scala/collection/immutable/$colon$colon/hd$1()|",
      "|java+method:///java/lang/Class/getMethod(java.lang.String,java.lang.Class%5B%5D)|",
      "|java+constructor:///org/apache/spark/util/ChildFirstURLClassLoader/ChildFirstURLClassLoader(java.net.URL%5B%5D,java.lang.ClassLoader)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager,boolean)|",
      "|java+method:///scala/collection/mutable/ArrayOps/toList()|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///java/lang/Thread/getContextClassLoader()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/worker/DriverWrapper$$anonfun$1/DriverWrapper$$anonfun$1()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///scala/collection/immutable/$colon$colon/tl$1()|",
      "|java+method:///scala/collection/immutable/List/toArray(scala.reflect.ClassTag)|",
      "|java+method:///scala/collection/immutable/StringOps/toBoolean()|",
      "|java+method:///scala/sys/package$/props()|",
      "|java+method:///java/io/PrintStream/println(java.lang.String)|",
      "|java+method:///java/lang/Thread/setContextClassLoader(java.lang.ClassLoader)|",
      "|java+constructor:///org/apache/spark/deploy/worker/WorkerWatcher/WorkerWatcher(org.apache.spark.rpc.RpcEnv,java.lang.String,boolean)|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/util/Utils$/classForName(java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv$/create$default$6()|",
      "|java+constructor:///org/apache/spark/SparkConf/SparkConf()|",
      "|java+method:///java/lang/Thread/currentThread()|",
      "|java+method:///java/net/URI/toURL()|",
      "|java+method:///java/lang/System/exit(int)|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///scala/sys/SystemProperties/getOrElse(java.lang.Object,scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/worker/WorkerWatcher$/$lessinit$greater$default$3()|",
      "|java+method:///java/io/File/toURI()|",
      "|java+constructor:///org/apache/spark/util/MutableURLClassLoader/MutableURLClassLoader(java.net.URL%5B%5D,java.lang.ClassLoader)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
      "|java+method:///java/lang/reflect/Method/invoke(java.lang.Object,java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/collection/Utils$/takeOrdered(scala.collection.Iterator,int,scala.math.Ordering)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///org/spark-project/guava/collect/Ordering/leastOf(java.util.Iterator,int)|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterator(scala.collection.Iterator)|",
      "|java+method:///scala/collection/Iterable/iterator()|",
      "|java+method:///scala/collection/JavaConversions$/collectionAsScalaIterable(java.util.Collection)|",
      "|java+constructor:///org/apache/spark/util/collection/Utils$$anon$1/Utils$$anon$1(scala.math.Ordering)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+constructor:///org/apache/spark/util/collection/Utils$$anon$1/Utils$$anon$1(scala.math.Ordering)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaIteratorConverter(java.util.Iterator)|",
      "|java+method:///org/spark-project/guava/collect/Ordering/leastOf(java.util.Iterator,int)|",
      "|java+method:///java/util/List/iterator()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$class/parseConstraintString(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils,java.lang.String)|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/spark-project/guava/base/Splitter/trimResults()|",
      "|java+method:///org/spark-project/guava/base/Splitter/withKeyValueSeparator(char)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/spark-project/guava/base/Splitter/on(char)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$parseConstraintString$1/MesosSchedulerUtils$$anonfun$parseConstraintString$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|",
      "|java+method:///scala/collection/mutable/Map$/canBuildFrom()|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///scala/collection/mutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/MapLike/$plus$plus(scala.collection.GenTraversableOnce)|",
      "|java+method:///java/lang/String/isEmpty()|",
      "|java+method:///org/spark-project/guava/base/Splitter$MapSplitter/split(java.lang.CharSequence)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/immutable/Map$/apply(scala.collection.Seq)|",
      "|java+method:///scala/Predef$/Map()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/spark-project/guava/base/Splitter/trimResults()|",
      "|java+method:///scala/collection/immutable/Map/mapValues(scala.Function1)|",
      "|java+method:///org/spark-project/guava/base/Splitter/withKeyValueSeparator(char)|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///org/spark-project/guava/base/Splitter/on(char)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///java/lang/String/isEmpty()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/MesosSchedulerUtils$$anonfun$parseConstraintString$1/MesosSchedulerUtils$$anonfun$parseConstraintString$1(org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils)|",
      "|java+constructor:///java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String,java.lang.Throwable)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/spark-project/guava/base/Splitter$MapSplitter/split(java.lang.CharSequence)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/Task/run(long,int,org.apache.spark.metrics.MetricsSystem)|",
    "called": "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/Utils$/localHostName()|",
      "|java+method:///org/apache/spark/TaskContextImpl/markTaskCompleted()|",
      "|java+constructor:///org/apache/spark/scheduler/Task$$anonfun$run$3/Task$$anonfun$run$3(org.apache.spark.scheduler.Task,org.apache.spark.TaskContextImpl)|",
      "|java+method:///org/apache/spark/scheduler/Task/partitionId()|",
      "|java+method:///org/apache/spark/scheduler/Task/taskMemoryManager()|",
      "|java+constructor:///org/apache/spark/TaskContextImpl/TaskContextImpl(int,int,long,int,org.apache.spark.unsafe.memory.TaskMemoryManager,org.apache.spark.metrics.MetricsSystem,scala.collection.Seq,boolean,org.apache.spark.executor.TaskMetrics)|",
      "|java+constructor:///org/apache/spark/scheduler/Task$$anonfun$run$2/Task$$anonfun$run$2(org.apache.spark.scheduler.Task)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/TaskContextImpl$/$lessinit$greater$default$9()|",
      "|java+method:///org/apache/spark/TaskContextImpl/collectAccumulators()|",
      "|java+method:///org/apache/spark/scheduler/Task/stageId()|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/setHostname(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/Task/_killed()|",
      "|java+method:///org/apache/spark/scheduler/Task/taskThread_$eq(java.lang.Thread)|",
      "|java+method:///org/apache/spark/scheduler/Task/context_$eq(org.apache.spark.TaskContextImpl)|",
      "|java+method:///org/apache/spark/util/Utils$/tryLogNonFatalError(scala.Function0)|",
      "|java+method:///org/apache/spark/TaskContextImpl/taskMetrics()|",
      "|java+method:///org/apache/spark/TaskContext$/setTaskContext(org.apache.spark.TaskContext)|",
      "|java+method:///java/lang/Thread/currentThread()|",
      "|java+method:///org/apache/spark/TaskContext$/unset()|",
      "|java+method:///org/apache/spark/scheduler/Task/kill(boolean)|",
      "|java+constructor:///org/apache/spark/scheduler/Task$$anonfun$run$1/Task$$anonfun$run$1(org.apache.spark.scheduler.Task)|",
      "|java+method:///org/apache/spark/scheduler/Task/runTask(org.apache.spark.TaskContext)|",
      "|java+method:///org/apache/spark/scheduler/Task/context()|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/setAccumulatorsUpdater(scala.Function0)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/Utils$/localHostName()|",
      "|java+method:///org/apache/spark/TaskContextImpl/markTaskCompleted()|",
      "|java+method:///org/apache/spark/scheduler/Task/partitionId()|",
      "|java+method:///org/apache/spark/scheduler/Task/taskMemoryManager()|",
      "|java+method:///org/apache/spark/TaskContextImpl$/$lessinit$greater$default$9()|",
      "|java+method:///org/apache/spark/TaskContextImpl/collectAccumulators()|",
      "|java+method:///org/apache/spark/scheduler/Task/stageId()|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/setHostname(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/Task/_killed()|",
      "|java+method:///org/apache/spark/scheduler/Task/taskThread_$eq(java.lang.Thread)|",
      "|java+method:///org/apache/spark/scheduler/Task/context_$eq(org.apache.spark.TaskContextImpl)|",
      "|java+method:///org/apache/spark/util/Utils$/tryLogNonFatalError(scala.Function0)|",
      "|java+method:///org/apache/spark/TaskContextImpl/taskMetrics()|",
      "|java+method:///org/apache/spark/TaskContext$/setTaskContext(org.apache.spark.TaskContext)|",
      "|java+method:///java/lang/Thread/currentThread()|",
      "|java+constructor:///org/apache/spark/scheduler/Task$$anonfun$run$2/Task$$anonfun$run$2(org.apache.spark.scheduler.Task,org.apache.spark.TaskContextImpl)|",
      "|java+method:///org/apache/spark/TaskContext$/unset()|",
      "|java+method:///org/apache/spark/scheduler/Task/kill(boolean)|",
      "|java+constructor:///org/apache/spark/scheduler/Task$$anonfun$run$1/Task$$anonfun$run$1(org.apache.spark.scheduler.Task)|",
      "|java+method:///org/apache/spark/scheduler/Task/runTask(org.apache.spark.TaskContext)|",
      "|java+method:///org/apache/spark/scheduler/Task/context()|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/setAccumulatorsUpdater(scala.Function0)|",
      "|java+constructor:///org/apache/spark/TaskContextImpl/TaskContextImpl(int,int,long,int,org.apache.spark.memory.TaskMemoryManager,org.apache.spark.metrics.MetricsSystem,scala.collection.Seq,boolean,org.apache.spark.executor.TaskMetrics)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/JettyUtils$/createServletHandler(java.lang.String,javax.servlet.http.HttpServlet,java.lang.String)|",
    "called": "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/spark-project/jetty/servlet/ServletContextHandler/setContextPath(java.lang.String)|",
      "|java+constructor:///org/spark-project/jetty/servlet/ServletHolder/ServletHolder(javax.servlet.Servlet)|",
      "|java+constructor:///org/spark-project/jetty/servlet/ServletContextHandler/ServletContextHandler()|",
      "|java+method:///org/spark-project/jetty/servlet/ServletContextHandler/addServlet(org.spark-project.jetty.servlet.ServletHolder,java.lang.String)|",
      "|java+method:///org/apache/spark/ui/JettyUtils$/attachPrefix(java.lang.String,java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///org/spark-project/jetty/servlet/ServletContextHandler/setContextPath(java.lang.String)|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+constructor:///org/spark-project/jetty/servlet/ServletHolder/ServletHolder(javax.servlet.Servlet)|",
      "|java+constructor:///org/spark-project/jetty/servlet/ServletContextHandler/ServletContextHandler()|",
      "|java+method:///org/spark-project/jetty/servlet/ServletContextHandler/addServlet(org.spark-project.jetty.servlet.ServletHolder,java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///scala/collection/immutable/StringOps/stripSuffix(java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$33/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$33/apply(org.apache.spark.scheduler.AccumulableInfo)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$33/apply(java.lang.String)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$73/apply(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
    "v1Body": [
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Int()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/SparkDeploySchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.SparkContext,java.lang.String%5B%5D)|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/CoarseGrainedSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.rpc.RpcEnv)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/maxCores()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/conf()|",
      "|java+constructor:///java/util/concurrent/Semaphore/Semaphore(int)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$2/SparkDeploySchedulerBackend$$anonfun$2(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$1/SparkDeploySchedulerBackend$$anonfun$1(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend/CoarseGrainedSchedulerBackend(org.apache.spark.scheduler.TaskSchedulerImpl,org.apache.spark.rpc.RpcEnv)|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/maxCores()|",
      "|java+method:///org/apache/spark/SparkContext/env()|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anon$1/SparkDeploySchedulerBackend$$anon$1(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+method:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend/conf()|",
      "|java+constructor:///java/util/concurrent/Semaphore/Semaphore(int)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$2/SparkDeploySchedulerBackend$$anonfun$2(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/SparkDeploySchedulerBackend$$anonfun$1/SparkDeploySchedulerBackend$$anonfun$1(org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rpc/akka/AkkaRpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)|",
    "called": "|java+constructor:///scala/runtime/VolatileObjectRef/VolatileObjectRef(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/rpc/akka/AkkaRpcEndpointRef/init()|",
      "|java+constructor:///scala/runtime/VolatileByteRef/VolatileByteRef(byte)|",
      "|java+constructor:///scala/runtime/VolatileObjectRef/VolatileObjectRef(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/rpc/akka/AkkaRpcEndpointRef/AkkaRpcEndpointRef(org.apache.spark.rpc.RpcAddress,scala.Function0,org.apache.spark.SparkConf,boolean)|",
      "|java+method:///org/apache/spark/rpc/akka/AkkaRpcEnv/registerEndpoint(org.apache.spark.rpc.RpcEndpoint,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+constructor:///org/apache/spark/rpc/akka/AkkaRpcEnv$$anonfun$setupEndpoint$1/AkkaRpcEnv$$anonfun$setupEndpoint$1(org.apache.spark.rpc.akka.AkkaRpcEnv,java.lang.String,org.apache.spark.rpc.RpcEndpoint,scala.runtime.VolatileObjectRef,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)|",
      "|java+method:///org/apache/spark/rpc/akka/AkkaRpcEnv/org$apache$spark$rpc$akka$AkkaRpcEnv$$defaultAddress()|",
      "|java+constructor:///scala/runtime/ObjectRef/ObjectRef(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rpc/akka/AkkaRpcEndpointRef/init()|",
      "|java+method:///org/apache/spark/rpc/akka/AkkaRpcEnv/org$apache$spark$rpc$akka$AkkaRpcEnv$$defaultAddress()|",
      "|java+constructor:///scala/runtime/VolatileObjectRef/VolatileObjectRef(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/rpc/akka/AkkaRpcEnv$$anonfun$3/AkkaRpcEnv$$anonfun$3(org.apache.spark.rpc.akka.AkkaRpcEnv,java.lang.String,org.apache.spark.rpc.RpcEndpoint,scala.runtime.VolatileObjectRef)|",
      "|java+constructor:///org/apache/spark/rpc/akka/AkkaRpcEndpointRef/AkkaRpcEndpointRef(org.apache.spark.rpc.RpcAddress,scala.Function0,org.apache.spark.SparkConf,boolean)|",
      "|java+method:///org/apache/spark/rpc/akka/AkkaRpcEnv/registerEndpoint(org.apache.spark.rpc.RpcEndpoint,org.apache.spark.rpc.RpcEndpointRef)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/ui/jobs/TaskDataSource$$anonfun$92/TaskDataSource$$anonfun$92(org.apache.spark.ui.jobs.TaskDataSource)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2/applyOrElse(java.lang.Throwable,scala.Function1)|",
    "called": "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
    "v1Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1/org$apache$spark$scheduler$cluster$YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$$$outer()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2$$anonfun$applyOrElse$8/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2$$anonfun$applyOrElse$8(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/sendFailure(java.lang.Throwable)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/logError(scala.Function0,java.lang.Throwable)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1/org$apache$spark$scheduler$cluster$YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$$$outer()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcCallContext/sendFailure(java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2$$anonfun$applyOrElse$10/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2$$anonfun$applyOrElse$10(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/logError(scala.Function0,java.lang.Throwable)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler$$anonfun$15/apply(int)|",
    "called": "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/getPreferredLocs(org.apache.spark.rdd.RDD,int)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/Stage/rdd()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/getPreferredLocs(org.apache.spark.rdd.RDD,int)|",
      "|java+method:///org/apache/spark/scheduler/ResultStage/partitions()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/scheduler/Stage/rdd()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$$anonfun$fn$9$1/apply(scala.collection.Iterator)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/asScalaIterator(java.util.Iterator)|",
      "|java+method:///java/lang/Iterable/iterator()|",
      "|java+method:///scala/collection/JavaConversions$/asJavaIterator(scala.collection.Iterator)|",
      "|java+method:///org/apache/spark/api/java/function/PairFlatMapFunction/call(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/asJavaIteratorConverter(scala.collection.Iterator)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///java/lang/Iterable/iterator()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaIteratorConverter(java.util.Iterator)|",
      "|java+method:///org/apache/spark/api/java/function/PairFlatMapFunction/call(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/r/SerDe$/readMap(java.io.DataInputStream)|",
    "called": "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/r/SerDe$/readObjectType(java.io.DataInputStream)|",
      "|java+method:///scala/collection/immutable/Range/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/immutable/IndexedSeq$/canBuildFrom()|",
      "|java+method:///scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)|",
      "|java+constructor:///org/apache/spark/api/r/SerDe$$anonfun$1/SerDe$$anonfun$1(java.io.DataInputStream,char)|",
      "|java+constructor:///org/apache/spark/api/r/SerDe$$anonfun$2/SerDe$$anonfun$2(java.io.DataInputStream)|",
      "|java+method:///scala/collection/immutable/IndexedSeq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readInt(java.io.DataInputStream)|",
      "|java+method:///scala/runtime/RichInt$/until$extension0(int,int)|",
      "|java+method:///scala/Predef$/intWrapper(int)|",
      "|java+method:///scala/collection/JavaConversions$/mapAsJavaMap(scala.collection.Map)|",
      "|java+constructor:///java/util/HashMap/HashMap()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readList(java.io.DataInputStream)|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/mutable/ArrayOps/toMap(scala.Predef$$less$colon$less)|",
      "|java+constructor:///java/util/HashMap/HashMap()|",
      "|java+method:///scala/collection/JavaConverters$/mapAsJavaMapConverter(scala.collection.Map)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readArray(java.io.DataInputStream)|",
      "|java+method:///org/apache/spark/api/r/SerDe$/readInt(java.io.DataInputStream)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/collection/mutable/ArrayOps/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$cogroup$3/apply()|",
    "called": "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD/CoGroupedRDD(scala.collection.Seq,org.apache.spark.Partitioner)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$cogroup$3$$anonfun$apply$48/PairRDDFunctions$$anonfun$cogroup$3$$anonfun$apply$48(org.apache.spark.rdd.PairRDDFunctions$$anonfun$cogroup$3)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/keyClass()|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///java/lang/Class/isArray()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$cogroup$3/apply()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/mapValues(scala.Function1)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD/CoGroupedRDD(scala.collection.Seq,org.apache.spark.Partitioner,scala.reflect.ClassTag)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$cogroup$3$$anonfun$apply$48/PairRDDFunctions$$anonfun$cogroup$3$$anonfun$apply$48(org.apache.spark.rdd.PairRDDFunctions$$anonfun$cogroup$3)|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/keyClass()|",
      "|java+method:///org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///java/lang/Class/isArray()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions$$anonfun$cogroup$3/apply()|",
      "|java+method:///org/apache/spark/rdd/PairRDDFunctions/mapValues(scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/RDD/debugSelf$1(org.apache.spark.rdd.RDD)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///scala/collection/mutable/ArrayOps/$plus$colon(java.lang.Object,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$35/RDD$$anonfun$35(org.apache.spark.rdd.RDD,org.apache.spark.rdd.RDD)|",
      "|java+method:///org/apache/spark/SparkContext/getRDDStorageInfo()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///org/apache/spark/storage/StorageLevel$/NONE()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/RDD/storageLevel()|",
      "|java+method:///org/apache/spark/rdd/RDD/context()|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$36/RDD$$anonfun$36(org.apache.spark.rdd.RDD)|",
      "|java+method:///scala/Predef$DummyImplicit$/dummyImplicit()|",
      "|java+method:///scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/description()|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/ArrayOps/filter(scala.Function1)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/SparkContext/getRDDStorageInfo(scala.Function1)|",
      "|java+method:///scala/collection/mutable/ArrayOps/$plus$colon(java.lang.Object,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$35/RDD$$anonfun$35(org.apache.spark.rdd.RDD,org.apache.spark.rdd.RDD)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/storage/StorageLevel$/NONE()|",
      "|java+method:///scala/Array$/canBuildFrom(scala.reflect.ClassTag)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/RDD/storageLevel()|",
      "|java+method:///org/apache/spark/rdd/RDD/context()|",
      "|java+method:///scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/rdd/RDD$$anonfun$36/RDD$$anonfun$36(org.apache.spark.rdd.RDD)|",
      "|java+method:///scala/Predef$DummyImplicit$/dummyImplicit()|",
      "|java+method:///scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)|",
      "|java+method:///org/apache/spark/storage/StorageLevel/description()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/SparkContext$$anonfun$24/SparkContext$$anonfun$24(org.apache.spark.SparkContext)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/history/HistoryServer$/main(java.lang.String%5B%5D)|",
    "called": "|java+method:///scala/Option/getOrElse(scala.Function0)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/SignalLogger$/register(org.slf4j.Logger)|",
      "|java+method:///java/lang/reflect/Constructor/newInstance(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer$$anonfun$main$1/HistoryServer$$anonfun$main$1(org.apache.spark.deploy.history.HistoryServer)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServer$/conf()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/util/ShutdownHookManager$/addShutdownHook(scala.Function0)|",
      "|java+method:///java/lang/Thread/sleep(long)|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServerArguments/HistoryServerArguments(org.apache.spark.SparkConf,java.lang.String%5B%5D)|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServer/bind()|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/classForName(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer/HistoryServer(org.apache.spark.SparkConf,org.apache.spark.deploy.history.ApplicationHistoryProvider,org.apache.spark.SecurityManager,int)|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServer$/initSecurity()|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer$$anonfun$3/HistoryServer$$anonfun$3()|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServer$/log()|",
      "|java+method:///java/lang/Class/getConstructor(java.lang.Class%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/SignalLogger$/register(org.slf4j.Logger)|",
      "|java+method:///java/lang/reflect/Constructor/newInstance(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer$$anonfun$main$1/HistoryServer$$anonfun$main$1(org.apache.spark.deploy.history.HistoryServer)|",
      "|java+method:///org/apache/spark/SparkConf/getInt(java.lang.String,int)|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServer$/conf()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/util/ShutdownHookManager$/addShutdownHook(scala.Function0)|",
      "|java+method:///java/lang/Thread/sleep(long)|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServerArguments/HistoryServerArguments(org.apache.spark.SparkConf,java.lang.String%5B%5D)|",
      "|java+constructor:///org/apache/spark/SecurityManager/SecurityManager(org.apache.spark.SparkConf)|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServer/bind()|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///org/apache/spark/util/Utils$/classForName(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer/HistoryServer(org.apache.spark.SparkConf,org.apache.spark.deploy.history.ApplicationHistoryProvider,org.apache.spark.SecurityManager,int)|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServer$/initSecurity()|",
      "|java+method:///org/apache/spark/deploy/history/HistoryServer$/log()|",
      "|java+constructor:///org/apache/spark/deploy/history/HistoryServer$$anonfun$5/HistoryServer$$anonfun$5()|",
      "|java+method:///java/lang/Class/getConstructor(java.lang.Class%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/rdd/AsyncRDDActions$/AsyncRDDActions$()|",
    "called": "|java+method:///scala/concurrent/ExecutionContext$/fromExecutorService(java.util.concurrent.ExecutorService)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonCachedThreadPool(java.lang.String,int)|",
      "|java+method:///scala/concurrent/ExecutionContext$/fromExecutorService(java.util.concurrent.ExecutorService)|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonCachedThreadPool$default$3()|",
      "|java+method:///org/apache/spark/util/ThreadUtils$/newDaemonCachedThreadPool(java.lang.String,int,int)|",
      "|java+method:///scala/concurrent/ExecutionContext$/fromExecutorService(java.util.concurrent.ExecutorService)|",
      "|java+constructor:///java/lang/Object/Object()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet/buildDriverDescription(org.apache.spark.deploy.rest.CreateSubmissionRequest)|",
    "called": "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/SparkConf/SparkConf(boolean)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
      "|java+method:///org/apache/spark/deploy/rest/CreateSubmissionRequest/appResource()|",
      "|java+method:///org/apache/spark/deploy/rest/CreateSubmissionRequest/sparkProperties()|",
      "|java+constructor:///org/apache/spark/deploy/Command/Command(java.lang.String,scala.collection.Seq,scala.collection.Map,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkConf/setAll(scala.collection.Traversable)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///java/util/Date/Date()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/rest/CreateSubmissionRequest/mainClass()|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$10/MesosSubmitRequestServlet$$anonfun$10(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$11/MesosSubmitRequestServlet$$anonfun$11(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$7/MesosSubmitRequestServlet$$anonfun$7(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$1/MesosSubmitRequestServlet$$anonfun$1(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$8/MesosSubmitRequestServlet$$anonfun$8(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$2/MesosSubmitRequestServlet$$anonfun$2(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$9/MesosSubmitRequestServlet$$anonfun$9(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$3/MesosSubmitRequestServlet$$anonfun$3(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$12/MesosSubmitRequestServlet$$anonfun$12(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$4/MesosSubmitRequestServlet$$anonfun$4(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$5/MesosSubmitRequestServlet$$anonfun$5(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$6/MesosSubmitRequestServlet$$anonfun$6(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet/newDriverId(java.util.Date)|",
      "|java+method:///org/apache/spark/util/Utils$/sparkJavaOpts(org.apache.spark.SparkConf,scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/rest/CreateSubmissionRequest/environmentVariables()|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$13/MesosSubmitRequestServlet$$anonfun$13(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/Iterable/toSeq()|",
      "|java+method:///org/apache/spark/util/Utils$/sparkJavaOpts$default$2()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription$/$lessinit$greater$default$10()|",
      "|java+constructor:///org/apache/spark/deploy/mesos/MesosDriverDescription/MesosDriverDescription(java.lang.String,java.lang.String,int,double,boolean,org.apache.spark.deploy.Command,scala.collection.immutable.Map,java.lang.String,java.util.Date,scala.Option)|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/rest/CreateSubmissionRequest/appArgs()|",
      "|java+method:///scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToDouble(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/SparkConf/SparkConf(boolean)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/deploy/rest/CreateSubmissionRequest/appResource()|",
      "|java+method:///org/apache/spark/deploy/rest/CreateSubmissionRequest/sparkProperties()|",
      "|java+constructor:///org/apache/spark/deploy/Command/Command(java.lang.String,scala.collection.Seq,scala.collection.Map,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///org/apache/spark/SparkConf/setAll(scala.collection.Traversable)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///java/util/Date/Date()|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/rest/CreateSubmissionRequest/mainClass()|",
      "|java+method:///scala/collection/immutable/Map/filter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$10/MesosSubmitRequestServlet$$anonfun$10(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$11/MesosSubmitRequestServlet$$anonfun$11(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$6/MesosSubmitRequestServlet$$anonfun$6(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$1/MesosSubmitRequestServlet$$anonfun$1(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$8/MesosSubmitRequestServlet$$anonfun$8(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$2/MesosSubmitRequestServlet$$anonfun$2(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$9/MesosSubmitRequestServlet$$anonfun$9(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$3/MesosSubmitRequestServlet$$anonfun$3(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$12/MesosSubmitRequestServlet$$anonfun$12(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$4/MesosSubmitRequestServlet$$anonfun$4(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$5/MesosSubmitRequestServlet$$anonfun$5(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$7/MesosSubmitRequestServlet$$anonfun$7(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet/newDriverId(java.util.Date)|",
      "|java+method:///org/apache/spark/util/Utils$/sparkJavaOpts(org.apache.spark.SparkConf,scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/rest/CreateSubmissionRequest/environmentVariables()|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$13/MesosSubmitRequestServlet$$anonfun$13(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
      "|java+constructor:///org/apache/spark/deploy/rest/mesos/MesosSubmitRequestServlet$$anonfun$14/MesosSubmitRequestServlet$$anonfun$14(org.apache.spark.deploy.rest.mesos.MesosSubmitRequestServlet)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/Iterable/toSeq()|",
      "|java+method:///org/apache/spark/util/Utils$/sparkJavaOpts$default$2()|",
      "|java+method:///org/apache/spark/deploy/mesos/MesosDriverDescription$/$lessinit$greater$default$10()|",
      "|java+constructor:///org/apache/spark/deploy/mesos/MesosDriverDescription/MesosDriverDescription(java.lang.String,java.lang.String,int,double,boolean,org.apache.spark.deploy.Command,scala.collection.immutable.Map,java.lang.String,java.util.Date,scala.Option)|",
      "|java+method:///scala/collection/immutable/Map/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/rest/CreateSubmissionRequest/appArgs()|",
      "|java+method:///scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToDouble(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/onDisconnected(org.apache.spark.rpc.RpcAddress)|",
    "called": "|java+method:///scala/Option/exists(scala.Function1)|",
    "v1Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/logWarning(scala.Function0)|",
      "|java+method:///scala/Option/exists(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/org$apache$spark$scheduler$cluster$YarnSchedulerBackend$YarnSchedulerEndpoint$$amEndpoint()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$onDisconnected$2/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$onDisconnected$2(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint,org.apache.spark.rpc.RpcAddress)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$onDisconnected$1/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$onDisconnected$1(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint,org.apache.spark.rpc.RpcAddress)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/logWarning(scala.Function0)|",
      "|java+method:///scala/Option/exists(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint/org$apache$spark$scheduler$cluster$YarnSchedulerBackend$YarnSchedulerEndpoint$$amEndpoint()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$onDisconnected$2/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$onDisconnected$2(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint,org.apache.spark.rpc.RpcAddress)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$onDisconnected$3/YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$onDisconnected$3(org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint,org.apache.spark.rpc.RpcAddress)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/initialize()|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/splitLocalRemoteBlocks()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/size()|",
      "|java+method:///scala/collection/mutable/Queue/nonEmpty()|",
      "|java+constructor:///org/apache/spark/storage/ShuffleBlockFetcherIterator$$anonfun$initialize$1/ShuffleBlockFetcherIterator$$anonfun$initialize$1(org.apache.spark.storage.ShuffleBlockFetcherIterator)|",
      "|java+method:///org/apache/spark/TaskContext/addTaskCompletionListener(scala.Function1)|",
      "|java+method:///scala/collection/mutable/Queue/$plus$plus$eq(scala.collection.TraversableOnce)|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator$FetchRequest/size()|",
      "|java+method:///scala/collection/mutable/Queue/dequeue()|",
      "|java+constructor:///org/apache/spark/storage/ShuffleBlockFetcherIterator$$anonfun$initialize$2/ShuffleBlockFetcherIterator$$anonfun$initialize$2(org.apache.spark.storage.ShuffleBlockFetcherIterator,int)|",
      "|java+method:///scala/collection/mutable/Queue/size()|",
      "|java+constructor:///org/apache/spark/storage/ShuffleBlockFetcherIterator$$anonfun$initialize$3/ShuffleBlockFetcherIterator$$anonfun$initialize$3(org.apache.spark.storage.ShuffleBlockFetcherIterator)|",
      "|java+method:///org/apache/spark/util/Utils$/randomize(scala.collection.TraversableOnce,scala.reflect.ClassTag)|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/mutable/Queue/front()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/fetchLocalBlocks()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/sendRequest(org.apache.spark.storage.ShuffleBlockFetcherIterator$FetchRequest)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/logDebug(scala.Function0)|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/splitLocalRemoteBlocks()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/size()|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/fetchUpToMaxBytes()|",
      "|java+constructor:///org/apache/spark/storage/ShuffleBlockFetcherIterator$$anonfun$initialize$1/ShuffleBlockFetcherIterator$$anonfun$initialize$1(org.apache.spark.storage.ShuffleBlockFetcherIterator)|",
      "|java+method:///org/apache/spark/TaskContext/addTaskCompletionListener(scala.Function1)|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/mutable/Queue/$plus$plus$eq(scala.collection.TraversableOnce)|",
      "|java+method:///org/apache/spark/storage/ShuffleBlockFetcherIterator/fetchLocalBlocks()|",
      "|java+constructor:///org/apache/spark/storage/ShuffleBlockFetcherIterator$$anonfun$initialize$2/ShuffleBlockFetcherIterator$$anonfun$initialize$2(org.apache.spark.storage.ShuffleBlockFetcherIterator,int)|",
      "|java+method:///scala/collection/mutable/Queue/size()|",
      "|java+constructor:///org/apache/spark/storage/ShuffleBlockFetcherIterator$$anonfun$initialize$3/ShuffleBlockFetcherIterator$$anonfun$initialize$3(org.apache.spark.storage.ShuffleBlockFetcherIterator)|",
      "|java+method:///org/apache/spark/util/Utils$/randomize(scala.collection.TraversableOnce,scala.reflect.ClassTag)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/resourceOffers(org.apache.mesos.SchedulerDriver,java.util.List)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///org/apache/mesos/Protos$Filters$Builder/setRefuseSeconds(double)|",
      "|java+method:///scala/collection/mutable/Buffer/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/stateLock()|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,org.apache.mesos.SchedulerDriver,org.apache.mesos.Protos$Filters)|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+method:///org/apache/mesos/Protos$Filters$Builder/build()|",
      "|java+method:///org/apache/mesos/Protos$Filters/newBuilder()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/mesos/Protos$Filters$Builder/setRefuseSeconds(double)|",
      "|java+method:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend/stateLock()|",
      "|java+method:///org/apache/mesos/Protos$Filters$Builder/build()|",
      "|java+method:///org/apache/mesos/Protos$Filters/newBuilder()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+constructor:///org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1/CoarseMesosSchedulerBackend$$anonfun$resourceOffers$1(org.apache.spark.scheduler.cluster.mesos.CoarseMesosSchedulerBackend,org.apache.mesos.SchedulerDriver,org.apache.mesos.Protos$Filters)|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$5/apply(scala.Tuple2)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/taskIdToTaskSetManager()|",
      "|java+method:///scala/Tuple2/_1$mcJ$sp()|",
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$5$$anonfun$apply$8/TaskSchedulerImpl$$anonfun$5$$anonfun$apply$8(org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$5,long,org.apache.spark.executor.TaskMetrics)|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/Option/map(scala.Function1)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/scheduler/TaskSchedulerImpl$$anonfun$5$$anonfun$apply$9/TaskSchedulerImpl$$anonfun$5$$anonfun$apply$9(org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$5,long,org.apache.spark.executor.TaskMetrics)|",
      "|java+method:///scala/Option$/option2Iterable(scala.Option)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/TaskSchedulerImpl/taskIdToTaskSetManager()|",
      "|java+method:///scala/Tuple2/_1$mcJ$sp()|",
      "|java+method:///scala/Tuple2/_2()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+method:///scala/Option/map(scala.Function1)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rpc/akka/AkkaRpcEnvFactory/create(org.apache.spark.rpc.RpcEnvConfig)|",
    "called": "|java+method:///scala/Tuple2/_2$mcI$sp()|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/AkkaUtils$/createActorSystem(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///akka/actor/ActorSystem/actorOf(akka.actor.Props,java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/port()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/conf()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/name()|",
      "|java+method:///akka/actor/Props$/apply(java.lang.Class,scala.collection.Seq)|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/rpc/akka/AkkaRpcEnv/AkkaRpcEnv(akka.actor.ActorSystem,org.apache.spark.SparkConf,int)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/host()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/securityManager()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/AkkaUtils$/createActorSystem(java.lang.String,java.lang.String,int,org.apache.spark.SparkConf,org.apache.spark.SecurityManager)|",
      "|java+constructor:///org/apache/spark/rpc/akka/AkkaRpcEnv/AkkaRpcEnv(akka.actor.ActorSystem,org.apache.spark.SecurityManager,org.apache.spark.SparkConf,int)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///akka/actor/ActorSystem/actorOf(akka.actor.Props,java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/port()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/conf()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/name()|",
      "|java+method:///akka/actor/Props$/apply(java.lang.Class,scala.collection.Seq)|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/host()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/securityManager()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/BlockManagerSlaveEndpoint$$anonfun$org$apache$spark$storage$BlockManagerSlaveEndpoint$$doAsync$2$$anonfun$applyOrElse$6/apply()|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///org/apache/spark/rpc/RpcCallContext/sender()|",
      "|java+method:///org/apache/spark/storage/BlockManagerSlaveEndpoint$$anonfun$org$apache$spark$storage$BlockManagerSlaveEndpoint$$doAsync$2$$anonfun$applyOrElse$6/apply()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/rpc/RpcCallContext/senderAddress()|",
      "|java+method:///org/apache/spark/storage/BlockManagerSlaveEndpoint$$anonfun$org$apache$spark$storage$BlockManagerSlaveEndpoint$$doAsync$2$$anonfun$applyOrElse$6/apply()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/LocalSparkCluster/stop()|",
    "called": "|java+method:///scala/collection/mutable/ArrayBuffer/clear()|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/ArrayBuffer/clear()|",
      "|java+constructor:///org/apache/spark/deploy/LocalSparkCluster$$anonfun$stop$3/LocalSparkCluster$$anonfun$stop$3(org.apache.spark.deploy.LocalSparkCluster)|",
      "|java+constructor:///org/apache/spark/deploy/LocalSparkCluster$$anonfun$stop$1/LocalSparkCluster$$anonfun$stop$1(org.apache.spark.deploy.LocalSparkCluster)|",
      "|java+method:///org/apache/spark/deploy/LocalSparkCluster/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/LocalSparkCluster/masterRpcEnvs()|",
      "|java+method:///org/apache/spark/deploy/LocalSparkCluster/org$apache$spark$deploy$LocalSparkCluster$$workerRpcEnvs()|",
      "|java+constructor:///org/apache/spark/deploy/LocalSparkCluster$$anonfun$stop$2/LocalSparkCluster$$anonfun$stop$2(org.apache.spark.deploy.LocalSparkCluster)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/deploy/LocalSparkCluster$$anonfun$stop$4/LocalSparkCluster$$anonfun$stop$4(org.apache.spark.deploy.LocalSparkCluster)|",
      "|java+constructor:///org/apache/spark/deploy/LocalSparkCluster$$anonfun$stop$3/LocalSparkCluster$$anonfun$stop$3(org.apache.spark.deploy.LocalSparkCluster)|",
      "|java+constructor:///org/apache/spark/deploy/LocalSparkCluster$$anonfun$stop$1/LocalSparkCluster$$anonfun$stop$1(org.apache.spark.deploy.LocalSparkCluster)|",
      "|java+method:///org/apache/spark/deploy/LocalSparkCluster/logInfo(scala.Function0)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/LocalSparkCluster/masterRpcEnvs()|",
      "|java+constructor:///org/apache/spark/deploy/LocalSparkCluster$$anonfun$stop$5/LocalSparkCluster$$anonfun$stop$5(org.apache.spark.deploy.LocalSparkCluster)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/clear()|",
      "|java+method:///org/apache/spark/deploy/LocalSparkCluster/org$apache$spark$deploy$LocalSparkCluster$$workerRpcEnvs()|",
      "|java+constructor:///org/apache/spark/deploy/LocalSparkCluster$$anonfun$stop$2/LocalSparkCluster$$anonfun$stop$2(org.apache.spark.deploy.LocalSparkCluster)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$getShuffleMapStage(org.apache.spark.ShuffleDependency,int)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/shuffleToMapStage()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/registerShuffleDependencies(org.apache.spark.ShuffleDependency,int)|",
      "|java+method:///org/apache/spark/ShuffleDependency/shuffleId()|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/newOrUsedShuffleStage(org.apache.spark.ShuffleDependency,int)|"
    ],
    "v2Body": [
      "|java+method:///scala/Some/x()|",
      "|java+constructor:///org/apache/spark/scheduler/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getShuffleMapStage$1/DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getShuffleMapStage$1(org.apache.spark.scheduler.DAGScheduler,int)|",
      "|java+method:///scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/getAncestorShuffleDependencies(org.apache.spark.rdd.RDD)|",
      "|java+method:///scala/collection/mutable/Stack/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/org$apache$spark$scheduler$DAGScheduler$$newOrUsedShuffleStage(org.apache.spark.ShuffleDependency,int)|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/scheduler/DAGScheduler/shuffleToMapStage()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/ShuffleDependency/rdd()|",
      "|java+method:///org/apache/spark/ShuffleDependency/shuffleId()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/PythonAccumulatorParam/addInPlace(java.util.List,java.util.List)|",
    "called": "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
    "v1Body": [
      "|java+method:///java/net/Socket/getInputStream()|",
      "|java+method:///scala/collection/mutable/Buffer/foreach(scala.Function1)|",
      "|java+method:///java/util/List/size()|",
      "|java+method:///java/io/InputStream/read()|",
      "|java+method:///org/apache/spark/api/python/PythonAccumulatorParam/bufferSize()|",
      "|java+method:///scala/collection/JavaConversions$/asScalaBuffer(java.util.List)|",
      "|java+constructor:///java/io/DataOutputStream/DataOutputStream(java.io.OutputStream)|",
      "|java+constructor:///org/apache/spark/api/python/PythonAccumulatorParam$$anonfun$addInPlace$1/PythonAccumulatorParam$$anonfun$addInPlace$1(org.apache.spark.api.python.PythonAccumulatorParam,java.io.DataOutputStream)|",
      "|java+method:///java/io/DataOutputStream/writeInt(int)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///org/apache/spark/api/python/PythonAccumulatorParam/openSocket()|",
      "|java+method:///java/util/List/addAll(java.util.Collection)|",
      "|java+method:///java/io/DataOutputStream/flush()|",
      "|java+method:///java/net/Socket/getOutputStream()|",
      "|java+constructor:///java/io/BufferedOutputStream/BufferedOutputStream(java.io.OutputStream,int)|"
    ],
    "v2Body": [
      "|java+method:///java/net/Socket/getInputStream()|",
      "|java+method:///java/util/List/size()|",
      "|java+method:///java/io/InputStream/read()|",
      "|java+method:///scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)|",
      "|java+method:///org/apache/spark/api/python/PythonAccumulatorParam/bufferSize()|",
      "|java+constructor:///java/io/DataOutputStream/DataOutputStream(java.io.OutputStream)|",
      "|java+method:///org/apache/spark/api/python/PythonAccumulatorParam/serverHost()|",
      "|java+constructor:///org/apache/spark/api/python/PythonAccumulatorParam$$anonfun$addInPlace$1/PythonAccumulatorParam$$anonfun$addInPlace$1(org.apache.spark.api.python.PythonAccumulatorParam,java.io.DataOutputStream)|",
      "|java+method:///java/io/DataOutputStream/writeInt(int)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///scala/collection/IterableLike/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/api/python/PythonAccumulatorParam/openSocket()|",
      "|java+method:///java/util/List/addAll(java.util.Collection)|",
      "|java+method:///java/io/DataOutputStream/flush()|",
      "|java+method:///java/net/Socket/getOutputStream()|",
      "|java+constructor:///java/io/BufferedOutputStream/BufferedOutputStream(java.io.OutputStream,int)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/java/JavaRDDLike$$anonfun$collectPartitions$1/apply(java.lang.Object)|",
    "called": "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+constructor:///java/util/ArrayList/ArrayList(java.util.Collection)|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike$$anonfun$collectPartitions$1/apply(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/mutable/ArrayOps/toSeq()|",
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/api/java/JavaRDDLike$$anonfun$collectPartitions$1/apply(java.lang.Object)|",
      "|java+method:///scala/Predef$/genericArrayOps(java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/scope/RDDOperationGraph$$anonfun$org$apache$spark$ui$scope$RDDOperationGraph$$makeDotSubgraph$2/apply(org.apache.spark.ui.scope.RDDOperationCluster)|",
    "called": "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
    "v1Body": [
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.String)|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationGraph$/org$apache$spark$ui$scope$RDDOperationGraph$$makeDotSubgraph(org.apache.spark.ui.scope.RDDOperationCluster,java.lang.String)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///org/apache/spark/ui/scope/RDDOperationGraph$/org$apache$spark$ui$scope$RDDOperationGraph$$makeDotSubgraph(scala.collection.mutable.StringBuilder,org.apache.spark.ui.scope.RDDOperationCluster,java.lang.String)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/partial/GroupedCountEvaluator$$anonfun$currentResult$1/apply(scala.Tuple2)|",
    "called": "|java+method:///scala/Tuple2/_2$mcJ$sp()|",
    "v1Body": [
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+constructor:///org/apache/spark/partial/BoundedDouble/BoundedDouble(double,double,double,double)|",
      "|java+method:///scala/collection/mutable/Map/update(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/Tuple2/_2$mcJ$sp()|"
    ],
    "v2Body": [
      "|java+method:///scala/Tuple2/_1()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/Tuple2/_2$mcJ$sp()|",
      "|java+constructor:///org/apache/spark/partial/BoundedDouble/BoundedDouble(double,double,double,double)|",
      "|java+method:///java/util/HashMap/put(java.lang.Object,java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$/accumulableInfoFromJson(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/apache/spark/scheduler/AccumulableInfo$/apply(long,java.lang.String,scala.Option,java.lang.String,boolean)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/reflect/ManifestFactory$/Boolean()|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$10/JsonProtocol$$anonfun$10()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$69/JsonProtocol$$anonfun$69()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extractOpt(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/apache/spark/util/Utils$/jsonOption(org.json4s.JsonAST$JValue)|",
      "|java+method:///org/apache/spark/scheduler/AccumulableInfo$/apply(long,java.lang.String,scala.Option,java.lang.String,boolean)|",
      "|java+method:///org/json4s/package$/jvalue2monadic(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+method:///scala/reflect/ManifestFactory$/Boolean()|",
      "|java+method:///org/json4s/MonadicJValue/$bslash(java.lang.String)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$72/JsonProtocol$$anonfun$72()|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$10/JsonProtocol$$anonfun$10()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extractOpt(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/jobs/StagePage$$anonfun$43/apply(org.apache.spark.ui.jobs.UIData$TaskUIData)|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$43$$anonfun$apply$15/StagePage$$anonfun$43$$anonfun$apply$15(org.apache.spark.ui.jobs.StagePage$$anonfun$43)|",
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///org/apache/spark/ui/jobs/StagePage$$anonfun$43$$anonfun$apply$2/StagePage$$anonfun$43$$anonfun$apply$2(org.apache.spark.ui.jobs.StagePage$$anonfun$43)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/inputMetrics()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///scala/Option/get()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/jobs/UIData$TaskUIData/taskMetrics()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/resultSerializationTime()|",
      "|java+method:///scala/Option/get()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rdd/CoGroupedRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)|",
    "called": "|java+method:///scala/collection/Seq$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$1/CoGroupedRDD$$anonfun$1(org.apache.spark.rdd.CoGroupedRDD,int)|",
      "|java+method:///scala/collection/TraversableLike/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/diskBytesSpilled()|",
      "|java+method:///org/apache/spark/TaskContext/taskMetrics()|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$compute$5/CoGroupedRDD$$anonfun$compute$5(org.apache.spark.rdd.CoGroupedRDD,org.apache.spark.util.collection.ExternalAppendOnlyMap)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/incDiskBytesSpilled(long)|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$compute$1/CoGroupedRDD$$anonfun$compute$1(org.apache.spark.rdd.CoGroupedRDD)|",
      "|java+constructor:///org/apache/spark/InterruptibleIterator/InterruptibleIterator(org.apache.spark.TaskContext,scala.collection.Iterator)|",
      "|java+method:///org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$compute$2/CoGroupedRDD$$anonfun$compute$2(org.apache.spark.rdd.CoGroupedRDD,org.apache.spark.TaskContext,org.apache.spark.rdd.CoGroupPartition,scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/rdd/CoGroupedRDD/dependencies()|",
      "|java+method:///scala/collection/immutable/Map/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkEnv$/get()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/memoryBytesSpilled()|",
      "|java+method:///org/apache/spark/util/collection/AppendOnlyMap/iterator()|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/rdd/CoGroupedRDD/createExternalMap(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+constructor:///org/apache/spark/util/collection/AppendOnlyMap/AppendOnlyMap(int)|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/util/collection/AppendOnlyMap$/$lessinit$greater$default$1()|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$compute$3/CoGroupedRDD$$anonfun$compute$3(org.apache.spark.rdd.CoGroupedRDD,scala.Function1)|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$2/CoGroupedRDD$$anonfun$2(org.apache.spark.rdd.CoGroupedRDD,org.apache.spark.util.collection.AppendOnlyMap,scala.Function2)|",
      "|java+method:///org/apache/spark/SparkEnv/conf()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/peakMemoryUsedBytes()|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$compute$4/CoGroupedRDD$$anonfun$compute$4(org.apache.spark.rdd.CoGroupedRDD)|",
      "|java+method:///org/apache/spark/InternalAccumulator$/PEAK_EXECUTION_MEMORY()|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/incMemoryBytesSpilled(long)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/iterator()|",
      "|java+method:///scala/collection/Seq/length()|",
      "|java+method:///org/apache/spark/TaskContext/internalMetricsToAccumulators()|",
      "|java+method:///org/apache/spark/Accumulable/add(java.lang.Object)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/InterruptibleIterator/InterruptibleIterator(org.apache.spark.TaskContext,scala.collection.Iterator)|",
      "|java+method:///scala/collection/TraversableLike/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/diskBytesSpilled()|",
      "|java+method:///org/apache/spark/TaskContext/taskMetrics()|",
      "|java+method:///scala/collection/generic/FilterMonadic/foreach(scala.Function1)|",
      "|java+constructor:///scala/collection/mutable/ArrayBuffer/ArrayBuffer()|",
      "|java+method:///scala/collection/mutable/ArrayBuffer/withFilter(scala.Function1)|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/incDiskBytesSpilled(long)|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$compute$1/CoGroupedRDD$$anonfun$compute$1(org.apache.spark.rdd.CoGroupedRDD)|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$compute$2/CoGroupedRDD$$anonfun$compute$2(org.apache.spark.rdd.CoGroupedRDD,org.apache.spark.TaskContext,org.apache.spark.rdd.CoGroupPartition,scala.collection.mutable.ArrayBuffer)|",
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///org/apache/spark/rdd/CoGroupedRDD/dependencies()|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/memoryBytesSpilled()|",
      "|java+method:///scala/collection/immutable/Map/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/rdd/CoGroupedRDD/createExternalMap(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$compute$4/CoGroupedRDD$$anonfun$compute$4(org.apache.spark.rdd.CoGroupedRDD,org.apache.spark.util.collection.ExternalAppendOnlyMap)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/peakMemoryUsedBytes()|",
      "|java+method:///org/apache/spark/InternalAccumulator$/PEAK_EXECUTION_MEMORY()|",
      "|java+method:///org/apache/spark/executor/TaskMetrics/incMemoryBytesSpilled(long)|",
      "|java+constructor:///org/apache/spark/rdd/CoGroupedRDD$$anonfun$compute$3/CoGroupedRDD$$anonfun$compute$3(org.apache.spark.rdd.CoGroupedRDD)|",
      "|java+method:///org/apache/spark/util/collection/ExternalAppendOnlyMap/iterator()|",
      "|java+method:///scala/collection/Seq/length()|",
      "|java+method:///org/apache/spark/TaskContext/internalMetricsToAccumulators()|",
      "|java+method:///org/apache/spark/Accumulable/add(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/uploadBlock(java.lang.String,int,java.lang.String,org.apache.spark.storage.BlockId,org.apache.spark.network.buffer.ManagedBuffer,org.apache.spark.storage.StorageLevel)|",
    "called": "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
    "v1Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/network/netty/NettyBlockTransferService$$anon$2/NettyBlockTransferService$$anon$2(org.apache.spark.network.netty.NettyBlockTransferService,org.apache.spark.storage.BlockId,scala.concurrent.Promise)|",
      "|java+constructor:///org/apache/spark/network/shuffle/protocol/UploadBlock/UploadBlock(java.lang.String,java.lang.String,java.lang.String,byte%5B%5D,byte%5B%5D)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/serializer()|",
      "|java+method:///org/apache/spark/network/shuffle/protocol/UploadBlock/toByteArray()|",
      "|java+method:///java/nio/ByteBuffer/hasArray()|",
      "|java+method:///org/apache/spark/network/client/TransportClientFactory/createClient(java.lang.String,int)|",
      "|java+method:///org/apache/spark/network/buffer/ManagedBuffer/nioByteBuffer()|",
      "|java+method:///scala/concurrent/Promise/future()|",
      "|java+method:///org/apache/spark/serializer/JavaSerializer/newInstance()|",
      "|java+method:///java/nio/ByteBuffer/get(byte%5B%5D)|",
      "|java+method:///org/apache/spark/storage/BlockId/toString()|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/serialize(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///scala/concurrent/Promise$/apply()|",
      "|java+method:///java/nio/ByteBuffer/remaining()|",
      "|java+method:///org/apache/spark/network/client/TransportClient/sendRpc(byte%5B%5D,org.apache.spark.network.client.RpcResponseCallback)|",
      "|java+method:///java/nio/ByteBuffer/array()|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+constructor:///org/apache/spark/network/netty/NettyBlockTransferService$$anon$2/NettyBlockTransferService$$anon$2(org.apache.spark.network.netty.NettyBlockTransferService,org.apache.spark.storage.BlockId,scala.concurrent.Promise)|",
      "|java+constructor:///org/apache/spark/network/shuffle/protocol/UploadBlock/UploadBlock(java.lang.String,java.lang.String,java.lang.String,byte%5B%5D,byte%5B%5D)|",
      "|java+method:///org/apache/spark/network/client/TransportClient/sendRpc(java.nio.ByteBuffer,org.apache.spark.network.client.RpcResponseCallback)|",
      "|java+method:///org/apache/spark/network/netty/NettyBlockTransferService/serializer()|",
      "|java+method:///java/nio/ByteBuffer/array()|",
      "|java+method:///org/apache/spark/network/client/TransportClientFactory/createClient(java.lang.String,int)|",
      "|java+method:///org/apache/spark/network/buffer/ManagedBuffer/nioByteBuffer()|",
      "|java+method:///scala/concurrent/Promise/future()|",
      "|java+method:///org/apache/spark/serializer/JavaSerializer/newInstance()|",
      "|java+method:///java/nio/ByteBuffer/get(byte%5B%5D)|",
      "|java+method:///org/apache/spark/storage/BlockId/toString()|",
      "|java+method:///org/apache/spark/serializer/SerializerInstance/serialize(java.lang.Object,scala.reflect.ClassTag)|",
      "|java+method:///scala/concurrent/Promise$/apply()|",
      "|java+method:///java/nio/ByteBuffer/remaining()|",
      "|java+method:///org/apache/spark/network/shuffle/protocol/UploadBlock/toByteBuffer()|",
      "|java+method:///java/nio/ByteBuffer/hasArray()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/storage/ExternalBlockStore$$anonfun$createBlkManager$1/ExternalBlockStore$$anonfun$createBlkManager$1(org.apache.spark.storage.ExternalBlockStore)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction0$mcV$sp/AbstractFunction0$mcV$sp()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0/AbstractFunction0()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction0$mcV$sp/AbstractFunction0$mcV$sp()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient$/run(java.lang.String,java.lang.String,java.lang.String%5B%5D,org.apache.spark.SparkConf,scala.collection.immutable.Map)|",
    "called": "|java+method:///scala/Option/getOrElse(scala.Function0)|",
    "v1Body": [
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$4/RestSubmissionClient$$anonfun$4()|",
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$5/RestSubmissionClient$$anonfun$5()|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/toMap(scala.Predef$$less$colon$less)|",
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient/RestSubmissionClient(java.lang.String)|",
      "|java+method:///scala/collection/immutable/Map/filter(scala.Function1)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient/constructSubmitRequest(java.lang.String,java.lang.String,java.lang.String%5B%5D,scala.collection.immutable.Map,scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient/createSubmission(org.apache.spark.deploy.rest.CreateSubmissionRequest)|",
      "|java+method:///org/apache/spark/SparkConf/getAll()|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient$$anonfun$5/RestSubmissionClient$$anonfun$5()|",
      "|java+method:///org/apache/spark/SparkConf/getOption(java.lang.String)|",
      "|java+method:///scala/collection/mutable/ArrayOps/toMap(scala.Predef$$less$colon$less)|",
      "|java+constructor:///org/apache/spark/deploy/rest/RestSubmissionClient/RestSubmissionClient(java.lang.String)|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient/constructSubmitRequest(java.lang.String,java.lang.String,java.lang.String%5B%5D,scala.collection.immutable.Map,scala.collection.immutable.Map)|",
      "|java+method:///org/apache/spark/deploy/rest/RestSubmissionClient/createSubmission(org.apache.spark.deploy.rest.CreateSubmissionRequest)|",
      "|java+method:///org/apache/spark/SparkConf/getAll()|",
      "|java+method:///scala/Predef$/conforms()|",
      "|java+method:///scala/Predef$/refArrayOps(java.lang.Object%5B%5D)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$1/apply()|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$1/apply()|",
      "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$1/apply$mcJ$sp()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$unrollMemoryThreshold()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/storage/MemoryStore$$anonfun$1/apply()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/MemoryStore/org$apache$spark$storage$MemoryStore$$maxMemory()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/util/Utils$/bytesToString(long)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkContext$$anonfun$assertNoOtherContextIsRunning$1/apply(org.apache.spark.SparkContext)|",
    "called": "|java+method:///scala/Option/map(scala.Function1)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/CallSite/longForm()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$apply$16/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$apply$16(org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$41/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$41(org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$42/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$42(org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///java/util/concurrent/atomic/AtomicReference/get()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$creationSite()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$apply$15/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$apply$15(org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1,java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext$/org$apache$spark$SparkContext$$activeContext()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext$/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext$/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "v2Body": [
      "|java+constructor:///org/apache/spark/SparkException/SparkException(java.lang.String)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$apply$17/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$apply$17(org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$42/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$42(org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1)|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$43/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$43(org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///java/util/concurrent/atomic/AtomicReference/get()|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext/org$apache$spark$SparkContext$$creationSite()|",
      "|java+method:///org/apache/spark/util/CallSite/longForm()|",
      "|java+constructor:///org/apache/spark/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$apply$16/SparkContext$$anonfun$assertNoOtherContextIsRunning$1$$anonfun$apply$16(org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$1,java.lang.String)|",
      "|java+method:///scala/Option/map(scala.Function1)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkContext$/org$apache$spark$SparkContext$$activeContext()|",
      "|java+method:///scala/Option/getOrElse(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext$/logWarning(scala.Function0)|",
      "|java+method:///org/apache/spark/SparkContext$/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/rpc/RpcEnvConfig/productElement(int)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
    "v1Body": [
      "|java+method:///java/lang/Object/toString()|",
      "|java+constructor:///java/lang/IndexOutOfBoundsException/IndexOutOfBoundsException(java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/port()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/conf()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/host()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/securityManager()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/name()|"
    ],
    "v2Body": [
      "|java+method:///java/lang/Object/toString()|",
      "|java+constructor:///java/lang/IndexOutOfBoundsException/IndexOutOfBoundsException(java.lang.String)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/port()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToBoolean(boolean)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/conf()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/host()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/securityManager()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/name()|",
      "|java+method:///org/apache/spark/rpc/RpcEnvConfig/clientMode()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/util/JsonProtocol$$anonfun$64/apply(org.json4s.JsonAST$JValue)|",
    "called": "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)|",
      "|java+method:///scala/reflect/ManifestFactory$/Long()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "v2Body": [
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class)|",
      "|java+method:///scala/collection/immutable/List$/canBuildFrom()|",
      "|java+method:///org/json4s/ExtractableJsonAstNode/extract(org.json4s.Formats,scala.reflect.Manifest)|",
      "|java+method:///org/apache/spark/util/JsonProtocol$/org$apache$spark$util$JsonProtocol$$format()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/reflect/ManifestFactory$/classType(java.lang.Class,scala.reflect.Manifest,scala.collection.Seq)|",
      "|java+method:///scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///org/apache/spark/util/JsonProtocol$$anonfun$64$$anonfun$apply$15/JsonProtocol$$anonfun$64$$anonfun$apply$15(org.apache.spark.util.JsonProtocol$$anonfun$64)|",
      "|java+method:///org/json4s/package$/jvalue2extractable(org.json4s.JsonAST$JValue)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/deploy/master/Master$$anonfun$receive$1/applyOrElse(java.lang.Object,scala.Function1)|",
    "called": "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///java/util/concurrent/ScheduledExecutorService/schedule(java.lang.Runnable,long,java.util.concurrent.TimeUnit)|",
      "|java+method:///org/apache/spark/deploy/master/WorkerInfo/lastHeartbeat_$eq(long)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$createApplication(org.apache.spark.deploy.ApplicationDescription,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/driver()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$21/Master$$anonfun$receive$1$$anonfun$applyOrElse$21(org.apache.spark.deploy.master.Master$$anonfun$receive$1)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$state_$eq(scala.Enumeration$Value)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anon$2/Master$$anonfun$receive$1$$anon$2(org.apache.spark.deploy.master.Master$$anonfun$receive$1)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/master/Master/rpcEnv()|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$20/Master$$anonfun$receive$1$$anonfun$applyOrElse$20(org.apache.spark.deploy.master.Master$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$WorkerSchedulerStateResponse/driverIds()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$masterUrl()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$9/Master$$anonfun$receive$1$$anonfun$applyOrElse$9(org.apache.spark.deploy.master.Master$$anonfun$receive$1,org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/logWarning(scala.Function0)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$17/Master$$anonfun$receive$1$$anonfun$applyOrElse$17(org.apache.spark.deploy.master.Master$$anonfun$receive$1,org.apache.spark.deploy.master.WorkerInfo)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$11/Master$$anonfun$receive$1$$anonfun$applyOrElse$11(org.apache.spark.deploy.master.Master$$anonfun$receive$1)|",
      "|java+method:///scala/collection/Seq/isEmpty()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/state()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/removeExecutor(org.apache.spark.deploy.master.ExecutorDesc)|",
      "|java+method:///org/apache/spark/deploy/master/WorkerInfo/endpoint()|",
      "|java+method:///org/apache/spark/deploy/master/Master/self()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$18/Master$$anonfun$receive$1$$anonfun$applyOrElse$18(org.apache.spark.deploy.master.Master$$anonfun$receive$1,org.apache.spark.deploy.master.WorkerInfo)|",
      "|java+constructor:///org/apache/spark/deploy/master/WorkerInfo/WorkerInfo(java.lang.String,java.lang.String,int,int,int,org.apache.spark.rpc.RpcEndpointRef,int,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$state()|",
      "|java+method:///org/apache/spark/deploy/master/Master/idToApp()|",
      "|java+method:///scala/collection/immutable/List/filter(scala.Function1)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$3/Master$$anonfun$receive$1$$anonfun$applyOrElse$3(org.apache.spark.deploy.master.Master$$anonfun$receive$1,java.lang.String,int,int,int)|",
      "|java+method:///org/apache/spark/deploy/master/PersistenceEngine/readPersistedData(org.apache.spark.rpc.RpcEnv)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterWorker/webUiPort()|",
      "|java+method:///org/apache/spark/deploy/master/Master/logInfo(scala.Function0)|",
      "|java+method:///scala/Option/flatMap(scala.Function1)|",
      "|java+method:///scala/Tuple3/_1()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$masterWebUiUrl()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterWorker/publicAddress()|",
      "|java+method:///scala/collection/Iterable/exists(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterApplication/appDescription()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$5/Master$$anonfun$receive$1$$anonfun$applyOrElse$5(org.apache.spark.deploy.master.Master$$anonfun$receive$1,org.apache.spark.deploy.ApplicationDescription)|",
      "|java+method:///scala/Tuple3/_2()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$UnregisterApplication/appId()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/immutable/List/foreach(scala.Function1)|",
      "|java+method:///scala/Tuple3/_3()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$forwardMessageThread()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$6/Master$$anonfun$receive$1$$anonfun$applyOrElse$6(org.apache.spark.deploy.master.Master$$anonfun$receive$1,org.apache.spark.deploy.ApplicationDescription,org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/address()|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$ReconnectWorker/DeployMessages$ReconnectWorker(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$registerWorker(org.apache.spark.deploy.master.WorkerInfo)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$RegisterWorkerFailed/DeployMessages$RegisterWorkerFailed(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$7/Master$$anonfun$receive$1$$anonfun$applyOrElse$7(org.apache.spark.deploy.master.Master$$anonfun$receive$1,scala.Enumeration$Value,org.apache.spark.deploy.master.ExecutorDesc)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$recoveryCompletionTask_$eq(java.util.concurrent.ScheduledFuture)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$RegisteredWorker/DeployMessages$RegisteredWorker(org.apache.spark.rpc.RpcEndpointRef,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$completeRecovery()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/execId()|",
      "|java+method:///org/apache/spark/deploy/master/ExecutorDesc/application()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationState$/WAITING()|",
      "|java+method:///org/apache/spark/deploy/master/RecoveryState$/ALIVE()|",
      "|java+method:///org/apache/spark/deploy/master/RecoveryState$/STANDBY()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$19/Master$$anonfun$receive$1$$anonfun$applyOrElse$19(org.apache.spark.deploy.master.Master$$anonfun$receive$1,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$8/Master$$anonfun$receive$1$$anonfun$8(org.apache.spark.deploy.master.Master$$anonfun$receive$1,int)|",
      "|java+method:///org/apache/spark/deploy/master/WorkerInfo/removeExecutor(org.apache.spark.deploy.master.ExecutorDesc)|",
      "|java+method:///scala/collection/mutable/HashSet/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/appId()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$DriverStateChanged/driverId()|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/RUNNING()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$timeOutDeadWorkers()|",
      "|java+method:///scala/collection/mutable/HashSet$/canBuildFrom()|",
      "|java+method:///org/apache/spark/deploy/master/PersistenceEngine/addWorker(org.apache.spark.deploy.master.WorkerInfo)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$RegisteredApplication/DeployMessages$RegisteredApplication(java.lang.String,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///java/lang/System/exit(int)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$registerApplication(org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+method:///org/apache/spark/deploy/master/Master/workers()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationState$/FAILED()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$2/Master$$anonfun$receive$1$$anonfun$applyOrElse$2(org.apache.spark.deploy.master.Master$$anonfun$receive$1)|",
      "|java+method:///org/apache/spark/deploy/master/WorkerInfo/state_$eq(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/resetRetryCount()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$8/Master$$anonfun$receive$1$$anonfun$applyOrElse$8(org.apache.spark.deploy.master.Master$$anonfun$receive$1)|",
      "|java+method:///org/apache/spark/deploy/master/RecoveryState$/RECOVERING()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/exitStatus()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$1/Master$$anonfun$receive$1$$anonfun$applyOrElse$1(org.apache.spark.deploy.master.Master$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$9/Master$$anonfun$receive$1$$anonfun$9(org.apache.spark.deploy.master.Master$$anonfun$receive$1)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/incrementRetryCount()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterWorker/id()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterWorker/cores()|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$ExecutorUpdated/DeployMessages$ExecutorUpdated(int,scala.Enumeration$Value,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/master/DriverState$/FINISHED()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$WorkerSchedulerStateResponse/executors()|",
      "|java+method:///org/apache/spark/deploy/master/PersistenceEngine/addApplication(org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterChangeAcknowledged/appId()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$canCompleteRecovery()|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$idToWorker()|",
      "|java+constructor:///scala/collection/mutable/StringBuilder/StringBuilder()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$16/Master$$anonfun$receive$1$$anonfun$applyOrElse$16(org.apache.spark.deploy.master.Master$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationState$/MAX_NUM_RETRY()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterWorker/worker()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/id()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/removeApplication(org.apache.spark.deploy.master.ApplicationInfo,scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/master/ExecutorDesc/worker()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/isFinished()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$removeDriver(java.lang.String,scala.Enumeration$Value,scala.Option)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$15/Master$$anonfun$receive$1$$anonfun$applyOrElse$15(org.apache.spark.deploy.master.Master$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$persistenceEngine()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$schedule()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterApplication/driver()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterWorker/memory()|",
      "|java+method:///org/apache/spark/deploy/master/WorkerState$/ALIVE()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$DriverStateChanged/exception()|",
      "|java+method:///org/apache/spark/deploy/master/Master/logError(scala.Function0)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterWorker/port()|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///scala/collection/mutable/StringBuilder/append(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$Heartbeat/workerId()|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/isFinished(scala.Enumeration$Value)|",
      "|java+constructor:///java/lang/Exception/Exception(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$14/Master$$anonfun$receive$1$$anonfun$applyOrElse$14(org.apache.spark.deploy.master.Master$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$DriverStateChanged/state()|",
      "|java+method:///org/apache/spark/deploy/master/ExecutorDesc/state_$eq(scala.Enumeration$Value)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///scala/collection/mutable/HashMap/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/message()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$13/Master$$anonfun$receive$1$$anonfun$applyOrElse$13(org.apache.spark.deploy.master.Master$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$WORKER_TIMEOUT_MS()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/executors()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/state_$eq(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/master/DriverState$/ERROR()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$12/Master$$anonfun$receive$1$$anonfun$applyOrElse$12(org.apache.spark.deploy.master.Master$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$beginRecovery(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$10/Master$$anonfun$receive$1$$anonfun$applyOrElse$10(org.apache.spark.deploy.master.Master$$anonfun$receive$1,java.lang.String,int)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///scala/collection/mutable/StringBuilder/toString()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$Heartbeat/worker()|",
      "|java+method:///org/apache/spark/deploy/master/DriverState$/FAILED()|",
      "|java+method:///org/apache/spark/deploy/master/DriverState$/KILLED()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$WorkerSchedulerStateResponse/id()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterWorker/host()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$4/Master$$anonfun$receive$1$$anonfun$applyOrElse$4(org.apache.spark.deploy.master.Master$$anonfun$receive$1,org.apache.spark.rpc.RpcAddress)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/master/Master/self()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$11/Master$$anonfun$receive$1$$anonfun$applyOrElse$11(org.apache.spark.deploy.master.Master$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///scala/Some/x()|",
      "|java+method:///java/util/concurrent/ScheduledExecutorService/schedule(java.lang.Runnable,long,java.util.concurrent.TimeUnit)|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/LAUNCHING()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$20/Master$$anonfun$receive$1$$anonfun$applyOrElse$20(org.apache.spark.deploy.master.Master$$anonfun$receive$1)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$appIdToUI()|",
      "|java+method:///org/apache/spark/deploy/master/WorkerInfo/lastHeartbeat_$eq(long)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$createApplication(org.apache.spark.deploy.ApplicationDescription,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/driver()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$10/Master$$anonfun$receive$1$$anonfun$applyOrElse$10(org.apache.spark.deploy.master.Master$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$21/Master$$anonfun$receive$1$$anonfun$applyOrElse$21(org.apache.spark.deploy.master.Master$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$16/Master$$anonfun$receive$1$$anonfun$applyOrElse$16(org.apache.spark.deploy.master.Master$$anonfun$receive$1,org.apache.spark.deploy.master.WorkerInfo)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$state_$eq(scala.Enumeration$Value)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anon$2/Master$$anonfun$receive$1$$anon$2(org.apache.spark.deploy.master.Master$$anonfun$receive$1)|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/master/Master/rpcEnv()|",
      "|java+method:///scala/collection/mutable/HashMap/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$WorkerSchedulerStateResponse/driverIds()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$masterUrl()|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/logWarning(scala.Function0)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$17/Master$$anonfun$receive$1$$anonfun$applyOrElse$17(org.apache.spark.deploy.master.Master$$anonfun$receive$1,org.apache.spark.deploy.master.WorkerInfo)|",
      "|java+method:///scala/Predef$/assert(boolean,scala.Function0)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$8/Master$$anonfun$receive$1$$anonfun$applyOrElse$8(org.apache.spark.deploy.master.Master$$anonfun$receive$1,org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+method:///scala/collection/Seq/isEmpty()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/state()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/removeExecutor(org.apache.spark.deploy.master.ExecutorDesc)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$state()|",
      "|java+method:///org/apache/spark/deploy/master/Master/idToApp()|",
      "|java+method:///scala/collection/immutable/List/filter(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/master/PersistenceEngine/readPersistedData(org.apache.spark.rpc.RpcEnv)|",
      "|java+method:///org/apache/spark/deploy/master/Master/logInfo(scala.Function0)|",
      "|java+method:///scala/Option/flatMap(scala.Function1)|",
      "|java+method:///scala/Tuple3/_1()|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/collection/Iterable/exists(scala.Function1)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterApplication/appDescription()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$UnregisterApplication/appId()|",
      "|java+method:///scala/Tuple3/_2()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$3/Master$$anonfun$receive$1$$anonfun$applyOrElse$3(org.apache.spark.deploy.master.Master$$anonfun$receive$1,org.apache.spark.deploy.ApplicationDescription)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/collection/immutable/List/foreach(scala.Function1)|",
      "|java+method:///scala/Tuple3/_3()|",
      "|java+method:///scala/collection/mutable/HashMap/get(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$4/Master$$anonfun$receive$1$$anonfun$applyOrElse$4(org.apache.spark.deploy.master.Master$$anonfun$receive$1,org.apache.spark.deploy.ApplicationDescription,org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$forwardMessageThread()|",
      "|java+method:///java/util/concurrent/ConcurrentHashMap/get(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$ReconnectWorker/DeployMessages$ReconnectWorker(java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$recoveryCompletionTask_$eq(java.util.concurrent.ScheduledFuture)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$completeRecovery()|",
      "|java+method:///org/apache/spark/deploy/master/ExecutorDesc/application()|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$9/Master$$anonfun$receive$1$$anonfun$applyOrElse$9(org.apache.spark.deploy.master.Master$$anonfun$receive$1,java.lang.String,int)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationState$/WAITING()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$6/Master$$anonfun$receive$1$$anonfun$applyOrElse$6(org.apache.spark.deploy.master.Master$$anonfun$receive$1,scala.Enumeration$Value,org.apache.spark.deploy.master.ExecutorDesc)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/execId()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$5/Master$$anonfun$receive$1$$anonfun$applyOrElse$5(org.apache.spark.deploy.master.Master$$anonfun$receive$1,int,scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/master/RecoveryState$/ALIVE()|",
      "|java+method:///org/apache/spark/deploy/master/RecoveryState$/STANDBY()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$19/Master$$anonfun$receive$1$$anonfun$applyOrElse$19(org.apache.spark.deploy.master.Master$$anonfun$receive$1,java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$8/Master$$anonfun$receive$1$$anonfun$8(org.apache.spark.deploy.master.Master$$anonfun$receive$1,int)|",
      "|java+method:///org/apache/spark/deploy/master/ExecutorDesc/state()|",
      "|java+method:///org/apache/spark/deploy/master/WorkerInfo/removeExecutor(org.apache.spark.deploy.master.ExecutorDesc)|",
      "|java+method:///scala/collection/mutable/HashSet/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/appId()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$DriverStateChanged/driverId()|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/RUNNING()|",
      "|java+method:///scala/collection/mutable/HashSet$/canBuildFrom()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$timeOutDeadWorkers()|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$RegisteredApplication/DeployMessages$RegisteredApplication(java.lang.String,org.apache.spark.rpc.RpcEndpointRef)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$18/Master$$anonfun$receive$1$$anonfun$applyOrElse$18(org.apache.spark.deploy.master.Master$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///java/lang/System/exit(int)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$registerApplication(org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+method:///org/apache/spark/deploy/master/Master/workers()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$7/Master$$anonfun$receive$1$$anonfun$applyOrElse$7(org.apache.spark.deploy.master.Master$$anonfun$receive$1)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationState$/FAILED()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$2/Master$$anonfun$receive$1$$anonfun$applyOrElse$2(org.apache.spark.deploy.master.Master$$anonfun$receive$1)|",
      "|java+method:///org/apache/spark/deploy/master/WorkerInfo/state_$eq(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/resetRetryCount()|",
      "|java+method:///org/apache/spark/deploy/master/RecoveryState$/RECOVERING()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/exitStatus()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$1/Master$$anonfun$receive$1$$anonfun$applyOrElse$1(org.apache.spark.deploy.master.Master$$anonfun$receive$1)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$9/Master$$anonfun$receive$1$$anonfun$9(org.apache.spark.deploy.master.Master$$anonfun$receive$1)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/incrementRetryCount()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///org/apache/spark/rpc/RpcEndpointRef/send(java.lang.Object)|",
      "|java+constructor:///org/apache/spark/deploy/DeployMessages$ExecutorUpdated/DeployMessages$ExecutorUpdated(int,scala.Enumeration$Value,scala.Option,scala.Option)|",
      "|java+method:///org/apache/spark/deploy/master/DriverState$/FINISHED()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$WorkerSchedulerStateResponse/executors()|",
      "|java+method:///org/apache/spark/deploy/master/PersistenceEngine/addApplication(org.apache.spark.deploy.master.ApplicationInfo)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$MasterChangeAcknowledged/appId()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$canCompleteRecovery()|",
      "|java+method:///scala/collection/mutable/HashSet/contains(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$idToWorker()|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationState$/MAX_NUM_RETRY()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/id()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+constructor:///scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/removeApplication(org.apache.spark.deploy.master.ApplicationInfo,scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/master/ExecutorDesc/worker()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/isFinished()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$removeDriver(java.lang.String,scala.Enumeration$Value,scala.Option)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$15/Master$$anonfun$receive$1$$anonfun$applyOrElse$15(org.apache.spark.deploy.master.Master$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$persistenceEngine()|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$schedule()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$RegisterApplication/driver()|",
      "|java+method:///org/apache/spark/deploy/master/WorkerState$/ALIVE()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$DriverStateChanged/exception()|",
      "|java+method:///org/apache/spark/deploy/master/Master/logError(scala.Function0)|",
      "|java+method:///scala/Function1/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$Heartbeat/workerId()|",
      "|java+method:///org/apache/spark/deploy/master/MasterMessages$AttachCompletedRebuildUI/appId()|",
      "|java+method:///org/apache/spark/deploy/ExecutorState$/isFinished(scala.Enumeration$Value)|",
      "|java+constructor:///java/lang/Exception/Exception(java.lang.String)|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$14/Master$$anonfun$receive$1$$anonfun$applyOrElse$14(org.apache.spark.deploy.master.Master$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$DriverStateChanged/state()|",
      "|java+method:///org/apache/spark/deploy/master/ExecutorDesc/state_$eq(scala.Enumeration$Value)|",
      "|java+method:///java/lang/System/currentTimeMillis()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$ExecutorStateChanged/message()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$13/Master$$anonfun$receive$1$$anonfun$applyOrElse$13(org.apache.spark.deploy.master.Master$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$WORKER_TIMEOUT_MS()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/executors()|",
      "|java+method:///org/apache/spark/deploy/master/ApplicationInfo/state_$eq(scala.Enumeration$Value)|",
      "|java+method:///org/apache/spark/deploy/master/DriverState$/ERROR()|",
      "|java+constructor:///org/apache/spark/deploy/master/Master$$anonfun$receive$1$$anonfun$applyOrElse$12/Master$$anonfun$receive$1$$anonfun$applyOrElse$12(org.apache.spark.deploy.master.Master$$anonfun$receive$1,java.lang.String)|",
      "|java+method:///org/apache/spark/deploy/master/Master/org$apache$spark$deploy$master$Master$$beginRecovery(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$Heartbeat/worker()|",
      "|java+method:///org/apache/spark/deploy/master/DriverState$/FAILED()|",
      "|java+method:///org/apache/spark/deploy/master/DriverState$/KILLED()|",
      "|java+method:///org/apache/spark/deploy/DeployMessages$WorkerSchedulerStateResponse/id()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/partial/GroupedSumEvaluator/currentResult()|",
    "called": "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/StatCounter/sampleVariance()|",
      "|java+method:///org/apache/spark/partial/GroupedSumEvaluator/sums()|",
      "|java+constructor:///org/apache/spark/partial/StudentTCacher/StudentTCacher(double)|",
      "|java+method:///org/apache/spark/partial/GroupedSumEvaluator/currentResult()|",
      "|java+method:///scala/collection/mutable/Map/update(java.lang.Object,java.lang.Object)|",
      "|java+method:///java/util/Map$Entry/getValue()|",
      "|java+method:///org/apache/spark/util/StatCounter/sum()|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///org/apache/spark/util/StatCounter/count()|",
      "|java+method:///org/apache/spark/partial/StudentTCacher/get(long)|",
      "|java+method:///java/util/Iterator/hasNext()|",
      "|java+method:///scala/collection/JavaConversions$/mapAsScalaMap(java.util.Map)|",
      "|java+method:///java/util/Map$Entry/getKey()|",
      "|java+method:///java/util/HashMap/entrySet()|",
      "|java+method:///org/apache/spark/util/StatCounter/mean()|",
      "|java+method:///java/util/Iterator/next()|",
      "|java+constructor:///org/apache/spark/partial/BoundedDouble/BoundedDouble(double,double,double,double)|",
      "|java+constructor:///java/util/HashMap/HashMap(int)|",
      "|java+method:///java/util/Set/iterator()|",
      "|java+method:///org/apache/spark/partial/GroupedSumEvaluator/outputsMerged()|",
      "|java+method:///java/util/HashMap/size()|",
      "|java+method:///scala/math/package$/sqrt(double)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/StatCounter/sampleVariance()|",
      "|java+method:///org/apache/spark/partial/GroupedSumEvaluator/sums()|",
      "|java+method:///java/util/Map$Entry/getKey()|",
      "|java+constructor:///org/apache/spark/partial/StudentTCacher/StudentTCacher(double)|",
      "|java+method:///scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)|",
      "|java+method:///java/util/Map$Entry/getValue()|",
      "|java+method:///org/apache/spark/util/StatCounter/sum()|",
      "|java+constructor:///scala/collection/mutable/HashMap/HashMap()|",
      "|java+method:///org/apache/spark/util/StatCounter/count()|",
      "|java+method:///org/apache/spark/partial/StudentTCacher/get(long)|",
      "|java+method:///java/util/Iterator/hasNext()|",
      "|java+method:///java/util/HashMap/entrySet()|",
      "|java+method:///org/apache/spark/util/StatCounter/mean()|",
      "|java+method:///scala/collection/convert/Decorators$AsScala/asScala()|",
      "|java+method:///org/apache/spark/partial/GroupedSumEvaluator/currentResult()|",
      "|java+method:///java/util/Iterator/next()|",
      "|java+constructor:///org/apache/spark/partial/BoundedDouble/BoundedDouble(double,double,double,double)|",
      "|java+method:///java/util/HashMap/put(java.lang.Object,java.lang.Object)|",
      "|java+constructor:///java/util/HashMap/HashMap(int)|",
      "|java+method:///java/util/Set/iterator()|",
      "|java+method:///org/apache/spark/partial/GroupedSumEvaluator/outputsMerged()|",
      "|java+method:///java/util/HashMap/size()|",
      "|java+method:///scala/math/package$/sqrt(double)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/metrics/MetricsSystem$$anonfun$getServletHandlers$2/MetricsSystem$$anonfun$getServletHandlers$2(org.apache.spark.metrics.MetricsSystem)|",
    "called": "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
    "v1Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|"
    ],
    "v2Body": [
      "|java+constructor:///scala/runtime/AbstractFunction1/AbstractFunction1()|",
      "|java+constructor:///java/lang/NullPointerException/NullPointerException()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/buildCommand(java.util.Map)|",
    "called": "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/Command/javaOpts()|",
      "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/getenv(java.lang.String)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/Command/classPathEntries()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/launcher/WorkerCommandBuilder$$anonfun$buildCommand$1/WorkerCommandBuilder$$anonfun$buildCommand$1(org.apache.spark.launcher.WorkerCommandBuilder,java.util.List)|",
      "|java+method:///java/util/List/add(java.lang.Object)|",
      "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/addOptionString(java.util.List,java.lang.String)|",
      "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/buildJavaCommand(java.lang.String)|",
      "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/addPermGenSizeOpt(java.util.List)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/Command/javaOpts()|",
      "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/getenv(java.lang.String)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/Command/classPathEntries()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/launcher/WorkerCommandBuilder$$anonfun$buildCommand$1/WorkerCommandBuilder$$anonfun$buildCommand$1(org.apache.spark.launcher.WorkerCommandBuilder,java.util.List)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///java/util/List/add(java.lang.Object)|",
      "|java+method:///org/apache/spark/launcher/CommandBuilderUtils/addPermGenSizeOpt(java.util.List)|",
      "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/addOptionString(java.util.List,java.lang.String)|",
      "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/buildJavaCommand(java.lang.String)|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/SparkEnv/stop()|",
    "called": "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
    "v1Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///scala/Option/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/broadcast/BroadcastManager/stop()|",
      "|java+method:///org/apache/spark/SparkEnv/mapOutputTracker()|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+method:///org/apache/spark/SparkEnv/metricsSystem()|",
      "|java+method:///org/apache/spark/SparkEnv/httpFileServer()|",
      "|java+method:///org/apache/spark/SparkEnv/pythonWorkers()|",
      "|java+method:///org/apache/spark/util/Utils$/deleteRecursively(java.io.File)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/shutdown()|",
      "|java+method:///org/apache/spark/SparkEnv/isStopped_$eq(boolean)|",
      "|java+method:///org/apache/spark/SparkEnv/shuffleManager()|",
      "|java+method:///org/apache/spark/SparkEnv/isStopped()|",
      "|java+method:///org/apache/spark/MapOutputTracker/stop()|",
      "|java+method:///org/apache/spark/SparkEnv/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMaster/stop()|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$stop$3/SparkEnv$$anonfun$stop$3(org.apache.spark.SparkEnv,java.lang.String)|",
      "|java+method:///scala/Option$/apply(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/BlockManager/stop()|",
      "|java+method:///org/apache/spark/storage/BlockManager/master()|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/stop()|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/stop()|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$stop$1/SparkEnv$$anonfun$stop$1(org.apache.spark.SparkEnv)|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$stop$2/SparkEnv$$anonfun$stop$2(org.apache.spark.SparkEnv)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkEnv/org$apache$spark$SparkEnv$$driverTmpDirToDelete()|",
      "|java+method:///org/apache/spark/shuffle/ShuffleManager/stop()|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///org/apache/spark/SparkEnv/broadcastManager()|",
      "|java+method:///org/apache/spark/SparkEnv/outputCommitCoordinator()|"
    ],
    "v2Body": [
      "|java+method:///scala/Some/x()|",
      "|java+method:///akka/actor/ActorSystem/shutdown()|",
      "|java+method:///org/apache/spark/SparkEnv/mapOutputTracker()|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$stop$2/SparkEnv$$anonfun$stop$2(org.apache.spark.SparkEnv,java.lang.String)|",
      "|java+method:///scala/collection/mutable/HashMap/values()|",
      "|java+constructor:///org/apache/spark/SparkEnv$$anonfun$stop$1/SparkEnv$$anonfun$stop$1(org.apache.spark.SparkEnv)|",
      "|java+method:///org/apache/spark/SparkEnv/metricsSystem()|",
      "|java+method:///org/apache/spark/SparkEnv/pythonWorkers()|",
      "|java+method:///org/apache/spark/util/Utils$/deleteRecursively(java.io.File)|",
      "|java+method:///java/lang/Object/equals(java.lang.Object)|",
      "|java+method:///scala/collection/Iterable/foreach(scala.Function1)|",
      "|java+method:///org/apache/spark/rpc/RpcEnv/shutdown()|",
      "|java+method:///org/apache/spark/SparkEnv/isStopped_$eq(boolean)|",
      "|java+method:///org/apache/spark/SparkEnv/shuffleManager()|",
      "|java+method:///org/apache/spark/SparkEnv/isStopped()|",
      "|java+method:///org/apache/spark/MapOutputTracker/stop()|",
      "|java+method:///org/apache/spark/SparkEnv/actorSystem()|",
      "|java+method:///org/apache/spark/broadcast/BroadcastManager/stop()|",
      "|java+method:///org/apache/spark/SparkEnv/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///org/apache/spark/storage/BlockManagerMaster/stop()|",
      "|java+method:///org/apache/spark/storage/BlockManager/stop()|",
      "|java+method:///org/apache/spark/storage/BlockManager/master()|",
      "|java+method:///org/apache/spark/metrics/MetricsSystem/stop()|",
      "|java+method:///org/apache/spark/SparkEnv/blockManager()|",
      "|java+constructor:///java/io/File/File(java.lang.String)|",
      "|java+method:///org/apache/spark/scheduler/OutputCommitCoordinator/stop()|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///org/apache/spark/SparkEnv/org$apache$spark$SparkEnv$$driverTmpDirToDelete()|",
      "|java+method:///org/apache/spark/shuffle/ShuffleManager/stop()|",
      "|java+method:///org/apache/spark/SparkEnv/rpcEnv()|",
      "|java+method:///org/apache/spark/SparkEnv/broadcastManager()|",
      "|java+method:///org/apache/spark/SparkEnv/outputCommitCoordinator()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/JettyUtils$/startJettyServer(java.lang.String,int,scala.collection.Seq,org.apache.spark.SparkConf,java.lang.String)|",
    "called": "|java+method:///scala/collection/Seq$/canBuildFrom()|",
    "v1Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/Seq/toArray(scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/ui/ServerInfo/ServerInfo(org.spark-project.jetty.server.Server,int,org.spark-project.jetty.server.handler.ContextHandlerCollection)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/ui/JettyUtils$/addFilters(scala.collection.Seq,org.apache.spark.SparkConf)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/spark-project/jetty/server/handler/ContextHandlerCollection/setHandlers(org.spark-project.jetty.server.Handler%5B%5D)|",
      "|java+constructor:///org/spark-project/jetty/server/handler/ContextHandlerCollection/ContextHandlerCollection()|",
      "|java+method:///org/apache/spark/util/Utils$/startServiceOnPort(int,scala.Function1,org.apache.spark.SparkConf,java.lang.String)|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|",
      "|java+constructor:///org/apache/spark/ui/JettyUtils$$anonfun$3/JettyUtils$$anonfun$3(java.lang.String,org.spark-project.jetty.server.handler.ContextHandlerCollection)|",
      "|java+constructor:///org/apache/spark/ui/JettyUtils$$anonfun$2/JettyUtils$$anonfun$2()|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/Seq$/canBuildFrom()|",
      "|java+method:///scala/collection/Seq/toArray(scala.reflect.ClassTag)|",
      "|java+constructor:///org/apache/spark/ui/ServerInfo/ServerInfo(org.spark-project.jetty.server.Server,int,org.spark-project.jetty.server.handler.ContextHandlerCollection)|",
      "|java+method:///scala/Tuple2/_1()|",
      "|java+method:///org/apache/spark/ui/JettyUtils$/addFilters(scala.collection.Seq,org.apache.spark.SparkConf)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///scala/MatchError/MatchError(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)|",
      "|java+method:///scala/Tuple2/_2$mcI$sp()|",
      "|java+constructor:///scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/reflect/ClassTag$/apply(java.lang.Class)|",
      "|java+method:///org/spark-project/jetty/server/handler/ContextHandlerCollection/setHandlers(org.spark-project.jetty.server.Handler%5B%5D)|",
      "|java+method:///org/apache/spark/util/Utils$/startServiceOnPort(int,scala.Function1,org.apache.spark.SparkConf,java.lang.String)|",
      "|java+constructor:///org/apache/spark/ui/JettyUtils$$anonfun$5/JettyUtils$$anonfun$5(java.lang.String,org.spark-project.jetty.server.handler.ContextHandlerCollection)|",
      "|java+constructor:///org/apache/spark/ui/JettyUtils$$anonfun$4/JettyUtils$$anonfun$4()|",
      "|java+constructor:///org/spark-project/jetty/server/handler/ContextHandlerCollection/ContextHandlerCollection()|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/r/SerDe$$anonfun$readList$1/apply(java.lang.Object)|",
    "called": "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|",
    "v1Body": [
      "|java+method:///org/apache/spark/api/r/SerDe$$anonfun$readList$1/apply(java.lang.String)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/api/r/SerDe$$anonfun$readList$1/apply(int)|",
      "|java+method:///scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/ui/SparkUI/getApplicationInfoList()|",
    "called": "|java+method:///scala/collection/Iterator$/apply(scala.collection.Seq)|",
    "v1Body": [
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/status/api/v1/ApplicationInfo/ApplicationInfo(java.lang.String,java.lang.String,scala.collection.Seq)|",
      "|java+method:///scala/collection/Iterator$/apply(scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/package$/Iterator()|",
      "|java+constructor:///java/util/Date/Date(long)|",
      "|java+method:///org/apache/spark/ui/SparkUI/appName()|",
      "|java+method:///org/apache/spark/ui/SparkUI/startTime()|",
      "|java+constructor:///org/apache/spark/status/api/v1/ApplicationAttemptInfo/ApplicationAttemptInfo(scala.Option,java.util.Date,java.util.Date,java.lang.String,boolean)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/ui/SparkUI/appId()|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///org/apache/spark/status/api/v1/ApplicationInfo/ApplicationInfo(java.lang.String,java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option,scala.collection.Seq)|",
      "|java+method:///scala/collection/Iterator$/apply(scala.collection.Seq)|",
      "|java+method:///scala/collection/Seq$/apply(scala.collection.Seq)|",
      "|java+method:///scala/package$/Iterator()|",
      "|java+constructor:///java/util/Date/Date(long)|",
      "|java+method:///org/apache/spark/ui/SparkUI/appName()|",
      "|java+method:///org/apache/spark/ui/SparkUI/startTime()|",
      "|java+constructor:///org/apache/spark/status/api/v1/ApplicationAttemptInfo/ApplicationAttemptInfo(scala.Option,java.util.Date,java.util.Date,java.lang.String,boolean)|"
    ],
    "affectedLib": "org.scala-lang:scala-library:2.10.5",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/api/python/SerDeUtil$$anonfun$pairRDDToPython$1$$anonfun$apply$2/apply(scala.collection.Seq)|",
    "called": "|java+method:///net/razorvine/pickle/Pickler/dumps(java.lang.Object)|",
    "v1Body": [
      "|java+method:///net/razorvine/pickle/Pickler/dumps(java.lang.Object)|",
      "|java+method:///scala/collection/JavaConversions$/seqAsJavaList(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///net/razorvine/pickle/Pickler/dumps(java.lang.Object)|"
    ],
    "affectedLib": "net.razorvine:pyrolite:4.9",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/TachyonBlockManager/putBytes(org.apache.spark.storage.BlockId,java.nio.ByteBuffer)|",
    "called": "|java+method:///tachyon/client/TachyonFile/getOutStream(tachyon.client.WriteType)|",
    "v1Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///org/apache/spark/storage/TachyonBlockManager$$anonfun$putBytes$1/TachyonBlockManager$$anonfun$putBytes$1(org.apache.spark.storage.TachyonBlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///tachyon/client/OutStream/cancel()|",
      "|java+method:///tachyon/client/OutStream/write(byte%5B%5D)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/getFile(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///tachyon/client/TachyonFile/getOutStream(tachyon.client.WriteType)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///tachyon/client/OutStream/close()|",
      "|java+method:///java/nio/ByteBuffer/array()|"
    ],
    "v2Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///tachyon/client/file/FileOutStream/cancel()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///org/apache/spark/storage/TachyonBlockManager$$anonfun$putBytes$1/TachyonBlockManager$$anonfun$putBytes$1(org.apache.spark.storage.TachyonBlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///java/nio/ByteBuffer/array()|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/getFile(org.apache.spark.storage.BlockId)|",
      "|java+method:///tachyon/client/file/FileOutStream/write(byte%5B%5D)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///tachyon/client/TachyonFile/getOutStream(tachyon.client.WriteType)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///tachyon/client/file/FileOutStream/close()|"
    ],
    "affectedLib": "org.tachyonproject:tachyon-client:0.8.2",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/TachyonBlockManager/getBytes(org.apache.spark.storage.BlockId)|",
    "called": "|java+method:///tachyon/client/TachyonFile/length()|",
    "v1Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///tachyon/client/TachyonFile/getLocationHosts()|",
      "|java+method:///tachyon/client/InStream/close()|",
      "|java+method:///tachyon/client/TachyonFile/length()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/spark-project/guava/io/ByteStreams/readFully(java.io.InputStream,byte%5B%5D)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/storage/TachyonBlockManager$$anonfun$getBytes$1/TachyonBlockManager$$anonfun$getBytes$1(org.apache.spark.storage.TachyonBlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///java/util/List/size()|",
      "|java+method:///tachyon/client/TachyonFile/getInStream(tachyon.client.ReadType)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/getFile(org.apache.spark.storage.BlockId)|",
      "|java+method:///java/nio/ByteBuffer/wrap(byte%5B%5D)|"
    ],
    "v2Body": [
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///tachyon/client/TachyonFile/getLocationHosts()|",
      "|java+method:///tachyon/client/TachyonFile/length()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///scala/Some/Some(java.lang.Object)|",
      "|java+method:///org/spark-project/guava/io/ByteStreams/readFully(java.io.InputStream,byte%5B%5D)|",
      "|java+method:///tachyon/client/file/FileInStream/close()|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+constructor:///org/apache/spark/storage/TachyonBlockManager$$anonfun$getBytes$1/TachyonBlockManager$$anonfun$getBytes$1(org.apache.spark.storage.TachyonBlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///java/util/List/size()|",
      "|java+method:///tachyon/client/TachyonFile/getInStream(tachyon.client.ReadType)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/getFile(org.apache.spark.storage.BlockId)|",
      "|java+method:///java/nio/ByteBuffer/wrap(byte%5B%5D)|"
    ],
    "affectedLib": "org.tachyonproject:tachyon-client:0.8.2",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/TachyonBlockManager/putValues(org.apache.spark.storage.BlockId,scala.collection.Iterator)|",
    "called": "|java+method:///tachyon/client/TachyonFile/getOutStream(tachyon.client.WriteType)|",
    "v1Body": [
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/blockManager()|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataSerializeStream(org.apache.spark.storage.BlockId,java.io.OutputStream,scala.collection.Iterator,org.apache.spark.serializer.Serializer)|",
      "|java+method:///tachyon/client/OutStream/close()|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/getFile(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///tachyon/client/TachyonFile/getOutStream(tachyon.client.WriteType)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataSerializeStream$default$4()|",
      "|java+constructor:///org/apache/spark/storage/TachyonBlockManager$$anonfun$putValues$1/TachyonBlockManager$$anonfun$putValues$1(org.apache.spark.storage.TachyonBlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///tachyon/client/OutStream/cancel()|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/blockManager()|",
      "|java+method:///org/apache/spark/storage/BlockManager/dataSerializeStream(org.apache.spark.storage.BlockId,java.io.OutputStream,scala.collection.Iterator)|",
      "|java+method:///scala/util/control/NonFatal$/unapply(java.lang.Throwable)|",
      "|java+method:///tachyon/client/file/FileOutStream/cancel()|",
      "|java+method:///scala/Option/isEmpty()|",
      "|java+constructor:///org/apache/spark/storage/TachyonBlockManager$$anonfun$putValues$1/TachyonBlockManager$$anonfun$putValues$1(org.apache.spark.storage.TachyonBlockManager,org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/getFile(org.apache.spark.storage.BlockId)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/logWarning(scala.Function0,java.lang.Throwable)|",
      "|java+method:///tachyon/client/TachyonFile/getOutStream(tachyon.client.WriteType)|",
      "|java+method:///scala/Option/get()|",
      "|java+method:///tachyon/client/file/FileOutStream/close()|"
    ],
    "affectedLib": "org.tachyonproject:tachyon-client:0.8.2",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/storage/TachyonBlockManager/getFile(java.lang.String)|",
    "called": "|java+constructor:///tachyon/TachyonURI/TachyonURI(java.lang.String)|",
    "v1Body": [
      "|java+method:///org/apache/spark/util/Utils$/nonNegativeHash(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/tachyonDirs()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///tachyon/client/TachyonFS/getFile(tachyon.TachyonURI)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/subDirs()|",
      "|java+method:///tachyon/client/TachyonFS/createFile(tachyon.TachyonURI)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///tachyon/client/TachyonFS/mkdir(tachyon.TachyonURI)|",
      "|java+constructor:///tachyon/TachyonURI/TachyonURI(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/client()|",
      "|java+method:///tachyon/client/TachyonFS/exist(tachyon.TachyonURI)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/org$apache$spark$storage$TachyonBlockManager$$subDirsPerTachyonDir()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/util/Utils$/nonNegativeHash(java.lang.Object)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/tachyonDirs()|",
      "|java+method:///scala/Predef$/augmentString(java.lang.String)|",
      "|java+method:///tachyon/client/TachyonFS/getFile(tachyon.TachyonURI)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+constructor:///scala/collection/immutable/StringOps/StringOps(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/subDirs()|",
      "|java+method:///tachyon/client/TachyonFS/createFile(tachyon.TachyonURI)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|",
      "|java+method:///tachyon/client/TachyonFS/mkdir(tachyon.TachyonURI)|",
      "|java+constructor:///tachyon/TachyonURI/TachyonURI(java.lang.String)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/client()|",
      "|java+method:///tachyon/client/TachyonFS/exist(tachyon.TachyonURI)|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToLong(long)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/storage/TachyonBlockManager/org$apache$spark$storage$TachyonBlockManager$$subDirsPerTachyonDir()|",
      "|java+method:///scala/collection/immutable/StringOps/format(scala.collection.Seq)|"
    ],
    "affectedLib": "org.tachyonproject:tachyon-client:0.8.2",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/scheduler/HighlyCompressedMapStatus$/apply(org.apache.spark.storage.BlockManagerId,long%5B%5D)|",
    "called": "|java+method:///org/roaringbitmap/RoaringBitmap/runOptimize()|",
    "v1Body": [
      "|java+constructor:///org/roaringbitmap/RoaringBitmap/RoaringBitmap()|",
      "|java+constructor:///org/apache/spark/scheduler/HighlyCompressedMapStatus/HighlyCompressedMapStatus(org.apache.spark.storage.BlockManagerId,int,org.roaringbitmap.RoaringBitmap,long)|",
      "|java+method:///org/roaringbitmap/RoaringBitmap/add(int)|"
    ],
    "v2Body": [
      "|java+method:///org/roaringbitmap/RoaringBitmap/runOptimize()|",
      "|java+constructor:///org/roaringbitmap/RoaringBitmap/RoaringBitmap()|",
      "|java+constructor:///org/apache/spark/scheduler/HighlyCompressedMapStatus/HighlyCompressedMapStatus(org.apache.spark.storage.BlockManagerId,int,org.roaringbitmap.RoaringBitmap,long)|",
      "|java+method:///org/roaringbitmap/RoaringBitmap/add(int)|",
      "|java+method:///org/roaringbitmap/RoaringBitmap/trim()|"
    ],
    "affectedLib": "org.roaringbitmap:RoaringBitmap:0.5.11",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+constructor:///org/apache/spark/launcher/WorkerCommandBuilder/WorkerCommandBuilder(java.lang.String,int,org.apache.spark.deploy.Command)|",
    "called": "|java+constructor:///org/apache/spark/launcher/AbstractCommandBuilder/AbstractCommandBuilder()|",
    "v1Body": [
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/JavaConversions$/mapAsJavaMap(scala.collection.Map)|",
      "|java+method:///org/apache/spark/deploy/Command/environment()|",
      "|java+method:///java/util/Map/putAll(java.util.Map)|",
      "|java+constructor:///org/apache/spark/launcher/AbstractCommandBuilder/AbstractCommandBuilder()|"
    ],
    "v2Body": [
      "|java+method:///java/util/Map/put(java.lang.Object,java.lang.Object)|",
      "|java+method:///scala/collection/convert/Decorators$AsJava/asJava()|",
      "|java+method:///scala/collection/JavaConverters$/mapAsJavaMapConverter(scala.collection.Map)|",
      "|java+method:///org/apache/spark/deploy/Command/environment()|",
      "|java+method:///java/util/Map/putAll(java.util.Map)|",
      "|java+constructor:///org/apache/spark/launcher/AbstractCommandBuilder/AbstractCommandBuilder()|"
    ],
    "affectedLib": "org.apache.spark:spark-launcher_2.10:1.6.0",
    "change": "UPDATED"
  },
  {
    "coordinatesV1": "org.apache.spark:spark-core_2.10:1.5.2",
    "coordinatesV2": "org.apache.spark:spark-core_2.10:1.6.0",
    "caller": "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/buildCommand(java.util.Map)|",
    "called": "|java+method:///org/apache/spark/launcher/CommandBuilderUtils/addPermGenSizeOpt(java.util.List)|",
    "v1Body": [
      "|java+method:///org/apache/spark/deploy/Command/javaOpts()|",
      "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/getenv(java.lang.String)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/Command/classPathEntries()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/launcher/WorkerCommandBuilder$$anonfun$buildCommand$1/WorkerCommandBuilder$$anonfun$buildCommand$1(org.apache.spark.launcher.WorkerCommandBuilder,java.util.List)|",
      "|java+method:///java/util/List/add(java.lang.Object)|",
      "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/addOptionString(java.util.List,java.lang.String)|",
      "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/buildJavaCommand(java.lang.String)|",
      "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/addPermGenSizeOpt(java.util.List)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "v2Body": [
      "|java+method:///org/apache/spark/deploy/Command/javaOpts()|",
      "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/getenv(java.lang.String)|",
      "|java+method:///scala/collection/Seq/foreach(scala.Function1)|",
      "|java+method:///scala/Predef$/wrapRefArray(java.lang.Object%5B%5D)|",
      "|java+constructor:///scala/StringContext/StringContext(scala.collection.Seq)|",
      "|java+method:///org/apache/spark/deploy/Command/classPathEntries()|",
      "|java+method:///scala/runtime/BoxesRunTime/boxToInteger(int)|",
      "|java+constructor:///org/apache/spark/launcher/WorkerCommandBuilder$$anonfun$buildCommand$1/WorkerCommandBuilder$$anonfun$buildCommand$1(org.apache.spark.launcher.WorkerCommandBuilder,java.util.List)|",
      "|java+method:///scala/Predef$/genericWrapArray(java.lang.Object)|",
      "|java+method:///java/util/List/add(java.lang.Object)|",
      "|java+method:///org/apache/spark/launcher/CommandBuilderUtils/addPermGenSizeOpt(java.util.List)|",
      "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/addOptionString(java.util.List,java.lang.String)|",
      "|java+method:///org/apache/spark/launcher/WorkerCommandBuilder/buildJavaCommand(java.lang.String)|",
      "|java+method:///scala/collection/Seq/mkString(java.lang.String)|",
      "|java+method:///scala/StringContext/s(scala.collection.Seq)|"
    ],
    "affectedLib": "org.apache.spark:spark-launcher_2.10:1.6.0",
    "change": "UPDATED"
  }
]